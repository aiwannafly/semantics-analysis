# text =  В современном мире всё большую популярность приобретает методика под названием customer development для тестирования идей и гипотез о будущем продукте.
В O
современном O
мире O
всё O
большую O
популярность O
приобретает O
методика O
под O
названием O
customer B-TERM
development I-TERM
для O
тестирования O
идей O
и O
гипотез O
о O
будущем O
продукте O
. O

# text =  В данном решении была использована готовая нейросеть от сервиса RusVectores, обученная на корпусе НКРЯ с использованием алгоритма word2vec CBOW с длиной вектора 300.
В O
данном O
решении O
была O
использована O
готовая O
нейросеть O
от O
сервиса O
RusVectores B-TERM
, O
обученная O
на O
корпусе O
НКРЯ B-TERM
с O
использованием O
алгоритма O
word2vec B-TERM
CBOW B-TERM
с O
длиной O
вектора O
300.

# text = НКРЯ – это совокупность русскоязычных текстов, Национальный Корпус Русского Языка в полном объёме.
НКРЯ B-TERM
– O
это O
совокупность O
русскоязычных O
текстов O
, O
Национальный B-TERM
Корпус I-TERM
Русского I-TERM
Языка I-TERM
в O
полном O
объёме O
. O

# text = Word2vec CBOW — алгоритм, благодаря которому слово на естественном языке представляется в виде числового вектора.
Word2vec B-TERM
CBOW B-TERM
— O
алгоритм O
, O
благодаря O
которому O
слово O
на O
естественном O
языке O
представляется O
в O
виде O
числового O
вектора O
. O

# text =  CBOW – это аббревиатура Continuous Bag of Words.
CBOW B-TERM
– O
это O
аббревиатура O
Continuous B-TERM
Bag I-TERM
of I-TERM
Words I-TERM
. O

# text =  Она обозначает алгоритм, который есть в word2vec.
Она O
обозначает O
алгоритм O
, O
который O
есть O
в O
word2vec B-TERM
. O

# text =  Данный алгоритм называют моделью «мешка слов», он предсказывает слово по контексту.
Данный O
алгоритм O
называют O
моделью O
« O
мешка B-TERM
слов I-TERM
» O
, O
он O
предсказывает B-TERM
слово I-TERM
по I-TERM
контексту I-TERM
. O

# text =  Ещё один алгоритм в word2vec - Skip-gram предсказывает контекст по слову.
Ещё O
один O
алгоритм O
в O
word2vec B-TERM
- O
Skip B-TERM
- I-TERM
gram I-TERM
предсказывает O
контекст O
по O
слову O
. O

# text = С помощью данных алгоритмов генерируют близкие по смыслу слова при запросе в поисковой системе, сравнивают документы по смыслу, определяют смысловую близость слов и предложений.
С O
помощью O
данных O
алгоритмов O
генерируют O
близкие O
по O
смыслу O
слова O
при O
запросе O
в O
поисковой O
системе O
, O
сравнивают B-TERM
документы I-TERM
по I-TERM
смыслу I-TERM
, O
определяют B-TERM
смысловую I-TERM
близость I-TERM
слов I-TERM
и O
предложений O
. O

# text = Более подробно о word2vec можно почитать в статье "Немного про word2vec: полезная теория".
Более O
подробно O
о O
word2vec B-TERM
можно O
почитать O
в O
статье O
" O
Немного O
про O
word2vec O
: O
полезная O
теория" O
. O

# text = О векторном представлении слов (эмбеддинге) хорошо и с примерами описано в статье "Что такое эмбеддинги и как они помогают машинам понимать тексты".
О O
векторном B-TERM
представлении I-TERM
слов I-TERM
( O
эмбеддинге B-TERM
) O
хорошо O
и O
с O
примерами O
описано O
в O
статье O
" O
Что O
такое O
эмбеддинги O
и O
как O
они O
помогают O
машинам O
понимать O
тексты" O
. O

# text =  Т.к. у меня таких мощностей нет, я воспользовался доступным онлайн сервисом RusVectores.
Т.к O
. O
у O
меня O
таких O
мощностей O
нет O
, O
я O
воспользовался O
доступным O
онлайн O
сервисом O
RusVectores B-TERM
. O
