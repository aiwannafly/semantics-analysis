# text =  Как научить свою нейросеть генерировать стихи
Как O
научить O
свою O
нейросеть O
генерировать B-TERM
стихи I-TERM

# text =  Языковые модели определяют вероятность появления последовательности слов  в данном языке: .
Языковые O
модели O
определяют B-TERM
вероятность I-TERM
появления I-TERM
последовательности I-TERM
слов I-TERM
в O
данном O
языке B-TERM
: O
. O

# text =  Кажется, самым простым способом построить такую модель является использование N-граммной статистики.
Кажется O
, O
самым O
простым O
способом O
построить O
такую O
модель O
является O
использование O
N B-TERM
- I-TERM
граммной I-TERM
статистики I-TERM
. O

# text =  Для решения такой проблемы используют обычно сглаживание Kneser–Ney или Katz’s backing-off.
Для O
решения O
такой O
проблемы O
используют O
обычно O
сглаживание B-TERM
Kneser I-TERM
– I-TERM
Ney I-TERM
или O
Katz B-TERM
’s I-TERM
backing I-TERM
- I-TERM
off I-TERM
. O

# text =  За более подробной информацией про методы сглаживания N-грамм стоит обратиться к известной книге Кристофера Маннинга “Foundations of Statistical Natural Language Processing”.
За O
более O
подробной O
информацией O
про O
методы O
сглаживания B-TERM
N I-TERM
- I-TERM
грамм I-TERM
стоит O
обратиться O
к O
известной O
книге O
Кристофера B-TERM
Маннинга I-TERM
“ O
Foundations B-TERM
of I-TERM
Statistical I-TERM
Natural I-TERM
Language I-TERM
Processing I-TERM
” O
. O

# text =  Хочу заметить, что 5-граммы слов я назвал не просто так: именно их (со сглаживанием, конечно) Google демонстрирует в статье “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling” — и показывает результаты, весьма сопоставимые с результатами у рекуррентных нейронных сетей — о которых, собственно, и пойдет далее речь.
Хочу O
заметить O
, O
что O
5-граммы O
слов O
я O
назвал O
не O
просто O
так O
: O
именно O
их O
( O
со O
сглаживанием O
, O
конечно O
) O
Google B-TERM
демонстрирует O
в O
статье O
“ B-TERM
One I-TERM
Billion I-TERM
Word I-TERM
Benchmark I-TERM
for I-TERM
Measuring I-TERM
Progress I-TERM
in I-TERM
Statistical I-TERM
Language I-TERM
Modeling I-TERM
” I-TERM
— O
и O
показывает O
результаты O
, O
весьма O
сопоставимые O
с O
результатами O
у O
рекуррентных B-TERM
нейронных I-TERM
сетей I-TERM
— O
о O
которых O
, O
собственно O
, O
и O
пойдет O
далее O
речь O
. O

# text =  Преимущество рекуррентных нейронных сетей — в возможности использовать неограниченно длинный контекст.
Преимущество O
рекуррентных B-TERM
нейронных I-TERM
сетей I-TERM
— O
в O
возможности O
использовать O
неограниченно O
длинный O
контекст O
. O

# text =  На практике классические RNN страдают от затухания градиента — по сути, отсутствия возможности помнить контекст дальше, чем на несколько слов.
На O
практике O
классические O
RNN B-TERM
страдают O
от O
затухания O
градиента O
— O
по O
сути O
, O
отсутствия O
возможности O
помнить O
контекст O
дальше O
, O
чем O
на O
несколько O
слов O
. O

# text =  Самыми популярными являются LSTM и GRU.
Самыми O
популярными O
являются O
LSTM B-TERM
и O
GRU B-TERM
. O

# text =  В дальнейшем, говоря о рекуррентном слое, я всегда буду подразумевать LSTM.
В O
дальнейшем O
, O
говоря O
о O
рекуррентном O
слое O
, O
я O
всегда O
буду O
подразумевать O
LSTM B-TERM
. O

# text =  Вспомним теперь, что для нашей задачи языковая модель нужна для выбора наиболее подходящего следующего слова по уже сгенерированной последовательности.
Вспомним O
теперь O
, O
что O
для O
нашей O
задачи O
языковая O
модель O
нужна O
для O
выбора B-TERM
наиболее I-TERM
подходящего I-TERM
следующего I-TERM
слова I-TERM
по O
уже O
сгенерированной B-TERM
последовательности I-TERM
. O

# text =  Метрические правила определяют последовательность ударных и безударных слогов в строке.
Метрические B-TERM
правила I-TERM
определяют O
последовательность O
ударных O
и O
безударных O
слогов B-TERM
в O
строке O
. O

# text =  Для решения этой проблемы мы делаем лучевой поиск (beam search), выбирая на каждом шаге вместо одного сразу N путей с наивысшими вероятностями.
Для O
решения O
этой O
проблемы O
мы O
делаем O
лучевой B-TERM
поиск I-TERM
( O
beam B-TERM
search I-TERM
) O
, O
выбирая O
на O
каждом O
шаге O
вместо O
одного O
сразу O
N O
путей O
с O
наивысшими O
вероятностями O
. O