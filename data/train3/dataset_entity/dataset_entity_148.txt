# text =   Как научить свою нейросеть генерировать стихи
Как O
научить O
свою O
нейросеть O
генерировать B-Task
стихи I-Task

# text =   Языковые модели определяют вероятность появления последовательности слов  в данном языке: .
Языковые O
модели O
определяют B-Task
вероятность I-Task
появления I-Task
последовательности I-Task
слов I-Task
в O
данном O
языке B-Object
: O
. O

# text =   Кажется, самым простым способом построить такую модель является использование N-граммной статистики.
Кажется O
, O
самым O
простым O
способом O
построить O
такую O
модель B-Object
является O
использование O
N B-Method
- I-Method
граммной I-Method
статистики I-Method
. O

# text =   Для решения такой проблемы используют обычно сглаживание Kneser–Ney или Katz’s backing-off.
Для O
решения O
такой O
проблемы O
используют O
обычно O
сглаживание B-Method
Kneser I-Method
– I-Method
Ney I-Method
или O
Katz B-Method
’s I-Method
backing I-Method
- I-Method
off I-Method
. O

# text =   За более подробной информацией про методы сглаживания N-грамм стоит обратиться к известной книге Кристофера Маннинга “Foundations of Statistical Natural Language Processing”.
За O
более O
подробной O
информацией O
про O
методы O
сглаживания B-Method
N I-Method
- I-Method
грамм I-Method
стоит O
обратиться O
к O
известной O
книге O
Кристофера B-Person
Маннинга I-Person
“ O
Foundations B-Publication
of I-Publication
Statistical I-Publication
Natural I-Publication
Language I-Publication
Processing I-Publication
” O
. O

# text =   Хочу заметить, что 5-граммы слов я назвал не просто так: именно их (со сглаживанием, конечно) Google демонстрирует в статье “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling” — и показывает результаты, весьма сопоставимые с результатами у рекуррентных нейронных сетей — о которых, собственно, и пойдет далее речь.
Хочу O
заметить O
, O
что O
5-граммы O
слов O
я O
назвал O
не O
просто O
так O
: O
именно O
их O
( O
со O
сглаживанием O
, O
конечно O
) O
Google B-Organization
демонстрирует O
в O
статье O
“ B-Publication
One I-Publication
Billion I-Publication
Word I-Publication
Benchmark I-Publication
for I-Publication
Measuring I-Publication
Progress I-Publication
in I-Publication
Statistical I-Publication
Language I-Publication
Modeling I-Publication
” I-Publication
— O
и O
показывает O
результаты O
, O
весьма O
сопоставимые O
с O
результатами O
у O
рекуррентных B-Method
нейронных I-Method
сетей I-Method
— O
о O
которых O
, O
собственно O
, O
и O
пойдет O
далее O
речь O
. O

# text =   Преимущество рекуррентных нейронных сетей — в возможности использовать неограниченно длинный контекст.
Преимущество O
рекуррентных B-Method
нейронных I-Method
сетей I-Method
— O
в O
возможности O
использовать O
неограниченно O
длинный O
контекст O
. O

# text =   На практике классические RNN страдают от затухания градиента — по сути, отсутствия возможности помнить контекст дальше, чем на несколько слов.
На O
практике O
классические O
RNN B-ShortName_Method
страдают O
от O
затухания O
градиента O
— O
по O
сути O
, O
отсутствия O
возможности O
помнить O
контекст O
дальше O
, O
чем O
на O
несколько O
слов O
. O

# text =   Самыми популярными являются LSTM и GRU.
Самыми O
популярными O
являются O
LSTM B-ShortName_Method
и O
GRU B-ShortName_Method
. O

# text =   В дальнейшем, говоря о рекуррентном слое, я всегда буду подразумевать LSTM.
В O
дальнейшем O
, O
говоря O
о O
рекуррентном O
слое O
, O
я O
всегда O
буду O
подразумевать O
LSTM B-ShortName_Method
. O

# text =   Вспомним теперь, что для нашей задачи языковая модель нужна для выбора наиболее подходящего следующего слова по уже сгенерированной последовательности.
Вспомним O
теперь O
, O
что O
для O
нашей O
задачи O
языковая O
модель B-Object
нужна O
для O
выбора B-Task
наиболее I-Task
подходящего I-Task
следующего I-Task
слова I-Task
по O
уже O
сгенерированной B-Object
последовательности I-Object
. O

# text =   Метрические правила определяют последовательность ударных и безударных слогов в строке.
Метрические B-Object
правила I-Object
определяют O
последовательность O
ударных O
и O
безударных O
слогов B-Subject
в O
строке O
. O

# text =   Для решения этой проблемы мы делаем лучевой поиск (beam search), выбирая на каждом шаге вместо одного сразу N путей с наивысшими вероятностями.
Для O
решения O
этой O
проблемы O
мы O
делаем O
лучевой B-Method
поиск I-Method
( O
beam B-Method
search I-Method
) O
, O
выбирая O
на O
каждом O
шаге O
вместо O
одного O
сразу O
N O
путей O
с O
наивысшими O
вероятностями O
. O
