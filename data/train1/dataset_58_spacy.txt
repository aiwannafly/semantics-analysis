# text =  На основании триггера на определенные ключевые слова она сможет определять, к примеру, признаки обмана, мошенничества.
На O
основании O
триггера O
на O
определенные O
ключевые B-TERM
слова I-TERM
она O
сможет O
определять O
, O
к O
примеру O
, O
признаки O
обмана O
, O
мошенничества O
. O

# text =  То есть, сформировав некоторый корпус слов-триггеров, вполне возможно классифицировать сайты по их текстовому содержанию.
То O
есть O
, O
сформировав O
некоторый O
корпус B-TERM
слов I-TERM
- I-TERM
триггеров I-TERM
, O
вполне O
возможно O
классифицировать O
сайты O
по O
их O
текстовому O
содержанию O
. O

# text =  Задача распознавания текста относится к сфере обработки естественного языка или NLP (natural language processing).
Задача B-TERM
распознавания I-TERM
текста I-TERM
относится O
к O
сфере O
обработки B-TERM
естественного I-TERM
языка I-TERM
или O
NLP B-TERM
( O
natural B-TERM
language I-TERM
processing I-TERM
) O
. O

# text =  NLP — направление искусственного интеллекта, нацеленное на обработку и анализ данных на естественном языке и обучение машин взаимодействию с людьми [1].
NLP B-TERM
— O
направление O
искусственного I-TERM
интеллекта I-TERM
, O
нацеленное O
на O
обработку O
и O
анализ I-TERM
данных I-TERM
на O
естественном O
языке O
и O
обучение O
машин O
взаимодействию O
с O
людьми O
[ O
1 O
] O
. O

# text =  Такой подход называется методом вложения слов (word embedding).
Такой O
подход O
называется O
методом B-TERM
вложения I-TERM
слов I-TERM
( O
word B-TERM
embedding I-TERM
) O
. O

# text =  Используя данные, состоящие из таких векторов, мы можем применять различные методы Machine Learning.
Используя O
данные O
, O
состоящие O
из O
таких O
векторов O
, O
мы O
можем O
применять O
различные O
методы O
Machine B-TERM
Learning I-TERM
. O

# text =  И поскольку искусственные нейронные сети лучшим образом справляются с векторно-матричными вычислениями, то выбор в их пользу становиться очевидным.
И O
поскольку O
искусственные B-TERM
нейронные I-TERM
сети I-TERM
лучшим O
образом O
справляются O
с O
векторно B-TERM
- I-TERM
матричными I-TERM
вычислениями I-TERM
, O
то O
выбор O
в O
их O
пользу O
становиться O
очевидным O
. O

# text =  Искусственная нейронная сеть — это математическая модель, а также ее программное или аппаратное воплощение, построенные по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.
Искусственная B-TERM
нейронная I-TERM
сеть I-TERM
— O
это O
математическая B-TERM
модель I-TERM
, O
а O
также O
ее O
программное O
или O
аппаратное O
воплощение O
, O
построенные O
по O
принципу O
организации O
и O
функционирования O
биологических O
нейронных O
сетей O
— O
сетей O
нервных O
клеток O
живого O
организма O
. O

# text =  Современные модели представляют собой так называемые глубокие модели.
Современные O
модели O
представляют O
собой O
так O
называемые O
глубокие B-TERM
модели I-TERM
. O

# text =  И в ее решении наилучшие метрики точности были достигнуты рекуррентными нейронными сетями, LSTM (сети с долгой краткосрочной памятью).
И O
в O
ее O
решении O
наилучшие O
метрики O
точности B-TERM
были O
достигнуты O
рекуррентными B-TERM
нейронными I-TERM
сетями I-TERM
, O
LSTM B-TERM
( O
сети B-TERM
с I-TERM
долгой I-TERM
краткосрочной I-TERM
памятью I-TERM
) O
. O

# text =  Позже свое превосходство в этой нише обрели NLP-модели-трансформеры.
Позже O
свое O
превосходство O
в O
этой O
нише O
обрели O
NLP B-TERM
- I-TERM
модели I-TERM
- I-TERM
трансформеры I-TERM
. O

# text = Описание упомянутых рекуррентных нейросетей (RNN), LSTM и GRU выходит за рамки темы статьи.
Описание O
упомянутых O
рекуррентных B-TERM
нейросетей I-TERM
( O
RNN B-TERM
) O
, O
LSTM B-TERM
и O
GRU B-TERM
выходит O
за O
рамки O
темы O
статьи O
. O

# text =  Однако RNN способны фиксировать зависимости только в одном направлении языка.
Однако O
RNN B-TERM
способны O
фиксировать B-TERM
зависимости I-TERM
только O
в O
одном O
направлении O
языка O
. O

# text =  Кроме этого, RNN не очень хороши в захвате долгосрочных зависимостей.
Кроме O
этого O
, O
RNN B-TERM
не O
очень O
хороши O
в O
захвате B-TERM
долгосрочных I-TERM
зависимостей I-TERM
. O

# text = LSTM избегают проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.
LSTM B-TERM
избегают O
проблемы O
долговременной O
зависимости O
, O
запоминая O
значения O
как O
на O
короткие O
, O
так O
и O
на O
длинные O
промежутки O
времени O
. O

# text =  Это объясняется тем, что LSTM не использует функцию активации внутри своих рекуррентных компонентов.
Это O
объясняется O
тем O
, O
что O
LSTM B-TERM
не O
использует O
функцию B-TERM
активации I-TERM
внутри O
своих O
рекуррентных O
компонентов O
. O

# text =  LSTM часто используются в машинном переводе и в задачах генерирования текстов на естественном языке.
LSTM B-TERM
часто O
используются O
в O
машинном B-TERM
переводе I-TERM
и O
в O
задачах O
генерирования B-TERM
текстов I-TERM
на O
естественном O
языке O
. O

# text =  Прежде чем использовать такой мощный и в то же время сложный инструмент, наша команда протестировала и более простые NLP-методы машинного обучения, в том числе «наивный байесовский классификатор», алгоритмы, использующие bag-of-words («мешок слов» — метод представления слов) и tf-idf (метрика определения частоты вхождения слов), а также простейшие модели нейронных сетей, состоящие из небольшого количества скрытых слоев.
Прежде O
чем O
использовать O
такой O
мощный O
и O
в O
то O
же O
время O
сложный O
инструмент O
, O
наша O
команда O
протестировала O
и O
более O
простые O
NLP O
- O
методы O
машинного O
обучения O
, O
в O
том O
числе O
« O
наивный B-TERM
байесовский I-TERM
классификатор I-TERM
» O
, O
алгоритмы O
, O
использующие O
bag B-TERM
- I-TERM
of I-TERM
- I-TERM
words I-TERM
( O
« O
мешок B-TERM
слов I-TERM
» O
— O
метод B-TERM
представления I-TERM
слов O
) O
и O
tf B-TERM
- I-TERM
idf I-TERM
( O
метрика B-TERM
определения I-TERM
частоты I-TERM
вхождения I-TERM
слов O
) O
, O
а O
также O
простейшие O
модели O
нейронных O
сетей O
, O
состоящие O
из O
небольшого O
количества O
скрытых O
слоев O
. O

# text =  BERT, или Bidirectional Encoder Representations from Transformers, — нейросетевая модель-трансформер от Google, на которой сегодня строится большинство инструментов автоматической обработки языка.
BERT B-TERM
, O
или O
Bidirectional B-TERM
Encoder I-TERM
Representations I-TERM
from I-TERM
Transformers I-TERM
, O
— O
нейросетевая O
модель O
- O
трансформер O
от O
Google B-TERM
, O
на O
которой O
сегодня O
строится O
большинство O
инструментов O
автоматической O
обработки O
языка O
. O

# text = Релиз BERT в 2018 году стал некоторой переломной точкой в развитии NLP-моделей.
Релиз O
BERT B-TERM
в O
2018 B-TERM
году I-TERM
стал O
некоторой O
переломной O
точкой O
в O
развитии O
NLP B-TERM
- I-TERM
моделей I-TERM
. O

# text =  Его появлению предшествовал ряд недавних разработок в области обработки естественного языка (BERT, ELMO и Ко в картинках — как в NLP пришло трансферное обучение): Semi-supervised Sequence learning (Andrew Dai и Quoc Le), ELMo (Matthew Peters и исследователи из AI2 и UW CSE), ULMFiT (Jeremy Howard и Sebastian Ruder), OpenAI Transformer (исследователи OpenAI Radford, Narasimhan, Salimans, и Sutskever) и Трансформер (Vaswani et al).
Его O
появлению O
предшествовал O
ряд O
недавних O
разработок O
в O
области O
обработки O
естественного O
языка O
( O
BERT B-TERM
, O
ELMO B-TERM
и O
Ко B-TERM
в O
картинках O
— O
как O
в O
NLP B-TERM
пришло O
трансферное B-TERM
обучение I-TERM
): O
Semi B-TERM
- I-TERM
supervised I-TERM
Sequence I-TERM
learning I-TERM
( O
Andrew B-TERM
Dai I-TERM
и O
Quoc B-TERM
Le I-TERM
) O
, O
ELMo B-TERM
( O
Matthew B-TERM
Peters I-TERM
и O
исследователи O
из O
AI2 B-TERM
и O
UW B-TERM
CSE B-TERM
) O
, O
ULMFiT B-TERM
( O
Jeremy B-TERM
Howard I-TERM
и O
Sebastian B-TERM
Ruder I-TERM
) O
, O
OpenAI B-TERM
Transformer I-TERM
( O
исследователи O
OpenAI B-TERM
Radford I-TERM
, O
Narasimhan B-TERM
, O
Salimans B-TERM
, O
и O
Sutskever B-TERM
) O
и O
Трансформер B-TERM
( O
Vaswani B-TERM
et O
al O
)O 
. O

# text = Трансформеры в машинном обучении — это семейство архитектур нейронных сетей, общая идея которых основана на так называемом «самовнимании» (self-attention).
Трансформеры B-TERM
в O
машинном B-TERM
обучении I-TERM
— O
это O
семейство O
архитектур O
нейронных O
сетей O
, O
общая O
идея O
которых O
основана O
на O
так O
называемом O
« O
самовнимании B-TERM
» O
( O
self B-TERM
- I-TERM
attention I-TERM
) O
. O

# text =  Однако алгоритм Self-attention не сразу поймет смысл предложения.
Однако O
алгоритм O
Self B-TERM
- I-TERM
attention I-TERM
не O
сразу O
поймет O
смысл O
предложения O
. O

# text =  Потом результаты сетей объединяется.По своей сути BERT — это обученный стек энкодеров Трансформера.
Потом O
результаты O
сетей O
объединяется O
. O
По O
своей O
сути O
BERT B-TERM
— O
это O
обученный O
стек O
энкодеров B-TERM
Трансформера I-TERM
. O

# text =  Разработкой и обучением модели BERT занималась целая группа исследователей Google AI Language на многомиллионном наборе слов на разных языках (более 100).
Разработкой O
и O
обучением O
модели O
BERT B-TERM
занималась O
целая O
группа O
исследователей O
Google B-TERM
AI I-TERM
Language I-TERM
на O
многомиллионном O
наборе O
слов O
на O
разных O
языках O
( O
более O
100 O
) O
. O

# text =  И мы дообучили BERT распознавать текстовое содержимое сайтов по 63 категориям (медицина, здоровье, видео, интернет-магазины, юмористические сайты, эротика, оружие, секты, криминал и пр.).
И O
мы O
дообучили O
BERT B-TERM
распознавать O
текстовое O
содержимое O
сайтов O
по O
63 O
категориям O
( O
медицина B-TERM
, O
здоровье O
, O
видео O
, O
интернет O
- O
магазины O
, O
юмористические O
сайты O
, O
эротика O
, O
оружие O
, O
секты O
, O
криминал O
и пр. O
) O
. O

# text =  Smart-Cat на первом этапе проводит их предобработку.
Smart B-TERM
- I-TERM
Cat I-TERM
на O
первом O
этапе O
проводит O
их O
предобработку O
. O

# text =  Для удобства работы с категоризатором Smart-Cat мы создали специальный Telegram-бот.
Для O
удобства O
работы O
с O
категоризатором O
Smart B-TERM
- I-TERM
Cat I-TERM
мы O
создали O
специальный O
Telegram B-TERM
- I-TERM
бот I-TERM
. O

# text =  После своей работы BERT-bot отправит CSV-таблицу.
После O
своей O
работы O
BERT B-TERM
- I-TERM
bot I-TERM
отправит O
CSV O
- O
таблицу O
. O

# text =  Но, как я уже сказал ранее, такие модели могут применяться не только в задачах классификации текста.
Но O
, O
как O
я O
уже O
сказал O
ранее O
, O
такие O
модели O
могут O
применяться O
не O
только O
в O
задачах O
классификации B-TERM
текста I-TERM
. O
