# text =  В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3.
В O
этой O
статье O
мы O
научим O
вас O
генерировать B-Task
текст I-Task
с O
помощью O
предварительно O
обученного O
GPT-2 B-Model
— O
более O
легкого O
предшественника O
GPT-3 B-Model
. O

# text =   Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface.
Мы O
будем O
использовать O
именитую O
библиотеку O
Transformers B-Library
, O
разработанную O
Huggingface B-Organization
. O

# text =   Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.
Модель B-Object
по O
умолчанию O
для O
конвейера O
генерации B-Task
текста I-Task
— O
GPT-2 B-Model
, O
самая O
популярная O
модель O
декодирующего O
трансформера B-Model
для O
генерации B-Task
языка I-Task
. O

# text =   Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.
Эта O
модель O
GPT2 B-Model
от O
CKIPLab B-Organization
предварительно O
обучена O
на O
китайском B-Lang
корпусе B-Object
, O
поэтому O
мы O
можем O
использовать O
их O
модель O
без O
необходимости O
заниматься O
настройкой O
самостоятельно O
. O
