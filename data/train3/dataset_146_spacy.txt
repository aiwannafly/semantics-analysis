# text =  AntiToxicBot — бот, распознающий токсичных пользователей в телеграм чатах.
AntiToxicBot O
— O
бот O
, O
распознающий B-TERM
токсичных I-TERM
пользователей I-TERM
в O
телеграм O
чатах O
. O

# text =  Почему же выбрано CNN+GRU, а не просто GRU или CNN?
Почему O
же O
выбрано O
CNN+GRU B-TERM
, O
а O
не O
просто O
GRU B-TERM
или O
CNN B-TERM
? O

# text =  Нейросеть состоит из 3-х основных частей(CNN, GRU, Linear).
Нейросеть O
состоит O
из O
3-х O
основных O
частей O
( O
CNN B-TERM
, O
GRU B-TERM
, O
Linear B-TERM
) O
. O

# text =  Как и в классификации картинок, свёрточный слой выделяет “признаки”, но в нашем случае векторизированный текст.
Как O
и O
в O
классификации O
картинок O
, O
свёрточный O
слой O
выделяет O
“ O
признаки O
” O
, O
но O
в O
нашем O
случае O
векторизированный B-TERM
текст I-TERM
. O

# text =  То-есть данная часть сети учится выделять признаки токсичных и позитивных сообщений.
То O
- O
есть O
данная O
часть O
сети O
учится O
выделять B-TERM
признаки I-TERM
токсичных I-TERM
и I-TERM
позитивных I-TERM
сообщений I-TERM
. O

# text =   GRU - Recurrent Neural Network
GRU B-TERM
- O
Recurrent B-TERM
Neural I-TERM
Network

# text =  Чтобы обрабатывать последовательности произвольной длины, используют рекуррентные слои.
Чтобы O
обрабатывать B-TERM
последовательности I-TERM
произвольной I-TERM
длины I-TERM
, O
используют O
рекуррентные O
слои O
. O

# text =  В архитектуре используется рекуррентный слой GRU.
В O
архитектуре O
используется O
рекуррентный O
слой O
GRU B-TERM
. O

# text = Данный слой учится делать заключительное решение по определению тональности текста на основе предыдущих слоёв.
Данный O
слой O
учится O
делать O
заключительное O
решение O
по O
определению B-TERM
тональности I-TERM
текста I-TERM
на O
основе O
предыдущих O
слоёв O
. O

# text =  Датасет был взят с сайта kaggle.
Датасет O
был O
взят O
с O
сайта O
kaggle B-TERM
. O

# text =  Около 14000 комментариев с разметкой токсичное сообщение или нет.
Около O
14000 O
комментариев O
с O
разметкой B-TERM
токсичное B-TERM
сообщение I-TERM
или O
нет O
. O

# text =  Для решения данной проблемы была использована библиотека Yandex Speller, которая исправляет орфографические ошибки.
Для O
решения O
данной O
проблемы O
была O
использована O
библиотека O
Yandex B-TERM
Speller I-TERM
, O
которая O
исправляет B-TERM
орфографические I-TERM
ошибки I-TERM
. O

# text =  Можно было обучить собственный Word2Vec на основе данного набора данных, но лучше взять уже обученный.
Можно O
было O
обучить O
собственный O
Word2Vec B-TERM
на O
основе O
данного O
набора O
данных O
, O
но O
лучше O
взять O
уже O
обученный O
. O

# text =  Например: Navec.
Например O
: O
Navec B-TERM
. O

# text =  Модель обучали на русской литературе (~150gb), что говорит о качественной векторизации текста.
Модель O
обучали O
на O
русской B-TERM
литературе I-TERM
( O
~150 O
gb O
) O
, O
что O
говорит O
о O
качественной O
векторизации O
текста O
. O

# text = Для классификации используется обыкновенная функция потерь – кросс энтропия.
Для O
классификации B-TERM
используется O
обыкновенная O
функция B-TERM
потерь I-TERM
– I-TERM
кросс I-TERM
энтропия I-TERM
. O

# text = При обучении сети надо обращать внимание на основные параметры такие, как loss, precision и accuracy.
При O
обучении O
сети O
надо O
обращать O
внимание O
на O
основные O
параметры O
такие O
, O
как O
loss B-TERM
, O
precision B-TERM
и O
accuracy B-TERM
. O

# text =  В ~80% случаев нейросеть классифицирует тональность текста правильно.
В O
~80 B-TERM
% O
случаев O
нейросеть O
классифицирует O
тональность O
текста O
правильно O
. O
