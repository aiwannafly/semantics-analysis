# sent_id = 1
# text =  Чатботы и искусственный интеллект для понимания естественного языка (NLU – Natural Language Understanding) тема достаточно горячая, про нее не раз говорилось на Хабре.
1	Чатботы O
2	и O
3	искусственный I-TERM
4	интеллект I-TERM
5	для O
6	понимания I-TERM
7	естественного I-TERM
8	языка I-TERM
9	( O
10	NLU I-TERM
11	– O
12	Natural I-TERM
13	Language I-TERM
14	Understanding I-TERM
15	) O
16	тема O
17	достаточно O
18	горячая O
19	, O
20	про O
21	нее O
22	не O
23	раз O
24	говорилось O
25	на O
26	Хабре O
27	. O

# sent_id = 2
# text =  Хотя AI — это достаточно широкая область, включающая в себя машинное зрение, предиктивный анализ, машинный перевод и другие области – понимание естественного языка (NLU) и его генерация (NLG) является значительной и быстрорастущей его частью.
1	Хотя O
2	AI I-TERM
3	— O
4	это O
5	достаточно O
6	широкая O
7	область O
8	, O
9	включающая O
10	в O
11	себя O
12	машинное I-TERM
13	зрение I-TERM
14	, O
15	предиктивный I-TERM
16	анализ I-TERM
17	, O
18	машинный I-TERM
19	перевод I-TERM
20	и O
21	другие O
22	области O
23	– O
24	понимание I-TERM
25	естественного I-TERM
26	языка I-TERM
27	( O
28	NLU I-TERM
29	) O
30	и O
31	его O
32	генерация I-TERM
33	( O
34	NLG I-TERM
35	) O
36	является O
37	значительной O
38	и O
39	быстрорастущей O
40	его O
41	частью O
42	. O

# sent_id = 3
# text =  Опуская историю, начавшуюся еще в 50-е годы с Алана Тьюринга и программы Элиза в 60-е годы, а также научные исследования в области лингвистики и машинного обучения 90-х годов, значимым событием более новой истории стало появление языка разметки AIML (Artificial Intelligence Markup Language), разработанной в 2001-м году Ричардом Уэлсом (Richard Wallace) и созданным на его основе чатботом A.L.I.C.E.
1	Опуская O
2	историю O
3	, O
4	начавшуюся O
5	еще O
6	в O
7	50-е O
8	годы O
9	с O
10	Алана I-TERM
11	Тьюринга I-TERM
12	и O
13	программы O
14	Элиза I-TERM
15	в O
16	60-е O
17	годы O
18	, O
19	а O
20	также O
21	научные O
22	исследования O
23	в O
24	области O
25	лингвистики O
26	и O
27	машинного I-TERM
28	обучения I-TERM
29	90-х O
30	годов O
31	, O
32	значимым O
33	событием O
34	более O
35	новой O
36	истории O
37	стало O
38	появление O
39	языка O
40	разметки O
41	AIML I-TERM
42	( O
43	Artificial I-TERM
44	Intelligence I-TERM
45	Markup I-TERM
46	Language I-TERM
47	) O
48	, O
49	разработанной O
50	в O
51	2001-м I-TERM
52	году O
53	Ричардом I-TERM
54	Уэлсом I-TERM
55	( O
56	Richard I-TERM
57	Wallace I-TERM
58	) O
59	и O
60	созданным O
61	на O
62	его O
63	основе O
64	чатботом O
65	A.L.I.C.E. I-TERM

# sent_id = 4
# text =  В течение последующих десяти лет подходы к написанию чатботов во многом представляли из себя переработки или улучшения этой методологии, получившей название «rule-based подход» или «подход на основе формальных правил».
1	В O
2	течение O
3	последующих O
4	десяти O
5	лет O
6	подходы O
7	к O
8	написанию O
9	чатботов O
10	во O
11	многом O
12	представляли O
13	из O
14	себя O
15	переработки O
16	или O
17	улучшения O
18	этой O
19	методологии O
20	, O
21	получившей O
22	название O
23	« O
24	rule I-TERM
25	- I-TERM
26	based I-TERM
27	подход O
28	» O
29	или O
30	« O
31	подход O
32	на O
33	основе O
34	формальных I-TERM
35	правил I-TERM
36	» O
37	. O

# sent_id = 5
# text =  Именно эти технологии, вместе с заметным продвижением в области технологий синтеза и распознавания речи, а также распространением мессенджеров и вебчатов – обусловили стремительный рост количества внедрений NLU-технологий в 2015-2018-м годах.
1	Именно O
2	эти O
3	технологии O
4	, O
5	вместе O
6	с O
7	заметным O
8	продвижением O
9	в O
10	области O
11	технологий O
12	синтеза I-TERM
13	и I-TERM
14	распознавания I-TERM
15	речи I-TERM
16	, O
17	а O
18	также O
19	распространением O
20	мессенджеров O
21	и O
22	вебчатов O
23	– O
24	обусловили O
25	стремительный O
26	рост O
27	количества O
28	внедрений O
29	NLU I-TERM
30	- I-TERM
31	технологий I-TERM
32	в O
33	2015 I-TERM
34	- I-TERM
35	2018-м I-TERM
36	годах O
37	. O

# sent_id = 6
# text =  Голосовые ассистенты (IVA): Alexa от Amazon, Google Assistant от Google, Siri от Apple, Cortana от Microsoft, Алиса от Яндекса – они определяют интенты (намерения) пользователей и исполняют команды.
1	Голосовые O
2	ассистенты O
3	( O
4	IVA O
5	) O
6	: O
7	Alexa I-TERM
8	от O
9	Amazon I-TERM
10	, O
11	Google I-TERM
12	Assistant I-TERM
13	от O
14	Google I-TERM
15	, O
16	Siri I-TERM
17	от O
18	Apple I-TERM
19	, O
20	Cortana I-TERM
21	от O
22	Microsoft I-TERM
23	, O
24	Алиса I-TERM
25	от O
26	Яндекса I-TERM
27	– O
28	они O
29	определяют O
30	интенты I-TERM
31	( O
32	намерения I-TERM
33	) O
34	пользователей O
35	и O
36	исполняют O
37	команды O
38	. O

# sent_id = 7
# text =  В качестве каналов могут выступать умные устройства, ассистенты, встроенные в устройства или мобильные телефоны, привычный звонок на номер телефона, мессенджеры или вебчаты, подобные популярным в России Livetex, Jivosite или Webim.
1	В O
2	качестве O
3	каналов O
4	могут O
5	выступать O
6	умные O
7	устройства O
8	, O
9	ассистенты O
10	, O
11	встроенные O
12	в O
13	устройства O
14	или O
15	мобильные O
16	телефоны O
17	, O
18	привычный O
19	звонок O
20	на O
21	номер O
22	телефона O
23	, O
24	мессенджеры O
25	или O
26	вебчаты O
27	, O
28	подобные O
29	популярным O
30	в O
31	России O
32	Livetex I-TERM
33	, O
34	Jivosite I-TERM
35	или O
36	Webim I-TERM
37	. O

# sent_id = 8
# text =  За эту конвертацию отвечают платформы ASR (распознавание речи), TTS (синтез речи), системы интеграции с телефонией.
1	За O
2	эту O
3	конвертацию O
4	отвечают O
5	платформы O
6	ASR I-TERM
7	( O
8	распознавание I-TERM
9	речи I-TERM
10	) O
11	, O
12	TTS I-TERM
13	( O
14	синтез I-TERM
15	речи I-TERM
16	) O
17	, O
18	системы O
19	интеграции O
20	с O
21	телефонией O
22	. O

# sent_id = 9
# text =  Наличие развитого rule-based синтаксиса может ускорить разработку чатботов в разы.
1	Наличие O
2	развитого O
3	rule I-TERM
4	- I-TERM
5	based I-TERM
6	синтаксиса O
7	может O
8	ускорить O
9	разработку O
10	чатботов O
11	в O
12	разы O
13	. O

# sent_id = 10
# text =  Анализ эмоций, богатая и глубокая аналитика, специальные фильтры (например, на использование ненормативной лексики), языковая поддержка, хранение контекста, как и собственно, точность работы используемых нейросетевых алгоритмов, а также производительность, масштабируемость и стабильность – все это также важные, хотя и не всегда очевидные со стороны, особенности диалоговых платформ.
1	Анализ I-TERM
2	эмоций I-TERM
3	, O
4	богатая O
5	и O
6	глубокая I-TERM
7	аналитика I-TERM
8	, O
9	специальные O
10	фильтры O
11	( O
12	например O
13	, O
14	на O
15	использование O
16	ненормативной O
17	лексики O
18	) O
19	, O
20	языковая O
21	поддержка O
22	, O
23	хранение O
24	контекста O
25	, O
26	как O
27	и O
28	собственно O
29	, O
30	точность O
31	работы O
32	используемых O
33	нейросетевых O
34	алгоритмов O
35	, O
36	а O
37	также O
38	производительность O
39	, O
40	масштабируемость O
41	и O
42	стабильность O
43	– O
44	все O
45	это O
46	также O
47	важные O
48	, O
49	хотя O
50	и O
51	не O
52	всегда O
53	очевидные O
54	со O
55	стороны O
56	, O
57	особенности O
58	диалоговых I-TERM
59	платформ I-TERM
60	. O

# sent_id = 11
# text =  Алгоритм понимания естественного языка (Natural Language Understanding, NLU) Microsoft DeBERTa превзошел человеческие возможности в одном из самых сложных тестов для подобных алгоритмов SuperGLUE.
1	Алгоритм O
2	понимания O
3	естественного O
4	языка O
5	( O
6	Natural I-TERM
7	Language I-TERM
8	Understanding I-TERM
9	, O
10	NLU I-TERM
11	) O
12	Microsoft I-TERM
13	DeBERTa I-TERM
14	превзошел O
15	человеческие O
16	возможности O
17	в O
18	одном O
19	из O
20	самых O
21	сложных O
22	тестов O
23	для O
24	подобных O
25	алгоритмов O
26	SuperGLUE I-TERM
27	. O

# sent_id = 12
# text =  На данный момент модель занимает первое место в рейтинге с показателем в 90,3, в то время как среднее значение человеческих возможностей составляет 89,8 баллов.
1	На O
2	данный O
3	момент O
4	модель O
5	занимает O
6	первое O
7	место O
8	в O
9	рейтинге O
10	с O
11	показателем O
12	в O
13	90,3 I-TERM
14	, O
15	в O
16	то O
17	время O
18	как O
19	среднее O
20	значение O
21	человеческих O
22	возможностей O
23	составляет O
24	89,8 I-TERM
25	баллов O
26	. O

# sent_id = 13
# text = Тест SuperGLUE включает в себя ряд задач, которые разработаны для оценки способности ИИ-моделей распознавать и понимать естественный язык, например, дать правильный ответ на вопрос на базе прочитанного абзаца, определить, правильно ли используется многозначное слово в определенном контексте и т.д.
1	Тест O
2	SuperGLUE I-TERM
3	включает O
4	в O
5	себя O
6	ряд O
7	задач O
8	, O
9	которые O
10	разработаны O
11	для O
12	оценки O
13	способности O
14	ИИ O
15	- O
16	моделей O
17	распознавать O
18	и O
19	понимать O
20	естественный O
21	язык O
22	, O
23	например O
24	, O
25	дать I-TERM
26	правильный I-TERM
27	ответ I-TERM
28	на I-TERM
29	вопрос I-TERM
30	на I-TERM
31	базе I-TERM
32	прочитанного I-TERM
33	абзаца I-TERM
34	, O
35	определить O
36	, O
37	правильно O
38	ли O
39	используется O
40	многозначное I-TERM
41	слово I-TERM
42	в O
43	определенном O
44	контексте O
45	и O
46	т.д. O

# sent_id = 14
# text =  Тест был разработан группой исследователей в 2019 году.
1	Тест O
2	был O
3	разработан O
4	группой O
5	исследователей O
6	в O
7	2019 I-TERM
8	году O
9	. O

# sent_id = 15
# text = Для того чтобы добиться текущего результата в 90,3 балла, DeBERTa получила масштабное обновление архитектуры: теперь она состоит из 48 слоев и имеет 1,5 млрд параметров.
1	Для O
2	того O
3	чтобы O
4	добиться O
5	текущего O
6	результата O
7	в O
8	90,3 I-TERM
9	балла O
10	, O
11	DeBERTa I-TERM
12	получила O
13	масштабное O
14	обновление O
15	архитектуры O
16	: O
17	теперь O
18	она O
19	состоит O
20	из O
21	48 I-TERM
22	слоев O
23	и O
24	имеет O
25	1,5 O
26	млрд O
27	параметров O
28	. O

# sent_id = 16
# text =  Кроме того, DeBERTa будет интегрирована в следующую версию Тьюринговой модели Microsoft Turing (Turing NLRv4).
1	Кроме O
2	того O
3	, O
4	DeBERTa I-TERM
5	будет O
6	интегрирована O
7	в O
8	следующую O
9	версию O
10	Тьюринговой I-TERM
11	модели I-TERM
12	Microsoft I-TERM
13	Turing I-TERM
14	( O
15	Turing I-TERM
16	NLRv4 I-TERM
17	) O
18	. O

# sent_id = 17
# text =  Тьюринговые модели используются в таких продуктах Microsoft, как Bing, Office, Dynamics и Azure Cognitive Services, чтобы совершенствовать, к примеру, взаимодействие с чат-ботами, предоставление рекомендаций и ответов на вопросы, поиск, автоматизацию поддержки клиентов, создание контента и решение многих других задач на пользу сотен миллионов пользователей.
1	Тьюринговые I-TERM
2	модели I-TERM
3	используются O
4	в O
5	таких O
6	продуктах O
7	Microsoft I-TERM
8	, O
9	как O
10	Bing I-TERM
11	, O
12	Office I-TERM
13	, O
14	Dynamics I-TERM
15	и O
16	Azure I-TERM
17	Cognitive I-TERM
18	Services I-TERM
19	, O
20	чтобы O
21	совершенствовать O
22	, O
23	к O
24	примеру O
25	, O
26	взаимодействие O
27	с O
28	чат I-TERM
29	- I-TERM
30	ботами I-TERM
31	, O
32	предоставление O
33	рекомендаций O
34	и O
35	ответов O
36	на O
37	вопросы O
38	, O
39	поиск O
40	, O
41	автоматизацию 
42	поддержки O
43	клиентов O
44	, O
45	создание I-TERM
46	контента I-TERM
47	и O
48	решение O
49	многих O
50	других O
51	задач O
52	на O
53	пользу O
54	сотен O
55	миллионов O
56	пользователей O
57	. O

# sent_id = 18
# text =  В отличии от машин, люди хорошо умеют использовать знания, ранее полученные при выполнении различных задач, для решения новых – это называется композиционным обобщением (англ. compositional generalization).
1	В O
2	отличии O
3	от O
4	машин O
5	, O
6	люди O
7	хорошо O
8	умеют O
9	использовать O
10	знания O
11	, O
12	ранее O
13	полученные O
14	при O
15	выполнении O
16	различных O
17	задач O
18	, O
19	для O
20	решения O
21	новых O
22	– O
23	это O
24	называется O
25	композиционным I-TERM
26	обобщением I-TERM
27	( O
28	англ O
29	. O
30	compositional I-TERM
31	generalization I-TERM
32	) O
33	. O

# sent_id = 19
# text =  Зайдя на несколько из них я увидел что большая половина типа Wix используют технологию Искусственного Интеллекта, чтобы создать шаблон разметки страницы и далее её уже заполнить.
1	Зайдя O
2	на O
3	несколько O
4	из O
5	них O
6	я O
7	увидел O
8	что O
9	большая O
10	половина O
11	типа O
12	Wix I-TERM
13	используют O
14	технологию O
15	Искусственного I-TERM
16	Интеллекта I-TERM
17	, O
18	чтобы O
19	создать O
20	шаблон O
21	разметки O
22	страницы O
23	и O
24	далее O
25	её O
26	уже O
27	заполнить O
28	. O

# sent_id = 20
# text =  В финале я могу его редактировать путем Drag & Drop.
1	В O
2	финале O
3	я O
4	могу O
5	его O
6	редактировать O
7	путем O
8	Drag I-TERM
9	& I-TERM
10	Drop I-TERM
11	. O

# sent_id = 21
# text =  Типичным методом обучения без учителя является кластеризация, благодаря которому обучающая выборка разбивается на устойчивые группы или кластеры.
1	Типичным O
2	методом O
3	обучения O
4	без O
5	учителя O
6	является O
7	кластеризация I-TERM
8	, O
9	благодаря O
10	которому O
11	обучающая O
12	выборка I-TERM
13	разбивается O
14	на O
15	устойчивые O
16	группы O
17	или O
18	кластеры I-TERM
19	. O

# sent_id = 22
# text =  Другой подход обучения без учителя для текстов называется тематическим моделированием (topic modeling), позволяющим выявить в неразмеченных текстах основные тематики.
1	Другой O
2	подход O
3	обучения O
4	без O
5	учителя O
6	для O
7	текстов O
8	называется O
9	тематическим I-TERM
10	моделированием I-TERM
11	( O
12	topic I-TERM
13	modeling I-TERM
14	) O
15	, O
16	позволяющим O
17	выявить O
18	в O
19	неразмеченных O
20	текстах O
21	основные O
22	тематики O
23	. O

# sent_id = 23
# text =  Если отказываемся от методов unsupervised learning, то логично обратиться к методам обучения с учителем (supervised learning) и в частности к классификации.
1	Если O
2	отказываемся O
3	от O
4	методов O
5	unsupervised I-TERM
6	learning I-TERM
7	, O
8	то O
9	логично O
10	обратиться O
11	к O
12	методам I-TERM
13	обучения I-TERM
14	с I-TERM
15	учителем I-TERM
16	( O
17	supervised I-TERM
18	learning I-TERM
19	) O
20	и O
21	в O
22	частности O
23	к O
24	классификации I-TERM
25	. O

# sent_id = 24
# text =  Результатом работы языковой модели являются эмбеддинги — это отображение из пространства слов в пространство векторов конкретной фиксированной длины, причем векторы, соответствующие близким по смыслу словам, будут расположены в новом пространстве рядом, а далекие по смыслу — далеко.
1	Результатом O
2	работы O
3	языковой O
4	модели O
5	являются O
6	эмбеддинги I-TERM
7	— O
8	это O
9	отображение O
10	из O
11	пространства O
12	слов I-TERM
13	в O
14	пространство O
15	векторов O
16	конкретной O
17	фиксированной O
18	длины O
19	, O
20	причем O
21	векторы O
22	, O
23	соответствующие O
24	близким O
25	по O
26	смыслу I-TERM
27	словам I-TERM
28	, O
29	будут O
30	расположены O
31	в O
32	новом O
33	пространстве O
34	рядом O
35	, O
36	а O
37	далекие O
38	по O
39	смыслу O
40	— O
41	далеко O
42	. O

# sent_id = 25
# text =  При использовании TF-IDF (например, вот) подхода с фильтром по частотам и логистической регрессии уже можно получить прекрасные результаты: изначально в краулер отправлялись очень разные тексты, и модель прекрасно справляется.
1	При O
2	использовании O
3	TF I-TERM
4	- I-TERM
5	IDF I-TERM
6	( O
7	например O
8	, O
9	вот O
10	) O
11	подхода O
12	с O
13	фильтром O
14	по O
15	частотам O
16	и O
17	логистической I-TERM
18	регрессии I-TERM
19	уже O
20	можно O
21	получить O
22	прекрасные O
23	результаты O
24	: O
25	изначально O
26	в O
27	краулер O
28	отправлялись O
29	очень O
30	разные O
31	тексты O
32	, O
33	и O
34	модель O
35	прекрасно O
36	справляется O
37	. O

# sent_id = 26
# text =  Для каждой из популяций рассчитаем word2vec расстояние до центра положительной обучающей выборки.
1	Для O
2	каждой O
3	из O
4	популяций O
5	рассчитаем O
6	word2vec I-TERM
7	расстояние O
8	до O
9	центра O
10	положительной O
11	обучающей O
12	выборки I-TERM
13	. O

# sent_id = 27
# text =  Распределения можно разделить, и для оценки расстояния между распределениями в первую очередь логично обратиться к Дивергенции Кульбака-Лейблера (ДКЛ).
1	Распределения O
2	можно O
3	разделить O
4	, O
5	и O
6	для O
7	оценки O
8	расстояния O
9	между O
10	распределениями O
11	в O
12	первую O
13	очередь O
14	логично O
15	обратиться O
16	к O
17	Дивергенции I-TERM
18	Кульбака I-TERM
19	- I-TERM
20	Лейблера I-TERM
21	( O
22	ДКЛ I-TERM
23	) O
24	. O

# sent_id = 28
# text =  Основатель компании Imagination Engines, Stephen L. Thaler продвигает свою нейронную сеть по имени DABUS (Device for the Autonomous Boot-strapping of Unified Sentience), указывая ее в качестве автора изобретения в заявках на патенты на разные изобретения, сгенерированные этой сетью.
1	Основатель O
2	компании O
3	Imagination I-TERM
4	Engines I-TERM
5	, O
6	Stephen I-TERM
7	L. I-TERM
8	Thaler I-TERM
9	продвигает O
10	свою O
11	нейронную O
12	сеть O
13	по O
14	имени O
15	DABUS I-TERM
16	( O
17	Device I-TERM
18	for I-TERM
19	the I-TERM
20	Autonomous I-TERM
21	Boot I-TERM
22	- I-TERM
23	strapping I-TERM
24	of I-TERM
25	Unified I-TERM
26	Sentience I-TERM
27	) O
28	, O
29	указывая O
30	ее O
31	в O
32	качестве O
33	автора O
34	изобретения O
35	в O
36	заявках O
37	на O
38	патенты O
39	на O
40	разные O
41	изобретения O
42	, O
43	сгенерированные O
44	этой O
45	сетью O
46	. O

# sent_id = 29
# text =  1 January 2010 at 07:59 Заметки об NLP (часть 2) Artificial Intelligence Natural Language Processing.
1	1 O
2	January O
3	2010 O
4	at O
5	07:59 O
6	Заметки O
7	об O
8	NLP I-TERM
9	( O
10	часть O
11	2 O
12	) O
13	Artificial I-TERM
14	Intelligence I-TERM
15	Natural I-TERM
16	Language I-TERM
17	Processing I-TERM
18	. O

# sent_id = 30
# text =  Хотя в первой части я и говорил, что не собираюсь останавливаться на морфологии, видимо, совсем без неё не получится
1	Хотя O
2	в O
3	первой O
4	части O
5	я O
6	и O
7	говорил O
8	, O
9	что O
10	не O
11	собираюсь O
12	останавливаться O
13	на O
14	морфологии I-TERM
15	, O
16	видимо O
17	, O
18	совсем O
19	без O
20	неё O
21	не O
22	получится O
23	. O

# sent_id = 31
# text =  Всё-таки обработка предложений сильно завязана на предшествующий морфологический анализ.
1	Всё O
2	- O
3	таки O
4	обработка O
5	предложений O
6	сильно O
7	завязана O
8	на O
9	предшествующий O
10	морфологический I-TERM
11	анализ I-TERM
12	. O

# sent_id = 32
# text =  Наш с вами родной русский язык очень хорош (для нас) и труден (для иностранцев) богатой фонетикой и разнообразием грамматических средств.
1	Наш O
2	с O
3	вами O
4	родной O
5	русский O
6	язык O
7	очень O
8	хорош O
9	( O
10	для O
11	нас O
12	) O
13	и O
14	труден O
15	( O
16	для O
17	иностранцев O
18	) O
19	богатой O
20	фонетикой I-TERM
21	и O
22	разнообразием O
23	грамматических I-TERM
24	средств I-TERM
25	. O

# sent_id = 33
# text =  Во-первых, в них не так много незнакомых нам фонем.
1	Во O
2	- O
3	первых O
4	, O
5	в O
6	них O
7	не O
8	так O
9	много O
10	незнакомых O
11	нам O
12	фонем I-TERM
13	. O

# sent_id = 34
# text =  Во-вторых, обилие грамматических явлений редко сталкивает нас с чем-либо непонятным.
1	Во O
2	- O
3	вторых O
4	, O
5	обилие O
6	грамматических I-TERM
7	явлений I-TERM
8	редко O
9	сталкивает O
10	нас O
11	с O
12	чем O
13	- O
14	либо O
15	непонятным O
16	. O

# sent_id = 35
# text =  А для американца, например, само понятие рода или падежа совершенно неочевидно.
1	А O
2	для O
3	американца O
4	, O
5	например O
6	, O
7	само O
8	понятие O
9	рода I-TERM
10	или O
11	падежа I-TERM
12	совершенно O
13	неочевидно O
14	. O

# sent_id = 36
# text =  Теперь о морфологии.
1	Теперь O
2	о O
3	морфологии I-TERM
4	. O

# sent_id = 37
# text =  Автоматические морфологические анализаторы работают хорошо.
1	Автоматические I-TERM
2	морфологические I-TERM
3	анализаторы I-TERM
4	работают O
5	хорошо O
6	. O

# sent_id = 38
# text =  Если кому интересно посмотреть, как работает автоматический анализатор — можно поэкспериментировать на сайте С.А. Старостина.
1	Если O
2	кому O
3	интересно O
4	посмотреть O
5	, O
6	как O
7	работает O
8	автоматический I-TERM
9	анализатор I-TERM
10	— O
11	можно O
12	поэкспериментировать O
13	на O
14	сайте O
15	С.А. I-TERM
16	Старостина I-TERM
17	. O

# sent_id = 39
# text =  Смею предположить, что едва ли не все морфологические анализаторы русского так или иначе опираются на Грамматический словарь Зализняка.
1	Смею O
2	предположить O
3	, O
4	что O
5	едва O
6	ли O
7	не O
8	все O
9	морфологические O
10	анализаторы O
11	русского O
12	так O
13	или O
14	иначе O
15	опираются O
16	на O
17	Грамматический I-TERM
18	словарь I-TERM
19	Зализняка I-TERM
20	. O

# sent_id = 40
# text =  Сам я пользуюсь разработками Алексея Сокирко, «обёрнутыми» в удобный интерфейс на сайте Lemmatizer.
1	Сам O
2	я O
3	пользуюсь O
4	разработками O
5	Алексея I-TERM
6	Сокирко I-TERM
7	, O
8	« O
9	обёрнутыми O
10	» O
11	в O
12	удобный O
13	интерфейс O
14	на O
15	сайте O
16	Lemmatizer I-TERM
17	. O

# sent_id = 41
# text =  Судите сами: упомянутый русский морфологический анализатор Алексея Сокирко оперирует базой данных в 18,5 мегабайт.
1	Судите O
2	сами O
3	: O
4	упомянутый O
5	русский I-TERM
6	морфологический I-TERM
7	анализатор I-TERM
8	Алексея I-TERM
9	Сокирко I-TERM
10	оперирует O
11	базой O
12	данных O
13	в O
14	18,5 O
15	мегабайт O
16	. O

# sent_id = 42
# text =  На Грамоте предлагают относить их к «предикативам», но общепринятого подхода нет.
1	На O
2	Грамоте I-TERM
3	предлагают O
4	относить O
5	их O
6	к O
7	« O
8	предикативам O
9	» O
10	, O
11	но O
12	общепринятого O
13	подхода O
14	нет O
15	. O

# sent_id = 43
# text =  Например, ещё одна «фича» анализатора Сокирко: он называет глаголы в личной форме («бегаю») глаголами, а в начальной форме («бегать») — инфинитивами.
1	Например O
2	, O
3	ещё O
4	одна O
5	« O
6	фича O
7	» O
8	анализатора I-TERM
9	Сокирко I-TERM
10	: O
11	он O
12	называет O
13	глаголы O
14	в O
15	личной O
16	форме O
17	( O
18	« O
19	бегаю O
20	» O
21	) O
22	глаголами O
23	, O
24	а O
25	в O
26	начальной O
27	форме O
28	( O
29	« O
30	бегать O
31	» O
32	) O
33	— O
34	инфинитивами O
35	. O

# sent_id = 44
# text =  Tags: NLP, обработка текстовб, компьютерная лингвистика.
1	Tags O
2	: O
3	NLP I-TERM
4	, O
5	обработка I-TERM
6	текстов
7	, O
8	компьютерная I-TERM
9	лингвистика I-TERM

# sent_id = 45
# text =  Туториал по фреймворку для программирования датасетов MTS AI corporate blog.
1	Туториал O
2	по O
3	фреймворку O
4	для O
5	программирования O
6	датасетов O
7	MTS I-TERM
8	AI I-TERM
9	corporate I-TERM
10	blog I-TERM

# sent_id = 46
# text =  Я Игорь Буянов, старший разработчик группы разметки данных MTS AI.
1	Я O
2	Игорь I-TERM
3	Буянов I-TERM
4	, O
5	старший O
6	разработчик O
7	группы O
8	разметки O
9	данных O
10	MTS I-TERM
11	AI I-TERM
12	. O

# sent_id = 47
# text =  Недавно рассказывал о том, как делать иерархически датасет из Википедии.
1	Недавно O
2	рассказывал O
3	о O
4	том O
5	, O
6	как O
7	делать O
8	иерархически O
9	датасет O
10	из O
11	Википедии I-TERM
12	. O

# sent_id = 48
# text =  В этом посте хочу рассказать вам о Сноркеле - фреймворке для программирования данных (data programming).
1	В O
2	этом O
3	посте O
4	хочу O
5	рассказать O
6	вам O
7	о O
8	Сноркеле I-TERM
9	- O
10	фреймворке I-TERM
11	для I-TERM
12	программирования I-TERM
13	данных I-TERM
14	( O
15	data I-TERM
16	programming I-TERM
17	) O
18	. O

# sent_id = 49
# text =  Проект стартовал в Стэнфорде как инструмент для помощи в разметке датасетов для задачи information extraction, а сейчас разработчики делают платформу для пользования внешними заказчиками. 
1	Проект O
2	стартовал O
3	в O
4	Стэнфорде I-TERM
5	как O
6	инструмент O
7	для O
8	помощи O
9	в O
10	разметке O
11	датасетов O
12	для O
13	задачи O
14	information I-TERM
15	extraction I-TERM
16	, O
17	а O
18	сейчас O
19	разработчики O
20	делают O
21	платформу O
22	для O
23	пользования O
24	внешними O
25	заказчиками O
26	. O

# sent_id = 50
# text =  В разметочные функции (labeling functions) закодированы все возможные правила, по которым можно поставить какую-либо метку каждому примеру из набора данных.
1	В O
2	разметочные I-TERM
3	функции I-TERM
4	( O
5	labeling I-TERM
6	functions I-TERM
7	) O
8	закодированы O
9	все O
10	возможные O
11	правила O
12	, O
13	по O
14	которым O
15	можно O
16	поставить O
17	какую O
18	- O
19	либо O
20	метку O
21	каждому O
22	примеру O
23	из O
24	набора O
25	данных O
26	. O

# sent_id = 51
# text =  В качестве основы для таких функций используются:внешние базы данных, такие как WordNet или WikiBase.
1	В O
2	качестве O
3	основы O
4	для O
5	таких O
6	функций O
7	используются O
8	: O
9	внешние O
10	базы O
11	данных O
12	, O
13	такие O
14	как O
15	WordNet I-TERM
16	или O
17	WikiBase I-TERM
18	. O

# sent_id = 52
# text =  Генеративная модель, являющаяся сердцем Сноркеля, попытается учесть недостатки отдельных функций.
1	Генеративная I-TERM
2	модель I-TERM
3	, O
4	являющаяся O
5	сердцем O
6	Сноркеля I-TERM
7	, O
8	попытается O
9	учесть O
10	недостатки O
11	отдельных O
12	функций O
13	. O

# sent_id = 53
# text =  Для наглядности оставляю здесь иллюстрацию с последовательностью работы со Снокрелем для задачи information extraction из оригинальной статьи.
1	Для O
2	наглядности O
3	оставляю O
4	здесь O
5	иллюстрацию O
6	с O
7	последовательностью O
8	работы O
9	со O
10	Снокрелем I-TERM
11	для O
12	задачи O
13	information I-TERM
14	extraction I-TERM
15	из O
16	оригинальной O
17	статьи O
18	. O

# sent_id = 54
# text =  Авторы оригинальной статьи представляют ее как факторный граф, или графическую вероятностную модель.
1	Авторы O
2	оригинальной O
3	статьи O
4	представляют O
5	ее O
6	как O
7	факторный I-TERM
8	граф I-TERM
9	, O
10	или O
11	графическую I-TERM
12	вероятностную I-TERM
13	модель I-TERM
14	. O

# sent_id = 55
# text =  Тогда модель определяется так, чтобы обучить эту модель без доступа к истинным меткам, это нужно обучаться с помощью логарифмического негативного маргинализированного правдоподобия, зная матрицу Оптимизацию авторы проводили с помощью SGD с семплированием Гиббса.
1	Тогда O
2	модель O
3	определяется O
4	так
5	, O
6	чтобы O
7	обучить O
8	эту O
9	модель O
10	без O
11	доступа O
12	к O
13	истинным O
14	меткам O
15	, O
16	это O
17	нужно O
18	обучаться O
19	с O
20	помощью O
21	логарифмического I-TERM
22	негативного I-TERM
23	маргинализированного I-TERM
24	правдоподобия I-TERM
25	, O
26	зная O
27	матрицу I-TERM
28	Оптимизацию I-TERM
29	авторы O
30	проводили O
31	с O
32	помощью O
33	SGD I-TERM
34	с O
35	семплированием I-TERM
36	Гиббса I-TERM
37	. O

# sent_id = 56
# text =  Загрузим заранее обученную модель fastText, чей выбор объясняется наличием огромного количества опечаток в текстах.
1	Загрузим O
2	заранее O
3	обученную O
4	модель O
5	fastText I-TERM
6	, O
7	чей O
8	выбор O
9	объясняется O
10	наличием O
11	огромного O
12	количества O
13	опечаток O
14	в O
15	текстах O
16	. O

# sent_id = 57
# text =  Таким образом мы получили опорный вектор для класса "диарея".
1	Таким O
2	образом O
3	мы O
4	получили O
5	опорный I-TERM
6	вектор I-TERM
7	для O
8	класса O
9	" O
10	диарея O
11	" O
12	. O

# sent_id = 58
# text =  Речь шла о морфологической разметке (part of speech tagging) современных текстов на русском языке.
1	Речь O
2	шла O
3	о O
4	морфологической I-TERM
5	разметке I-TERM
6	( O
7	part I-TERM
8	of I-TERM
9	speech I-TERM
10	tagging I-TERM
11	) O
12	современных O
13	текстов O
14	на O
15	русском I-TERM
16	языке I-TERM
17	. O

# sent_id = 59
# text =  Как обычно, результат разметки будет опубликован на условиях лицензии Creative Commons.
1	Как O
2	обычно O
3	, O
4	результат O
5	разметки I-TERM
6	будет O
7	опубликован O
8	на O
9	условиях O
10	лицензии O
11	Creative I-TERM
12	Commons I-TERM
13	. O

# sent_id = 60
# text =  Извлечение именованных сущностей из текста — одна из востребованных функций текстовой аналитики.
1	Извлечение I-TERM
2	именованных I-TERM
3	сущностей I-TERM
4	из O
5	текста O
6	— O
7	одна O
8	из O
9	востребованных O
10	функций O
11	текстовой O
12	аналитики O
13	. O

# sent_id = 61
# text =  Специалисты Data Science часто применяют различные методы получения датасетов.
1	Специалисты O
2	Data I-TERM
3	Science I-TERM
4	часто O
5	применяют O
6	различные O
7	методы O
8	получения O
9	датасетов O
10	. O

# sent_id = 62
# text =  Цель этой статьи — представить краткий обзор трех разных методов извлечения данных с использованием языка Python.
1	Цель O
2	этой O
3	статьи O
4	— O
5	представить O
6	краткий O
7	обзор O
8	трех O
9	разных O
10	методов O
11	извлечения O
12	данных O
13	с O
14	использованием O
15	языка O
16	Python I-TERM
17	. O

# sent_id = 63
# text =  Я расскажу, как делать это с помощью Jupyter Notebook.
1	Я O
2	расскажу O
3	, O
4	как O
5	делать O
6	это O
7	с O
8	помощью O
9	Jupyter I-TERM
10	Notebook I-TERM
11	. O

# sent_id = 64
# text =  Библиотека SQLAlchemy позволит связать ваш код в ноутбуке с наиболее распространенными типами баз данных.
1	Библиотека O
2	SQLAlchemy I-TERM
3	позволит O
4	связать O
5	ваш O
6	код O
7	в O
8	ноутбуке O
9	с O
10	наиболее O
11	распространенными O
12	типами O
13	баз O
14	данных O
15	. O

# sent_id = 65
# text =  Мы собираемся применить Beautiful Soup и библиотеку urllib, чтобы соскрапить названия отелей и цены на них с веб-сайта TripAdvisor.
1	Мы O
2	собираемся O
3	применить O
4	Beautiful I-TERM
5	Soup I-TERM
6	и O
7	библиотеку O
8	urllib I-TERM
9	, O
10	чтобы O
11	соскрапить O
12	названия O
13	отелей O
14	и O
15	цены O
16	на O
17	них O
18	с O
19	веб O
20	- O
21	сайта O
22	TripAdvisor I-TERM
23	. O

# sent_id = 66
# text =  Я приведу простой пример извлечения данных о погоде с общедоступного API Dark Sky.
1	Я O
2	приведу O
3	простой O
4	пример O
5	извлечения O
6	данных O
7	о O
8	погоде O
9	с O
10	общедоступного O
11	API I-TERM
12	Dark I-TERM
13	Sky I-TERM
14	. O

# sent_id = 67
# text =  Для доступа к данным из Dark Sky я воспользуюсь библиотекой requests.
1	Для O
2	доступа O
3	к O
4	данным O
5	из O
6	Dark I-TERM
7	Sky I-TERM
8	я O
9	воспользуюсь O
10	библиотекой O
11	requests I-TERM
12	. O

# sent_id = 68
# text =  Известный учёный Алан Тьюринг в 1950 году усомнился в том, что машина не может мыслить, и для проверки предложил свой знаменитый тест.
1	Известный O
2	учёный O
3	Алан I-TERM
4	Тьюринг I-TERM
5	в O
6	1950 I-TERM
7	году O
8	усомнился O
9	в O
10	том O
11	, O
12	что O
13	машина O
14	не O
15	может O
16	мыслить O
17	, O
18	и O
19	для O
20	проверки O
21	предложил O
22	свой O
23	знаменитый O
24	тест I-TERM
25	. O

# sent_id = 69
# text = В 1954 году прошёл Джорджтаунский эксперимент.
1	В O
2	1954 I-TERM
3	году O
4	прошёл O
5	Джорджтаунский I-TERM
6	эксперимент I-TERM
7	. O

# sent_id = 70
# text =  В его рамках демонстрировалась система, которая автоматически перевела 60 предложений с русского языка на французский.
1	В O
2	его O
3	рамках O
4	демонстрировалась O
5	система I-TERM
6	, O
7	которая O
8	автоматически O
9	перевела O
10	60 O
11	предложений I-TERM
12	с O
13	русского I-TERM
14	языка I-TERM
15	на O
16	французский I-TERM
17	. O

# sent_id = 71
# text =  В 1960-е годы появились первые чат-боты, очень примитивные: в основном они перефразировали то, что говорил им собеседник-человек.
1	В O
2	1960-е I-TERM
3	годы O
4	появились O
5	первые O
6	чат I-TERM
7	- I-TERM
8	боты I-TERM
9	, O
10	очень O
11	примитивные O
12	: O
13	в O
14	основном O
15	они O
16	перефразировали O
17	то O
18	, O
19	что O
20	говорил O
21	им O
22	собеседник O
23	- O
24	человек O
25	. O

# sent_id = 72
# text =  Даже знаменитый чат-бот Женя Густман, который, как считается, прошёл одну из версий теста Тьюринга, сделал это не благодаря хитрым алгоритмам.
1	Даже O
2	знаменитый O
3	чат I-TERM
4	- I-TERM
5	бот I-TERM
6	Женя I-TERM
7	Густман I-TERM
8	, O
9	который O
10	, O
11	как O
12	считается O
13	, O
14	прошёл O
15	одну O
16	из O
17	версий O
18	теста I-TERM
19	Тьюринга I-TERM
20	, O
21	сделал O
22	это O
23	не O
24	благодаря O
25	хитрым O
26	алгоритмам O
27	. O

# sent_id = 73
# text =  Учёные пытались всё формализовать, построить формальную модель, онтологию, понятия, связи, общие правила синтаксического разбора и универсальную грамматику.
1	Учёные O
2	пытались O
3	всё O
4	формализовать O
5	, O
6	построить O
7	формальную I-TERM
8	модель I-TERM
9	, O
10	онтологию I-TERM
11	, O
12	понятия I-TERM
13	, O
14	связи I-TERM
15	, O
16	общие O
17	правила O
18	синтаксического I-TERM
19	разбора I-TERM
20	и O
21	универсальную I-TERM
22	грамматику I-TERM
23	. O

# sent_id = 74
# text =  Тогда возникла теория грамматик Хомского.
1	Тогда O
2	возникла O
3	теория I-TERM
4	грамматик I-TERM
5	Хомского I-TERM
6	. O

# sent_id = 75
# text =  Поэтому в 1980-е годы внимание переключилось на систему другого класса: на алгоритмы машинного обучения и так называемую корпусную лингвистику.
1	Поэтому O
2	в O
3	1980-е I-TERM
4	годы O
5	внимание O
6	переключилось O
7	на O
8	систему O
9	другого O
10	класса I-TERM
11	: O
12	на O
13	алгоритмы I-TERM
14	машинного I-TERM
15	обучения I-TERM
16	и O
17	так O
18	называемую O
19	корпусную I-TERM
20	лингвистику I-TERM
21	. O

# sent_id = 76
# text =  В 1990-е годы эта область получила очень мощный толчок благодаря развитию Всемирной паутины с большим количеством слабоструктурированного текста, по которому нужно было искать, его требовалось каталогизировать.
1	В O
2	1990-е I-TERM
3	годы O
4	эта O
5	область O
6	получила O
7	очень O
8	мощный O
9	толчок O
10	благодаря O
11	развитию O
12	Всемирной I-TERM
13	паутины I-TERM
14	с O
15	большим O
16	количеством O
17	слабоструктурированного I-TERM
18	текста I-TERM
19	, O
20	по O
21	которому O
22	нужно O
23	было O
24	искать O
25	, O
26	его O
27	требовалось O
28	каталогизировать O
29	. O

# sent_id = 77
# text =  В 2000-е анализ естественных языков начал применяться уже не только для поиска в Интернете, но и для решения разнообразных задач.
1	В O
2	2000-е O
3	анализ I-TERM
4	естественных I-TERM
5	языков I-TERM
6	начал O
7	применяться O
8	уже O
9	не O
10	только O
11	для O
12	поиска O
13	в O
14	Интернете I-TERM
15	, O
16	но O
17	и O
18	для O
19	решения O
20	разнообразных O
21	задач O
22	. O

# sent_id = 78
# text =  Возникли модели, основанные на краудсорсинге: мы не только пытаемся что-то понять с помощью машины, а подключаем людей, которые за небольшую плату определяют, на каком языке написан текст.
1	Возникли O
2	модели O
3	, O
4	основанные O
5	на O
6	краудсорсинге I-TERM
7	: O
8	мы O
9	не O
10	только O
11	пытаемся O
12	что O
13	- O
14	то O
15	понять O
16	с O
17	помощью O
18	машины O
19	, O
20	а O
21	подключаем O
22	людей O
23	, O
24	которые O
25	за O
26	небольшую O
27	плату O
28	определяют O
29	, O
30	на O
31	каком O
32	языке O
33	написан O
34	текст O
35	. O

# sent_id = 79
# text =  В некотором смысле начали возрождаться идеи использования формальных онтологий, но теперь онтологии крутятся вокруг краудсорсинговых баз знаний, в частности баз на основе Linked Open Data.
1	В O
2	некотором O
3	смысле O
4	начали O
5	возрождаться O
6	идеи O
7	использования O
8	формальных I-TERM
9	онтологий I-TERM
10	, O
11	но O
12	теперь O
13	онтологии I-TERM
14	крутятся O
15	вокруг O
16	краудсорсинговых O
17	баз O
18	знаний O
19	, O
20	в O
21	частности O
22	баз O
23	на O
24	основе O
25	Linked I-TERM
26	Open I-TERM
27	Data I-TERM
28	. O

# sent_id = 80
# text =  Это целый набор баз знаний, его центр — машиночитаемый вариант «Википедии» DBpedia, который тоже наполняется по краудсорсинговой модели.
1	Это O
2	целый O
3	набор O
4	баз O
5	знаний O
6	, O
7	его O
8	центр O
9	— O
10	машиночитаемый O
11	вариант O
12	« O
13	Википедии I-TERM
14	» O
15	DBpedia I-TERM
16	, O
17	который O
18	тоже O
19	наполняется O
20	по O
21	краудсорсинговой I-TERM
22	модели I-TERM
23	. O

# sent_id = 81
# text =  В частности, семантический анализ (о чём документ?), генерация автоматической аннотации и автоматического summary, перевод и создание документов.
1	В O
2	частности O
3	, O
4	семантический I-TERM
5	анализ I-TERM
6	( O
7	о O
8	чём O
9	документ O
10	? O
11	) O 
12	, O
13	генерация I-TERM
14	автоматической I-TERM
15	аннотации I-TERM
16	и O
17	автоматического I-TERM
18	summary I-TERM
19	, O
20	перевод I-TERM
21	и O
22	создание I-TERM
23	документов I-TERM
24	. O

# sent_id = 82
# text =  Все наверняка слышали об известном генераторе научных статей SCIgen, который создал статью «Корчеватель: Алгоритм типичной унификации точек доступа и избыточности».
1	Все O
2	наверняка O
3	слышали O
4	об O
5	известном O
6	генераторе O
7	научных O
8	статей O
9	SCIgen I-TERM
10	, O
11	который O
12	создал O
13	статью O
14	« O
15	Корчеватель O
16	: O
17	Алгоритм O
18	типичной O
19	унификации O
20	точек O
21	доступа O
22	и O
23	избыточности O
24	» O
25	. O

# sent_id = 83
# text =  Но в случае с лентой такие рекомендации работают плохо: здесь постоянно возникает ситуация холодного старта.
1	Но O
2	в O
3	случае O
4	с O
5	лентой O
6	такие O
7	рекомендации O
8	работают O
9	плохо O
10	: O
11	здесь O
12	постоянно O
13	возникает O
14	ситуация I-TERM
15	холодного I-TERM
16	старта I-TERM
17	. O

# sent_id = 84
# text =  Поэтому применим классический воркэраунд для задачи холодного старта и построим систему контентных рекомендаций: попробуем научить машину понимать, о чём написан пост.
1	Поэтому O
2	применим O
3	классический O
4	воркэраунд I-TERM
5	для O
6	задачи O
7	холодного I-TERM
8	старта I-TERM
9	и O
10	построим O
11	систему O
12	контентных O
13	рекомендаций O
14	: O
15	попробуем O
16	научить O
17	машину O
18	понимать O
19	, O
20	о O
21	чём O
22	написан O
23	пост O
24	. O

# sent_id = 85
# text =  Соответственно, требуется метод семантического анализа.
1	Соответственно O
2	, O
3	требуется O
4	метод I-TERM
5	семантического I-TERM
6	анализа I-TERM
7	. O

# sent_id = 86
# text =  Тут поможет анализ эмоциональной окраски.
1	Тут O
2	поможет O
3	анализ I-TERM
4	эмоциональной I-TERM
5	окраски I-TERM
6	. O

# sent_id = 87
# text =  В частности, это Apache Tika, японская библиотека language-detection и одна из последних разработок — питоновский пакет Ldig, который как раз работает на инфинитиграммах.
1	В O
2	частности O
3	, O
4	это O
5	Apache I-TERM
6	Tika I-TERM
7	, O
8	японская O
9	библиотека O
10	language I-TERM
11	- I-TERM
12	detection I-TERM
13	и O
14	одна O
15	из O
16	последних O
17	разработок O
18	— O
19	питоновский O
20	пакет O
21	Ldig I-TERM
22	, O
23	который O
24	как O
25	раз O
26	работает O
27	на O
28	инфинитиграммах O
29	. O

# sent_id = 88
# text =  Но если текст короткий, из одного предложения или нескольких слов, то классический подход, основанный на триграммах, очень часто ошибается.
1	Но O
2	если O
3	текст I-TERM
4	короткий O
5	, O
6	из O
7	одного O
8	предложения I-TERM
9	или O
10	нескольких O
11	слов O
12	, O
13	то O
14	классический O
15	подход O
16	, O
17	основанный O
18	на O
19	триграммах I-TERM
20	, O
21	очень O
22	часто O
23	ошибается O
24	. O

# sent_id = 89
# text =  Исправить ситуацию могут инфинитиграммы, но это новая область, далеко не для всех языков уже есть обученные и готовые классификаторы.
1	Исправить O
2	ситуацию O
3	могут O
4	инфинитиграммы I-TERM
5	, O
6	но O
7	это O
8	новая O
9	область O
10	, O
11	далеко O
12	не O
13	для O
14	всех O
15	языков O
16	уже O
17	есть O
18	обученные O
19	и O
20	готовые O
21	классификаторы O
22	. O

# sent_id = 90
# text =  Первый основан на так называемом фонетическом матчинге.
1	Первый O
2	основан O
3	на O
4	так O
5	называемом O
6	фонетическом I-TERM
7	матчинге I-TERM
8	. O

# sent_id = 91
# text =  Альтернативный подход — так называемое редакционное расстояние, с помощью которого мы ищем в словаре максимально похожие слова-аналоги.
1	Альтернативный O
2	подход O
3	— O
4	так O
5	называемое O
6	редакционное I-TERM
7	расстояние I-TERM
8	, O
9	с O
10	помощью O
11	которого O
12	мы O
13	ищем O
14	в O
15	словаре O
16	максимально O
17	похожие O
18	слова O
19	- O
20	аналоги O
21	. O

# sent_id = 92
# text =  Первая концепция — стемминг, мы пытаемся найти основу слова.
1	Первая O
2	концепция O
3	— O
4	стемминг I-TERM
5	, O
6	мы O
7	пытаемся O
8	найти O
9	основу I-TERM
10	слова I-TERM
11	. O

# sent_id = 93
# text =  Здесь используется подход affix stripping.
1	Здесь O
2	используется O
3	подход O
4	affix I-TERM
5	stripping I-TERM
6	. O

# sent_id = 94
# text =  Есть известная реализация, так называемый стеммер Портера, или проект Snowball.
1	Есть O
2	известная O
3	реализация O
4	, O
5	так O
6	называемый O
7	стеммер I-TERM
8	Портера I-TERM
9	, O
10	или O
11	проект O
12	Snowball I-TERM
13	. O

# sent_id = 95
# text =  Самый распространённый, наверное, инструмент — реализация в пакете Apache Lucene.
1	Самый O
2	распространённый O
3	, O
4	наверное O
5	, O
6	инструмент O
7	— O
8	реализация O
9	в O
10	пакете O
11	Apache I-TERM
12	Lucene I-TERM
13	. O

# sent_id = 96
# text =  Вторая концепция, альтернатива стемминга — лемматизация.
1	Вторая O
2	концепция O
3	, O
4	альтернатива O
5	стемминга I-TERM
6	— O
7	лемматизация I-TERM
8	. O

# sent_id = 97
# text =  Она пытается привести слово не к основе или корню, а к базовой, словарной форме — т. е. лемме.
1	Она O
2	пытается O
3	привести O
4	слово I-TERM
5	не O
6	к O
7	основе I-TERM
8	или O
9	корню I-TERM
10	, O
11	а O
12	к O
13	базовой O
14	, O
15	словарной I-TERM
16	форме I-TERM
17	— O
18	т O
19	. O
20	  O
21	е O
22	. O
23	лемме I-TERM
24	. O

# sent_id = 98
# text =  Существует множество реализаций, и тема очень хорошо проработана именно для user generated текстов, пользовательски зашумлённых текстов.
1	Существует O
2	множество O
3	реализаций O
4	, O
5	и O
6	тема I-TERM
7	очень O
8	хорошо O
9	проработана O
10	именно O
11	для O
12	user I-TERM
13	generated I-TERM
14	текстов I-TERM
15	, O
16	пользовательски O
17	зашумлённых O
18	текстов I-TERM
19	. O

# sent_id = 99
# text =  Теперь отобразим это в векторном пространстве, потому что почти все математические модели работают в векторных пространствах больших размерностей.
1	Теперь O
2	отобразим O
3	это O
4	в O
5	векторном I-TERM
6	пространстве I-TERM
7	, O
8	потому O
9	что O
10	почти O
11	все O
12	математические I-TERM
13	модели I-TERM
14	работают O
15	в O
16	векторных I-TERM
17	пространствах I-TERM
18	больших O
19	размерностей O
20	. O

# sent_id = 100
# text =  Базовый подход, который используют многие модели, — метод "мешка слов".
1	Базовый O
2	подход O
3	, O
4	который O
5	используют O
6	многие O
7	модели O
8	, O
9	— O
10	метод I-TERM
11	" I-TERM
12	мешка I-TERM
13	слов I-TERM
14	" I-TERM
15	. O

# sent_id = 101
# text =  Доминирует так называемый TF-IDF.
1	Доминирует O
2	так O
3	называемый O
4	TF I-TERM
5	- I-TERM
6	IDF I-TERM
7	. O

# sent_id = 102
# text =  Частоту слова (term frequency, TF) определяют по-разному.
1	Частоту I-TERM
2	слова I-TERM
3	( O
4	term I-TERM
5	frequency I-TERM
6	, O
7	TF I-TERM
8	) O
9	определяют O
10	по O
11	- O
12	разному O
13	. O

# sent_id = 103
# text =  Определив TF в документе, мы перемножаем её с обратной частотой документа (inverse document frequency, IDF).
1	Определив O
2	TF I-TERM
3	в O
4	документе O
5	, O
6	мы O
7	перемножаем O
8	её O
9	с O
10	обратной I-TERM
11	частотой I-TERM
12	документа I-TERM
13	( O
14	inverse I-TERM
15	document I-TERM
16	frequency I-TERM
17	, O
18	IDF I-TERM
19	) O
20	. O

# sent_id = 104
# text =  IDF обычно вычисляют как логарифм от числа документов в корпусе, разделённый на количество документов, где это слово представлено.
1	IDF I-TERM
2	обычно O
3	вычисляют O
4	как O
5	логарифм I-TERM
6	от O
7	числа O
8	документов O
9	в O
10	корпусе I-TERM
11	, O
12	разделённый O
13	на O
14	количество O
15	документов O
16	, O
17	где O
18	это O
19	слово I-TERM
20	представлено O
21	. O

# sent_id = 105
# text =  Например, при анализе эмоциональной окраски очень важно, к чему относилось, условно говоря, слово «хороший» или «нет».
1	Например O
2	, O
3	при O
4	анализе I-TERM
5	эмоциональной I-TERM
6	окраски I-TERM
7	очень O
8	важно O
9	, O
10	к O
11	чему O
12	относилось O
13	, O
14	условно O
15	говоря O
16	, O
17	слово I-TERM
18	« O
19	хороший O
20	» O
21	или O
22	« O
23	нет O
24	» O
25	. O

# sent_id = 106
# text =  Тогда наряду с мешком слов поможет мешок N-грамм: мы добавляем в словарь не только слова, но и словосочетания.
1	Тогда O
2	наряду O
3	с O
4	мешком I-TERM
5	слов I-TERM
6	поможет O
7	мешок I-TERM
8	N I-TERM
9	- I-TERM
10	грамм I-TERM
11	: O
12	мы O
13	добавляем O
14	в O
15	словарь O
16	не O
17	только O
18	слова O
19	, O
20	но O
21	и O
22	словосочетания I-TERM
23	. O

# sent_id = 107
# text =  Мы не будем вносить все словосочетания, потому что это приведёт к комбинаторному взрыву, но часто используемые статистически значимые пары или пары, соответствующие именованным сущностям, можно добавить, и это повысит качество работы итоговой модели.
1	Мы O
2	не O
3	будем O
4	вносить O
5	все O
6	словосочетания I-TERM
7	, O
8	потому O
9	что O
10	это O
11	приведёт O
12	к O
13	комбинаторному O
14	взрыву O
15	, O
16	но O
17	часто O
18	используемые O
19	статистически O
20	значимые O
21	пары O
22	или O
23	пары O
24	, O
25	соответствующие O
26	именованным I-TERM
27	сущностям I-TERM
28	, O
29	можно O
30	добавить O
31	, O
32	и O
33	это O
34	повысит O
35	качество O
36	работы O
37	итоговой O
38	модели I-TERM
39	. O

# sent_id = 108
# text =  Отчасти эти ситуации позволяют обработать методы построения "векторных представлений слов", например, знаменитый word2vec или более модные skip-gramm.
1	Отчасти O
2	эти O
3	ситуации O
4	позволяют O
5	обработать O
6	методы O
7	построения O
8	" O
9	векторных I-TERM
10	представлений I-TERM
11	слов I-TERM
12	" O
13	, O
14	например O
15	, O
16	знаменитый O
17	word2vec I-TERM
18	или O
19	более O
20	модные O
21	skip I-TERM
22	- I-TERM
23	gramm I-TERM
24	. O

# sent_id = 109
# text =  Стандартные хеш-функции равномерно размазывают данные по пространству хешей.
1	Стандартные O
2	хеш I-TERM
3	- I-TERM
4	функции I-TERM
5	равномерно O
6	размазывают O
7	данные O
8	по O
9	пространству O
10	хешей O
11	. O

# sent_id = 110
# text =  Локально-чувствительный хеш похожие объекты поместит в пространстве объектов близко.
1	Локально I-TERM
2	- I-TERM
3	чувствительный I-TERM
4	хеш I-TERM
5	похожие O
6	объекты O
7	поместит O
8	в O
9	пространстве O
10	объектов O
11	близко O
12	. O

# sent_id = 111
# text =  Мы выбираем случайный базис из случайных векторов.
1	Мы O
2	выбираем O
3	случайный O
4	базис I-TERM
5	из O
6	случайных I-TERM
7	векторов I-TERM
8	. O

# sent_id = 112
# text =  Задача семантического анализа достаточно старая.
1	Задача O
2	семантического I-TERM
3	анализа I-TERM
4	достаточно O
5	старая O
6	. O

# sent_id = 113
# text =  Современный подход — анализ семантики без учителя, поэтому его называют анализом скрытой (латентной) семантики.
1	Современный O
2	подход O
3	— O
4	анализ I-TERM
5	семантики I-TERM
6	без I-TERM
7	учителя I-TERM
8	, O
9	поэтому O
10	его O
11	называют O
12	анализом I-TERM
13	скрытой I-TERM
14	( I-TERM
15	латентной I-TERM
16	) I-TERM
17	семантики I-TERM
18	. O

# sent_id = 114
# text =  Исторически первый подход к латентно-семантическому анализу — это латентно-семантическое индексирование.
1	Исторически O
2	первый O
3	подход O
4	к O
5	латентно I-TERM
6	- I-TERM
7	семантическому I-TERM
8	анализу I-TERM
9	— O
10	это O
11	латентно I-TERM
12	- I-TERM
13	семантическое I-TERM
14	индексирование I-TERM
15	. O

# sent_id = 115
# text =  Мы уже использовали для решения задач коллаборативных рекомендаций хорошо зарекомендовавшие себя техники факторизации матриц.
1	Мы O
2	уже O
3	использовали O
4	для O
5	решения O
6	задач I-TERM
7	коллаборативных I-TERM
8	рекомендаций I-TERM
9	хорошо O
10	зарекомендовавшие O
11	себя O
12	техники O
13	факторизации O
14	матриц O
15	. O

# sent_id = 116
# text =  В чём суть факторизации?
1	В O
2	чём O
3	суть O
4	факторизации I-TERM
5	? O

# sent_id = 117
# text =  Одной из альтернатив стал так называемый вероятностный латентно-семантический индекс.
1	Одной O
2	из O
3	альтернатив O
4	стал O
5	так O
6	называемый O
7	вероятностный I-TERM
8	латентно I-TERM
9	- I-TERM
10	семантический I-TERM
11	индекс I-TERM
12	. O

# sent_id = 118
# text =  Важно понять, что техника вероятностного латентно-семантического индекса — это техника факторизации матрицы.
1	Важно O
2	понять O
3	, O
4	что O
5	техника I-TERM
6	вероятностного I-TERM
7	латентно I-TERM
8	- I-TERM
9	семантического I-TERM
10	индекса I-TERM
11	— O
12	это O
13	техника I-TERM
14	факторизации I-TERM
15	матрицы I-TERM
16	. O

# sent_id = 119
# text =  По сравнению с классической факторизацией на основе сингулярного разложения у вероятностной генерирующей модели есть важное преимущество.
1	По O
2	сравнению O
3	с O
4	классической I-TERM
5	факторизацией I-TERM
6	на O
7	основе O
8	сингулярного I-TERM
9	разложения I-TERM
10	у O
11	вероятностной O
12	генерирующей O
13	модели O
14	есть O
15	важное O
16	преимущество O
17	. O

# sent_id = 120
# text =  Для этого используется перплексия.
1	Для O
2	этого O
3	используется O
4	перплексия I-TERM
5	. O

# sent_id = 121
# text =  Есть так называемый EM-алгоритм.
1	Есть O
2	так O
3	называемый O
4	EM I-TERM
5	- I-TERM
6	алгоритм I-TERM
7	. O

# sent_id = 122
# text = Как поясняет сам Томас Димсон, This Word Does Not Exist является вариацией нейросети GPT-2.
1	Как O
2	поясняет O
3	сам O
4	Томас I-TERM
5	Димсон I-TERM
6	, O
7	This I-TERM
8	Word I-TERM
9	Does I-TERM
10	Not I-TERM
11	Exist I-TERM
12	является O
13	вариацией O
14	нейросети O
15	GPT-2 I-TERM
16	. O

# sent_id = 123
# text =  Существует также твиттер-бот проекта.
1	Существует O
2	также O
3	твиттер I-TERM
4	- I-TERM
5	бот I-TERM
6	проекта O
7	. O

# sent_id = 124
# text =  Чтобы натренировать свою нейросеть на основе загруженных файлов, Димсон рекомендует воспользоваться контентом Apple Dictionary или Urban Dictionary.
1	Чтобы O
2	натренировать O
3	свою O
4	нейросеть O
5	на O
6	основе O
7	загруженных O
8	файлов O
9	, O
10	Димсон I-TERM
11	рекомендует O
12	воспользоваться O
13	контентом O
14	Apple I-TERM
15	Dictionary I-TERM
16	или O
17	Urban I-TERM
18	Dictionary I-TERM
19	. O

# sent_id = 125
# text = Правда, пользователи YCombinator уже заметили, что This Word Does Not Exist иногда предлагает уже существующие слова — например, refactoring.
1	Правда O
2	, O
3	пользователи O
4	YCombinator I-TERM
5	уже O
6	заметили O
7	, O
8	что O
9	This I-TERM
10	Word I-TERM
11	Does I-TERM
12	Not I-TERM
13	Exist I-TERM
14	иногда O
15	предлагает O
16	уже O
17	существующие O
18	слова O
19	— O
20	например O
21	, O
22	refactoring O
23	. O

# sent_id = 126
# text =  Он опубликовал программу (репозиторий на гитхабе), которая делает именно это: генерирует политические речи, удивительно похожие на настоящие.
1	Он O
2	опубликовал O
3	программу O
4	( O
5	репозиторий O
6	на O
7	гитхабе O
8	) O
9	, O
10	которая O
11	делает O
12	именно O
13	это O
14	: O
15	генерирует I-TERM
16	политические I-TERM
17	речи I-TERM
18	, O
19	удивительно O
20	похожие O
21	на O
22	настоящие O
23	. O

# sent_id = 127
# text = Ученые Новосибирского государственного технического университета НЭТИ завершают разработку системы распознавания русского жестового языка.
1	Ученые O
2	Новосибирского I-TERM
3	государственного I-TERM
4	технического I-TERM
5	университета I-TERM
6	НЭТИ I-TERM
7	завершают O
8	разработку O
9	системы O
10	распознавания O
11	русского I-TERM
12	жестового I-TERM
13	языка I-TERM
14	. O

# sent_id = 128
# text =  Точность распознавания составляет 92%.
1	Точность I-TERM
2	распознавания O
3	составляет O
4	92 I-TERM
5	% I-TERM
6	. O

# sent_id = 129
# text =  «Мы также вели работу над выделением эпентезы (межжестовое движение).
1	« O
2	Мы O
3	также O
4	вели O
5	работу O
6	над O
7	выделением I-TERM
8	эпентезы I-TERM
9	( O
10	межжестовое I-TERM
11	движение I-TERM
12	) O
13	. O

# sent_id = 130
# text =  Сейчас точность выделения жестов в видеопотоке составляет 85—90%.
1	Сейчас O
2	точность I-TERM
3	выделения I-TERM
4	жестов I-TERM
5	в O
6	видеопотоке O
7	составляет O
8	85—90 I-TERM
9	% I-TERM
10	. O

# sent_id = 131
# text =  После выхода учебника я читал курс на его основе в УрФУ, ШАДе, ИТМО и СПбГУ и убедился, что наличие перевода очень помогает.
1	После O
2	выхода O
3	учебника O
4	я O
5	читал O
6	курс O
7	на O
8	его O
9	основе O
10	в O
11	УрФУ I-TERM
12	, O
13	ШАДе I-TERM
14	, O
15	ИТМО I-TERM
16	и O
17	СПбГУ I-TERM
18	и O
19	убедился O
20	, O
21	что O
22	наличие O
23	перевода O
24	очень O
25	помогает O
26	. O

# sent_id = 132
# text =  В случае NLP потребность в «локализованных» учебных материалах еще заметнее, чем в информационном поиске.
1	В O
2	случае O
3	NLP I-TERM
4	потребность O
5	в O
6	« O
7	локализованных O
8	» O
9	учебных O
10	материалах O
11	еще O
12	заметнее O
13	, O
14	чем O
15	в O
16	информационном I-TERM
17	поиске I-TERM
18	. O

# sent_id = 133
# text =  Слушателям предлагается самостоятельно реализовать методы морфологического анализа, определения тональности текста, автоматического реферирования документов, извлечения именованных сущностей и машинного перевода.
1	Слушателям O
2	предлагается O
3	самостоятельно O
4	реализовать O
5	методы O
6	морфологического I-TERM
7	анализа I-TERM
8	, O
9	определения O
10	тональности I-TERM
11	текста I-TERM
12	, O
13	автоматического I-TERM
14	реферирования I-TERM
15	документов I-TERM
16	, O
17	извлечения I-TERM
18	именованных I-TERM
19	сущностей I-TERM
20	и O
21	машинного I-TERM
22	перевода I-TERM
23	. O

# sent_id = 134
# text =  Желательно, чтобы слушатели обладали базовыми знаниями линейной алгебры, теории вероятностей, математической статистики и машинного обучения, а также навыками программирования (необходимы для решения практических заданий).
1	Желательно O
2	, O
3	чтобы O
4	слушатели O
5	обладали O
6	базовыми O
7	знаниями O
8	линейной I-TERM
9	алгебры I-TERM
10	, O
11	теории I-TERM
12	вероятностей I-TERM
13	, O
14	математической I-TERM
15	статистики I-TERM
16	и O
17	машинного I-TERM
18	обучения I-TERM
19	, O
20	а O
21	также O
22	навыками O
23	программирования I-TERM
24	( O
25	необходимы O
26	для O
27	решения O
28	практических O
29	заданий O
30	) O
31	. O

# sent_id = 135
# text =  Прорывы #DeepPavlov в 2019 году: обзор и итоги года Московский физико-технический институт (МФТИ).
1	Прорывы O
2	# O
3	DeepPavlov I-TERM
4	в O
5	2019 O
6	году O
7	: O
8	обзор O
9	и O
10	итоги O
11	года O
12	Московский I-TERM
13	физико I-TERM
14	- I-TERM
15	технический I-TERM
16	институт I-TERM
17	( O
18	МФТИ I-TERM
19	) O
20	. O

# sent_id = 136
# text =  Библиотеке #DeepPavlov, на минуточку, уже два года, и мы рады, что наше сообщество с каждым днем растет.
1	Библиотеке O
2	# O
3	DeepPavlov I-TERM
4	, O
5	на O
6	минуточку O
7	, O
8	уже O
9	два O
10	года O
11	, O
12	и O
13	мы O
14	рады O
15	, O
16	что O
17	наше O
18	сообщество O
19	с O
20	каждым O
21	днем O
22	растет O
23	. O

# sent_id = 137
# text =  Увеличилось количество коммерческих решений за счет state-of-art технологий, реализованных в DeepPavlov, в разных отраслях от ритейла до промышленности.
1	Увеличилось O
2	количество O
3	коммерческих O
4	решений O
5	за O
6	счет O
7	state I-TERM
8	- I-TERM
9	of I-TERM
10	- I-TERM
11	art I-TERM
12	технологий O
13	, O
14	реализованных O
15	в O
16	DeepPavlov I-TERM
17	, O
18	в O
19	разных O
20	отраслях O
21	от O
22	ритейла O
23	до O
24	промышленности O
25	. O

# sent_id = 138
# text =  Вышел первый релиз DeepPavlov Agent.
1	Вышел O
2	первый O
3	релиз O
4	DeepPavlov I-TERM
5	Agent I-TERM
6	. O

# sent_id = 139
# text =  DeepPavlov решает проблемы такие как: классификация текста, исправление опечаток, распознавание именованных сущностей, ответы на вопросы по базе знаний и многие другие.
1	DeepPavlov I-TERM
2	решает O
3	проблемы O
4	такие O
5	как O
6	: O
7	классификация I-TERM
8	текста I-TERM
9	, O
10	исправление I-TERM
11	опечаток I-TERM
12	, O
13	распознавание I-TERM
14	именованных I-TERM
15	сущностей I-TERM
16	, O
17	ответы I-TERM
18	на I-TERM
19	вопросы I-TERM
20	по O
21	базе O
22	знаний O
23	и O
24	многие O
25	другие O
26	. O

# sent_id = 140
# text =  Библиотека поддерживает платформы Linux и Windows.
1	Библиотека O
2	поддерживает O
3	платформы O
4	Linux I-TERM
5	и O
6	Windows I-TERM
7	. O

# sent_id = 141
# text =  В настоящее время современные результаты во многих задачах были достигнуты благодаря применению моделей на основе BERT.
1	В O
2	настоящее O
3	время O
4	современные O
5	результаты O
6	во O
7	многих O
8	задачах O
9	были O
10	достигнуты O
11	благодаря O
12	применению O
13	моделей O
14	на O
15	основе O
16	BERT I-TERM
17	. O

# sent_id = 142
# text =  Команда DeepPavlov интегрировала BERT в три последующие задачи: классификация текста, распознавание именованных сущностей и ответы на вопросы.
1	Команда O
2	DeepPavlov I-TERM
3	интегрировала O
4	BERT I-TERM
5	в O
6	три O
7	последующие O
8	задачи O
9	: O
10	классификация I-TERM
11	текста I-TERM
12	, O
13	распознавание I-TERM
14	именованных I-TERM
15	сущностей I-TERM
16	и O
17	ответы I-TERM
18	на I-TERM
19	вопросы I-TERM
20	. O

# sent_id = 143
# text =  Модель классификации текста на основе BERT DeepPavlov служит, например, для решения проблемы обнаружения оскорблений.
1	Модель O
2	классификации O
3	текста O
4	на O
5	основе O
6	BERT I-TERM
7	DeepPavlov I-TERM
8	служит O
9	, O
10	например O
11	, O
12	для O
13	решения O
14	проблемы O
15	обнаружения I-TERM
16	оскорблений I-TERM
17	. O

# sent_id = 144
# text =  В дополнение к моделям классификации текста DeepPavlov содержит модель на основе BERT для распознавания именованных сущностей (NER).
1	В O
2	дополнение O
3	к O
4	моделям O
5	классификации I-TERM
6	текста I-TERM
7	DeepPavlov I-TERM
8	содержит O
9	модель O
10	на O
11	основе O
12	BERT I-TERM
13	для O
14	распознавания I-TERM
15	именованных I-TERM
16	сущностей I-TERM
17	( O
18	NER I-TERM
19	) O
20	. O

# sent_id = 145
# text =  Например, модель может извлечь важную информацию из резюме, чтобы облегчить работу специалистов по кадрам.
1	Например O
2	, O
3	модель O
4	может O
5	извлечь I-TERM
6	важную I-TERM
7	информацию I-TERM
8	из O
9	резюме O
10	, O
11	чтобы O
12	облегчить O
13	работу O
14	специалистов O
15	по O
16	кадрам O
17	. O

# sent_id = 146
# text =  Кроме того, NER может использоваться для идентификации соответствующих объектов в запросах клиентов, таких как спецификации продуктов, названия компаний или данные о филиалах компании.
1	Кроме O
2	того O
3	, O
4	NER I-TERM
5	может O
6	использоваться O
7	для O
8	идентификации I-TERM
9	соответствующих I-TERM
10	объектов I-TERM
11	в O
12	запросах O
13	клиентов O
14	, O
15	таких O
16	как O
17	спецификации O
18	продуктов O
19	, O
20	названия O
21	компаний O
22	или O
23	данные O
24	о O
25	филиалах O
26	компании O
27	. O

# sent_id = 147
# text =  Команда DeepPavlov обучила модель NER на англоязычном корпусе OntoNotes, который имеет 19 типов разметки, включая PER (человек), LOC (местоположение), ORG (организация) и многие другие.
1	Команда O
2	DeepPavlov I-TERM
3	обучила O
4	модель I-TERM
5	NER I-TERM
6	на O
7	англоязычном O
8	корпусе O
9	OntoNotes I-TERM
10	, O
11	который O
12	имеет O
13	19 O
14	типов O
15	разметки I-TERM
16	, O
17	включая O
18	PER I-TERM
19	( O
20	человек O
21	) O
22	, O
23	LOC I-TERM
24	( O
25	местоположение O
26	) O
27	, O
28	ORG I-TERM
29	( O
30	организация O
31	) O
32	и O
33	многие O
34	другие O
35	. O

# sent_id = 148
# text =  Одним из основных переломных моментов в этой области стал выпуск Стэнфордского набора данных для ответов на вопросы (SQuAD).
1	Одним O
2	из O
3	основных O
4	переломных O
5	моментов O
6	в O
7	этой O
8	области O
9	стал O
10	выпуск O
11	Стэнфордского I-TERM
12	набора I-TERM
13	данных I-TERM
14	для I-TERM
15	ответов I-TERM
16	на I-TERM
17	вопросы I-TERM
18	( O
19	SQuAD I-TERM
20	) O
21	. O

# sent_id = 149
# text =  Набор данных SQuAD привел к появлению бесчисленных подходов к решению задачи вопросно-ответных систем.
1	Набор O
2	данных O
3	SQuAD I-TERM
4	привел O
5	к O
6	появлению O
7	бесчисленных O
8	подходов O
9	к O
10	решению O
11	задачи O
12	вопросно I-TERM
13	- I-TERM
14	ответных I-TERM
15	систем I-TERM
16	. O

# sent_id = 150
# text =  Одной из наиболее успешных является модель DeepPavlov BERT.
1	Одной O
2	из O
3	наиболее O
4	успешных O
5	является O
6	модель O
7	DeepPavlov I-TERM
8	BERT I-TERM
9	. O

# sent_id = 151
# text =  Чтобы использовать модель QA на основе BERT с DeepPavlov, необходимо следующее.
1	Чтобы O
2	использовать O
3	модель O
4	QA I-TERM
5	на O
6	основе O
7	BERT I-TERM
8	с O
9	DeepPavlov I-TERM
10	, O
11	необходимо O
12	следующее O
13	. O

# sent_id = 152
# text =  DeepPavlov Agent — платформа для создания многозадачных чат-ботов.
1	DeepPavlov I-TERM
2	Agent I-TERM
3	— O
4	платформа O
5	для O
6	создания O
7	многозадачных O
8	чат O
9	- O
10	ботов O
11	. O

# sent_id = 153
# text =  При разработке разговорных агентов в основном применяется модульная архитектура для целенаправленного диалога, при котором разворачивается сценарий.
1	При O
2	разработке O
3	разговорных I-TERM
4	агентов I-TERM
5	в O
6	основном O
7	применяется O
8	модульная I-TERM
9	архитектура I-TERM
10	для O
11	целенаправленного I-TERM
12	диалога I-TERM
13	, O
14	при O
15	котором O
16	разворачивается O
17	сценарий O
18	. O

# sent_id = 154
# text =  Для решения этой задачи в октябре 2019 года вышел первый релиз DeepPavlov Agent 1.0 — платформы для создания многозадачных чат-ботов.
1	Для O
2	решения O
3	этой O
4	задачи O
5	в O
6	октябре O
7	2019 I-TERM
8	года I-TERM
9	вышел O
10	первый O
11	релиз O
12	DeepPavlov I-TERM
13	Agent I-TERM
14	1.0 I-TERM
15	— O
16	платформы O
17	для O
18	создания O
19	многозадачных O
20	чат O
21	- O
22	ботов O
23	. O

# sent_id = 155
# text =  Агент помогает разработчикам производственных чатботов организовать несколько NLP моделей в одном конвейере.
1	Агент O
2	помогает O
3	разработчикам O
4	производственных O
5	чатботов O
6	организовать I-TERM
7	несколько I-TERM
8	NLP I-TERM
9	моделей I-TERM
10	в O
11	одном O
12	конвейере O
13	. O

# sent_id = 156
# text =  Чтобы упростить работу с предобученными NLP моделями из DeepPavlov, в сентябрь 2019 года был запущен SaaS сервис.
1	Чтобы O
2	упростить O
3	работу O
4	с O
5	предобученными O
6	NLP O
7	моделями O
8	из O
9	DeepPavlov I-TERM
10	, O
11	в O
12	сентябрь O
13	2019 I-TERM
14	года I-TERM
15	был O
16	запущен O
17	SaaS I-TERM
18	сервис I-TERM
19	. O

# sent_id = 157
# text =  DeepPavlov Cloud позволяет анализировать текст, а также хранить документы в облачном хранилище.
1	DeepPavlov I-TERM
2	Cloud I-TERM
3	позволяет O
4	анализировать I-TERM
5	текст I-TERM
6	, O
7	а O
8	также O
9	хранить I-TERM
10	документы I-TERM
11	в O
12	облачном O
13	хранилище O
14	. O

# sent_id = 158
# text =  Оценка состояния диалога (DST — Dialogue State Traking) является основным компонентом в таких диалоговых системах.
1	Оценка I-TERM
2	состояния I-TERM
3	диалога I-TERM
4	( O
5	DST I-TERM
6	— O
7	Dialogue I-TERM
8	State I-TERM
9	Traking I-TERM
10	) O
11	является O
12	основным O
13	компонентом O
14	в O
15	таких O
16	диалоговых O
17	системах O
18	. O

# sent_id = 159
# text =  DST отвечает за перевод высказываний на человеческом языке в семантическое представление языка, в частности, за извлечение намерений (intets) и пар слот-значение (slot, value), соответствующих цели пользователя.
1	DST I-TERM
2	отвечает O
3	за O
4	перевод I-TERM
5	высказываний I-TERM
6	на O
7	человеческом O
8	языке O
9	в O
10	семантическое O
11	представление O
12	языка O
13	, O
14	в O
15	частности O
16	, O
17	за O
18	извлечение O
19	намерений O
20	( O
21	intets O
22	) O
23	и O
24	пар O
25	слот O
26	- O
27	значение O
28	( O
29	slot O
30	, O
31	value O
32	) O
33	, O
34	соответствующих O
35	цели O
36	пользователя O
37	. O

# sent_id = 160
# text =  В ходе участия команды в DSTC8 была разработана модель GOLOMB (GOaL-Oriented Multi-task BERT-based dialogue state tracker) — целеориентированная мультизадачная модель на базе BERT для отслеживания состояния диалога.
1	В O
2	ходе O
3	участия O
4	команды O
5	в O
6	DSTC8 O
7	была O
8	разработана O
9	модель O
10	GOLOMB I-TERM
11	( O
12	GOaL I-TERM
13	- I-TERM
14	Oriented I-TERM
15	Multi I-TERM
16	- I-TERM
17	task I-TERM
18	BERT I-TERM
19	- I-TERM
20	based I-TERM
21	dialogue I-TERM
22	state I-TERM
23	tracker I-TERM
24	) O
25	— O
26	целеориентированная O
27	мультизадачная O
28	модель O
29	на O
30	базе O
31	BERT I-TERM
32	для O
33	отслеживания I-TERM
34	состояния I-TERM
35	диалога I-TERM
36	. O

# sent_id = 161
# text =  Для предсказания состояния диалога модель решает несколько классификационных задач и задачу поиска подстроки.
1	Для O
2	предсказания O
3	состояния O
4	диалога O
5	модель O
6	решает O
7	несколько O
8	классификационных I-TERM
9	задач I-TERM
10	и O
11	задачу I-TERM
12	поиска I-TERM
13	подстроки I-TERM
14	. O

# sent_id = 162
# text =  В скором времени данная модель появится библиотеке DeepPavlov.
1	В O
2	скором O
3	времени O
4	данная O
5	модель O
6	появится O
7	библиотеке O
8	DeepPavlov I-TERM
9	. O

# sent_id = 163
# text =  Как было сказано ранее, DeepPavlov поставляется с несколькими предобученными компонентами, работающими на TensorFlow и Keras.
1	Как O
2	было O
3	сказано O
4	ранее O
5	, O
6	DeepPavlov I-TERM
7	поставляется O
8	с O
9	несколькими O
10	предобученными O
11	компонентами O
12	, O
13	работающими O
14	на O
15	TensorFlow I-TERM
16	и O
17	Keras I-TERM
18	. O

# sent_id = 164
# text =  На основании триггера на определенные ключевые слова она сможет определять, к примеру, признаки обмана, мошенничества.
1	На O
2	основании O
3	триггера O
4	на O
5	определенные O
6	ключевые I-TERM
7	слова I-TERM
8	она O
9	сможет O
10	определять O
11	, O
12	к O
13	примеру O
14	, O
15	признаки O
16	обмана O
17	, O
18	мошенничества O
19	. O

# sent_id = 165
# text =  То есть, сформировав некоторый корпус слов-триггеров, вполне возможно классифицировать сайты по их текстовому содержанию.
1	То O
2	есть O
3	, O
4	сформировав O
5	некоторый O
6	корпус I-TERM
7	слов I-TERM
8	- I-TERM
9	триггеров I-TERM
10	, O
11	вполне O
12	возможно O
13	классифицировать O
14	сайты O
15	по O
16	их O
17	текстовому O
18	содержанию O
19	. O

# sent_id = 166
# text =  Задача распознавания текста относится к сфере обработки естественного языка или NLP (natural language processing).
1	Задача I-TERM
2	распознавания I-TERM
3	текста I-TERM
4	относится O
5	к O
6	сфере O
7	обработки I-TERM
8	естественного I-TERM
9	языка I-TERM
10	или O
11	NLP I-TERM
12	( O
13	natural I-TERM
14	language I-TERM
15	processing I-TERM
16	) O
17	. O

# sent_id = 167
# text =  NLP — направление искусственного интеллекта, нацеленное на обработку и анализ данных на естественном языке и обучение машин взаимодействию с людьми [1].
1	NLP I-TERM
2	— O
3	направление O
4	искусственного I-TERM
5	интеллекта I-TERM
6	, O
7	нацеленное O
8	на O
9	обработку O
10	и O
11	анализ I-TERM
12	данных I-TERM
13	на O
14	естественном O
15	языке O
16	и O
17	обучение O
18	машин O
19	взаимодействию O
20	с O
21	людьми O
22	[ O
23	1 O
24	] O
25	. O

# sent_id = 168
# text =  Такой подход называется методом вложения слов (word embedding).
1	Такой O
2	подход O
3	называется O
4	методом I-TERM
5	вложения I-TERM
6	слов I-TERM
7	( O
8	word I-TERM
9	embedding I-TERM
10	) O
11	. O

# sent_id = 169
# text =  Используя данные, состоящие из таких векторов, мы можем применять различные методы Machine Learning.
1	Используя O
2	данные O
3	, O
4	состоящие O
5	из O
6	таких O
7	векторов O
8	, O
9	мы O
10	можем O
11	применять O
12	различные O
13	методы O
14	Machine I-TERM
15	Learning I-TERM
16	. O

# sent_id = 170
# text =  И поскольку искусственные нейронные сети лучшим образом справляются с векторно-матричными вычислениями, то выбор в их пользу становиться очевидным.
1	И O
2	поскольку O
3	искусственные I-TERM
4	нейронные I-TERM
5	сети I-TERM
6	лучшим O
7	образом O
8	справляются O
9	с O
10	векторно I-TERM
11	- I-TERM
12	матричными I-TERM
13	вычислениями I-TERM
14	, O
15	то O
16	выбор O
17	в O
18	их O
19	пользу O
20	становиться O
21	очевидным O
22	. O

# sent_id = 171
# text =  Искусственная нейронная сеть — это математическая модель, а также ее программное или аппаратное воплощение, построенные по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.
1	Искусственная I-TERM
2	нейронная I-TERM
3	сеть I-TERM
4	— O
5	это O
6	математическая I-TERM
7	модель I-TERM
8	, O
9	а O
10	также O
11	ее O
12	программное O
13	или O
14	аппаратное O
15	воплощение O
16	, O
17	построенные O
18	по O
19	принципу O
20	организации O
21	и O
22	функционирования O
23	биологических O
24	нейронных O
25	сетей O
26	— O
27	сетей O
28	нервных O
29	клеток O
30	живого O
31	организма O
32	. O

# sent_id = 172
# text =  Современные модели представляют собой так называемые глубокие модели.
1	Современные O
2	модели O
3	представляют O
4	собой O
5	так O
6	называемые O
7	глубокие I-TERM
8	модели I-TERM
9	. O

# sent_id = 173
# text =  И в ее решении наилучшие метрики точности были достигнуты рекуррентными нейронными сетями, LSTM (сети с долгой краткосрочной памятью).
1	И O
2	в O
3	ее O
4	решении O
5	наилучшие O
6	метрики O
7	точности I-TERM
8	были O
9	достигнуты O
10	рекуррентными I-TERM
11	нейронными I-TERM
12	сетями I-TERM
13	, O
14	LSTM I-TERM
15	( O
16	сети I-TERM
17	с I-TERM
18	долгой I-TERM
19	краткосрочной I-TERM
20	памятью I-TERM
21	) O
22	. O

# sent_id = 174
# text =  Позже свое превосходство в этой нише обрели NLP-модели-трансформеры.
1	Позже O
2	свое O
3	превосходство O
4	в O
5	этой O
6	нише O
7	обрели O
8	NLP I-TERM
9	- I-TERM
10	модели I-TERM
11	- I-TERM
12	трансформеры I-TERM
13	. O

# sent_id = 175
# text = Описание упомянутых рекуррентных нейросетей (RNN), LSTM и GRU выходит за рамки темы статьи.
1	Описание O
2	упомянутых O
3	рекуррентных I-TERM
4	нейросетей I-TERM
5	( O
6	RNN I-TERM
7	) O
8	, O
9	LSTM I-TERM
10	и O
11	GRU I-TERM
12	выходит O
13	за O
14	рамки O
15	темы O
16	статьи O
17	. O

# sent_id = 176
# text =  Однако RNN способны фиксировать зависимости только в одном направлении языка.
1	Однако O
2	RNN I-TERM
3	способны O
4	фиксировать I-TERM
5	зависимости I-TERM
6	только O
7	в O
8	одном O
9	направлении O
10	языка O
11	. O

# sent_id = 177
# text =  Кроме этого, RNN не очень хороши в захвате долгосрочных зависимостей.
1	Кроме O
2	этого O
3	, O
4	RNN I-TERM
5	не O
6	очень O
7	хороши O
8	в O
9	захвате I-TERM
10	долгосрочных I-TERM
11	зависимостей I-TERM
12	. O

# sent_id = 178
# text = LSTM избегают проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.
1	LSTM I-TERM
2	избегают O
3	проблемы O
4	долговременной O
5	зависимости O
6	, O
7	запоминая O
8	значения O
9	как O
10	на O
11	короткие O
12	, O
13	так O
14	и O
15	на O
16	длинные O
17	промежутки O
18	времени O
19	. O

# sent_id = 179
# text =  Это объясняется тем, что LSTM не использует функцию активации внутри своих рекуррентных компонентов.
1	Это O
2	объясняется O
3	тем O
4	, O
5	что O
6	LSTM I-TERM
7	не O
8	использует O
9	функцию I-TERM
10	активации I-TERM
11	внутри O
12	своих O
13	рекуррентных O
14	компонентов O
15	. O

# sent_id = 180
# text =  LSTM часто используются в машинном переводе и в задачах генерирования текстов на естественном языке.
1	LSTM I-TERM
2	часто O
3	используются O
4	в O
5	машинном I-TERM
6	переводе I-TERM
7	и O
8	в O
9	задачах O
10	генерирования I-TERM
11	текстов I-TERM
12	на O
13	естественном O
14	языке O
15	. O

# sent_id = 181
# text =  Прежде чем использовать такой мощный и в то же время сложный инструмент, наша команда протестировала и более простые NLP-методы машинного обучения, в том числе «наивный байесовский классификатор», алгоритмы, использующие bag-of-words («мешок слов» — метод представления слов) и tf-idf (метрика определения частоты вхождения слов), а также простейшие модели нейронных сетей, состоящие из небольшого количества скрытых слоев.
1	Прежде O
2	чем O
3	использовать O
4	такой O
5	мощный O
6	и O
7	в O
8	то O
9	же O
10	время O
11	сложный O
12	инструмент O
13	, O
14	наша O
15	команда O
16	протестировала O
17	и O
18	более O
19	простые O
20	NLP O
21	- O
22	методы O
23	машинного O
24	обучения O
25	, O
26	в O
27	том O
28	числе O
29	« O
30	наивный I-TERM
31	байесовский I-TERM
32	классификатор I-TERM
33	» O
34	, O
35	алгоритмы O
36	, O
37	использующие O
38	bag I-TERM
39	- I-TERM
40	of I-TERM
41	- I-TERM
42	words I-TERM
43	( O
44	« O
45	мешок I-TERM
46	слов I-TERM
47	» O
48	— O
49	метод I-TERM
50	представления I-TERM
51	слов O
52	) O
53	и O
54	tf I-TERM
55	- I-TERM
56	idf I-TERM
57	( O
58	метрика I-TERM
59	определения I-TERM
60	частоты I-TERM
61	вхождения I-TERM
62	слов O
63	) O
64	, O
65	а O
66	также O
67	простейшие O
68	модели O
69	нейронных O
70	сетей O
71	, O
72	состоящие O
73	из O
74	небольшого O
75	количества O
76	скрытых O
77	слоев O
78	. O

# sent_id = 182
# text =  BERT, или Bidirectional Encoder Representations from Transformers, — нейросетевая модель-трансформер от Google, на которой сегодня строится большинство инструментов автоматической обработки языка.
1	BERT I-TERM
2	, O
3	или O
4	Bidirectional I-TERM
5	Encoder I-TERM
6	Representations I-TERM
7	from I-TERM
8	Transformers I-TERM
9	, O
10	— O
11	нейросетевая O
12	модель O
13	- O
14	трансформер O
15	от O
16	Google I-TERM
17	, O
18	на O
19	которой O
20	сегодня O
21	строится O
22	большинство O
23	инструментов O
24	автоматической O
25	обработки O
26	языка O
27	. O

# sent_id = 183
# text = Релиз BERT в 2018 году стал некоторой переломной точкой в развитии NLP-моделей.
1	Релиз O
2	BERT I-TERM
3	в O
4	2018 I-TERM
5	году I-TERM
6	стал O
7	некоторой O
8	переломной O
9	точкой O
10	в O
11	развитии O
12	NLP I-TERM
13	- I-TERM
14	моделей I-TERM
15	. O

# sent_id = 184
# text =  Его появлению предшествовал ряд недавних разработок в области обработки естественного языка (BERT, ELMO и Ко в картинках — как в NLP пришло трансферное обучение): Semi-supervised Sequence learning (Andrew Dai и Quoc Le), ELMo (Matthew Peters и исследователи из AI2 и UW CSE), ULMFiT (Jeremy Howard и Sebastian Ruder), OpenAI Transformer (исследователи OpenAI Radford, Narasimhan, Salimans, и Sutskever) и Трансформер (Vaswani et al).
1	Его O
2	появлению O
3	предшествовал O
4	ряд O
5	недавних O
6	разработок O
7	в O
8	области O
9	обработки O
10	естественного O
11	языка O
12	( O
13	BERT I-TERM
14	, O
15	ELMO I-TERM
16	и O
17	Ко I-TERM
18	в O
19	картинках O
20	— O
21	как O
22	в O
23	NLP I-TERM
24	пришло O
25	трансферное I-TERM
26	обучение I-TERM
27	): O
28	Semi I-TERM
29	- I-TERM
30	supervised I-TERM
31	Sequence I-TERM
32	learning I-TERM
33	( O
34	Andrew I-TERM
35	Dai I-TERM
36	и O
37	Quoc I-TERM
38	Le I-TERM
39	) O
40	, O
41	ELMo I-TERM
42	( O
43	Matthew I-TERM
44	Peters I-TERM
45	и O
46	исследователи O
47	из O
48	AI2 I-TERM
49	и O
50	UW I-TERM
51	CSE I-TERM
52	) O
53	, O
54	ULMFiT I-TERM
55	( O
56	Jeremy I-TERM
57	Howard I-TERM
58	и O
59	Sebastian I-TERM
60	Ruder I-TERM
61	) O
62	, O
63	OpenAI I-TERM
64	Transformer I-TERM
65	( O
66	исследователи O
67	OpenAI I-TERM
68	Radford I-TERM
69	, O
70	Narasimhan I-TERM
71	, O
72	Salimans I-TERM
73	, O
74	и O
75	Sutskever I-TERM
76	) O
77	и O
78	Трансформер I-TERM
79	( O
80	Vaswani I-TERM
81	et O
82	al O
83	)O 
84	. O

# sent_id = 185
# text = Трансформеры в машинном обучении — это семейство архитектур нейронных сетей, общая идея которых основана на так называемом «самовнимании» (self-attention).
1	Трансформеры I-TERM
2	в O
3	машинном I-TERM
4	обучении I-TERM
5	— O
6	это O
7	семейство O
8	архитектур O
9	нейронных O
10	сетей O
11	, O
12	общая O
13	идея O
14	которых O
15	основана O
16	на O
17	так O
18	называемом O
19	« O
20	самовнимании I-TERM
21	» O
22	( O
23	self I-TERM
24	- I-TERM
25	attention I-TERM
26	) O
27	. O

# sent_id = 186
# text =  Однако алгоритм Self-attention не сразу поймет смысл предложения.
1	Однако O
2	алгоритм O
3	Self I-TERM
4	- I-TERM
5	attention I-TERM
6	не O
7	сразу O
8	поймет O
9	смысл O
10	предложения O
11	. O

# sent_id = 187
# text =  Потом результаты сетей объединяется.По своей сути BERT — это обученный стек энкодеров Трансформера.
1	Потом O
2	результаты O
3	сетей O
4	объединяется O
5	. O
6	По O
7	своей O
8	сути O
9	BERT I-TERM
10	— O
11	это O
12	обученный O
13	стек O
14	энкодеров I-TERM
15	Трансформера I-TERM
16	. O

# sent_id = 188
# text =  Разработкой и обучением модели BERT занималась целая группа исследователей Google AI Language на многомиллионном наборе слов на разных языках (более 100).
1	Разработкой O
2	и O
3	обучением O
4	модели O
5	BERT I-TERM
6	занималась O
7	целая O
8	группа O
9	исследователей O
10	Google I-TERM
11	AI I-TERM
12	Language I-TERM
13	на O
14	многомиллионном O
15	наборе O
16	слов O
17	на O
18	разных O
19	языках O
20	( O
21	более O
22	100 O
23	) O
24	. O

# sent_id = 189
# text =  И мы дообучили BERT распознавать текстовое содержимое сайтов по 63 категориям (медицина, здоровье, видео, интернет-магазины, юмористические сайты, эротика, оружие, секты, криминал и пр.).
1	И O
2	мы O
3	дообучили O
4	BERT I-TERM
5	распознавать O
6	текстовое O
7	содержимое O
8	сайтов O
9	по O
10	63 O
11	категориям O
12	( O
13	медицина I-TERM
14	, O
15	здоровье O
16	, O
17	видео O
18	, O
19	интернет O
20	- O
21	магазины O
22	, O
23	юмористические O
24	сайты O
25	, O
26	эротика O
27	, O
28	оружие O
29	, O
30	секты O
31	, O
32	криминал O
33	и пр. O
34	) O
35	. O

# sent_id = 190
# text =  Smart-Cat на первом этапе проводит их предобработку.
1	Smart I-TERM
2	- I-TERM
3	Cat I-TERM
4	на O
5	первом O
6	этапе O
7	проводит O
8	их O
9	предобработку O
10	. O

# sent_id = 191
# text =  Для удобства работы с категоризатором Smart-Cat мы создали специальный Telegram-бот.
1	Для O
2	удобства O
3	работы O
4	с O
5	категоризатором O
6	Smart I-TERM
7	- I-TERM
8	Cat I-TERM
9	мы O
10	создали O
11	специальный O
12	Telegram I-TERM
13	- I-TERM
14	бот I-TERM
15	. O

# sent_id = 192
# text =  После своей работы BERT-bot отправит CSV-таблицу.
1	После O
2	своей O
3	работы O
4	BERT I-TERM
5	- I-TERM
6	bot I-TERM
7	отправит O
8	CSV O
9	- O
10	таблицу O
11	. O

# sent_id = 193
# text =  Но, как я уже сказал ранее, такие модели могут применяться не только в задачах классификации текста.
1	Но O
2	, O
3	как O
4	я O
5	уже O
6	сказал O
7	ранее O
8	, O
9	такие O
10	модели O
11	могут O
12	применяться O
13	не O
14	только O
15	в O
16	задачах O
17	классификации I-TERM
18	текста I-TERM
19	. O

# sent_id = 194
# text =  С моделью от OpenAI связано сразу несколько новостей — хорошая и не очень.
1	С O
2	моделью O
3	от O
4	OpenAI I-TERM
5	связано O
6	сразу O
7	несколько O
8	новостей O
9	— O
10	хорошая O
11	и O
12	не O
13	очень O
14	. O

# sent_id = 195
# text =  Сделка OpenAI и Microsoft. Начать придется с менее приятной — компания Майкрософт завладела эксклюзивными правами на GPT-3.
1	Сделка O
2	OpenAI I-TERM
3	и O
4	Microsoft I-TERM
5	. O
6	Начать O
7	придется O
8	с O
9	менее O
10	приятной O
11	— O
12	компания O
13	Майкрософт I-TERM
14	завладела O
15	эксклюзивными O
16	правами O
17	на O
18	GPT-3 I-TERM
19	. O

# sent_id = 196
# text =  Сделка предсказуемо вызвала негодование — Элон Маск, основатель OpenAI, а ныне бывший член совета директоров компании, заявил, что Майкрософт по сути захватили OpenAI.
1	Сделка O
2	предсказуемо O
3	вызвала O
4	негодование O
5	— O
6	Элон I-TERM
7	Маск I-TERM
8	, O
9	основатель O
10	OpenAI I-TERM
11	, O
12	а O
13	ныне O
14	бывший O
15	член O
16	совета O
17	директоров O
18	компании O
19	, O
20	заявил O
21	, O
22	что O
23	Майкрософт I-TERM
24	по O
25	сути O
26	захватили O
27	OpenAI I-TERM
28	. O

# sent_id = 197
# text =  Дело в том, что OpenAI изначально создавалась как некоммерческая организация с высокой миссией — не позволить искусственному интеллекту оказаться в руках отдельного государства или корпорации.
1	Дело O
2	в O
3	том O
4	, O
5	что O
6	OpenAI I-TERM
7	изначально O
8	создавалась O
9	как O
10	некоммерческая O
11	организация O
12	с O
13	высокой O
14	миссией O
15	— O
16	не O
17	позволить O
18	искусственному I-TERM
19	интеллекту I-TERM
20	оказаться O
21	в O
22	руках O
23	отдельного O
24	государства O
25	или O
26	корпорации O
27	. O

# sent_id = 198
# text =  ruGPT3 от Сбера. Теперь к более приятной новости — исследователи из Сбера выложили в открытый доступ модель, которая повторяет архитектуру GPT-3 и основана на коде GPT-2 и, самое главное, обучена на русскоязычном корпусе.
1	ruGPT3 I-TERM
2	от O
3	Сбера I-TERM
4	. O
5	Теперь O
6	к O
7	более O
8	приятной O
9	новости O
10	— O
11	исследователи O
12	из O
13	Сбера I-TERM
14	выложили O
15	в O
16	открытый O
17	доступ O
18	модель O
19	, O
20	которая O
21	повторяет O
22	архитектуру O
23	GPT-3 I-TERM
24	и O
25	основана O
26	на O
27	коде O
28	GPT-2 I-TERM
29	и O
30	, O
31	самое O
32	главное O
33	, O
34	обучена O
35	на O
36	русскоязычном O
37	корпусе I-TERM
38	. O

# sent_id = 199
# text =  Если коммерческие организации можно оправдать тем, что код часто вплетен в инфраструктуру проектов, то что говорить про исследовательские институты и некоммерческие компании вроде DeepMind и OpenAI?
1	Если O
2	коммерческие O
3	организации O
4	можно O
5	оправдать O
6	тем O
7	, O
8	что O
9	код O
10	часто O
11	вплетен O
12	в O
13	инфраструктуру O
14	проектов O
15	, O
16	то O
17	что O
18	говорить O
19	про O
20	исследовательские O
21	институты O
22	и O
23	некоммерческие O
24	компании O
25	вроде O
26	DeepMind I-TERM
27	и O
28	OpenAI I-TERM
29	? O

# sent_id = 200
# text =  Платформа для видеозвонков Maxine объединяет в себе целый зоопарк ML-алгоритмов.
1	Платформа O
2	для O
3	видеозвонков O
4	Maxine I-TERM
5	объединяет O
6	в O
7	себе O
8	целый O
9	зоопарк O
10	ML O
11	- O
12	алгоритмов O
13	. O

# sent_id = 201
# text =  Google Meet поделились кейсом создания своего алгоритма для качественного удаления фона на основе фреймворка от Mediapipe (который умеет отслеживание движение глаз, головы и рук).
1	Google I-TERM
2	Meet I-TERM
3	поделились O
4	кейсом O
5	создания O
6	своего O
7	алгоритма O
8	для O
9	качественного O
10	удаления O
11	фона O
12	на O
13	основе O
14	фреймворка O
15	от O
16	Mediapipe I-TERM
17	( O
18	который O
19	умеет O
20	отслеживание O
21	движение O
22	глаз O
23	, O
24	головы O
25	и O
26	рук O
27	) O
28	. O

# sent_id = 202
# text =  Google также запустил новую функцию для сервиса YouTube Stories на iOS, который позволяет улучшать качество речи.
1	Google I-TERM
2	также O
3	запустил O
4	новую O
5	функцию O
6	для O
7	сервиса O
8	YouTube I-TERM
9	Stories I-TERM
10	на O
11	iOS I-TERM
12	, O
13	который O
14	позволяет O
15	улучшать O
16	качество O
17	речи O
18	. O

# sent_id = 203
# text =  В современном мире всё большую популярность приобретает методика под названием customer development для тестирования идей и гипотез о будущем продукте.
1	В O
2	современном O
3	мире O
4	всё O
5	большую O
6	популярность O
7	приобретает O
8	методика O
9	под O
10	названием O
11	customer I-TERM
12	development I-TERM
13	для O
14	тестирования O
15	идей O
16	и O
17	гипотез O
18	о O
19	будущем O
20	продукте O
21	. O

# sent_id = 204
# text =  В данном решении была использована готовая нейросеть от сервиса RusVectores, обученная на корпусе НКРЯ с использованием алгоритма word2vec CBOW с длиной вектора 300.
1	В O
2	данном O
3	решении O
4	была O
5	использована O
6	готовая O
7	нейросеть O
8	от O
9	сервиса O
10	RusVectores I-TERM
11	, O
12	обученная O
13	на O
14	корпусе O
15	НКРЯ I-TERM
16	с O
17	использованием O
18	алгоритма O
19	word2vec I-TERM
20	CBOW I-TERM
21	с O
22	длиной O
23	вектора O
24	300.

# sent_id = 205
# text = НКРЯ – это совокупность русскоязычных текстов, Национальный Корпус Русского Языка в полном объёме.
1	НКРЯ I-TERM
2	– O
3	это O
4	совокупность O
5	русскоязычных O
6	текстов O
7	, O
8	Национальный I-TERM
9	Корпус I-TERM
10	Русского I-TERM
11	Языка I-TERM
12	в O
13	полном O
14	объёме O
15	. O

# sent_id = 206
# text = Word2vec CBOW — алгоритм, благодаря которому слово на естественном языке представляется в виде числового вектора.
1	Word2vec I-TERM
2	CBOW I-TERM
3	— O
4	алгоритм O
5	, O
6	благодаря O
7	которому O
8	слово O
9	на O
10	естественном O
11	языке O
12	представляется O
13	в O
14	виде O
15	числового O
16	вектора O
17	. O

# sent_id = 207
# text =  CBOW – это аббревиатура Continuous Bag of Words.
1	CBOW I-TERM
2	– O
3	это O
4	аббревиатура O
5	Continuous I-TERM
6	Bag I-TERM
7	of I-TERM
8	Words I-TERM
9	. O

# sent_id = 208
# text =  Она обозначает алгоритм, который есть в word2vec.
1	Она O
2	обозначает O
3	алгоритм O
4	, O
5	который O
6	есть O
7	в O
8	word2vec I-TERM
9	. O

# sent_id = 209
# text =  Данный алгоритм называют моделью «мешка слов», он предсказывает слово по контексту.
1	Данный O
2	алгоритм O
3	называют O
4	моделью O
5	« O
6	мешка I-TERM
7	слов I-TERM
8	» O
9	, O
10	он O
11	предсказывает I-TERM
12	слово I-TERM
13	по I-TERM
14	контексту I-TERM
15	. O

# sent_id = 210
# text =  Ещё один алгоритм в word2vec - Skip-gram предсказывает контекст по слову.
1	Ещё O
2	один O
3	алгоритм O
4	в O
5	word2vec I-TERM
6	- O
7	Skip I-TERM
8	- I-TERM
9	gram I-TERM
10	предсказывает O
11	контекст O
12	по O
13	слову O
14	. O

# sent_id = 211
# text = С помощью данных алгоритмов генерируют близкие по смыслу слова при запросе в поисковой системе, сравнивают документы по смыслу, определяют смысловую близость слов и предложений.
1	С O
2	помощью O
3	данных O
4	алгоритмов O
5	генерируют O
6	близкие O
7	по O
8	смыслу O
9	слова O
10	при O
11	запросе O
12	в O
13	поисковой O
14	системе O
15	, O
16	сравнивают I-TERM
17	документы I-TERM
18	по I-TERM
19	смыслу I-TERM
20	, O
21	определяют I-TERM
22	смысловую I-TERM
23	близость I-TERM
24	слов I-TERM
25	и O
26	предложений O
27	. O

# sent_id = 212
# text = Более подробно о word2vec можно почитать в статье "Немного про word2vec: полезная теория".
1	Более O
2	подробно O
3	о O
4	word2vec I-TERM
5	можно O
6	почитать O
7	в O
8	статье O
9	" O
10	Немного O
11	про O
12	word2vec O
13	: O
14	полезная O
15	теория" O
16	. O

# sent_id = 213
# text = О векторном представлении слов (эмбеддинге) хорошо и с примерами описано в статье "Что такое эмбеддинги и как они помогают машинам понимать тексты".
1	О O
2	векторном I-TERM
3	представлении I-TERM
4	слов I-TERM
5	( O
6	эмбеддинге I-TERM
7	) O
8	хорошо O
9	и O
10	с O
11	примерами O
12	описано O
13	в O
14	статье O
15	" O
16	Что O
17	такое O
18	эмбеддинги O
19	и O
20	как O
21	они O
22	помогают O
23	машинам O
24	понимать O
25	тексты" O
26	. O

# sent_id = 214
# text =  Т.к. у меня таких мощностей нет, я воспользовался доступным онлайн сервисом RusVectores.
1	Т.к O
2	. O
3	у O
4	меня O
5	таких O
6	мощностей O
7	нет O
8	, O
9	я O
10	воспользовался O
11	доступным O
12	онлайн O
13	сервисом O
14	RusVectores I-TERM
15	. O

# sent_id = 215
# text =  Эти модели всегда ищут синонимы — даже для устоявшихся словосочетаний.
1	Эти O
2	модели O
3	всегда O
4	ищут I-TERM
5	синонимы I-TERM
6	— O
7	даже O
8	для O
9	устоявшихся O
10	словосочетаний I-TERM
11	. O

# sent_id = 216
# text =  Вчера OpenAI выпустили Whisper.
1	Вчера O
2	OpenAI I-TERM
3	выпустили O
4	Whisper I-TERM
5	. O

# sent_id = 217
# text =  По сути авторы попытались: Исключить транскрипты других систем ASR из датасета; Привести пунктуации к некому стандарту.
1	По O
2	сути O
3	авторы O
4	попытались O
5	: O
6	Исключить O
7	транскрипты O
8	других O
9	систем O
10	ASR I-TERM
11	из O
12	датасета; O
13	Привести O
14	пунктуации O
15	к O
16	некому O
17	стандарту O
18	. O

# sent_id = 218
# text =  Серьезной нормализации или денормализации текста не делалось.
1	Серьезной O
2	нормализации I-TERM
3	или O
4	денормализации I-TERM
5	текста O
6	не O
7	делалось O
8	. O

# sent_id = 219
# text =  Под капотом же seq2seq модель, глядишь сама всё и так выучит.
1	Под O
2	капотом O
3	же O
4	seq2seq I-TERM
5	модель O
6	, O
7	глядишь O
8	сама O
9	всё O
10	и O
11	так O
12	выучит O
13	. O

# sent_id = 220
# text =  Ведь исходя из названий FAIR, OpenAI и прочие же FOSS - альтруисты, борющиеся за наше будущее, они же выложили код для тренировки (а повторить смогут лишь GAFA компании) и все датасеты, не так ли?
1	Ведь O
2	исходя O
3	из O
4	названий O
5	FAIR I-TERM
6	, O
7	OpenAI I-TERM
8	и O
9	прочие O
10	же O
11	FOSS I-TERM
12	- O
13	альтруисты O
14	, O
15	борющиеся O
16	за O
17	наше O
18	будущее O
19	, O
20	они O
21	же O
22	выложили O
23	код O
24	для O
25	тренировки O
26	( O
27	а O
28	повторить O
29	смогут O
30	лишь O
31	GAFA I-TERM
32	компании O
33	) O
34	и O
35	все O
36	датасеты O
37	, O
38	не O
39	так O
40	ли O
41	? O

# sent_id = 221
# text =  На практике OpenAI уже давно не Open, а недавняя история с DALLE-2 / Midjourney / Stable Diffusion скорее иллюстрируют тренд.
1	На O
2	практике O
3	OpenAI I-TERM
4	уже O
5	давно O
6	не O
7	Open O
8	, O
9	а O
10	недавняя O
11	история O
12	с O
13	DALLE-2 I-TERM
14	/ O
15	Midjourney I-TERM
16	/ O
17	Stable I-TERM
18	Diffusion I-TERM
19	скорее O
20	иллюстрируют O
21	тренд O
22	. O

# sent_id = 222
# text =  Наверное стоит только сказать, что это sequence-to-sequence encoder-decoder трансформерная модель, без особого снижения длины инпута с довольном стандартным окном в 25 миллисекунд и шагом в 10 миллисекунд, работающая на аудио в 16 килогерц.
1	Наверное O
2	стоит O
3	только O
4	сказать O
5	, O
6	что O
7	это O
8	sequence I-TERM
9	- I-TERM
10	to I-TERM
11	- I-TERM
12	sequence I-TERM
13	encoder I-TERM
14	- I-TERM
15	decoder I-TERM
16	трансформерная O
17	модель O
18	, O
19	без O
20	особого O
21	снижения O
22	длины O
23	инпута O
24	с O
25	довольном O
26	стандартным O
27	окном O
28	в O
29	25 O
30	миллисекунд O
31	и O
32	шагом O
33	в O
34	10 O
35	миллисекунд O
36	, O
37	работающая O
38	на O
39	аудио O
40	в O
41	16 O
42	килогерц O
43	. O

# sent_id = 223
# text =  Решать конечно вам для вашего конкретного приложения, но если сравнивать только саму модель распознавания, а не весь обвес в виде сервиса (понятно, что тут VAD и детектор языка запихали тоже в модель), например с древними бенчмарками из silero-models, то самые маленькие модели на CPU в расчете на 1 ядро (1 ядро = 2 потока) отличаются по скорости … на два порядка.
1	Решать O
2	конечно O
3	вам O
4	для O
5	вашего O
6	конкретного O
7	приложения O
8	, O
9	но O
10	если O
11	сравнивать O
12	только O
13	саму O
14	модель O
15	распознавания O
16	, O
17	а O
18	не O
19	весь O
20	обвес O
21	в O
22	виде O
23	сервиса O
24	( O
25	понятно O
26	, O
27	что O
28	тут O
29	VAD I-TERM
30	и O
31	детектор O
32	языка O
33	запихали O
34	тоже O
35	в O
36	модель O
37	) O
38	, O
39	например O
40	с O
41	древними O
42	бенчмарками O
43	из O
44	silero I-TERM
45	- I-TERM
46	models I-TERM
47	, O
48	то O
49	самые O
50	маленькие O
51	модели O
52	на O
53	CPU O
54	в O
55	расчете O
56	на O
57	1 O
58	ядро O
59	( O
60	1 O
61	ядро O
62	= O
63	2 O
64	потока O
65	) O
66	отличаются O
67	по O
68	скорости O
69	… O
70	на O
71	два O
72	порядка O
73	. O

# sent_id = 224
# text =  Для наших моделей из прошлого релиза, многие из этих датасетов тоже как бы "zero-shot" (то есть у нас нет соответствующего большого тренировочного датасета).
1	Для O
2	наших O
3	моделей O
4	из O
5	прошлого O
6	релиза O
7	, O
8	многие O
9	из O
10	этих O
11	датасетов O
12	тоже O
13	как O
14	бы O
15	" O
16	zero I-TERM
17	- I-TERM
18	shot I-TERM
19	" O
20	( O
21	то O
22	есть O
23	у O
24	нас O
25	нет O
26	соответствующего O
27	большого O
28	тренировочного O
29	датасета O
30	) O
31	. O

# sent_id = 225
# text =  В течение четырех лет вышло несколько версий модели, способных транскрибировать лекции, телефонные разговоры, телевизионные программы, радиошоу и другие прямые трансляции с «человеческой точностью».
1	В O
2	течение O
3	четырех O
4	лет O
5	вышло O
6	несколько O
7	версий O
8	модели O
9	, O
10	способных O
11	транскрибировать I-TERM
12	лекции I-TERM
13	, O
14	телефонные O
15	разговоры O
16	, O
17	телевизионные O
18	программы O
19	, O
20	радиошоу O
21	и O
22	другие O
23	прямые O
24	трансляции O
25	с O
26	« O
27	человеческой O
28	точностью O
29	» O
30	. O

# sent_id = 226
# text =  Модель DeepSpeech представляет собой сквозную обучаемую архитектуру на уровне символов, которая может транскрибировать аудио на различных языках.
1	Модель O
2	DeepSpeech I-TERM
3	представляет O
4	собой O
5	сквозную O
6	обучаемую O
7	архитектуру O
8	на O
9	уровне O
10	символов O
11	, O
12	которая O
13	может O
14	транскрибировать I-TERM
15	аудио I-TERM
16	на O
17	различных O
18	языках O
19	. O

# sent_id = 227
# text =  Вдохновленная этими усилиями по сбору данных, исследовательская группа Mozilla в сотрудничестве с группой открытых инноваций запустила проект Common Voice, цель которого заключалась в сборе и проверке речевых данных.
1	Вдохновленная O
2	этими O
3	усилиями O
4	по O
5	сбору O
6	данных O
7	, O
8	исследовательская O
9	группа O
10	Mozilla I-TERM
11	в O
12	сотрудничестве O
13	с O
14	группой O
15	открытых O
16	инноваций O
17	запустила O
18	проект O
19	Common I-TERM
20	Voice I-TERM
21	, O
22	цель O
23	которого O
24	заключалась O
25	в O
26	сборе O
27	и O
28	проверке I-TERM
29	речевых I-TERM
30	данных I-TERM
31	. O

# sent_id = 228
# text =  Common Voice включает не только речевые записи, но и из добровольно предоставленные метаданные, такие как возраст, пол и акцент говорящего.
1	Common I-TERM
2	Voice I-TERM
3	включает O
4	не O
5	только O
6	речевые I-TERM
7	записи I-TERM
8	, O
9	но O
10	и O
11	из O
12	добровольно O
13	предоставленные O
14	метаданные I-TERM
15	, O
16	такие O
17	как O
18	возраст O
19	, O
20	пол O
21	и O
22	акцент O
23	говорящего O
24	. O

# sent_id = 229
# text =  Сегодня Common Voice является одним из крупнейших в мире мультиязычных корпусов, являющихся общественным достоянием, с более чем 9 тысячами часов голосовых данных на 60 различных языках, включая такие редкие языки, как валлийский и киньяруанда.
1	Сегодня O
2	Common I-TERM
3	Voice I-TERM
4	является O
5	одним O
6	из O
7	крупнейших O
8	в O
9	мире O
10	мультиязычных I-TERM
11	корпусов I-TERM
12	, O
13	являющихся O
14	общественным O
15	достоянием O
16	, O
17	с O
18	более O
19	чем O
20	9 O
21	тысячами O
22	часов O
23	голосовых O
24	данных O
25	на O
26	60 O
27	различных O
28	языках O
29	, O
30	включая O
31	такие O
32	редкие O
33	языки O
34	, O
35	как O
36	валлийский I-TERM
37	и O
38	киньяруанда I-TERM
39	. O

# sent_id = 230
# text = В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3.
1	В O
2	этой O
3	статье O
4	мы O
5	научим O
6	вас O
7	генерировать I-TERM
8	текст I-TERM
9	с O
10	помощью O
11	предварительно O
12	обученного O
13	GPT-2 I-TERM
14	— O
15	более O
16	легкого O
17	предшественника O
18	GPT-3 I-TERM
19	. O

# sent_id = 231
# text =  Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface.
1	Мы O
2	будем O
3	использовать O
4	именитую O
5	библиотеку O
6	Transformers I-TERM
7	, O
8	разработанную O
9	Huggingface I-TERM
10	. O

# sent_id = 232
# text =  Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.
1	Модель O
2	по O
3	умолчанию O
4	для O
5	конвейера O
6	генерации O
7	текста O
8	— O
9	GPT-2 I-TERM
10	, O
11	самая O
12	популярная O
13	модель O
14	декодирующего O
15	трансформера I-TERM
16	для O
17	генерации I-TERM
18	языка I-TERM
19	. O

# sent_id = 233
# text =  Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.
1	Эта O
2	модель O
3	GPT2 I-TERM
4	от O
5	CKIPLab I-TERM
6	предварительно O
7	обучена O
8	на O
9	китайском I-TERM
10	корпусе I-TERM
11	, O
12	поэтому O
13	мы O
14	можем O
15	использовать O
16	их O
17	модель O
18	без O
19	необходимости O
20	заниматься O
21	настройкой O
22	самостоятельно O
23	. O

# sent_id = 234
# text =  Перед переходом к самим метрикам необходимо ввести важную концепцию для описания этих метрик в терминах ошибок классификации — confusion matrix (матрица ошибок).
1	Перед O
2	переходом O
3	к O
4	самим O
5	метрикам O
6	необходимо O
7	ввести O
8	важную O
9	концепцию O
10	для O
11	описания O
12	этих O
13	метрик O
14	в O
15	терминах O
16	ошибок O
17	классификации O
18	— O
19	confusion I-TERM
20	matrix I-TERM
21	( O
22	матрица I-TERM
23	ошибок I-TERM
24	) O
25	. O

# sent_id = 235
# text =  Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма.
1	Интуитивно O
2	понятной O
3	, O
4	очевидной O
5	и O
6	почти O
7	неиспользуемой O
8	метрикой O
9	является O
10	accuracy I-TERM
11	— O
12	доля O
13	правильных O
14	ответов O
15	алгоритма O
16	. O

# sent_id = 236
# text =  Для оценки качества работы алгоритма на каждом из классов по отдельности введем метрики precision (точность) и recall (полнота).
1	Для O
2	оценки O
3	качества O
4	работы O
5	алгоритма O
6	на O
7	каждом O
8	из O
9	классов O
10	по O
11	отдельности O
12	введем O
13	метрики O
14	precision I-TERM
15	( O
16	точность I-TERM
17	) O
18	и O
19	recall I-TERM
20	( O
21	полнота I-TERM
22	) O
23	. O

# sent_id = 237
# text =  Precision можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными, а recall показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.
1	Precision I-TERM
2	можно O
3	интерпретировать O
4	как O
5	долю O
6	объектов O
7	, O
8	названных O
9	классификатором O
10	положительными O
11	и O
12	при O
13	этом O
14	действительно O
15	являющимися O
16	положительными O
17	, O
18	а O
19	recall I-TERM
20	показывает O
21	, O
22	какую O
23	долю O
24	объектов O
25	положительного O
26	класса O
27	из O
28	всех O
29	объектов O
30	положительного O
31	класса O
32	нашел O
33	алгоритм O
34	. O

# sent_id = 238
# text =  F-мера достигает максимума при полноте и точности, равными единице, и близка к нулю, если один из аргументов близок к нулю.
1	F I-TERM
2	- I-TERM
3	мера I-TERM
4	достигает O
5	максимума O
6	при O
7	полноте I-TERM
8	и O
9	точности I-TERM
10	, O
11	равными O
12	единице O
13	, O
14	и O
15	близка O
16	к O
17	нулю O
18	, O
19	если O
20	один O
21	из O
22	аргументов O
23	близок O
24	к O
25	нулю O
26	. O

# sent_id = 239
# text =  Одним из способов оценить модель в целом, не привязываясь к конкретному порогу, является AUC-ROC (или ROC AUC) — площадь (Area Under Curve) под кривой ошибок (Receiver Operating Characteristic curve ).
1	Одним O
2	из O
3	способов O
4	оценить O
5	модель O
6	в O
7	целом O
8	, O
9	не O
10	привязываясь O
11	к O
12	конкретному O
13	порогу O
14	, O
15	является O
16	AUC I-TERM
17	- I-TERM
18	ROC I-TERM
19	( O
20	или O
21	ROC I-TERM
22	AUC I-TERM
23	) O
24	— O
25	площадь I-TERM
26	( O
27	Area I-TERM
28	Under I-TERM
29	Curve I-TERM
30	) O
31	под O
32	кривой O
33	ошибок O
34	( O
35	Receiver I-TERM
36	Operating I-TERM
37	Characteristic I-TERM
38	curve I-TERM
39	) O
40	. O

# sent_id = 240
# text =  Интуитивно можно представить минимизацию logloss как задачу максимизации accuracy путем штрафа за неверные предсказания.
1	Интуитивно O
2	можно O
3	представить O
4	минимизацию O
5	logloss I-TERM
6	как O
7	задачу I-TERM
8	максимизации I-TERM
9	accuracy I-TERM
10	путем O
11	штрафа O
12	за O
13	неверные O
14	предсказания O
15	. O

# sent_id = 241
# text =  Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком.
1	Однажды O
2	нам O
3	понадобилось O
4	выбрать O
5	синтаксический I-TERM
6	парсер I-TERM
7	для O
8	работы O
9	с O
10	русским O
11	языком O
12	. O

# sent_id = 242
# text =  Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение.
1	Для O
2	этого O
3	мы O
4	углубились O
5	в O
6	дебри O
7	морфологии I-TERM
8	и O
9	токенизации I-TERM
10	, O
11	протестировали O
12	разные O
13	варианты O
14	и O
15	оценили O
16	их O
17	применение O
18	. O

# sent_id = 243
# text =  В первой строке предложение разобрано в рамках грамматики зависимостей.
1	В O
2	первой O
3	строке O
4	предложение O
5	разобрано O
6	в O
7	рамках O
8	грамматики I-TERM
9	зависимостей I-TERM
10	. O

# sent_id = 244
# text =  Во второй строке разбор идет в соответствии с грамматикой непосредственно составляющих.
1	Во O
2	второй O
3	строке O
4	разбор O
5	идет O
6	в O
7	соответствии O
8	с O
9	  O
10	грамматикой I-TERM
11	непосредственно I-TERM
12	составляющих I-TERM
13	. O

# sent_id = 245
# text =  Поэтому в автоматическом парсинге русского языка принято работать исходя из грамматики зависимостей.
1	Поэтому O
2	в O
3	автоматическом I-TERM
4	парсинге I-TERM
5	русского I-TERM
6	языка O
7	принято O
8	работать O
9	исходя O
10	из O
11	грамматики O
12	зависимостей O
13	. O

# sent_id = 246
# text =  Чтобы облегчить себе выбор парсера, мы обратили свой взгляд на проект Universal Dependencies и недавно прошедшее в его рамках соревнование CoNLL Shared Task.
1	Чтобы O
2	облегчить O
3	себе O
4	выбор O
5	парсера O
6	, O
7	мы O
8	обратили O
9	свой O
10	взгляд O
11	на O
12	проект O
13	Universal I-TERM
14	Dependencies I-TERM
15	и O
16	недавно O
17	прошедшее O
18	в O
19	его O
20	рамках O
21	соревнование O
22	CoNLL O
23	Shared O
24	Task O
25	. O

# sent_id = 247
# text =  Universal Dependencies — это проект по унификации разметки синтаксических корпусов (трибанков) в рамках грамматики зависимостей.
1	Universal I-TERM
2	Dependencies I-TERM
3	— O
4	это O
5	проект O
6	по O
7	унификации I-TERM
8	разметки I-TERM
9	синтаксических I-TERM
10	корпусов I-TERM
11	( O
12	трибанков I-TERM
13	) O
14	в O
15	рамках O
16	грамматики I-TERM
17	зависимостей I-TERM
18	. O

# sent_id = 248
# text =  Мы можем оценивать, правильно ли нашли вершину слова — метрика UAS (Unlabeled attachment score).
1	Мы O
2	можем O
3	оценивать O
4	, O
5	правильно O
6	ли O
7	нашли O
8	вершину O
9	слова O
10	— O
11	метрика O
12	UAS I-TERM
13	( O
14	Unlabeled I-TERM
15	attachment I-TERM
16	score I-TERM
17	) O
18	. O

# sent_id = 249
# text =  Или оценивать, правильно ли найдена как вершина, так и тип зависимости — метрика LAS (Labeled attachment score).
1	Или O
2	оценивать O
3	, O
4	правильно O
5	ли O
6	найдена O
7	как O
8	вершина O
9	, O
10	так O
11	и O
12	тип O
13	зависимости O
14	— O
15	метрика O
16	LAS I-TERM
17	( O
18	Labeled I-TERM
19	attachment I-TERM
20	score I-TERM
21	) O
22	. O

# sent_id = 250
# text =  Казалось бы, здесь напрашивается оценка точности (accuracy) — считаем, сколько раз мы попали из общего количества случаев.
1	Казалось O
2	бы O
3	, O
4	здесь O
5	напрашивается O
6	оценка O
7	точности I-TERM
8	( O
9	accuracy I-TERM
10	) O
11	— O
12	считаем O
13	, O
14	сколько O
15	раз O
16	мы O
17	попали O
18	из O
19	общего O
20	количества O
21	случаев O
22	. O

# sent_id = 251
# text =  Разработчики, решающие задачи автоматического парсинга, часто берут на вход сырой текст, который в соответствии с пирамидой анализа проходит этапы токенизации и морфологического анализа.
1	Разработчики O
2	, O
3	решающие O
4	задачи O
5	автоматического I-TERM
6	парсинга I-TERM
7	, O
8	часто O
9	берут O
10	на O
11	вход O
12	сырой O
13	текст O
14	, O
15	который O
16	в O
17	соответствии O
18	с O
19	пирамидой O
20	анализа O
21	проходит O
22	этапы O
23	токенизации I-TERM
24	и O
25	морфологического I-TERM
26	анализа I-TERM
27	. O

# sent_id = 252
# text =  Поэтому формулой оценки в данном случае является ф-мера, где точность (precision) — доля точных попаданий относительно общего числа предсказаний, а полнота — доля точных попаданий относительно числа связей в размеченных данных.
1	Поэтому O
2	формулой O
3	оценки O
4	в O
5	данном O
6	случае O
7	является O
8	ф I-TERM
9	- I-TERM
10	мера I-TERM
11	, O
12	где O
13	точность I-TERM
14	( O
15	precision I-TERM
16	) O
17	— O
18	доля O
19	точных O
20	попаданий O
21	относительно O
22	общего O
23	числа O
24	предсказаний O
25	, O
26	а O
27	полнота I-TERM
28	— O
29	доля O
30	точных O
31	попаданий O
32	относительно O
33	числа O
34	связей O
35	в O
36	размеченных O
37	данных O
38	. O

# sent_id = 253
# text =  Очевидно, что все эксперименты проводятся на SynTagRus (разработка ИППИ РАН), в котором более миллиона токенов.
1	Очевидно O
2	, O
3	что O
4	все O
5	эксперименты O
6	проводятся O
7	на O
8	SynTagRus I-TERM
9	( O
10	разработка O
11	ИППИ I-TERM
12	РАН I-TERM
13	) O
14	, O
15	в O
16	котором O
17	более O
18	миллиона O
19	токенов I-TERM
20	. O

# sent_id = 254
# text =  По итогам соревнования прошлого года модели, которые обучались на одном и том же SynTagRus, достигли следующих показателей LAS:
1	По O
2	итогам O
3	соревнования O
4	прошлого O
5	года O
6	модели O
7	, O
8	которые O
9	обучались O
10	на O
11	одном O
12	и O
13	том O
14	же O
15	SynTagRus I-TERM
16	, O
17	достигли O
18	следующих O
19	показателей O
20	LAS I-TERM
21	: O

# sent_id = 255
# text =  Забегая вперед, заметим, что новая версия UDPipe (Future) оказалась еще выше в этом году.
1	Забегая O
2	вперед O
3	, O
4	заметим O
5	, O
6	что O
7	новая O
8	версия O
9	UDPipe I-TERM
10	( O
11	Future I-TERM
12	) O
13	оказалась O
14	еще O
15	выше O
16	в O
17	этом O
18	году O
19	. O

# sent_id = 256
# text =  В список не вошел Syntaxnet — парсер Google.
1	В O
2	список O
3	не O
4	вошел O
5	Syntaxnet I-TERM
6	— O
7	парсер O
8	Google I-TERM
9	. O

# sent_id = 257
# text =  Ответ прост: Syntaxnet начинался лишь с этапа морфологического анализа.
1	Ответ O
2	прост O
3	: O
4	Syntaxnet I-TERM
5	начинался O
6	лишь O
7	с O
8	этапа O
9	морфологического I-TERM
10	анализа I-TERM
11	. O

# sent_id = 258
# text =  В качестве начальных данных у нас есть табличка выше с лидирующим Syntaxnet и с UDPipe 2.0 где-то на 7 месте.
1	В O
2	качестве O
3	начальных O
4	данных O
5	у O
6	нас O
7	есть O
8	табличка O
9	выше O
10	с O
11	лидирующим O
12	Syntaxnet I-TERM
13	и O
14	с O
15	UDPipe I-TERM
16	2.0 I-TERM
17	где O
18	- O
19	то O
20	на O
21	7 O
22	месте O
23	. O

# sent_id = 259
# text =  Синтаксис, разумеется, далеко не единственный модуль «под капотом» real-time системы, поэтому тратить на него больше десятка миллисекунд не стоит.
1	Синтаксис I-TERM
2	, O
3	разумеется O
4	, O
5	далеко O
6	не O
7	единственный O
8	модуль O
9	« O
10	под O
11	капотом O
12	» O
13	real I-TERM
14	- I-TERM
15	time I-TERM
16	системы O
17	, O
18	поэтому O
19	тратить O
20	на O
21	него O
22	больше O
23	десятка O
24	миллисекунд O
25	не O
26	стоит O
27	. O

# sent_id = 260
# text =  Для русского языка у нас есть достаточно хорошие морфологические анализаторы, которые могут встроиться в нашу пирамиду.
1	Для O
2	русского O
3	языка O
4	у O
5	нас O
6	есть O
7	достаточно O
8	хорошие O
9	морфологические I-TERM
10	анализаторы I-TERM
11	, O
12	которые O
13	могут O
14	встроиться O
15	в O
16	нашу O
17	пирамиду O
18	. O

# sent_id = 261
# text =  Затем начинает работу теггер — штука, которая предсказывает морфологические свойства токена: в каком падеже слово стоит, в каком числе.
1	Затем O
2	начинает O
3	работу O
4	теггер I-TERM
5	— O
6	штука O
7	, O
8	которая O
9	предсказывает O
10	морфологические I-TERM
11	свойства I-TERM
12	токена O
13	: O
14	в O
15	каком O
16	падеже O
17	слово O
18	стоит O
19	, O
20	в O
21	каком O
22	числе O
23	. O

# sent_id = 262
# text =  В UDPipe есть еще лемматизатор, который подбирает для слов начальную форму.
1	В O
2	UDPipe I-TERM
3	есть O
4	еще O
5	лемматизатор I-TERM
6	, O
7	который O
8	подбирает O
9	для O
10	слов O
11	начальную I-TERM
12	форму I-TERM
13	. O

# sent_id = 263
# text =  UDPipe — это transition-based архитектура: она работает быстро, за линейное время проходя по всем токенам один раз.
1	UDPipe I-TERM
2	— O
3	это O
4	transition I-TERM
5	- I-TERM
6	based I-TERM
7	архитектура O
8	: O
9	она O
10	работает O
11	быстро O
12	, O
13	за O
14	линейное O
15	время O
16	проходя O
17	по O
18	всем O
19	токенам I-TERM
20	один O
21	раз O
22	. O

# sent_id = 264
# text =  RightArc — то же самое, но зависимость строится в другую сторону, и отбрасывается верхушка.
1	RightArc I-TERM
2	— O
3	то O
4	же O
5	самое O
6	, O
7	но O
8	зависимость O
9	строится O
10	в O
11	другую O
12	сторону O
13	, O
14	и O
15	отбрасывается O
16	верхушка O
17	. O

# sent_id = 265
# text =  У классических transition-based parser возможны три операции, перечисленные выше: стрелка в одну сторону, стрелка в другую сторону и шифт.
1	У O
2	классических O
3	transition I-TERM
4	- I-TERM
5	based I-TERM
6	parser I-TERM
7	возможны O
8	три O
9	операции O
10	, O
11	перечисленные O
12	выше O
13	: O
14	стрелка O
15	в O
16	одну O
17	сторону O
18	, O
19	стрелка O
20	в O
21	другую O
22	сторону O
23	и O
24	шифт O
25	. O

# sent_id = 266
# text =  Анализатор Mystem (разработка яндекса) в определении частей речи достигает лучших результатов, чем UDPipe.
1	Анализатор O
2	Mystem I-TERM
3	( O
4	разработка O
5	яндекса I-TERM
6	) O
7	в O
8	определении O
9	частей O
10	речи O
11	достигает O
12	лучших O
13	результатов O
14	, O
15	чем O
16	UDPipe I-TERM
17	. O

# sent_id = 267
# text =  Многие знают, что Mystem не полностью понимает морфологическую омонимию.
1	Многие O
2	знают O
3	, O
4	что O
5	Mystem I-TERM
6	не O
7	полностью O
8	понимает O
9	морфологическую I-TERM
10	омонимию I-TERM
11	. O

# sent_id = 268
# text =  При помощи анализатора RNNMorph.
1	При O
2	помощи O
3	анализатора O
4	RNNMorph I-TERM
5	. O

# sent_id = 269
# text =  Про него мало кто слышал, но в прошлом году он выиграл соревнование среди морфологических анализаторов, проводившееся в рамках конференции «Диалог».
1	Про O
2	него O
3	мало O
4	кто O
5	слышал O
6	, O
7	но O
8	в O
9	прошлом O
10	году O
11	он O
12	выиграл O
13	соревнование O
14	среди O
15	морфологических I-TERM
16	анализаторов I-TERM
17	, O
18	проводившееся O
19	в O
20	рамках O
21	конференции O
22	« O
23	Диалог O
24	» O
25	. O

# sent_id = 270
# text =  Хотя если сравнивать их чисто на уровне качества морфологической разметки (данные с MorphoRuEval-2017), то проигрыш получается значительный — порядка 15%, если считать accuracy по словам.
1	Хотя O
2	если O
3	сравнивать O
4	их O
5	чисто O
6	на O
7	уровне O
8	качества O
9	морфологической I-TERM
10	разметки I-TERM
11	( O
12	данные O
13	с O
14	MorphoRuEval-2017 O
15	) O
16	, O
17	то O
18	проигрыш O
19	получается O
20	значительный O
21	— O
22	порядка O
23	15 O
24	% O
25	, O
26	если O
27	считать O
28	accuracy I-TERM
29	по O
30	словам O
31	. O

# sent_id = 271
# text =  Дальше буду сравнивать нас с Syntaxnet и остальными алгоритмами.
1	Дальше O
2	буду O
3	сравнивать O
4	нас O
5	с O
6	Syntaxnet I-TERM
7	и O
8	остальными O
9	алгоритмами O
10	. O

# sent_id = 272
# text =  Интересно, что мы почти дотянулись по метрике LAS до версии Syntaxnet.
1	Интересно O
2	, O
3	что O
4	мы O
5	почти O
6	дотянулись O
7	по O
8	метрике O
9	LAS I-TERM
10	до O
11	версии O
12	Syntaxnet I-TERM
13	. O

# sent_id = 273
# text =  В архитектуре стенфордского парсера и Syntaxnet заложена другая концепия: сначала они генерируют полный ориентированный граф, и дальше работа алгоритма состоит в том, чтобы оставить тот скелет (минимальное остовное дерево), который будет наиболее вероятным.
1	В O
2	архитектуре O
3	стенфордского O
4	парсера O
5	и O
6	Syntaxnet I-TERM
7	заложена O
8	другая O
9	концепия O
10	: O
11	сначала O
12	они O
13	генерируют O
14	полный O
15	ориентированный I-TERM
16	граф I-TERM
17	, O
18	и O
19	дальше O
20	работа O
21	алгоритма O
22	состоит O
23	в O
24	том O
25	, O
26	чтобы O
27	оставить O
28	тот O
29	скелет O
30	( O
31	минимальное I-TERM
32	остовное I-TERM
33	дерево I-TERM
34	) O
35	, O
36	который O
37	будет O
38	наиболее O
39	вероятным O
40	. O

# sent_id = 274
# text =  Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
1	Чаще O
2	всего O
3	на O
4	практике O
5	в O
6	NLP I-TERM
7	приходится O
8	сталкиваться O
9	с O
10	задачей O
11	построения I-TERM
12	эмбеддингов I-TERM
13	. O

# sent_id = 275
# text =  Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
1	Для O
2	ее O
3	решения O
4	обычно O
5	используют O
6	один O
7	из O
8	следующих O
9	инструментов O
10	: O
11	Готовые O
12	векторы I-TERM
13	/ O
14	эмбеддинги I-TERM
15	слов I-TERM
16	[ O
17	6 O
18	] O
19	; O
20	Внутренние O
21	состояния O
22	CNN I-TERM
23	, O
24	натренированных O
25	на O
26	таких O
27	задачах O
28	как O
29	, O
30	как O
31	определение I-TERM
32	фальшивых I-TERM
33	предложений I-TERM
34	/ O
35	языковое I-TERM
36	моделирование I-TERM
37	/ O
38	классификация I-TERM
39	[ O
40	7 O
41	] O
42	; O
43	Комбинация O
44	выше O
45	перечисленных O
46	методов O
47	; O
48	Кроме O
49	того O
50	, O
51	уже O
52	много O
53	раз O
54	было O
55	показано O
56	[ O
57	9 O
58	] O
59	, O
60	что O
61	в O
62	качестве O
63	хорошего O
64	бейслайна O
65	для O
66	эмбеддингов O
67	предложений O
68	можно O
69	взять O
70	и O
71	просто O
72	усредненные O
73	( O
74	с O
75	парой O
76	незначительных O
77	деталей O
78	, O
79	которые O
80	сейчас O
81	опустим O
82	) O
83	векторы O
84	слов O
85	. O

# sent_id = 276
# text =  Выраженная в тексте эмоциональная оценка называется тональностью или сентиментом (от англ. sentiment — чувство; мнение, настроение) текста.
1	Выраженная O
2	в O
3	тексте O
4	эмоциональная I-TERM
5	оценка I-TERM
6	называется O
7	тональностью I-TERM
8	или O
9	сентиментом I-TERM
10	( O
11	от O
12	англ O
13	. O 
14	sentiment O
15	— O
16	чувство O
17	; O
18	мнение O
19	, O
20	настроение O
21	) O
22	текста O
23	. O

# sent_id = 277
# text =  Исторически сложилось так, что традиционный подход к сентимент анализу представляет собой задачу классификации текста (части текста) на две-три категории (негативный, позитивный, нейтральный или просто: негативный или позитивный) [Pang & Lee; Turney ].
1	Исторически O
2	сложилось O
3	так O
4	, O
5	что O
6	традиционный O
7	подход O
8	к O
9	сентимент I-TERM
10	анализу I-TERM
11	представляет O
12	собой O
13	задачу O
14	классификации I-TERM
15	текста O
16	( O
17	части O
18	текста O
19	) O
20	на O
21	две O
22	- O
23	три O
24	категории O
25	( O
26	негативный O
27	, O
28	позитивный O
29	, O
30	нейтральный O
31	или O
32	просто O
33	: O
34	негативный O
35	или O
36	позитивный O
37	) O
38	[ O
39	Pang I-TERM
40	& O
41	Lee I-TERM
42	; O
43	Turney I-TERM
44	] O
45	. O

# sent_id = 278
# text =  Такой вид сентимент анализа называется объектной тональностью (object-based).
1	Такой O
2	вид O
3	сентимент I-TERM
4	анализа I-TERM
5	называется O
6	объектной I-TERM
7	тональностью I-TERM
8	( O
9	object I-TERM
10	- I-TERM
11	based I-TERM
12	) O
13	. O

# sent_id = 279
# text =  Таким образом, тональность высказывания определяется тремя компонентами: субъектом тональности (кто высказал оценку), объектом тональности (о ком или о чём высказана оценка) и собственно тональной оценкой (как оценили).
1	Таким O
2	образом O
3	, O
4	тональность I-TERM
5	высказывания I-TERM
6	определяется O
7	тремя O
8	компонентами O
9	: O
10	субъектом I-TERM
11	тональности I-TERM
12	( O
13	кто O
14	высказал O
15	оценку O
16	) O
17	, O
18	объектом I-TERM
19	тональности I-TERM
20	( O
21	о O
22	ком O
23	или O
24	о O
25	чём O
26	высказана O
27	оценка O
28	) O
29	и O
30	собственно O
31	тональной I-TERM
32	оценкой I-TERM
33	( O
34	как O
35	оценили O
36	) O
37	. O

# sent_id = 280
# text =  Еще одним направлением сентимент анализа является выявление негативности / позитивности атрибутов объекта тональности (feature-based / aspect-based sentiment analysis).
1	Еще O
2	одним O
3	направлением O
4	сентимент O
5	анализа O
6	является O
7	выявление O
8	негативности I-TERM
9	/ I-TERM
10	позитивности I-TERM
11	атрибутов I-TERM
12	объекта I-TERM
13	тональности I-TERM
14	( O
15	feature I-TERM
16	- I-TERM
17	based I-TERM
18	/ I-TERM
19	aspect I-TERM
20	- I-TERM
21	based I-TERM
22	sentiment I-TERM
23	analysis I-TERM
24	) O
25	. O

# sent_id = 281
# text =  При статистическом подходе для решения задачи общей классификации текстов на классы тональности широко используют метод опорных векторов (SVM), Байесовы модели, различного рода регрессии [Chetviorkin & Loukachevitch — описание соревнования ROMIP-2011 по сентимент анализу данных, практически все участники использовали SVM или Байес].
1	При O
2	статистическом O
3	подходе O
4	для O
5	решения O
6	задачи O
7	общей O
8	классификации I-TERM
9	текстов O
10	на O
11	классы O
12	тональности O
13	широко O
14	используют O
15	метод I-TERM
16	опорных I-TERM
17	векторов I-TERM
18	( O
19	SVM I-TERM
20	) O
21	, O
22	Байесовы I-TERM
23	модели I-TERM
24	, O
25	различного O
26	рода O
27	регрессии I-TERM
28	[ O
29	Chetviorkin I-TERM
30	& O
31	Loukachevitch I-TERM
32	— O
33	описание O
34	соревнования O
35	ROMIP-2011 O
36	по O
37	сентимент I-TERM
38	анализу I-TERM
39	данных O
40	, O
41	практически O
42	все O
43	участники O
44	использовали O
45	SVM I-TERM
46	или O
47	Байес I-TERM
48	] O
49	. O

# sent_id = 282
# text =  Если же целью является определение тональности у определенного, заранее заданного объекта (нескольких объектов), то применяют более сложные статистические алгоритмы, такие как CRF [Антонова и Соловьев], алгоритмы семантической близости (например, латентно-семантический анализ – LSA, латентное размещение Дирихле — LDA) и др., а также методы, основанные на правилах [Пазельская и Соловьев].
1	Если O
2	же O
3	целью O
4	является O
5	определение I-TERM
6	тональности I-TERM
7	у O
8	определенного O
9	, O
10	заранее O
11	заданного O
12	объекта O
13	( O
14	нескольких O
15	объектов O
16	) O
17	, O
18	то O
19	применяют O
20	более O
21	сложные O
22	статистические I-TERM
23	алгоритмы I-TERM
24	, O
25	такие O
26	как O
27	CRF I-TERM
28	[ O
29	Антонова I-TERM
30	и O
31	Соловьев I-TERM
32	] O
33	, O
34	алгоритмы I-TERM
35	семантической I-TERM
36	близости I-TERM
37	( O
38	например O
39	, O
40	латентно I-TERM
41	- I-TERM
42	семантический I-TERM
43	анализ I-TERM
44	– O
45	LSA I-TERM
46	, O
47	латентное I-TERM
48	размещение I-TERM
49	Дирихле I-TERM
50	— O
51	LDA I-TERM
52	) O
53	и др. O
54	, O
55	а O
56	также O
57	методы O
58	, O
59	основанные O
60	на O
61	правилах O
62	[ O
63	Пазельская I-TERM
64	и O
65	Соловьев I-TERM
66	] O
67	. O

# sent_id = 283
# text =  Удалось найти лишь это упоминание про систему Deepgram.
1	Удалось O
2	найти O
3	лишь O
4	это O
5	упоминание O
6	про O
7	систему O
8	Deepgram I-TERM
9	. O

# sent_id = 284
# text =  Также очень похожая функциональность есть у Microsoft в Streams, но нигде не нашел упоминания про поддержку русского языка, судя по всему, ее тоже нет.
1	Также O
2	очень O
3	похожая O
4	функциональность O
5	есть O
6	у O
7	Microsoft I-TERM
8	в O
9	Streams I-TERM
10	, O
11	но O
12	нигде O
13	не O
14	нашел O
15	упоминания O
16	про O
17	поддержку O
18	русского O
19	языка O
20	, O
21	судя O
22	по O
23	всему O
24	, O
25	ее O
26	тоже O
27	нет O
28	. O

# sent_id = 285
# text =  Нейросети, которые могут преобразовывать речь в текст называются (сюрприз-сюрприз), speech-to-text.
1	Нейросети O
2	, O
3	которые O
4	могут O
5	преобразовывать I-TERM
6	речь I-TERM
7	в I-TERM
8	текст I-TERM
9	называются O
10	( O
11	сюрприз O
12	- O
13	сюрприз O
14	) O
15	, O
16	speech I-TERM
17	- I-TERM
18	to I-TERM
19	- I-TERM
20	text I-TERM
21	. O

# sent_id = 286
# text =  Если получится найти публичный сервис speech-to-text, то его можно использовать, чтобы «оцифровать» речь во всех вебинарах, а сделать потом нечеткий поиск по тексту – более простая задача.
1	Если O
2	получится O
3	найти O
4	публичный O
5	сервис O
6	speech I-TERM
7	- I-TERM
8	to I-TERM
9	- I-TERM
10	text I-TERM
11	, O
12	то O
13	его O
14	можно O
15	использовать O
16	, O
17	чтобы O
18	« O
19	оцифровать O
20	» O
21	речь O
22	во O
23	всех O
24	вебинарах O
25	, O
26	а O
27	сделать O
28	потом O
29	нечеткий O
30	поиск O
31	по O
32	тексту O
33	– O
34	более O
35	простая O
36	задача O
37	. O

# sent_id = 287
# text =  Поиск сервисов, способных делать speech-to-text показал, что таких систем масса, в том числе и разработанных в России, есть среди них также глобальные облачные провайдеры вроде Google, Amazon, MS Azure.
1	Поиск O
2	сервисов O
3	, O
4	способных O
5	делать O
6	speech I-TERM
7	- I-TERM
8	to I-TERM
9	- I-TERM
10	text I-TERM
11	показал O
12	, O
13	что O
14	таких O
15	систем O
16	масса O
17	, O
18	в O
19	том O
20	числе O
21	и O
22	разработанных O
23	в O
24	России O
25	, O
26	есть O
27	среди O
28	них O
29	также O
30	глобальные O
31	облачные O
32	провайдеры O
33	вроде O
34	Google I-TERM
35	, O
36	Amazon I-TERM
37	, O
38	MS I-TERM
39	Azure I-TERM
40	. O

# sent_id = 288
# text =  Custom Vocabularies – позволяет создать «словарь» из тех, слов, которые должна «выучить» нейросеть перед тем, как приступить к распознаванию.
1	Custom I-TERM
2	Vocabularies I-TERM
3	– O
4	позволяет O
5	создать O
6	« O
7	словарь O
8	» O
9	из O
10	тех O
11	, O
12	слов O
13	, O
14	которые O
15	должна O
16	« O
17	выучить O
18	» O
19	нейросеть O
20	перед O
21	тем O
22	, O
23	как O
24	приступить O
25	к O
26	распознаванию I-TERM
27	. O

# sent_id = 289
# text =  Можно попробовать прикрутить к итоговому набору текстов алгоритм BERT (Bi-directional Encoder Representation from Transformer), описание есть тут.
1	Можно O
2	попробовать O
3	прикрутить O
4	к O
5	итоговому O
6	набору O
7	текстов O
8	алгоритм O
9	BERT I-TERM
10	( O
11	Bi I-TERM
12	- I-TERM
13	directional I-TERM
14	Encoder I-TERM
15	Representation I-TERM
16	from I-TERM
17	Transformer I-TERM
18	) O
19	, O
20	описание O
21	есть O
22	тут O
23	. O

# sent_id = 290
# text =  Некоторое время назад к нам обратился заказчик с не совсем обычной задачей — воспроизвести сервис IBM Watson Personality Insights, который анализировал текст, написанный человеком и определял по нему ряд личностных характеристик.
1	Некоторое O
2	время O
3	назад O
4	к O
5	нам O
6	обратился O
7	заказчик O
8	с O
9	не O
10	совсем O
11	обычной O
12	задачей O
13	— O
14	воспроизвести O
15	сервис O
16	IBM I-TERM
17	Watson I-TERM
18	Personality I-TERM
19	Insights I-TERM
20	, O
21	который O
22	анализировал I-TERM
23	текст I-TERM
24	, O
25	написанный O
26	человеком O
27	и O
28	определял O
29	по O
30	нему O
31	ряд O
32	личностных I-TERM
33	характеристик I-TERM
34	. O

# sent_id = 291
# text =  Основная идея данного сервиса состояла в том, что он получает на вход текст написанный определенным человеком и определяет по этому тексту четыре группы характеристик личности.
1	Основная O
2	идея O
3	данного O
4	сервиса O
5	состояла O
6	в O
7	том O
8	, O
9	что O
10	он O
11	получает O
12	на O
13	вход O
14	текст O
15	написанный O
16	определенным O
17	человеком O
18	и O
19	определяет I-TERM
20	по O
21	этому O
22	тексту O
23	четыре O
24	группы I-TERM
25	характеристик I-TERM
26	личности I-TERM
27	. O

# sent_id = 292
# text =  Например, Personality Insights использовался в психотерапии для оценки состояния пациентов [5], в искусстве (оценка личности персонажей пьес Шекспира) [6], определении спама [7] а также в научных исследованиях.
1	Например O
2	, O
3	Personality I-TERM
4	Insights I-TERM
5	использовался O
6	в O
7	психотерапии I-TERM
8	для O
9	оценки I-TERM
10	состояния I-TERM
11	пациентов I-TERM
12	[ O
13	5 O
14	] O
15	, O
16	в O
17	искусстве O
18	( O
19	оценка I-TERM
20	личности I-TERM
21	персонажей I-TERM
22	пьес I-TERM
23	Шекспира I-TERM
24	) O
25	[ O
26	6 O
27	] O
28	, O
29	определении I-TERM
30	спама I-TERM
31	[ O
32	7 O
33	] O
34	а O
35	также O
36	в O
37	научных O
38	исследованиях O
39	. O

# sent_id = 293
# text =  На сайте Personality Insights качество моделей Watson оценивалось с помощью двух показателей — средней абсолютной ошибки (MAE) и коэффициента корреляции.
1	На O
2	сайте O
3	Personality I-TERM
4	Insights I-TERM
5	качество O
6	моделей O
7	Watson I-TERM
8	оценивалось O
9	с O
10	помощью O
11	двух O
12	показателей O
13	— O
14	средней I-TERM
15	абсолютной I-TERM
16	ошибки I-TERM
17	( O
18	MAE I-TERM
19	) O
20	и O
21	коэффициента I-TERM
22	корреляции I-TERM
23	. O

# sent_id = 294
# text =  В литературе для предсказания характеристик Big 5 использовались различные методы линейная регрессия с использованием признаков полученных латентным семантическим анализом [11], ридж-регрессия по большому набору собранных вручную признаков [12], SVM с признаками TF/IDF [13], word2vec и doc2vec [14].
1	В O
2	литературе O
3	для O
4	предсказания O
5	характеристик O
6	Big O
7	5 O
8	использовались O
9	различные O
10	методы O
11	линейная I-TERM
12	регрессия I-TERM
13	с O
14	использованием O
15	признаков O
16	полученных O
17	латентным O
18	семантическим O
19	анализом O
20	[ O
21	11 O
22	] O
23	, O
24	ридж I-TERM
25	- I-TERM
26	регрессия I-TERM
27	по O
28	большому O
29	набору O
30	собранных O
31	вручную O
32	признаков O
33	[ O
34	12 O
35	] O
36	, O
37	SVM I-TERM
38	с O
39	признаками O
40	TF I-TERM
41	/ O
42	IDF I-TERM
43	[ O
44	13 O
45	] O
46	, O
47	word2vec I-TERM
48	и O
49	doc2vec I-TERM
50	[ O
51	14 O
52	] O
53	. O

# sent_id = 295
# text =  В более современных работах присутствуют сверточные нейронные сети [15, 16], а также предобученные модели BERT [17]
1	В O
2	более O
3	современных O
4	работах O
5	присутствуют O
6	сверточные I-TERM
7	нейронные I-TERM
8	сети I-TERM
9	[ O
10	15 O
11	, O
12	16 O
13	] O
14	, O
15	а O
16	также O
17	предобученные O
18	модели O
19	BERT I-TERM
20	[ O
21	17 O
22	] O

# sent_id = 296
# text = Модель, которую построил заказчик использовала вектора слов word2vec и рекуррентную нейронную сеть на базе GRU (gated recurrent unit) (Рис 1а).
1	Модель O
2	, O
3	которую O
4	построил O
5	заказчик O
6	использовала O
7	вектора O
8	слов O
9	word2vec I-TERM
10	и O
11	рекуррентную O
12	нейронную O
13	сеть O
14	на O
15	базе O
16	GRU I-TERM
17	( O
18	gated I-TERM
19	recurrent I-TERM
20	unit I-TERM
21	) O
22	( O
23	Рис O
24	1а O
25	) O
26	. O

# sent_id = 297
# text =  Обучалась модель с функцией ошибки MSE (среднеквадратичное отклонение).
1	Обучалась O
2	модель O
3	с O
4	функцией O
5	ошибки O
6	MSE I-TERM
7	( O
8	среднеквадратичное I-TERM
9	отклонение I-TERM
10	) O
11	. O

# sent_id = 298
# text =  Сигмоидная функция активации обычно не очень хорошо подходит для задачи регрессии.
1	Сигмоидная I-TERM
2	функция I-TERM
3	активации I-TERM
4	обычно O
5	не O
6	очень O
7	хорошо O
8	подходит O
9	для O
10	задачи O
11	регрессии I-TERM
12	. O

# sent_id = 299
# text =  В литературе для регрессии рекомендуют использовать линейную активацию или RelU.
1	В O
2	литературе O
3	для O
4	регрессии I-TERM
5	рекомендуют O
6	использовать O
7	линейную I-TERM
8	активацию I-TERM
9	или O
10	RelU I-TERM
11	. O

# sent_id = 300
# text =  Вычислив MAE отдельно для характеристики личности и отдельно для потребительских предпочтений получили значения 0.11 и 0.148 соответственно, т. е. потребительские предпочтения сильно портят общую картину.
1	Вычислив O
2	MAE I-TERM
3	отдельно O
4	для O
5	характеристики O
6	личности O
7	и O
8	отдельно O
9	для O
10	потребительских O
11	предпочтений O
12	получили O
13	значения O
14	0.11 I-TERM
15	и O
16	0.148 I-TERM
17	соответственно O
18	, O
19	т O
20	. O
21	  O
22	е O
23	. O
24	потребительские O
25	предпочтения O
26	сильно O
27	портят O
28	общую O
29	картину O
30	. O

# sent_id = 301
# text =  Замена BERT на более современную модель XLM RoBERTa large позволило улучшить результаты (эта модель более ресурсозатратная и медленная, но заказчик сказал, что скорость работы не критична).
1	Замена O
2	BERT O
3	на O
4	более O
5	современную O
6	модель O
7	XLM I-TERM
8	RoBERTa I-TERM
9	large I-TERM
10	позволило O
11	улучшить O
12	результаты O
13	( O
14	эта O
15	модель O
16	более O
17	ресурсозатратная O
18	и O
19	медленная O
20	, O
21	но O
22	заказчик O
23	сказал O
24	, O
25	что O
26	скорость O
27	работы O
28	не O
29	критична O
30	) O
31	. O

# sent_id = 302
# text =  Итоговый MAE составил 0.073 для характеристик личности и 0.098 для потребительских предпочтений.
1	Итоговый O
2	MAE I-TERM
3	составил O
4	0.073 I-TERM
5	для O
6	характеристик O
7	личности O
8	и O
9	0.098 I-TERM
10	для O
11	потребительских O
12	предпочтений O
13	. O

# sent_id = 303
# text =  Получились немного разные цифры, но средний коэффициент корреляции по всем параметрам составил 0.68, что говорит о том, что характеристики, выдаваемые с разных переводов одного текста должны быть весьма похожи.
1	Получились O
2	немного O
3	разные O
4	цифры O
5	, O
6	но O
7	средний O
8	коэффициент I-TERM
9	корреляции I-TERM
10	по O
11	всем O
12	параметрам O
13	составил O
14	0.68 I-TERM
15	, O
16	что O
17	говорит O
18	о O
19	том O
20	, O
21	что O
22	характеристики O
23	, O
24	выдаваемые O
25	с O
26	разных O
27	переводов O
28	одного O
29	текста O
30	должны O
31	быть O
32	весьма O
33	похожи O
34	. O

# sent_id = 304
# text =  Надо сказать, что признаки, формируемые верхними слоями подобных моделей не всегда являются самыми лучшими, точнее даже сказать, как правило, не являются — в классических задачах, таких как поиск именованных сущностей или ответы на вопросы по тексту, признаки верхних уровней работают хуже, чем признаки промежуточных [18].
1	Надо O
2	сказать O
3	, O
4	что O
5	признаки O
6	, O
7	формируемые O
8	верхними O
9	слоями O
10	подобных O
11	моделей O
12	не O
13	всегда O
14	являются O
15	самыми O
16	лучшими O
17	, O
18	точнее O
19	даже O
20	сказать O
21	, O
22	как O
23	правило O
24	, O
25	не O
26	являются O
27	— O
28	в O
29	классических O
30	задачах O
31	, O
32	таких O
33	как O
34	поиск I-TERM
35	именованных I-TERM
36	сущностей I-TERM
37	или O
38	ответы O
39	на O
40	вопросы O
41	по O
42	тексту O
43	, O
44	признаки O
45	верхних O
46	уровней O
47	работают O
48	хуже O
49	, O
50	чем O
51	признаки O
52	промежуточных O
53	[ O
54	18 O
55	] O
56	. O

# sent_id = 305
# text =  Veridical Data Science — программная статья о методологии верификации моделей.
1	Veridical I-TERM
2	Data I-TERM
3	Science I-TERM
4	— O
5	программная O
6	статья O
7	о O
8	методологии O
9	верификации I-TERM
10	моделей I-TERM
11	. O

# sent_id = 306
# text = Чтобы обеспечить надежную проверку и разработать механизмы проверки и пополнения знаний, нужны специалисты смежных областей, одновременно обладающие компетенциями в ML и в предметной области (медицине, лингвистике, нейробиологии, образовании и т.д.).
1	Чтобы O
2	обеспечить O
3	надежную O
4	проверку O
5	и O
6	разработать O
7	механизмы O
8	проверки O
9	и O
10	пополнения O
11	знаний O
12	, O
13	нужны O
14	специалисты O
15	смежных O
16	областей O
17	, O
18	одновременно O
19	обладающие O
20	компетенциями O
21	в O
22	ML O
23	и O
24	в O
25	предметной O
26	области O
27	( O
28	медицине I-TERM
29	, O
30	лингвистике I-TERM
31	, O
32	нейробиологии I-TERM
33	, O
34	образовании I-TERM
35	и O
36	т.д. O
37	) O
38	. O

# sent_id = 307
# text =  В частности, развивается causal inference и commonsense reasoning.
1	В O
2	частности O
3	, O
4	развивается O
5	causal I-TERM
6	inference I-TERM
7	и O
8	commonsense I-TERM
9	reasoning I-TERM
10	. O

# sent_id = 308
# text =  Часть докладов посвящена мета-обучению (о том, как учиться учиться) и соединению DL-технологий с логикой 1 и 2 порядка — термин Artificial General Intelligence (AGI) становится обычным термином в выступлениях спикеров.
1	Часть O
2	докладов O
3	посвящена O
4	мета O
5	- O
6	обучению O
7	( O
8	о O
9	том O
10	, O
11	как O
12	учиться O
13	учиться O
14	) O
15	и O
16	соединению O
17	DL I-TERM
18	- I-TERM
19	технологий I-TERM
20	с O
21	логикой O
22	1 O
23	и O
24	2 O
25	порядка O
26	— O
27	термин O
28	Artificial I-TERM
29	General I-TERM
30	Intelligence I-TERM
31	( O
32	AGI I-TERM
33	) O
34	становится O
35	обычным O
36	термином O
37	в O
38	выступлениях O
39	спикеров O
40	. O

# sent_id = 309
# text =  Google запускает Coral ai – аналог raspberry pi, мини-компьютер для внедрения нейросетей в экспериментальные установки.
1	Google I-TERM
2	запускает O
3	Coral I-TERM
4	ai I-TERM
5	– O
6	аналог O
7	raspberry I-TERM
8	pi I-TERM
9	, O
10	мини O
11	- O
12	компьютер O
13	для O
14	внедрения O
15	нейросетей O
16	в O
17	экспериментальные O
18	установки O
19	. O

# sent_id = 310
# text =  Federated learning – направление ML, в котором отдельные модели учатся независимо друг от друга, а затем объединяются в единую модель (без централизации исходных данных), с поправками на редкие события, аномалии, персонализацию и т.д.
1	Federated I-TERM
2	learning I-TERM
3	– O
4	направление O
5	ML I-TERM
6	, O
7	в O
8	котором O
9	отдельные O
10	модели O
11	учатся O
12	независимо O
13	друг O
14	от O
15	друга O
16	, O
17	а O
18	затем O
19	объединяются O
20	в O
21	единую O
22	модель O
23	( O
24	без O
25	централизации O
26	исходных O
27	данных O
28	) O
29	, O
30	с O
31	поправками O
32	на O
33	редкие O
34	события O
35	, O
36	аномалии O
37	, O
38	персонализацию O
39	и O
40	т.д. O

# sent_id = 311
# text =  Генеративные модели на основании federated learning – будущее перспективное направление по мнению Google, которое находится “в ранних стадиях экспоненциального роста”.
1	Генеративные I-TERM
2	модели I-TERM
3	на O
4	основании O
5	federated I-TERM
6	learning I-TERM
7	– O
8	будущее O
9	перспективное O
10	направление O
11	по O
12	мнению O
13	Google I-TERM
14	, O
15	которое O
16	находится O
17	“ O
18	в O
19	ранних O
20	стадиях O
21	экспоненциального O
22	роста O
23	” O
24	. O

# sent_id = 312
# text =  МТИ Technology Review протестировали два инструмента — MyInterview и Curious Thing.
1	МТИ I-TERM
2	Technology I-TERM
3	Review I-TERM
4	протестировали O
5	два O
6	инструмента O
7	— O
8	MyInterview I-TERM
9	и O
10	Curious I-TERM
11	Thing I-TERM
12	. O

# sent_id = 313
# text =  Потом появилась нейросеть GPT-2, которая была как минимум в 10 раз мощнее и была способна обрабатывать 1,5 миллиарда параметров (переменных, определяющих возможности машинного обучения).
1	Потом O
2	появилась O
3	нейросеть O
4	GPT-2 I-TERM
5	, O
6	которая O
7	была O
8	как O
9	минимум O
10	в O
11	10 O
12	раз O
13	мощнее O
14	и O
15	была O
16	способна O
17	обрабатывать O
18	1,5 O
19	миллиарда O
20	параметров O
21	( O
22	переменных O
23	, O
24	определяющих O
25	возможности O
26	машинного I-TERM
27	обучения I-TERM
28	) O
29	. O

# sent_id = 314
# text =  Используя новый алгоритм упаковки, в Graphcore ускорили обработку естественного языка более чем в 2 раза при обучении BERT-Large.
1	Используя O
2	новый O
3	алгоритм I-TERM
4	упаковки I-TERM
5	, O
6	в O
7	Graphcore I-TERM
8	ускорили O
9	обработку O
10	естественного O
11	языка O
12	более O
13	чем O
14	в O
15	2 O
16	раза O
17	при O
18	обучении O
19	BERT I-TERM
20	- I-TERM
21	Large I-TERM
22	. O

# sent_id = 315
# text =  В Graphcore предполагают, что алгоритм упаковки также может применяться в геномике, в моделях фолдинга белков и других моделях с перекошенным распределением длины, оказывая гораздо более широкое влияние на различные отрасли и приложения.
1	В O
2	Graphcore I-TERM
3	предполагают O
4	, O
5	что O
6	алгоритм I-TERM
7	упаковки I-TERM
8	также O
9	может O
10	применяться O
11	в O
12	геномике I-TERM
13	, O
14	в O
15	моделях I-TERM
16	фолдинга I-TERM
17	белков O
18	и O
19	других O
20	моделях O
21	с O
22	перекошенным O
23	распределением O
24	длины O
25	, O
26	оказывая O
27	гораздо O
28	более O
29	широкое O
30	влияние O
31	на O
32	различные O
33	отрасли O
34	и O
35	приложения O
36	. O

# sent_id = 316
# text =  В новой работе Graphcore представили высокоэффективный алгоритм гистограммной упаковки с неотрицательными наименьшими квадратами (или NNLSHP), а также алгоритм BERT, применяемый к упакованным последовательностям.
1	В O
2	новой O
3	работе O
4	Graphcore I-TERM
5	представили O
6	высокоэффективный O
7	алгоритм I-TERM
8	гистограммной I-TERM
9	упаковки I-TERM
10	с O
11	неотрицательными O
12	наименьшими O
13	квадратами O
14	( O
15	или O
16	NNLSHP I-TERM) O
17	, O
18	а O
19	также O
20	алгоритм O
21	BERT I-TERM
22	, O
23	применяемый O
24	к O
25	упакованным O
26	последовательностям O
27	. O

# sent_id = 317
# text =  Сбер создал и опубликовал в открытом доступе программную библиотеку PyTorch-LifeStream, содержащую несколько алгоритмов построения эмбеддингов событийных данных.
1	Сбер I-TERM
2	создал O
3	и O
4	опубликовал O
5	в O
6	открытом O
7	доступе O
8	программную O
9	библиотеку O
10	PyTorch I-TERM
11	- I-TERM
12	LifeStream I-TERM
13	, O
14	содержащую O
15	несколько O
16	алгоритмов O
17	построения O
18	эмбеддингов O
19	событийных O
20	данных O
21	. O

# sent_id = 318
# text =  Эмбеддинг (Embedding) – преобразования сложноструктурированных данных,  например слов, текстов, атрибутов событий, событий и их последовательностей, в машинно-читаемый набор чисел – числовой вектор.Событийные данные – разные последовательности.
1	Эмбеддинг I-TERM
2	( O
3	Embedding I-TERM
4	) O
5	– O
6	преобразования O
7	сложноструктурированных O
8	данных O
9	, O
10	   O
11	например O
12	слов O
13	, O
14	текстов O
15	, O
16	атрибутов O
17	событий O
18	, O
19	событий O
20	и O
21	их O
22	последовательностей O
23	, O
24	в O
25	машинно O
26	- O
27	читаемый O
28	набор O
29	чисел O
30	– O
31	числовой O
32	вектор O
33	. O
34	Событийные O
35	данные O
36	– O
37	разные O
38	последовательности O
39	. O

# sent_id = 319
# text =  Самой популярной из трёх задач соревнования стала главная – Matching.
1	Самой O
2	популярной O
3	из O
4	трёх O
5	задач O
6	соревнования O
7	стала O
8	главная O
9	– O
10	Matching I-TERM
11	. O

# sent_id = 320
# text =  Стоит отметить, что и для них всё было непросто – конкурсная задача матчинга позволила удачно применить разработанный в Лаборатории ИИ метод генерации эмбеддингов транзакционных данных одновременно для двух разных доменов событийных данных (транзакции и кликстрим – атрибуты посещения веб-страниц). 
1	Стоит O
2	отметить O
3	, O
4	что O
5	и O
6	для O
7	них O
8	всё O
9	было O
10	непросто O
11	– O
12	конкурсная O
13	задача O
14	матчинга O
15	позволила O
16	удачно O
17	применить O
18	разработанный O
19	в O
20	Лаборатории I-TERM
21	ИИ I-TERM
22	метод I-TERM
23	генерации I-TERM
24	эмбеддингов I-TERM
25	транзакционных I-TERM
26	данных I-TERM
27	одновременно O
28	для O
29	двух O
30	разных O
31	доменов O
32	событийных O
33	данных O
34	( O
35	транзакции I-TERM
36	и O
37	кликстрим I-TERM
38	– O
39	атрибуты O
40	посещения O
41	веб O
42	- O
43	страниц O
44	) O
45	. O

# sent_id = 321
# text =  Победители создали лучшее решение благодаря применению собственной библиотеки PyTorch-LifeStream, которая позволила ускорить разработку решения, так как содержит много готовых инструментов для работы с событийными данными, и дала возможность стать фаворитом престижного конкурса. 
1	Победители O
2	создали O
3	лучшее O
4	решение O
5	благодаря O
6	применению O
7	собственной O
8	библиотеки O
9	PyTorch I-TERM
10	- I-TERM
11	LifeStream I-TERM
12	, O
13	которая O
14	позволила O
15	ускорить O
16	разработку O
17	решения O
18	, O
19	так O
20	как O
21	содержит O
22	много O
23	готовых O
24	инструментов O
25	для O
26	работы O
27	с O
28	событийными O
29	данными O
30	, O
31	и O
32	дала O
33	возможность O
34	стать O
35	фаворитом O
36	престижного O
37	конкурса O

# sent_id = 322
# text =  Фичи для транзакций и кликов объединялись и подавались в алгоритм бустинга.
1	Фичи O
2	для O
3	транзакций O
4	и O
5	кликов O
6	объединялись O
7	и O
8	подавались O
9	в O
10	алгоритм O
11	бустинга I-TERM
12	. O

# sent_id = 323
# text =  Алгоритм обучался как задача бинарной классификации.
1	Алгоритм O
2	обучался O
3	как O
4	задача I-TERM
5	бинарной I-TERM
6	классификации I-TERM
7	. O

# sent_id = 324
# text =  Команда решила использовать схему обучения, похожую на сиамскую сеть.
1	Команда O
2	решила O
3	использовать O
4	схему O
5	обучения O
6	, O
7	похожую O
8	на O
9	сиамскую I-TERM
10	сеть I-TERM
11	. O

# sent_id = 325
# text =  Метки использованы для выборки положительных и отрицательных образцов для функции потерь Softmax Loss.
1	Метки O
2	использованы O
3	для O
4	выборки O
5	положительных O
6	и O
7	отрицательных O
8	образцов O
9	для O
10	функции O
11	потерь O
12	Softmax I-TERM
13	Loss I-TERM
14	. O

# sent_id = 326
# text =  SequenceEncoder – рекурентно-нейронная сеть (RNN), совместно используемая для транзакций и кликов.
1	SequenceEncoder I-TERM
2	– O
3	рекурентно I-TERM
4	- I-TERM
5	нейронная I-TERM
6	сеть I-TERM
7	( O
8	RNN I-TERM
9	) O
10	, O
11	совместно O
12	используемая O
13	для O
14	транзакций O
15	и O
16	кликов O
17	. O

# sent_id = 327
# text =  В итоге это дало самый большой прирост качества: для ансамбля из пяти моделей метрика качества R1 выросла с 0.2819 до 0.2949.
1	В O
2	итоге O
3	это O
4	дало O
5	самый O
6	большой O
7	прирост O
8	качества O
9	: O
10	для O
11	ансамбля O
12	из O
13	пяти O
14	моделей O
15	метрика O
16	качества O
17	R1 I-TERM
18	выросла O
19	с O
20	0.2819 I-TERM
21	до O
22	0.2949 I-TERM
23	. O

# sent_id = 328
# text =  Решать задачу будем с использованием нейронных сетей, но оптимизируемых генетическим алгоритмом (ГА) – такой процесс называют нейроэволюцией.
1	Решать O
2	задачу O
3	будем O
4	с O
5	использованием O
6	нейронных O
7	сетей O
8	, O
9	но O
10	оптимизируемых O
11	генетическим I-TERM
12	алгоритмом I-TERM
13	( O
14	ГА I-TERM
15	) O
16	– O
17	такой O
18	процесс O
19	называют O
20	нейроэволюцией O
21	. O

# sent_id = 329
# text =  Мы воспользовались методом NEAT (NeuroEvolution of Augmenting Topologies), изобретенным Кеннетом Стенли и Ристо Мииккулайненом в начале века [1]: во-первых, он хорошо зарекомендовал себя в важных для народного хозяйства проблемах, во-вторых, к началу работы над проектом у нас уже был свой фреймворк, реализующий NEAT.
1	Мы O
2	воспользовались O
3	методом O
4	NEAT I-TERM
5	( O
6	NeuroEvolution I-TERM
7	of I-TERM
8	Augmenting I-TERM
9	Topologies I-TERM
10	) O
11	, O
12	изобретенным O
13	Кеннетом I-TERM
14	Стенли I-TERM
15	и O
16	Ристо I-TERM
17	Мииккулайненом I-TERM
18	в O
19	начале O
20	века O
21	[ O
22	1 O
23	] O
24	: O
25	во O
26	- O
27	первых O
28	, O
29	он O
30	хорошо O
31	зарекомендовал O
32	себя O
33	в O
34	важных O
35	для O
36	народного O
37	хозяйства O
38	проблемах O
39	, O
40	во O
41	- O
42	вторых O
43	, O
44	к O
45	началу O
46	работы O
47	над O
48	проектом O
49	у O
50	нас O
51	уже O
52	был O
53	свой O
54	фреймворк O
55	, O
56	реализующий O
57	NEAT I-TERM
58	. O

# sent_id = 330
# text =  Строго говоря, нам нужно получить параллельный корпус из двух текстов.
1	Строго O
2	говоря O
3	, O
4	нам O
5	нужно O
6	получить O
7	параллельный I-TERM
8	корпус I-TERM
9	из O
10	двух O
11	текстов O
12	. O

# sent_id = 331
# text =  Для выравнивания воспользуемся библиотекой lingtrain-aligner, над которой я работаю около года и которая родилась из кучи скриптов на python, часть из которых еще ждет своего часа.
1	Для O
2	выравнивания O
3	воспользуемся O
4	библиотекой O
5	lingtrain I-TERM
6	- I-TERM
7	aligner I-TERM
8	, O
9	над O
10	которой O
11	я O
12	работаю O
13	около O
14	года O
15	и O
16	которая O
17	родилась O
18	из O
19	кучи O
20	скриптов O
21	на O
22	python O
23	, O
24	часть O
25	из O
26	которых O
27	еще O
28	ждет O
29	своего O
30	часа O
31	. O

# sent_id = 332
# text =  Хорошим решением мне видится регрессия на координаты строк при выравнивании батча и сдвиг окна на конец потока при выравнивании следующего.
1	Хорошим O
2	решением O
3	мне O
4	видится O
5	регрессия I-TERM
6	на O
7	координаты O
8	строк O
9	при O
10	выравнивании I-TERM
11	батча O
12	и O
13	сдвиг O
14	окна O
15	на O
16	конец O
17	потока O
18	при O
19	выравнивании O
20	следующего O
21	. O

# sent_id = 333
# text =  Facebook представила систему распознавания речи wav2vec-U.
1	Facebook I-TERM
2	представила O
3	систему O
4	распознавания I-TERM
5	речи I-TERM
6	wav2vec I-TERM
7	- O
8	U O

# sent_id = 334
# text =  Система разбивает запись на речевые единицы, которые приблизительно соответствуют отдельным звукам.
1	Система O
2	разбивает O
3	запись O
4	на O
5	речевые I-TERM
6	единицы I-TERM
7	, O
8	которые O
9	приблизительно O
10	соответствуют O
11	отдельным O
12	звукам O
13	. O

# sent_id = 335
# text =  Чтобы научиться распознавать слова в аудиозаписи, Facebook обучила генеративную состязательную сеть (GAN).
1	Чтобы O
2	научиться O
3	распознавать O
4	слова O
5	в O
6	аудиозаписи O
7	, O
8	Facebook I-TERM
9	обучила O
10	генеративную I-TERM
11	состязательную I-TERM
12	сеть I-TERM
13	( O
14	GAN I-TERM
15	) O
16	. O

# sent_id = 336
# text =  Генератор берет каждый аудиосегмент и предсказывает фонему, соответствующую звуку на языке.
1	Генератор O
2	берет O
3	каждый O
4	аудиосегмент O
5	и O
6	предсказывает O
7	фонему I-TERM
8	, O
9	соответствующую O
10	звуку O
11	на O
12	языке O
13	. O

# sent_id = 337
# text =  Новая модель распознавания речи Facebook AI — это последняя разработка за несколько лет работы над моделями распознавания речи.
1	Новая O
2	модель O
3	распознавания I-TERM
4	речи I-TERM
5	Facebook I-TERM
6	AI I-TERM
7	— O
8	это O
9	последняя O
10	разработка O
11	за O
12	несколько O
13	лет O
14	работы O
15	над O
16	моделями O
17	распознавания O
18	речи O
19	. O

# sent_id = 338
# text =  Ее предшественниками стали wav2letter, wav2vec, Librilight, wav2vec 2.0, XLSR и wav2vec 2.0.
1	ее O
2	предшественниками O
3	стали O
4	wav2letter I-TERM
5	, O
6	wav2vec I-TERM
7	, O
8	Librilight I-TERM
9	, O
10	wav2vec I-TERM
11	2.0 I-TERM
12	, O
13	XLSR I-TERM
14	и O
15	wav2vec I-TERM
16	2.0 I-TERM
17	. O

# sent_id = 339
# text =  В полку LLM прибыло: недавно специалисты из Французского национального центра научных исследований (French National Center for Scientific Research) объявили о релизе новой большой языковой модели под названием BLOOM (расшифровывается как BigScience Large Open-science Open-access Multilingual Language Model).
1	В O
2	полку O
3	LLM I-TERM
4	прибыло O
5	: O
6	недавно O
7	специалисты O
8	из O
9	Французского I-TERM
10	национального I-TERM
11	центра I-TERM
12	научных I-TERM
13	исследований I-TERM
14	( O
15	French I-TERM
16	National I-TERM
17	Center I-TERM
18	for I-TERM
19	Scientific I-TERM
20	Research I-TERM
21	) O
22	объявили O
23	о O
24	релизе O
25	новой O
26	большой O
27	языковой O
28	модели O
29	под O
30	названием O
31	BLOOM I-TERM
32	( O
33	расшифровывается O
34	как O
35	BigScience I-TERM
36	Large I-TERM
37	Open I-TERM
38	- I-TERM
39	science I-TERM
40	Open I-TERM
41	- I-TERM
42	access I-TERM
43	Multilingual I-TERM
44	Language I-TERM
45	Model I-TERM
46	) O
47	. O

# sent_id = 340
# text =  Большие языковые модели или LLM (Large Language Models) — это алгоритмы глубокого обучения, которые обучаются на огромных объемах данных.
1	Большие O
2	языковые O
3	модели O
4	или O
5	LLM I-TERM
6	( O
7	Large I-TERM
8	Language I-TERM
9	Models I-TERM
10	) O
11	— O
12	это O
13	алгоритмы O
14	глубокого O
15	обучения O
16	, O
17	которые O
18	обучаются O
19	на O
20	огромных O
21	объемах O
22	данных O
23	. O

# sent_id = 341
# text =  Их можно использовать в качестве чат-ботов, для поиска информации, модерации онлайн-контента, анализа литературы или для создания совершенно новых фрагментов текста на основе подсказок (чем занимается, например, «Порфирьевич», который способен генерировать весьма забавные короткие рассказы).
1	Их O
2	можно O
3	использовать O
4	в O
5	качестве O
6	чат I-TERM
7	- I-TERM
8	ботов I-TERM
9	, O
10	для O
11	поиска I-TERM
12	информации I-TERM
13	, O
14	модерации I-TERM
15	онлайн I-TERM
16	- I-TERM
17	контента I-TERM
18	, O
19	анализа I-TERM
20	литературы I-TERM
21	или O
22	для O
23	создания I-TERM
24	совершенно I-TERM
25	новых I-TERM
26	фрагментов I-TERM
27	текста I-TERM
28	на O
29	основе O
30	подсказок O
31	( O
32	чем O
33	занимается O
34	, O
35	например O
36	, O
37	« O
38	Порфирьевич I-TERM
39	» O
40	, O
41	который O
42	способен O
43	генерировать O
44	весьма O
45	забавные O
46	короткие O
47	рассказы O
48	) O
49	. O

# sent_id = 342
# text =  Новая LLM с открытым исходным кодом в отличие от таких известных LLM, как GPT-3 от OpenAI и LaMDA от Google, BLOOM является открытой языковой моделью, а исследователи охотно делятся подробностями о тех данных, на которых она обучалась, рассказывают о проблемах в ее разработке и о том, как они оценивали производительность BLOOM.
1	Новая O
2	LLM I-TERM
3	с O
4	открытым O
5	исходным O
6	кодом O
7	в O
8	отличие O
9	от O
10	таких O
11	известных O
12	LLM I-TERM
13	, O
14	как O
15	GPT-3 I-TERM
16	от O
17	OpenAI I-TERM
18	и O
19	LaMDA I-TERM
20	от O
21	Google I-TERM
22	, O
23	BLOOM I-TERM
24	является O
25	открытой O
26	языковой O
27	моделью O
28	, O
29	а O
30	исследователи O
31	охотно O
32	делятся O
33	подробностями O
34	о O
35	тех O
36	данных O
37	, O
38	на O
39	которых O
40	она O
41	обучалась O
42	, O
43	рассказывают O
44	о O
45	проблемах O
46	в O
47	ее O
48	разработке O
49	и O
50	о O
51	том O
52	, O
53	как O
54	они O
55	оценивали O
56	производительность O
57	BLOOM I-TERM
58	. O

# sent_id = 343
# text =  OpenAI и Google не делились своим кодом и не делали свои модели общедоступными.
1	OpenAI I-TERM
2	и O
3	Google I-TERM
4	не O
5	делились O
6	своим O
7	кодом O
8	и O
9	не O
10	делали O
11	свои O
12	модели O
13	общедоступными O
14	. O

# sent_id = 344
# text =  И уже сейчас над BLOOM работают более тысячи исследователей-добровольцев в рамках проекта под названием BigScience, который координирует стартап Hugging Face, существующий за счет финансовой поддержки французского правительства.
1	И O
2	уже O
3	сейчас O
4	над O
5	BLOOM I-TERM
6	работают O
7	более O
8	тысячи O
9	исследователей O
10	- O
11	добровольцев O
12	в O
13	рамках O
14	проекта O
15	под O
16	названием O
17	BigScience I-TERM
18	, O
19	который O
20	координирует O
21	стартап O
22	Hugging I-TERM
23	Face I-TERM
24	, O
25	существующий O
26	за O
27	счет O
28	финансовой O
29	поддержки O
30	французского O
31	правительства O
32	. O

# sent_id = 345
# text =  BLOOM может обрабатывать 46 языков, включая французский, испанский, арабский, вьетнамский, китайский, индонезийский, каталанский, целых 13 языков Индии (хинди, бенгали, маратхи и ряд других) и аж 20 африканских.
1	BLOOM I-TERM
2	может O
3	обрабатывать O
4	46 O
5	языков O
6	, O
7	включая O
8	французский I-TERM
9	, O
10	испанский I-TERM
11	, O
12	арабский I-TERM
13	, O
14	вьетнамский I-TERM
15	, O
16	китайский I-TERM
17	, O
18	индонезийский I-TERM
19	, O
20	каталанский I-TERM
21	, O
22	целых O
23	13 O
24	языков O
25	Индии O
26	( O
27	хинди I-TERM
28	, O
29	бенгали I-TERM
30	, O
31	маратхи I-TERM
32	и O
33	ряд O
34	других O
35	) O
36	и O
37	аж O
38	20 O
39	африканских O
40	. O

# sent_id = 346
# text =  На русском BLOOM тоже пишет, но пока довольно вяло.
1	На O
2	русском I-TERM
3	тоже O
4	пишет O
5	, O
6	но O
7	пока O
8	довольно O
9	вяло O
10	. O

# sent_id = 347
# text =  Почти треть обучающих данных была введена в модель BLOOM на английском языке: следствие того, что именно английский является наиболее часто используемым языком в интернете.
1	Почти O
2	треть O
3	обучающих O
4	данных O
5	была O
6	введена O
7	в O
8	модель O
9	BLOOM I-TERM
10	на O
11	английском I-TERM
12	языке O
13	: O
14	следствие O
15	того O
16	, O
17	что O
18	именно O
19	английский O
20	является O
21	наиболее O
22	часто O
23	используемым O
24	языком O
25	в O
26	интернете O
27	. O

# sent_id = 348
# text =  В июне текущего года инженер Google Блейк Лемуан (он на фото выше) ошарашил мировую общественность заявлением, что LLM LaMDA, над которой он работал вместе с другими программистами, может обладать некоторым подобием разума.
1	В O
2	июне O
3	текущего O
4	года O
5	инженер O
6	Google I-TERM
7	Блейк I-TERM
8	Лемуан I-TERM
9	( O
10	он O
11	на O
12	фото O
13	выше O
14	) O
15	ошарашил O
16	мировую O
17	общественность O
18	заявлением O
19	, O
20	что O
21	LLM I-TERM
22	LaMDA I-TERM
23	, O
24	над O
25	которой O
26	он O
27	работал O
28	вместе O
29	с O
30	другими O
31	программистами O
32	, O
33	может O
34	обладать O
35	некоторым O
36	подобием O
37	разума O
38	. O

# sent_id = 349
# text =  Американский ученый и исследователь ИИ Гэри Маркус еще до появления в сети откровений Лемуана опубликовал на портале Scientific American материал под названием «Общий ИИ не так неизбежен, как вы думаете».
1	Американский O
2	ученый O
3	и O
4	исследователь O
5	ИИ I-TERM
6	Гэри I-TERM
7	Маркус I-TERM
8	еще O
9	до O
10	появления O
11	в O
12	сети O
13	откровений O
14	Лемуана I-TERM
15	опубликовал O
16	на O
17	портале O
18	Scientific I-TERM
19	American I-TERM
20	материал O
21	под O
22	названием O
23	« I-TERM
24	Общий I-TERM
25	ИИ I-TERM
26	не I-TERM
27	так I-TERM
28	неизбежен I-TERM
29	, I-TERM
30	как I-TERM
31	вы I-TERM
32	думаете I-TERM
33	» I-TERM
34	. O

# sent_id = 350
# text =  В частности, DALL-E 2 от OpenAI провалил тест на различение изображений астронавтов, едущих на лошадях, перепутав их с лошадьми, оседлавшими астронавтов.
1	В O
2	частности O
3	, O
4	DALL I-TERM
5	- I-TERM
6	E I-TERM
7	2 I-TERM
8	от O
9	OpenAI I-TERM
10	провалил O
11	тест O
12	на O
13	различение O
14	изображений O
15	астронавтов O
16	, O
17	едущих O
18	на O
19	лошадях O
20	, O
21	перепутав O
22	их O
23	с O
24	лошадьми O
25	, O
26	оседлавшими O
27	астронавтов O
28	. O

# sent_id = 351
# text =  «Сбер» представил mGPT — версию нейросети GPT-3, способную генерировать тексты на 61 языке Open source *Machine learning *Artificial Intelligence IT-companies       
1	« O
2	Сбер I-TERM
3	» O
4	представил O
5	mGPT I-TERM
6	— O
7	версию O
8	нейросети O
9	GPT-3 I-TERM
10	, O
11	способную O
12	генерировать O
13	тексты O
14	на O
15	61 O
16	языке O

# sent_id = 352
# text =  21 апреля 2022 года команда разработчиков SberDevices представила многоязычную версию нейросети GPT-3 под названием mGPT.
1	21 I-TERM
2	апреля I-TERM
3	2022 I-TERM
4	года O
5	команда O
6	разработчиков O
7	SberDevices I-TERM
8	представила O
9	многоязычную O
10	версию O
11	нейросети O
12	GPT-3 I-TERM
13	под O
14	названием O
15	mGPT I-TERM
16	. O

# sent_id = 353
# text =  «Сбер» рассказал, что модель mGPT может использоваться как просто для генерации текста, так и для решения различных задач в области обработки естественного языка на одном из поддерживаемых языков путем дообучения или в составе ансамблей моделей.
1	« O
2	Сбер I-TERM
3	» O
4	рассказал O
5	, O
6	что O
7	модель O
8	mGPT I-TERM
9	может O
10	использоваться O
11	как O
12	просто O
13	для O
14	генерации I-TERM
15	текста I-TERM
16	, O
17	так O
18	и O
19	для O
20	решения O
21	различных O
22	задач O
23	в O
24	области O
25	обработки I-TERM
26	естественного I-TERM
27	языка I-TERM
28	на O
29	одном O
30	из O
31	поддерживаемых O
32	языков O
33	путем O
34	дообучения O
35	или O
36	в O
37	составе O
38	ансамблей I-TERM
39	моделей I-TERM
40	. O

# sent_id = 354
# text =  Разработчики уточнили, что модель mGPT показывает выдающиеся результаты на многих задачах few-shot и zero-shot learning: в этой области машинного обучения не требуется отдельно доучивать модель, достаточно сформулировать задачу текстом и привести несколько примеров, после чего mGPT научится выполнять новую задачу.
1	Разработчики O
2	уточнили O
3	, O
4	что O
5	модель O
6	mGPT I-TERM
7	показывает O
8	выдающиеся O
9	результаты O
10	на O
11	многих O
12	задачах O
13	few I-TERM
14	- I-TERM
15	shot I-TERM
16	и O
17	zero I-TERM
18	- I-TERM
19	shot I-TERM
20	learning I-TERM
21	: O
22	в O
23	этой O
24	области O
25	машинного O
26	обучения O
27	не O
28	требуется O
29	отдельно O
30	доучивать O
31	модель O
32	, O
33	достаточно O
34	сформулировать O
35	задачу O
36	текстом O
37	и O
38	привести O
39	несколько O
40	примеров O
41	, O
42	после O
43	чего O
44	mGPT I-TERM
45	научится O
46	выполнять O
47	новую O
48	задачу O
49	. O

# sent_id = 355
# text =  Это может использоваться для того, чтобы научить автоматизированную систему отвечать на вопросы, определять эмоциональную окраску текста, извлекать из текста имена, фамилии, названия компаний и тому подобное.
1	Это O
2	может O
3	использоваться O
4	для O
5	того O
6	, O
7	чтобы O
8	научить O
9	автоматизированную O
10	систему O
11	отвечать O
12	на O
13	вопросы O
14	, O
15	определять I-TERM
16	эмоциональную I-TERM
17	окраску I-TERM
18	текста I-TERM
19	, O
20	извлекать O
21	из O
22	текста O
23	имена O
24	, O
25	фамилии O
26	, O
27	названия O
28	компаний O
29	и O
30	тому O
31	подобное O
32	. O

# sent_id = 356
# text =  «Сбер» раскрыл, что модель mGPT может также использоваться как компонент различных речевых технологий — например, для улучшения качества распознавания речи, генерации сценариев диалоговых систем и других задачах.
1	« O
2	Сбер I-TERM
3	» O
4	раскрыл O
5	, O
6	что O
7	модель O
8	mGPT I-TERM
9	может O
10	также O
11	использоваться O
12	как O
13	компонент O
14	различных O
15	речевых I-TERM
16	технологий I-TERM
17	— O
18	например O
19	, O
20	для O
21	улучшения I-TERM
22	качества I-TERM
23	распознавания I-TERM
24	речи I-TERM
25	, O
26	генерации I-TERM
27	сценариев I-TERM
28	диалоговых I-TERM
29	систем I-TERM
30	и O
31	других O
32	задачах O
33	. O

# sent_id = 357
# text =  Полный перечень языков, доступный в модели mGPT: азербайджанский, английский, арабский, армянский, африкаанс, баскский, башкирский, белорусский, бенгали, бирманский, болгарский, бурятский, венгерский, вьетнамский, голландский, греческий, грузинский, датский, иврит, индонезийский, испанский, итальянский, йоруба, казахский, калмыцкий, киргизский, китайский, корейский, латышский, литовский, малайский, малаялам, маратхи, молдавский, монгольский, немецкий, осетинский, персидский, польский, португальский, румынский, русский, суахили, таджикский, тайский, тамильский, татарский, телугу, тувинский, турецкий, туркменский, узбекский, украинский, урду, финский, французский, хинди, чувашский, шведский, якутский, японский.
1	Полный O
2	перечень O
3	языков O
4	, O
5	доступный O
6	в O
7	модели O
8	mGPT I-TERM
9	: O
10	азербайджанский I-TERM
11	, O
12	английский I-TERM
13	, O
14	арабский I-TERM
15	, O
16	армянский I-TERM
17	, O
18	африкаанс I-TERM
19	, O
20	баскский I-TERM
21	, O
22	башкирский I-TERM
23	, O
24	белорусский I-TERM
25	, O
26	бенгали I-TERM
27	, O
28	бирманский I-TERM
29	, O
30	болгарский I-TERM
31	, O
32	бурятский I-TERM
33	, O
34	венгерский I-TERM
35	, O
36	вьетнамский I-TERM
37	, O
38	голландский I-TERM
39	, O
40	греческий I-TERM
41	, O
42	грузинский I-TERM
43	, O
44	датский I-TERM
45	, O
46	иврит I-TERM
47	, O
48	индонезийский I-TERM
49	, O
50	испанский I-TERM
51	, O
52	итальянский I-TERM
53	, O
54	йоруба I-TERM
55	, O
56	казахский I-TERM
57	, O
58	калмыцкий I-TERM
59	, O
60	киргизский I-TERM
61	, O
62	китайский I-TERM
63	, O
64	корейский I-TERM
65	, O
66	латышский I-TERM
67	, O
68	литовский I-TERM
69	, O
70	малайский I-TERM
71	, O
72	малаялам I-TERM
73	, O
74	маратхи I-TERM
75	, O
76	молдавский I-TERM
77	, O
78	монгольский I-TERM
79	, O
80	немецкий I-TERM
81	, O
82	осетинский I-TERM
83	, O
84	персидский I-TERM
85	, O
86	польский I-TERM
87	, O
88	португальский I-TERM
89	, O
90	румынский I-TERM
91	, O
92	русский I-TERM
93	, O
94	суахили I-TERM
95	, O
96	таджикский I-TERM
97	, O
98	тайский I-TERM
99	, O
100	тамильский I-TERM
101	, O
102	татарский I-TERM
103	, O
104	телугу I-TERM
105	, O
106	тувинский I-TERM
107	, O
108	турецкий I-TERM
109	, O
110	туркменский I-TERM
111	, O
112	узбекский I-TERM
113	, O
114	украинский I-TERM
115	, O
116	урду I-TERM
117	, O
118	финский I-TERM
119	, O
120	французский I-TERM
121	, O
122	хинди I-TERM
123	, O
124	чувашский I-TERM
125	, O
126	шведский I-TERM
127	, O
128	якутский I-TERM
129	, O
130	японский I-TERM
131	. O

# sent_id = 358
# text =  В 2020 году «Сбер» представил русскоязычную версию нейросети GPT-3, именно она используется в двух виртуальных ассистентах семейства «Салют» от «Сбера».
1	В O
2	2020 I-TERM
3	году O
4	« O
5	Сбер I-TERM
6	» O
7	представил O
8	русскоязычную I-TERM
9	версию O
10	нейросети O
11	GPT-3 I-TERM
12	, O
13	именно O
14	она O
15	используется O
16	в O
17	двух O
18	виртуальных O
19	ассистентах O
20	семейства O
21	« O
22	Салют I-TERM
23	» O
24	от O
25	« O
26	Сбера I-TERM
27	» O
28	. O

# sent_id = 359
# text =  Русскоязычная версия GPT-3, разработанная «Сбером», доступна на платформе SmartMarket.
1	Русскоязычная O
2	версия O
3	GPT-3 I-TERM
4	, O
5	разработанная O
6	« O
7	Сбером I-TERM
8	» O
9	, O
10	доступна O
11	на O
12	платформе O
13	SmartMarket I-TERM
14	. O

# sent_id = 360
# text =  В ноябре 2021 года «Сбер» обучил нейросеть ruGPT-3 автоматически писать код и назвал эту функцию JARVIS.
1	В O
2	ноябре O
3	2021 I-TERM
4	года O
5	« O
6	Сбер I-TERM
7	» O
8	обучил O
9	нейросеть O
10	ruGPT-3 I-TERM
11	автоматически O
12	писать O
13	код O
14	и O
15	назвал O
16	эту O
17	функцию O
18	JARVIS I-TERM
19	. O

# sent_id = 361
# text =  Не заблокированы: Sber AI — на GitHub; ruDALL-E — на GitHub; Russian GPT-3 models — GitHub.
1	Не O
2	заблокированы O
3	: O
4	Sber I-TERM
5	AI I-TERM
6	— O
7	на O
8	GitHub I-TERM
9	; O
10	ruDALL I-TERM
11	- I-TERM
12	E I-TERM
13	— O
14	на O
15	GitHub I-TERM
16	; O
17	Russian I-TERM
18	GPT-3 I-TERM
19	models I-TERM
20	— O
21	GitHub I-TERM
22	. O

# sent_id = 362
# text =  Заблокированы: большая часть ссылок на открытом портале Open Source от разработчиков «Сбера»; SberDevices; Sberbank AI Lab; Open source software developed by Sberbank-Technology.
1	Заблокированы O
2	: O
3	большая O
4	часть O
5	ссылок O
6	на O
7	открытом O
8	портале O
9	Open I-TERM
10	Source I-TERM
11	от O
12	разработчиков O
13	« O
14	Сбера I-TERM
15	» O
16	; O
17	SberDevices I-TERM
18	; O
19	Sberbank I-TERM
20	AI I-TERM
21	Lab I-TERM
22	; O
23	Open I-TERM
24	source I-TERM
25	software I-TERM
26	developed I-TERM
27	by I-TERM
28	Sberbank I-TERM
29	- I-TERM
30	Technology I-TERM
31	. O

# sent_id = 363
# text =  В этой статье мы расскажем о методе Propensity Score Adjustment, который применим для коррекции смещений и улучшения данных, полученных на онлайн-панелях.
1	В O
2	этой O
3	статье O
4	мы O
5	расскажем O
6	о O
7	методе I-TERM
8	Propensity I-TERM
9	Score I-TERM
10	Adjustment I-TERM
11	, O
12	который O
13	применим O
14	для O
15	коррекции O
16	смещений O
17	и O
18	улучшения O
19	данных O
20	, O
21	полученных O
22	на O
23	онлайн O
24	- O
25	панелях O
26	. O

# sent_id = 364
# text =  Итоговые коэффициенты, корректирующие смещение онлайн-выборки, можно рассчитать по методу Хорвица-Томпсона.
1	Итоговые O
2	коэффициенты O
3	, O
4	корректирующие O
5	смещение O
6	онлайн O
7	- O
8	выборки O
9	, O
10	можно O
11	рассчитать O
12	по O
13	методу I-TERM
14	Хорвица I-TERM
15	- I-TERM
16	Томпсона I-TERM
17	. O

# sent_id = 365
# text =  Взвешивание (Weighting) - метод предназначен для коррекции известных перекосов выборок по социально-демографическим атрибутам.
1	Взвешивание I-TERM
2	( O
3	Weighting I-TERM
4	) O
5	метод O
6	предназначен O
7	для O
8	коррекции O
9	известных O
10	перекосов O
11	выборок O
12	по O
13	социально O
14	- O
15	демографическим O
16	атрибутам O
17	. O

# sent_id = 366
# text =  Авторы оригинального исследования Pew Research рекомендуют использовать для корректировки онлайн-опросов модели случайных лесов (random forest).
1	Авторы O
2	оригинального O
3	исследования O
4	Pew I-TERM
5	Research I-TERM
6	рекомендуют O
7	использовать O
8	для O
9	корректировки O
10	онлайн O
11	- O
12	опросов O
13	модели O
14	случайных I-TERM
15	лесов I-TERM
16	( O
17	random I-TERM
18	forest I-TERM
19	) O
20	. O

# sent_id = 367
# text =  Сейчас стандарт коррекции онлайн-выборок находится на стадии обсуждения и разработки и метод Propensity Score Adjustment, который мы рассмотрели, может стать общепринятым способом коррекции онлайн-панелей.
1	Сейчас O
2	стандарт O
3	коррекции O
4	онлайн O
5	- O
6	выборок O
7	находится O
8	на O
9	стадии O
10	обсуждения O
11	и O
12	разработки O
13	и O
14	метод I-TERM
15	Propensity I-TERM
16	Score I-TERM
17	Adjustment I-TERM
18	, O
19	который O
20	мы O
21	рассмотрели O
22	, O
23	может O
24	стать O
25	общепринятым O
26	способом O
27	коррекции O
28	онлайн O
29	- O
30	панелей O
31	. O

# sent_id = 368
# text =  В данной статье мы будем использовать модель трансформера для бинарной классификации текста.
1	В O
2	данной O
3	статье O
4	мы O
5	будем O
6	использовать O
7	модель I-TERM
8	трансформера I-TERM
9	для O
10	бинарной I-TERM
11	классификации I-TERM
12	текста I-TERM
13	. O

# sent_id = 369
# text =  Самая простая и популярная связка – TF-IDF + линейная модель.
1	Самая O
2	простая O
3	и O
4	популярная O
5	связка O
6	– O
7	TF I-TERM
8	- I-TERM
9	IDF I-TERM
10	+ O
11	линейная I-TERM
12	модель I-TERM
13	. O

# sent_id = 370
# text =  В случае с BERT можно (даже нужно) опустить препроцессинг и сразу перейти к токенизации и обучению.
1	В O
2	случае O
3	с O
4	BERT I-TERM
5	можно O
6	( O
7	даже O
8	нужно O
9	) O
10	опустить O
11	препроцессинг I-TERM
12	и O
13	сразу O
14	перейти O
15	к O
16	токенизации I-TERM
17	и O
18	обучению O
19	. O

# sent_id = 371
# text =  Необходимо обучить модель находить обращения с жалобой на сотрудника или другими словами – бинарная классификация.
1	Необходимо O
2	обучить O
3	модель O
4	находить O
5	обращения O
6	с O
7	жалобой O
8	на O
9	сотрудника O
10	или O
11	другими O
12	словами O
13	– O
14	бинарная I-TERM
15	классификация I-TERM
16	. O

# sent_id = 372
# text = Для решения описанной задачи используется модель от DeepPavlov rubert-base-cased-sentence.
1	Для O
2	решения O
3	описанной O
4	задачи O
5	используется O
6	модель O
7	от O
8	DeepPavlov I-TERM
9	rubert I-TERM
10	- I-TERM
11	base I-TERM
12	- I-TERM
13	cased I-TERM
14	- I-TERM
15	sentence I-TERM
16	. O

# sent_id = 373
# text =  На выходе мы получаем метрику f1 = 0.91 Посмотрим, как модель классифицировала данные показанные в начале статьи.
1	На O
2	выходе O
3	мы O
4	получаем O
5	метрику O
6	f1 I-TERM
7	= O
8	0.91 I-TERM
9	Посмотрим O
10	, O
11	как O
12	модель O
13	классифицировала O
14	данные O
15	показанные O
16	в O
17	начале O
18	статьи O
19	. O

# sent_id = 374
# text =  Обученные модели можно найти на сайтах HuggingFace и DeepPavlov.
1	Обученные O
2	модели O
3	можно O
4	найти O
5	на O
6	сайтах O
7	HuggingFace I-TERM
8	и O
9	DeepPavlov I-TERM
10	. O

# sent_id = 375
# text =  Соответственно, мы приходим к стандартной задаче Machine Learning (ML) – «многоклассовая классификация».
1	Соответственно O
2	, O
3	мы O
4	приходим O
5	к O
6	стандартной O
7	задаче O
8	Machine I-TERM
9	Learning I-TERM
10	( O
11	ML I-TERM
12	) O
13	– O
14	« O
15	многоклассовая I-TERM
16	классификация I-TERM
17	» O
18	. O

# sent_id = 376
# text =  В результате данного анализа решается задача — сбор сводной аналитики по организации.
1	В O
2	результате O
3	данного O
4	анализа O
5	решается O
6	задача O
7	— O
8	сбор I-TERM
9	сводной I-TERM
10	аналитики I-TERM
11	по I-TERM
12	организации I-TERM
13	. O

# sent_id = 377
# text =  В случае многоклассовой классификации число классов должно быть более 2 и может достигать даже многих тысяч.
1	В O
2	случае O
3	многоклассовой I-TERM
4	классификации I-TERM
5	число O
6	классов O
7	должно O
8	быть O
9	более O
10	2 O
11	и O
12	может O
13	достигать O
14	даже O
15	многих O
16	тысяч O
17	. O

# sent_id = 378
# text =  Во-вторых, такого разброса тематик, связанных с техническими текстами, у нас еще не было: нейросети, переиспользование контента, автоматическое тестирование документации, встраивание текста в интерфейс, мастерство технических коммуникаций, построение процессов перевода и принципы написания документов с расчетом на их последующую локализацию.
1	Во O
2	- O
3	вторых O
4	, O
5	такого O
6	разброса O
7	тематик O
8	, O
9	связанных O
10	с O
11	техническими I-TERM
12	текстами I-TERM
13	, O
14	у O
15	нас O
16	еще O
17	не O
18	было O
19	: O
20	нейросети O
21	, O
22	переиспользование I-TERM
23	контента I-TERM
24	, O
25	автоматическое I-TERM
26	тестирование I-TERM
27	документации I-TERM
28	, O
29	встраивание I-TERM
30	текста I-TERM
31	в I-TERM
32	интерфейс I-TERM
33	, O
34	мастерство I-TERM
35	технических I-TERM
36	коммуникаций I-TERM
37	, O
38	построение I-TERM
39	процессов I-TERM
40	перевода I-TERM
41	и O
42	принципы I-TERM
43	написания I-TERM
44	документов I-TERM
45	с O
46	расчетом O
47	на O
48	их O
49	последующую O
50	локализацию O
51	. O

# sent_id = 379
# text =  Зачастую суммаризация предполагает работу с большими генеративными текстовыми моделями, куда надо «положить» все отзывы.
1	Зачастую O
2	суммаризация I-TERM
3	предполагает O
4	работу O
5	с O
6	большими O
7	генеративными I-TERM
8	текстовыми I-TERM
9	моделями I-TERM
10	, O
11	куда O
12	надо O
13	« O
14	положить O
15	» O
16	все O
17	отзывы O
18	. O

# sent_id = 380
# text =  То есть какие аспекты искать; выделять эти аспекты в отзывах; оценивать тональность высказываний.
1	То O
2	есть O
3	какие O
4	аспекты O
5	искать O
6	; O
7	выделять O
8	эти O
9	аспекты O
10	в O
11	отзывах O
12	; O
13	оценивать O
14	тональность I-TERM
15	высказываний I-TERM
16	. O

# sent_id = 381
# text =  Мы начали со внутреннего инструмента Яндекса — библиотеки регулярных выражений под названием Remorph.
1	Мы O
2	начали O
3	со O
4	внутреннего O
5	инструмента O
6	Яндекса I-TERM
7	— O
8	библиотеки I-TERM
9	регулярных I-TERM
10	выражений I-TERM
11	под O
12	названием O
13	Remorph I-TERM
14	. O

# sent_id = 382
# text =  Исследователи Массачусетского технологического университета разработали систему искусственного интеллекта, которая способна переписывать устаревшие предложения в статьях «Википедии».
1	Исследователи O
2	Массачусетского I-TERM
3	технологического I-TERM
4	университета I-TERM
5	разработали O
6	систему I-TERM
7	искусственного I-TERM
8	интеллекта I-TERM
9	, O
10	которая O
11	способна O
12	переписывать O
13	устаревшие O
14	предложения O
15	в O
16	статьях O
17	« O
18	Википедии I-TERM
19	» O
20	. O

# sent_id = 383
# text =  Расширение статей, серьезные переписывания или другие рутинные изменения, такие как обновление номеров, дат, имен и местоположений в настоящее время добровольно выполняются пользователями из разных стран.
1	Расширение I-TERM
2	статей I-TERM
3	, O
4	серьезные O
5	переписывания O
6	или O
7	другие O
8	рутинные O
9	изменения O
10	, O
11	такие O
12	как O
13	обновление O
14	номеров O
15	, O
16	дат O
17	, O
18	имен O
19	и O
20	местоположений O
21	в O
22	настоящее O
23	время O
24	добровольно O
25	выполняются O
26	пользователями O
27	из O
28	разных O
29	стран O
30	. O

# sent_id = 384
# text =  Если она видит какие-либо противоречия между этими двумя высказываниями, то использует «маску нейтральности», чтобы определить те противоречивые слова, которые нужно удалить, и те, которые обязательно нужно сохранить.
1	Если O
2	она O
3	видит O
4	какие O
5	- O
6	либо O
7	противоречия O
8	между O
9	этими O
10	двумя O
11	высказываниями O
12	, O
13	то O
14	использует O
15	« O
16	маску I-TERM
17	нейтральности I-TERM
18	» O
19	, O
20	чтобы O
21	определить O
22	те O
23	противоречивые O
24	слова O
25	, O
26	которые O
27	нужно O
28	удалить O
29	, O
30	и O
31	те O
32	, O
33	которые O
34	обязательно O
35	нужно O
36	сохранить O
37	. O

# sent_id = 385
# text =  Отмечается, что систему также можно использовать для дополнения наборов данных, предназначенных для обучения детекторов фейкньюс, что потенциально снижает предвзятость и повышает точность информации.
1	Отмечается O
2	, O
3	что O
4	систему O
5	также O
6	можно O
7	использовать O
8	для O
9	дополнения I-TERM
10	наборов I-TERM
11	данных I-TERM
12	, O
13	предназначенных O
14	для O
15	обучения I-TERM
16	детекторов I-TERM
17	фейкньюс I-TERM
18	, O
19	что O
20	потенциально O
21	снижает O
22	предвзятость O
23	и O
24	повышает O
25	точность O
26	информации O
27	. O

# sent_id = 386
# text =  Наше выработанное решение – обучить нейронную сеть, которая способна по тексту обращения автоматически распознавать заранее ранжированные по классам проблемы, извлекать сущность (номер заказа и телефон клиента) и по определённым классам сделать автоматизацию решения.
1	Наше O
2	выработанное O
3	решение O
4	– O
5	обучить O
6	нейронную O
7	сеть O
8	, O
9	которая O
10	способна O
11	по O
12	тексту O
13	обращения O
14	автоматически I-TERM
15	распознавать I-TERM
16	заранее I-TERM
17	ранжированные I-TERM
18	по I-TERM
19	классам I-TERM
20	проблемы I-TERM
21	, O
22	извлекать O
23	сущность O
24	( O
25	номер O
26	заказа O
27	и O
28	телефон O
29	клиента O
30	) O
31	и O
32	по O
33	определённым O
34	классам O
35	сделать O
36	автоматизацию O
37	решения O
38	. O

# sent_id = 387
# text =  На самом деле уже существуют продвинутые и проверенные методы ее обработки, использующие нейронные сети, с распознаванием смысла и контекста – BERT (Bidirectional Encoder Representations from Transformers).
1	На O
2	самом O
3	деле O
4	уже O
5	существуют O
6	продвинутые O
7	и O
8	проверенные O
9	методы O
10	ее O
11	обработки O
12	, O
13	использующие O
14	нейронные I-TERM
15	сети I-TERM
16	, O
17	с O
18	распознаванием O
19	смысла O
20	и O
21	контекста O
22	– O
23	BERT I-TERM
24	( O
25	Bidirectional I-TERM
26	Encoder I-TERM
27	Representations I-TERM
28	from I-TERM
29	Transformers I-TERM
30	) O
31	. O

# sent_id = 388
# text =  Перед тем как выбрать нейронные сети, мы протестировали несколько более стандартных архитектур, случайные леса и бустинг.
1	Архитектура O
2	модели O
3	Перед O
4	тем O
5	как O
6	выбрать O
7	нейронные O
8	сети O
9	, O
10	мы O
11	протестировали O
12	несколько O
13	более O
14	стандартных O
15	архитектур O
16	, O
17	случайные I-TERM
18	леса I-TERM
19	и O
20	бустинг I-TERM
21	. O

# sent_id = 389
# text =  Эта модель была обучена на огромном корпусе русскоязычного текста с двумя задачами – предсказать замаскированное слово в предложениях и предсказать, если одно из предложений следует по смыслу за вторым.
1	Эта O
2	модель O
3	была O
4	обучена O
5	на O
6	огромном O
7	корпусе O
8	русскоязычного O
9	текста O
10	с O
11	двумя O
12	задачами O
13	– O
14	предсказать I-TERM
15	замаскированное I-TERM
16	слово I-TERM
17	в I-TERM
18	предложениях I-TERM
19	и O
20	предсказать O
21	, O
22	если O
23	одно O
24	из O
25	предложений O
26	следует O
27	по O
28	смыслу O
29	за O
30	вторым O
31	. O

# sent_id = 390
# text =  Наша задача – дообучить эту языковую модель для нашего приложения (одна модель для классификации и одна – для извлечения сущности).
1	Наша O
2	задача O
3	– O
4	дообучить O
5	эту O
6	языковую O
7	модель O
8	для O
9	нашего O
10	приложения O
11	( O
12	одна O
13	модель O
14	для O
15	классификации I-TERM
16	и O
17	одна O
18	– O
19	для O
20	извлечения I-TERM
21	сущности I-TERM
22	) O
23	. O

# sent_id = 391
# text =  результат первой модели – точность 77%
1	результат O
2	первой O
3	модели O
4	– O
5	точность I-TERM
6	77% I-TERM

# sent_id = 392
# text =  Чтобы определить, какие ещё есть потенциальные классы, мы повели так называемое тематическое моделирование, используя несколько подходов: начиная от пробалистических моделей (латентное распределение Дирихле, ARTM) и всё те же нейронные сети (BERT).
1	Чтобы O
2	определить O
3	, O
4	какие O
5	ещё O
6	есть O
7	потенциальные O
8	классы O
9	, O
10	мы O
11	повели O
12	так O
13	называемое O
14	тематическое I-TERM
15	моделирование I-TERM
16	, O
17	используя O
18	несколько O
19	подходов O
20	: O
21	начиная O
22	от O
23	пробалистических I-TERM
24	моделей I-TERM
25	( O
26	латентное I-TERM
27	распределение I-TERM
28	Дирихле I-TERM
29	, O
30	ARTM I-TERM
31	) O
32	и O
33	всё O
34	те O
35	же O
36	нейронные I-TERM
37	сети I-TERM
38	( O
39	BERT I-TERM
40	) O
41	. O

# sent_id = 393
# text =  Теперь нам нужно было использовать некоторые технические способы, чтобы сделать максимально высоким качество модели, которая на новых классах давала точность 72%.
1	Теперь O
2	нам O
3	нужно O
4	было O
5	использовать O
6	некоторые O
7	технические O
8	способы O
9	, O
10	чтобы O
11	сделать O
12	максимально O
13	высоким O
14	качество O
15	модели O
16	, O
17	которая O
18	на O
19	новых O
20	классах O
21	давала O
22	точность I-TERM
23	72 I-TERM
24	% I-TERM
25	. O

# sent_id = 394
# text =  Второе, мы стандартно провели экстенсивный тюнинг гиперпараметров и изменили нашу метрику с точности на F1, чтобы ставить больше акцента на точность по каждому классу, так как общая точность предвзято относится к доминирующим классам.
1	Второе O
2	, O
3	мы O
4	стандартно O
5	провели O
6	экстенсивный I-TERM
7	тюнинг I-TERM
8	гиперпараметров I-TERM
9	и O
10	изменили O
11	нашу O
12	метрику O
13	с O
14	точности I-TERM
15	на O
16	F1 I-TERM
17	, O
18	чтобы O
19	ставить O
20	больше O
21	акцента O
22	на O
23	точность I-TERM
24	по O
25	каждому O
26	классу O
27	, O
28	так O
29	как O
30	общая O
31	точность I-TERM
32	предвзято O
33	относится O
34	к O
35	доминирующим O
36	классам O
37	. O

# sent_id = 395
# text =  Изменение оптимизирующей метрики на F1 позволило алгоритму обучения дольше обучаться, так как почти на каждом этапе происходило улучшение по F1, когда метрика была точность, мы достигали плато гораздо быстрее.
1	Изменение O
2	оптимизирующей O
3	метрики O
4	на O
5	F1 I-TERM
6	позволило O
7	алгоритму O
8	обучения O
9	дольше O
10	обучаться O
11	, O
12	так O
13	как O
14	почти O
15	на O
16	каждом O
17	этапе O
18	происходило O
19	улучшение O
20	по O
21	F1 I-TERM
22	, O
23	когда O
24	метрика O
25	была O
26	точность I-TERM
27	, O
28	мы O
29	достигали O
30	плато O
31	гораздо O
32	быстрее O
33	. O

# sent_id = 396
# text =  Изначально на этапе MVP (minimum viable product) мы применяли регулярные выражения для извлечения сущности.
1	Изначально O
2	на O
3	этапе O
4	MVP I-TERM
5	( O
6	minimum I-TERM
7	viable I-TERM
8	product I-TERM
9	) O
10	мы O
11	применяли O
12	регулярные I-TERM
13	выражения I-TERM
14	для O
15	извлечения I-TERM
16	сущности I-TERM
17	. O

# sent_id = 397
# text =  Протестировав поведение модели на продовских данных, мы обнаружили, что точность извлечения была около 50%.
1	Протестировав O
2	поведение O
3	модели O
4	на O
5	продовских O
6	данных O
7	, O
8	мы O
9	обнаружили O
10	, O
11	что O
12	точность I-TERM
13	извлечения O
14	была O
15	около O
16	50 I-TERM
17	% I-TERM
18	. O

# sent_id = 398
# text =  Мы поняли, что даже извлечение сущности зависит от контекста, и решили использовать BERT.
1	Мы O
2	поняли O
3	, O
4	что O
5	даже O
6	извлечение I-TERM
7	сущности I-TERM
8	зависит O
9	от O
10	контекста O
11	, O
12	и O
13	решили O
14	использовать O
15	BERT I-TERM
16	. O

# sent_id = 399
# text =  На вход необходимо представить размеченные данные с маркировкой BIO (beginning, intermediate, O – пустота).
1	На O
2	вход O
3	необходимо O
4	представить O
5	размеченные O
6	данные O
7	с O
8	маркировкой I-TERM
9	BIO I-TERM
10	( O
11	beginning O
12	, O
13	intermediate O
14	, O
15	O O
16	– O
17	пустота O
18	) O
19	. O

# sent_id = 400
# text =  Мы производили разметку 800 обращений на DataTurcks: Точность подхода BERT – 94% на этапе обучения, она валидирована на тестовых данных.
1	Мы O
2	производили O
3	разметку I-TERM
4	800 O
5	обращений O
6	на O
7	DataTurcks I-TERM
8	: O
9	Точность I-TERM
10	подхода O
11	BERT O
12	– O
13	94 I-TERM
14	% I-TERM
15	на O
16	этапе O
17	обучения O
18	, O
19	она O
20	валидирована O
21	на O
22	тестовых O
23	данных O
24	. O

# sent_id = 401
# text =  Это постобработка увеличила точность до 98%.
1	Это O
2	постобработка O
3	увеличила O
4	точность I-TERM
5	до O
6	98% I-TERM

# sent_id = 402
# text =  В иностранной литературе можно встретить термин Continuous Learning (CL), который объединяет различные методы использования новых данных для поддержания эффективности моделей.
1	В O
2	иностранной O
3	литературе O
4	можно O
5	встретить O
6	термин O
7	Continuous I-TERM
8	Learning I-TERM
9	( O
10	CL I-TERM
11	) O
12	, O
13	который O
14	объединяет O
15	различные O
16	методы O
17	использования O
18	новых O
19	данных O
20	для O
21	поддержания O
22	эффективности O
23	моделей O
24	. O

# sent_id = 403
# text =  Методы CL были положены в основу пайплайна переобучения.
1	Методы O
2	CL I-TERM
3	были O
4	положены O
5	в O
6	основу O
7	пайплайна O
8	переобучения O
9	. O

# sent_id = 404
# text =  Для решения этой проблемы требуется архитектура, которая позволяет GPT-3 анализировать содержание письма и оценивать, какая информация актуальна для ответа.
1	Для O
2	решения O
3	этой O
4	проблемы O
5	требуется O
6	архитектура O
7	, O
8	которая O
9	позволяет O
10	GPT-3 I-TERM
11	анализировать I-TERM
12	содержание I-TERM
13	письма I-TERM
14	и O
15	оценивать I-TERM
16	, I-TERM
17	какая I-TERM
18	информация I-TERM
19	актуальна I-TERM
20	для O
21	ответа O
22	. O

# sent_id = 405
# text =  OpenAI представила модель машинного обучения GPT-3, обученную на 175 млрд параметров, в июне 2020 года.
1	OpenAI I-TERM
2	представила O
3	модель O
4	машинного O
5	обучения O
6	GPT-3 I-TERM
7	, O
8	обученную O
9	на O
10	175 O
11	млрд O
12	параметров O
13	, O
14	в O
15	июне I-TERM
16	2020 I-TERM
17	года I-TERM
18	. O

# sent_id = 406
# text =  В отличие от предшественников GPT-2 и GPT-1 ее исходный код или обучающий набор данных решили не открывать.
1	В O
2	отличие O
3	от O
4	предшественников O
5	GPT-2 I-TERM
6	и O
7	GPT-1 I-TERM
8	ее O
9	исходный O
10	код O
11	или O
12	обучающий O
13	набор O
14	данных O
15	решили O
16	не O
17	открывать O
18	. O

# sent_id = 407
# text =  Модель уже попытались применить в медицинской сфере для общения с пациентами, но результаты эксперимента оказались неутешительными.
1	Модель O
2	уже O
3	попытались O
4	применить O
5	в O
6	медицинской I-TERM
7	сфере I-TERM
8	для O
9	общения O
10	с O
11	пациентами O
12	, O
13	но O
14	результаты O
15	эксперимента O
16	оказались O
17	неутешительными O
18	. O

# sent_id = 408
# text = Между тем создатели проекта GPT-Neo от EleutherAI решили воссоздать аналог GPT-3, но с открытым исходным кодом.
1	Между O
2	тем O
3	создатели O
4	проекта O
5	GPT I-TERM
6	- I-TERM
7	Neo I-TERM
8	от O
9	EleutherAI I-TERM
10	решили O
11	воссоздать O
12	аналог O
13	GPT-3 I-TERM
14	, O
15	но O
16	с O
17	открытым O
18	исходным O
19	кодом O
20	. O

# sent_id = 409
# text =  Однако только сейчас мы немного приблизились к сюжетам фантастических фильмов: можем попросить Алису убавить громкость, Google Assistant — заказать такси или Siri — завести будильник.
1	Однако O
2	только O
3	сейчас O
4	мы O
5	немного O
6	приблизились O
7	к O
8	сюжетам O
9	фантастических O
10	фильмов O
11	: O
12	можем O
13	попросить O
14	Алису I-TERM
15	убавить I-TERM
16	громкость I-TERM
17	, O
18	Google I-TERM
19	Assistant I-TERM
20	— O
21	заказать I-TERM
22	такси I-TERM
23	или O
24	Siri I-TERM
25	— O
26	завести I-TERM
27	будильник I-TERM
28	. O

# sent_id = 410
# text =  Технологии языкового процессинга востребованы в разработках, связанных с построением искусственного интеллекта: в поисковых системах, для извлечения фактов, оценки тональности текста, машинного перевода и диалога.
1	Технологии I-TERM
2	языкового I-TERM
3	процессинга I-TERM
4	востребованы O
5	в O
6	разработках O
7	, O
8	связанных O
9	с O
10	построением O
11	искусственного O
12	интеллекта O
13	: O
14	в O
15	поисковых I-TERM
16	системах I-TERM
17	, O
18	для O
19	извлечения I-TERM
20	фактов I-TERM
21	, O
22	оценки I-TERM
23	тональности I-TERM
24	текста I-TERM
25	, O
26	машинного I-TERM
27	перевода I-TERM
28	и O
29	диалога O
30	. O

# sent_id = 411
# text =  Первые разговоры об обработке естественного языка компьютером начались еще в 30-е годы XX-го века с философских рассуждений Айера — он предлагал отличать разумного человека от глупой машины с помощью эмпирического теста.
1	Первые O
2	разговоры O
3	об O
4	обработке I-TERM
5	естественного I-TERM
6	языка I-TERM
7	компьютером O
8	начались O
9	еще O
10	в O
11	30-е I-TERM
12	годы I-TERM
13	XX I-TERM
14	- I-TERM
15	го I-TERM
16	века I-TERM
17	с O
18	философских O
19	рассуждений O
20	Айера I-TERM
21	— O
22	он O
23	предлагал O
24	отличать O
25	разумного O
26	человека O
27	от O
28	глупой O
29	машины O
30	с O
31	помощью O
32	эмпирического O
33	теста O
34	. O

# sent_id = 412
# text =  В 1950 году Алан Тьюринг в философском журнале Mind предложил такой тест, где судья должен определить, с кем он ведет диалог: с человеком или компьютером.
1	В O
2	1950 I-TERM
3	году O
4	Алан I-TERM
5	Тьюринг I-TERM
6	в O
7	философском O
8	журнале O
9	Mind I-TERM
10	предложил O
11	такой O
12	тест I-TERM
13	, O
14	где O
15	судья O
16	должен O
17	определить O
18	, O
19	с O
20	кем O
21	он O
22	ведет O
23	диалог O
24	: O
25	с O
26	человеком O
27	или O
28	компьютером O
29	. O

# sent_id = 413
# text =  В 1954 году Джорджтаунский университет совместно с компанией IBM продемонстрировали программу машинного перевода с русского на английский, которая работала на базе словаря из 250 слов и набора из 6 грамматических правил.
1	В O
2	1954 I-TERM
3	году O
4	Джорджтаунский I-TERM
5	университет I-TERM
6	совместно O
7	с O
8	компанией O
9	IBM I-TERM
10	продемонстрировали O
11	программу I-TERM
12	машинного I-TERM
13	перевода I-TERM
14	с O
15	русского I-TERM
16	на O
17	английский I-TERM
18	, O
19	которая O
20	работала O
21	на O
22	базе O
23	словаря O
24	из O
25	250 O
26	слов O
27	и O
28	набора O
29	из O
30	6 O
31	грамматических O
32	правил O
33	. O

# sent_id = 414
# text =  Параллельно с попытками научить компьютер переводить текст, ученые и целые университеты думали над созданием робота, способного имитировать речевое поведение человека.
1	Параллельно O
2	с O
3	попытками O
4	научить O
5	компьютер O
6	переводить O
7	текст O
8	, O
9	ученые O
10	и O
11	целые O
12	университеты O
13	думали O
14	над O
15	созданием O
16	робота O
17	, O
18	способного O
19	имитировать I-TERM
20	речевое I-TERM
21	поведение I-TERM
22	человека I-TERM
23	. O

# sent_id = 415
# text =  Первой успешной реализацией чат-бота стал виртуальный собеседник ELIZA, написанный в 1966 году Джозефом Вейценбаумом.
1	Первой O
2	успешной O
3	реализацией O
4	чат O
5	- O
6	бота O
7	стал O
8	виртуальный I-TERM
9	собеседник I-TERM
10	ELIZA I-TERM
11	, O
12	написанный O
13	в O
14	1966 I-TERM
15	году I-TERM
16	Джозефом I-TERM
17	Вейценбаумом I-TERM
18	. O

# sent_id = 416
# text =  Элиза пародировала поведение психотерапевта, выделяя значимые слова из фразы собеседника и задавая встречный вопрос.
1	Элиза I-TERM
2	пародировала O
3	поведение O
4	психотерапевта O
5	, O
6	выделяя O
7	значимые O
8	слова O
9	из O
10	фразы O
11	собеседника O
12	и O
13	задавая O
14	встречный O
15	вопрос O
16	. O

# sent_id = 417
# text =  Можно считать, что это был первый чат-бот, построенный на правилах (rule-based bot), и он положил начало целому классу таких систем.
1	Можно O
2	считать O
3	, O
4	что O
5	это O
6	был O
7	первый O
8	чат O
9	- O
10	бот O
11	, O
12	построенный O
13	на O
14	правилах O
15	( O
16	rule I-TERM
17	- I-TERM
18	based I-TERM
19	bot I-TERM
20	) O
21	, O
22	и O
23	он O
24	положил O
25	начало O
26	целому O
27	классу O
28	таких O
29	систем O
30	. O

# sent_id = 418
# text =  Без Элизы не появились бы такие программы-собеседники, как Cleverbot, WeChat Xiaoice, Eugene Goostman — формально прошедший тест Тьюринга в 2014 году, — и даже Siri, Jarvis и Alexa.
1	Без O
2	Элизы I-TERM
3	не O
4	появились O
5	бы O
6	такие O
7	программы O
8	- O
9	собеседники O
10	, O
11	как O
12	Cleverbot I-TERM
13	, O
14	WeChat I-TERM
15	Xiaoice I-TERM
16	, O
17	Eugene I-TERM
18	Goostman I-TERM
19	— O
20	формально O
21	прошедший O
22	тест I-TERM
23	Тьюринга I-TERM
24	в O
25	2014 I-TERM
26	году O
27	, O
28	— O
29	и O
30	даже O
31	Siri I-TERM
32	, O
33	Jarvis I-TERM
34	и O
35	Alexa I-TERM
36	. O

# sent_id = 419
# text =  В 1968 году Терри Виноградом на языке LISP была разработана программа SHRDLU.
1	В O
2	1968 I-TERM
3	году O
4	Терри I-TERM
5	Виноградом I-TERM
6	на O
7	языке O
8	LISP I-TERM
9	была O
10	разработана O
11	программа O
12	SHRDLU I-TERM
13	. O

# sent_id = 420
# text =  Следующим шагом в развитии чат-ботов стала программа A.L.I.C.E., для которой Ричард Уоллес разработал специальный язык разметки — AIML (англ. Artificial Intelligence Markup Language).
1	Следующим O
2	шагом O
3	в O
4	развитии O
5	чат O
6	- O
7	ботов O
8	стала O
9	программа O
10	A.L.I.C.E. I-TERM
11	, O
12	для O
13	которой O
14	Ричард I-TERM
15	Уоллес I-TERM
16	разработал O
17	специальный O
18	язык O
19	разметки I-TERM
20	— O
21	AIML I-TERM
22	( O
23	англ O
24	. O
25	Artificial I-TERM
26	Intelligence I-TERM
27	Markup I-TERM
28	Language I-TERM
29	) O
30	. O

# sent_id = 421
# text =  Разговоры о нейронных сетях и глубоком обучении ходили уже в 90-е годы, а первый нейрокомпьютер «Марк-1» появился вообще в 1958 году.
1	Разговоры O
2	о O
3	нейронных I-TERM
4	сетях I-TERM
5	и O
6	глубоком O
7	обучении O
8	ходили O
9	уже O
10	в O
11	90-е O
12	годы O
13	, O
14	а O
15	первый O
16	нейрокомпьютер I-TERM
17	« O
18	Марк-1 I-TERM
19	» O
20	появился O
21	вообще O
22	в O
23	1958 I-TERM
24	году I-TERM
25	. O

# sent_id = 422
# text =  1970 г. Машинный перевод на основе правил (англ. RBMT) был первой попыткой научить машину переводить.
1	1970 O
2	г. O
3	Машинный I-TERM
4	перевод I-TERM
5	на I-TERM
6	основе I-TERM
7	правил I-TERM
8	( O
9	англ O
10	RBMT I-TERM
11	. O
12	) O
13	был O
14	первой O
15	попыткой O
16	научить O
17	машину O
18	переводить O
19	. O

# sent_id = 423
# text =  1984 г. Машинный перевод на основе примеров (англ. EBMT) был способен переводить даже совсем не похожие друг на друга языки, где задавать какие-то правила было бесполезно.
1	1984 I-TERM
2	г. I-TERM
3	Машинный I-TERM
4	перевод I-TERM
5	на I-TERM
6	основе I-TERM
7	примеров I-TERM
8	( O
9	англ O
10	. O
11	EBMT I-TERM
12	) O
13	был O
14	способен O
15	переводить O
16	даже O
17	совсем O
18	не O
19	похожие O
20	друг O
21	на O
22	друга O
23	языки O
24	, O
25	где O
26	задавать O
27	какие O
28	- O
29	то O
30	правила O
31	было O
32	бесполезно O
33	. O

# sent_id = 424
# text =  1990 г. Статистический машинный перевод (англ. SMT) в эпоху развития интернета позволил использовать не только готовые языковые корпуса, но даже книги и вольно переведенные статьи.
1	1990 I-TERM
2	г. O
3	Статистический I-TERM
4	машинный I-TERM
5	перевод I-TERM
6	( O
7	англ I-TERM
8	. O 
9	SMT I-TERM
10	) O
11	в O
12	эпоху O
13	развития O
14	интернета O
15	позволил O
16	использовать O
17	не O
18	только O
19	готовые O
20	языковые O
21	корпуса O
22	, O
23	но O
24	даже O
25	книги O
26	и O
27	вольно O
28	переведенные O
29	статьи O
30	. O

# sent_id = 425
# text =  Статистические методы и сейчас активно используются в языковом процессинге.
1	Статистические I-TERM
2	методы I-TERM
3	и O
4	сейчас O
5	активно O
6	используются O
7	в O
8	языковом I-TERM
9	процессинге I-TERM
10	. O

# sent_id = 426
# text =  По мере развития обработки естественного языка множество задач решалось классическими статистическими методами и множеством правил, однако проблему нечеткости и неоднозначности в языке это не решало.
1	По O
2	мере O
3	развития O
4	обработки I-TERM
5	естественного I-TERM
6	языка I-TERM
7	множество O
8	задач O
9	решалось O
10	классическими O
11	статистическими I-TERM
12	методами I-TERM
13	и O
14	множеством O
15	правил O
16	, O
17	однако O
18	проблему I-TERM
19	нечеткости I-TERM
20	и I-TERM
21	неоднозначности I-TERM
22	в O
23	языке O
24	это O
25	не O
26	решало O
27	. O

# sent_id = 427
# text =  Так родился статистический метод анализа текста word2vec (англ. Word to vector).
1	Так O
2	родился O
3	статистический I-TERM
4	метод I-TERM
5	анализа I-TERM
6	текста I-TERM
7	word2vec I-TERM
8	( O
9	англ O
10	. O
11	Word I-TERM
12	to I-TERM
13	vector I-TERM
14	) O
15	. O

# sent_id = 428
# text =  Под эти критерии отлично подходит рекуррентная нейронная сеть (RNN), однако по мере увеличения расстояния между связанными частями текста необходимо увеличивать и размер RNN, из-за чего падает качество обработки информации.
1	Под O
2	эти O
3	критерии O
4	отлично O
5	подходит O
6	рекуррентная I-TERM
7	нейронная I-TERM
8	сеть I-TERM
9	( O
10	RNN I-TERM
11	) O
12	, O
13	однако O
14	по O
15	мере O
16	увеличения O
17	расстояния O
18	между O
19	связанными O
20	частями O
21	текста O
22	необходимо O
23	увеличивать O
24	и O
25	размер O
26	RNN I-TERM
27	, O
28	из O
29	- O
30	за O
31	чего O
32	падает O
33	качество O
34	обработки O
35	информации O
36	. O

# sent_id = 429
# text =  Эту проблему решает сеть LSTM (англ. Long short-term memory).
1	Эту O
2	проблему O
3	решает O
4	сеть O
5	LSTM I-TERM
6	( O
7	англ O
8	. O 
9	Long I-TERM
10	short I-TERM
11	- I-TERM
12	term I-TERM
13	memory I-TERM
14	) O
15	. O

# sent_id = 430
# text =  Если говорить о языке Python, который часто используется для анализа данных, то это NLTK и Spacy.
1	Если O
2	говорить O
3	о O
4	языке O
5	Python I-TERM
6	, O
7	который O
8	часто O
9	используется O
10	для O
11	анализа I-TERM
12	данных I-TERM
13	, O
14	то O
15	это O
16	NLTK I-TERM
17	и O
18	Spacy I-TERM
19	. O

# sent_id = 431
# text =  Крупные компании также принимают участие в разработке библиотек для NLP, как например NLP Architect от Intel или PyTorch от исследователей из Facebook и Uber.
1	Крупные O
2	компании O
3	также O
4	принимают O
5	участие O
6	в O
7	разработке O
8	библиотек O
9	для O
10	NLP I-TERM
11	, O
12	как O
13	например O
14	NLP I-TERM
15	Architect I-TERM
16	от O
17	Intel I-TERM
18	или O
19	PyTorch I-TERM
20	от O
21	исследователей O
22	из O
23	Facebook I-TERM
24	и O
25	Uber I-TERM
26	. O

# sent_id = 432
# text =  Направление b2c не единственное, где можно применять чат-ботов.
1	Направление O
2	b2c I-TERM
3	не O
4	единственное O
5	, O
6	где O
7	можно O
8	применять O
9	чат O
10	- O
11	ботов O
12	. O

# sent_id = 433
# text =  Участникам предлагалось определить потенциальные заболевания коров по реальным жалобам людей из открытых источников, а также научиться выделять из текстов симптомы заболеваний (NER - Named Entity Recognition).
1	Участникам O
2	предлагалось O
3	определить O
4	потенциальные O
5	заболевания O
6	коров O
7	по O
8	реальным O
9	жалобам O
10	людей O
11	из O
12	открытых O
13	источников O
14	, O
15	а O
16	также O
17	научиться O
18	выделять O
19	из O
20	текстов O
21	симптомы O
22	заболеваний O
23	( O
24	NER I-TERM
25	- O
26	Named I-TERM
27	Entity I-TERM
28	Recognition I-TERM
29	) O
30	. O

# sent_id = 434
# text =  Эта статья будет интересна не только тем, кто специализируется в NLP (Natural Language Processing), но и начинающим исследователям данных.
1	Эта O
2	статья O
3	будет O
4	интересна O
5	не O
6	только O
7	тем O
8	, O
9	кто O
10	специализируется O
11	в O
12	NLP I-TERM
13	( O
14	Natural I-TERM
15	Language I-TERM
16	Processing I-TERM
17	) O
18	, O
19	но O
20	и O
21	начинающим O
22	исследователям O
23	данных O
24	. O

# sent_id = 435
# text =  Спаны - это участки текста, которые содержат в себе определенный смысл.
1	Спаны I-TERM
2	  O
3	- O
4	  O
5	это O
6	участки O
7	текста I-TERM
8	, O
9	которые O
10	содержат O
11	в O
12	себе O
13	определенный O
14	смысл O
15	. O

# sent_id = 436
# text =  Программа для разметки YEDDA и процесс разметки.
1	Программа O
2	для O
3	разметки I-TERM
4	YEDDA I-TERM
5	и O
6	процесс O
7	разметки O
8	. O

# sent_id = 437
# text =  Так как задача является составной, то и метрика состояла из двух компонентов с весом 0.8 для задачи классификации и 0.2 для задачи NER.
1	Так O
2	как O
3	задача O
4	является O
5	составной O
6	, O
7	то O
8	и O
9	метрика O
10	состояла O
11	из O
12	двух O
13	компонентов O
14	с O
15	весом O
16	0.8 I-TERM
17	для O
18	задачи O
19	классификации I-TERM
20	и O
21	0.2 I-TERM
22	для O
23	задачи O
24	NER I-TERM
25	. O

# sent_id = 438
# text =  В задаче классификации использовался logloss, вычисляемый как среднее значение метрики sklearn.metrics.log_loss по классам болезней.
1	В O
2	задаче O
3	классификации I-TERM
4	использовался O
5	logloss I-TERM
6	, O
7	вычисляемый O
8	как O
9	среднее O
10	значение O
11	метрики O
12	sklearn.metrics.log_loss I-TERM
13	по O
14	классам O
15	болезней O
16	. O

# sent_id = 439
# text = В задаче NER использовался span-based F1-score, рассчитываемый следующим образом: для каждого текста берутся предсказанные индексы начала и конца размеченных признаков болезни, по ним выделяются из текста токены (отдельные слова, разделенные пробелом) и сравниваются с истинной (экспертной) разметкой.
1	В O
2	задаче O
3	NER I-TERM
4	использовался O
5	span I-TERM
6	- I-TERM
7	based I-TERM
8	F1-score I-TERM
9	, O
10	рассчитываемый O
11	следующим O
12	образом O
13	: O
14	для O
15	каждого O
16	текста I-TERM
17	берутся O
18	предсказанные O
19	индексы I-TERM
20	начала O
21	и O
22	конца O
23	размеченных O
24	признаков O
25	болезни O
26	, O
27	по O
28	ним O
29	выделяются O
30	из O
31	текста O
32	токены I-TERM
33	( O
34	отдельные O
35	слова I-TERM
36	, O
37	разделенные O
38	пробелом O
39	) O
40	и O
41	сравниваются O
42	с O
43	истинной O
44	( O
45	экспертной O
46	) O
47	разметкой I-TERM
48	. O

# sent_id = 440
# text =  Код для подсчета метрики span-based F1-score.
1	Код O
2	для O
3	подсчета O
4	метрики O
5	span I-TERM
6	- I-TERM
7	based I-TERM
8	F1-score I-TERM
9	.

# sent_id = 441
# text =  Этим решением стало использование классификатора CatBoost, который прямо из коробки может обрабатывать текстовые фичи.
1	Этим O
2	решением O
3	стало O
4	использование O
5	классификатора O
6	CatBoost I-TERM
7	, O
8	который O
9	прямо O
10	из O
11	коробки O
12	может O
13	обрабатывать O
14	текстовые O
15	фичи O
16	. O

# sent_id = 442
# text =  Решение для задачи распознавания симптомов мы давать не стали, чтобы участники Data Science чемпионата могли покреативить.
1	Решение O
2	для O
3	задачи I-TERM
4	распознавания I-TERM
5	симптомов I-TERM
6	мы O
7	давать O
8	не O
9	стали O
10	, O
11	чтобы O
12	участники O
13	Data I-TERM
14	Science I-TERM
15	чемпионата O
16	могли O
17	покреативить O
18	. O

# sent_id = 443
# text =  Во-первых, конкретно для этого соревнования наиболее эффективный подход - это доразметка спанов тренировочных данных для задачи NER.
1	Во O
2	- O
3	первых O
4	, O
5	конкретно O
6	для O
7	этого O
8	соревнования O
9	наиболее O
10	эффективный O
11	подход O
12	- O
13	это O
14	доразметка I-TERM
15	спанов I-TERM
16	тренировочных I-TERM
17	данных I-TERM
18	для O
19	задачи O
20	NER I-TERM
21	. O

# sent_id = 444
# text =  Во-вторых, участники использовали базовые подходы для NLP-задач: удаление стоп-слов и знаков пунктуации, приведение к нижнему регистру, стемминг и лемматизация.
1	Во O
2	- O
3	вторых O
4	, O
5	участники O
6	использовали O
7	базовые O
8	подходы O
9	для O
10	NLP I-TERM
11	- O
12	задач O
13	: O
14	удаление O
15	стоп O
16	- O
17	слов O
18	и O
19	знаков O
20	пунктуации O
21	, O
22	приведение O
23	к O
24	нижнему O
25	регистру O
26	, O
27	стемминг I-TERM
28	и O
29	лемматизация I-TERM
30	. O

# sent_id = 445
# text = Более же продвинутым подходом является аугментация данных.
1	Более O
2	же O
3	продвинутым O
4	подходом O
5	является O
6	аугментация I-TERM
7	данных I-TERM
8	. O

# sent_id = 446
# text =  Один из возможных способов аугментации текста - перифраз текста.
1	Один O
2	из O
3	возможных O
4	способов O
5	аугментации I-TERM
6	текста I-TERM
7	- O
8	перифраз I-TERM
9	текста I-TERM
10	. O

# sent_id = 447
# text =  Примером данного решения является использование парафрайзера на основе “rut5-base-paraphraser” из библиотеки huggingface.
1	Примером O
2	данного O
3	решения O
4	является O
5	использование O
6	парафрайзера I-TERM
7	на O
8	основе O
9	“ O
10	rut5-base I-TERM
11	- I-TERM
12	paraphraser I-TERM
13	” O
14	из O
15	библиотеки O
16	huggingface I-TERM
17	. O

# sent_id = 448
# text =  Реализуется данный метод аналогично с предыдущим, как модель можно использовать “LaBSE-en-ru”.
1	Реализуется O
2	данный O
3	метод O
4	аналогично O
5	с O
6	предыдущим O
7	, O
8	как O
9	модель O
10	можно O
11	использовать O
12	“ O
13	LaBSE I-TERM
14	- I-TERM
15	en I-TERM
16	- I-TERM
17	ru I-TERM
18	” O
19	. O

# sent_id = 449
# text =  Сначала решается задача выделения симптомов (NER), после чего в текстах убираются все слова, не являющиеся симптомами.
1	Сначала O
2	решается O
3	задача I-TERM
4	выделения I-TERM
5	симптомов I-TERM
6	( O
7	NER I-TERM
8	) O
9	, O
10	после O
11	чего O
12	в O
13	текстах I-TERM
14	убираются O
15	все O
16	слова I-TERM
17	, O
18	не O
19	являющиеся O
20	симптомами O
21	. O

# sent_id = 450
# text =  Базовым вариантом эмбеддингов является TF-IDF, который зависит от частоты употребления слова в документе.
1	Базовым O
2	вариантом O
3	эмбеддингов I-TERM
4	является O
5	TF I-TERM
6	- I-TERM
7	IDF I-TERM
8	, O
9	который O
10	зависит O
11	от O
12	частоты I-TERM
13	употребления I-TERM
14	слова I-TERM
15	в O
16	документе O
17	. O

# sent_id = 451
# text =  И чтобы его улучшить, можно использовать эмбеддинги предобученных моделей, таких как Word2Vec, FastText и тд.
1	И O
2	чтобы O
3	его O
4	улучшить O
5	, O
6	можно O
7	использовать O
8	эмбеддинги I-TERM
9	предобученных I-TERM
10	моделей I-TERM
11	, O
12	таких O
13	как O
14	Word2Vec I-TERM
15	, O
16	FastText I-TERM
17	и O
18	тд O
19	. O

# sent_id = 452
# text =  В частности, в одном из лучших решений использовался необычный FastText, предобученный на корпусе текстов RuDReC, который содержит отзывы потребителей на русском языке о фармацевтической продукции.
1	В O
2	частности O
3	, O
4	в O
5	одном O
6	из O
7	лучших O
8	решений O
9	использовался O
10	необычный O
11	FastText I-TERM
12	, O
13	предобученный O
14	на O
15	корпусе O
16	текстов O
17	RuDReC I-TERM
18	, O
19	который O
20	содержит O
21	отзывы O
22	потребителей O
23	на O
24	русском I-TERM
25	языке I-TERM
26	о O
27	фармацевтической O
28	продукции O
29	. O

# sent_id = 453
# text =  Напомним, что алгоритм работы с трансформерами можно представить следующим образом: сначала тексты преобразовываются токенизатором, далее обучается модель-трансформер.
1	Напомним O
2	, O
3	что O
4	алгоритм O
5	работы O
6	с O
7	трансформерами I-TERM
8	можно O
9	представить O
10	следующим O
11	образом O
12	: O
13	сначала O
14	тексты O
15	преобразовываются O
16	токенизатором I-TERM
17	, O
18	далее O
19	обучается O
20	модель I-TERM
21	- I-TERM
22	трансформер I-TERM
23	. O

# sent_id = 454
# text =  Если же говорить о выборе моделей, то наилучшие результаты были получены следующими из них: RuBERT-base, RuBERT-large, LaBSE-en-ru.
1	Если O
2	же O
3	говорить O
4	о O
5	выборе O
6	моделей O
7	, O
8	то O
9	наилучшие O
10	результаты O
11	были O
12	получены O
13	следующими O
14	из O
15	них O
16	: O
17	RuBERT I-TERM
18	- I-TERM
19	base I-TERM
20	, O
21	RuBERT I-TERM
22	- I-TERM
23	large I-TERM
24	, O
25	LaBSE I-TERM
26	- I-TERM
27	en I-TERM
28	- I-TERM
29	ru I-TERM
30	. O

# sent_id = 455
# text =  Предположим, что вы и так слышали о моделях семейства BERT (в предыдущей статье мы описывали, как применяем BERT в других задачах), а вот LaBSE - выбор совершенно неочевидный.
1	Предположим O
2	, O
3	что O
4	вы O
5	и O
6	так O
7	слышали O
8	о O
9	моделях O
10	семейства O
11	BERT I-TERM
12	( O
13	в O
14	предыдущей O
15	статье O
16	мы O
17	описывали O
18	, O
19	как O
20	применяем O
21	BERT I-TERM
22	в O
23	других O
24	задачах O
25	) O
26	, O
27	а O
28	вот O
29	LaBSE I-TERM
30	- O
31	выбор O
32	совершенно O
33	неочевидный O
34	. O

# sent_id = 456
# text = Далее слова в тестовом наборе текстов также приводятся к векторам и сравниваются со словами из тренировочной разметки при помощи косинусной близости.
1	Далее O
2	слова O
3	в O
4	тестовом O
5	наборе O
6	текстов O
7	также O
8	приводятся O
9	к O
10	векторам O
11	и O
12	сравниваются O
13	со O
14	словами O
15	из O
16	тренировочной O
17	разметки O
18	при O
19	помощи O
20	косинусной I-TERM
21	близости I-TERM
22	. O

# sent_id = 457
# text =  Архитектура в свою очередь может содержать LSTM, BiLSTM, RNN или GRU слои.
1	Архитектура O
2	в O
3	свою O
4	очередь O
5	может O
6	содержать O
7	LSTM I-TERM
8	, O
9	BiLSTM I-TERM
10	, O
11	RNN I-TERM
12	или O
13	GRU I-TERM
14	слои O
15	. O

# sent_id = 458
# text =  Из интересных решений один из участников представил BiLSTM-сеть с CRF слоем.
1	Из O
2	интересных O
3	решений O
4	один O
5	из O
6	участников O
7	представил O
8	BiLSTM I-TERM
9	- I-TERM
10	сеть I-TERM
11	с O
12	CRF I-TERM
13	слоем O
14	. O

# sent_id = 459
# text =  Используются те же модели, поэтому расскажем о различии в подготовке данных для моделей.Для задачи NER тексты преобразовываются с помощью токенизатора и теггинга.
1	Используются O
2	те O
3	же O
4	модели O
5	, O
6	поэтому O
7	расскажем O
8	о O
9	различии O
10	в O
11	подготовке O
12	данных O
13	для O
14	моделей O
15	. O
16	Для O
17	задачи O
18	NER I-TERM
19	тексты O
20	преобразовываются O
21	с O
22	помощью O
23	токенизатора I-TERM
24	и O
25	теггинга I-TERM
26	. O

# sent_id = 460
# text =  Сначала тексты при помощи токенизатора переводятся в вектора - это то, на чем обучается модель.
1	Сначала O
2	тексты O
3	при O
4	помощи O
5	токенизатора I-TERM
6	переводятся O
7	в O
8	вектора I-TERM
9	- O
10	это O
11	то O
12	, O
13	на O
14	чем O
15	обучается O
16	модель O
17	. O

# sent_id = 461
# text =  Далее создаются таргеты при помощи теггинга.
1	Далее O
2	создаются O
3	таргеты I-TERM
4	при O
5	помощи O
6	теггинга I-TERM
7	. O

# sent_id = 462
# text =  Самым распространенным алгоритмом теггинга является “Inside–outside–beginning”.
1	Самым O
2	распространенным O
3	алгоритмом O
4	теггинга O
5	является O
6	“ O
7	Inside I-TERM
8	– I-TERM
9	outside I-TERM
10	– I-TERM
11	beginning I-TERM
12	” 
13	. O

# sent_id = 463
# text =  Тег указывает на то, что слово находится внутри спана.
1	Тег I-TERM
2	указывает O
3	на O
4	то O
5	, O
6	что O
7	слово O
8	находится O
9	внутри O
10	спана I-TERM
11	. O

# sent_id = 464
# text =  Среди решений были как кастомный код для обучения и инференса, так и код от huggingface, который можно использовать из коробки.
1	Среди O
2	решений O
3	были O
4	как O
5	кастомный O
6	код O
7	для O
8	обучения O
9	и O
10	инференса O
11	, O
12	так O
13	и O
14	код O
15	от O
16	huggingface I-TERM
17	, O
18	который O
19	можно O
20	использовать O
21	из O
22	коробки O
23	. O

# sent_id = 465
# text =  Безусловно, основной метрикой оценивания являлся лидерборд.
1	Безусловно O
2	, O
3	основной O
4	метрикой O
5	оценивания O
6	являлся O
7	лидерборд I-TERM
8	. O

# sent_id = 466
# text =  Для решения ситуации мы можем искусственно сгенерировать данные с помощью языка программирования.
1	Для O
2	решения O
3	ситуации O
4	мы O
5	можем O
6	искусственно O
7	сгенерировать I-TERM
8	данные I-TERM
9	с O
10	помощью O
11	языка O
12	программирования O
13	. O

# sent_id = 467
# text =  Пересмотрев множество примеров и статей, была найдена англоязычная статья, в которой рассмотрены три самых интересных, в плане функциональности и простоты использования, способа генерации синтетических данных с помощью пакетов Python.
1	Пересмотрев O
2	множество O
3	примеров O
4	и O
5	статей O
6	, O
7	была O
8	найдена O
9	англоязычная O
10	статья O
11	, O
12	в O
13	которой O
14	  O
15	рассмотрены O
16	три O
17	самых O
18	интересных O
19	, O
20	в O
21	плане O
22	функциональности O
23	и O
24	простоты O
25	использования O
26	, O
27	способа O
28	генерации I-TERM
29	синтетических I-TERM
30	данных I-TERM
31	с O
32	помощью O
33	пакетов O
34	  O
35	Python I-TERM
36	. O

# sent_id = 468
# text =  Faker - это пакет Python, разработанный для упрощения генерации синтетических данных.
1	Faker I-TERM
2	- O
3	это O
4	пакет O
5	Python I-TERM
6	, O
7	разработанный O
8	для O
9	упрощения O
10	генерации I-TERM
11	синтетических I-TERM
12	данных I-TERM
13	. O

# sent_id = 469
# text =  SDV или Synthetic Data Vault - это пакет Python для генерации синтетических данных на основе предоставленного набора данных.
1	SDV I-TERM
2	или O
3	Synthetic I-TERM
4	Data I-TERM
5	Vault I-TERM
6	- O
7	это O
8	пакет O
9	Python I-TERM
10	для O
11	генерации I-TERM
12	синтетических I-TERM
13	данных I-TERM
14	на O
15	основе O
16	предоставленного O
17	набора O
18	данных O
19	. O

# sent_id = 470
# text =  SDV генерирует данные, применяя математические методы и модели машинного обучения.
1	SDV I-TERM
2	генерирует O
3	данные O
4	, O
5	применяя O
6	математические I-TERM
7	методы I-TERM
8	и O
9	модели I-TERM
10	машинного I-TERM
11	обучения I-TERM
12	. O

# sent_id = 471
# text =  С помощью SVD можно обработать данные, даже если они содержат несколько типов данных и отсутствующие значения.
1	С O
2	помощью O
3	SVD I-TERM
4	можно O
5	обработать I-TERM
6	данные I-TERM
7	, O
8	даже O
9	если O
10	они O
11	содержат O
12	несколько O
13	типов O
14	данных O
15	и O
16	отсутствующие O
17	значения O
18	. O

# sent_id = 472
# text =  Используем для этого одну из доступных моделей SVD Singular Table GaussianCopula.
1	Используем O
2	для O
3	этого O
4	одну O
5	из O
6	доступных O
7	моделей O
8	SVD I-TERM
9	Singular I-TERM
10	Table I-TERM
11	GaussianCopula I-TERM
12	. O

# sent_id = 473
# text =  Воспользуемся функцией evaluate из SDV.
1	Воспользуемся O
2	функцией O
3	evaluate I-TERM
4	из O
5	SDV I-TERM
6	. O

# sent_id = 474
# text =  Возьмем для примера статистические метрики (критерии Колмогорова–Смирнова и Хи-квадрат) и метрику обнаружения, основанную на классификаторе логистической регрессии.
1	Возьмем O
2	для O
3	примера O
4	статистические I-TERM
5	метрики I-TERM
6	( O
7	критерии I-TERM
8	Колмогорова I-TERM
9	– I-TERM
10	Смирнова I-TERM
11	и O
12	Хи I-TERM
13	- I-TERM
14	квадрат I-TERM
15	) O
16	и O
17	метрику I-TERM
18	обнаружения I-TERM
19	, O
20	основанную O
21	на O
22	классификаторе I-TERM
23	логистической I-TERM
24	регрессии I-TERM
25	. O

# sent_id = 475
# text =  KSTest используется для сравнения столбцов с непрерывными данными, а CSTest с дискретными данными.
1	KSTest I-TERM
2	используется O
3	для O
4	сравнения O
5	столбцов O
6	с O
7	непрерывными O
8	данными O
9	, O
10	а O
11	CSTest I-TERM
12	с O
13	дискретными O
14	данными O
15	. O

# sent_id = 476
# text =  Метрика LogisticDetection при помощи машинного обучения позволяет оценить насколько сложно отличить синтетические данные от исходных.
1	Метрика O
2	LogisticDetection I-TERM
3	при O
4	помощи O
5	машинного O
6	обучения O
7	позволяет O
8	оценить O
9	насколько O
10	сложно O
11	отличить I-TERM
12	синтетические I-TERM
13	данные I-TERM
14	от I-TERM
15	исходных I-TERM
16	. O

# sent_id = 477
# text =  Gretel или Gretel Synthetics – это пакет Python с открытым исходным кодом, основанный на рекуррентной нейронной сети для создания структурированных и не структурированных данных.
1	Gretel I-TERM
2	или O
3	Gretel I-TERM
4	Synthetics I-TERM
5	– O
6	это O
7	пакет O
8	Python I-TERM
9	с O
10	открытым O
11	исходным O
12	кодом O
13	, O
14	основанный O
15	на O
16	рекуррентной I-TERM
17	нейронной I-TERM
18	сети I-TERM
19	для O
20	создания O
21	структурированных O
22	и O
23	не O
24	структурированных O
25	данных O
26	. O

# sent_id = 478
# text =  Этот модуль работает непосредственно с датафреймами данных Pandas и позволяет автоматически разбивать датафрейм на более мелкие датафреймы (по кластерам столбцов), выполнять обучение модели и генерацию для каждого фрейма независимо.
1	Этот O
2	модуль O
3	работает O
4	  O
5	непосредственно O
6	с O
7	датафреймами O
8	данных O
9	Pandas I-TERM
10	и O
11	позволяет O
12	автоматически O
13	разбивать O
14	датафрейм O
15	на O
16	более O
17	мелкие O
18	датафреймы O
19	( O
20	по O
21	кластерам O
22	столбцов O
23	) O
24	, O
25	выполнять O
26	обучение O
27	модели O
28	и O
29	генерацию O
30	для O
31	каждого O
32	фрейма O
33	независимо O
34	. O

# sent_id = 479
# text =  Теперь с помощью пакета Gretel cгенерируем синтетические данные для Stroke Prediction Dataset и проанализируем их относительно данных полученных с помощью пакета SVD из пункта 2.
1	Теперь O
2	с O
3	помощью O
4	пакета O
5	Gretel I-TERM
6	cгенерируем O
7	синтетические O
8	данные O
9	для O
10	Stroke I-TERM
11	Prediction I-TERM
12	Dataset I-TERM
13	и O
14	проанализируем O
15	их O
16	относительно O
17	данных O
18	полученных O
19	с O
20	помощью O
21	пакета O
22	SVD I-TERM
23	из O
24	пункта O
25	2 O
26	. O

# sent_id = 480
# text =  Метрикой оценки качества является ROC-AUC.
1	Метрикой O
2	оценки O
3	качества O
4	является O
5	ROC I-TERM
6	- I-TERM
7	AUC I-TERM
8	. O

# sent_id = 481
# text =  Разработанный подход для решения задачи кредитного скоринга в дальнейшем легко переносим и на прочие банковские задачи: модели склонности, оттока и дохода.
1	Разработанный O
2	подход O
3	для O
4	решения O
5	задачи I-TERM
6	кредитного I-TERM
7	скоринга I-TERM
8	в O
9	дальнейшем O
10	легко O
11	переносим O
12	и O
13	на O
14	прочие O
15	банковские O
16	задачи O
17	: O
18	модели O
19	склонности O
20	, O
21	оттока O
22	и O
23	дохода O
24	. O

# sent_id = 482
# text =  Токены, относящиеся к ФИО, мы выделяем с помощью клиентской базы и проверки с помощью библиотек для морфологического анализа.
1	Токены I-TERM
2	, O
3	относящиеся O
4	к O
5	ФИО O
6	, O
7	мы O
8	выделяем O
9	с O
10	помощью O
11	клиентской O
12	базы O
13	и O
14	проверки O
15	с O
16	помощью O
17	библиотек O
18	для O
19	морфологического I-TERM
20	анализа I-TERM
21	. O

# sent_id = 483
# text = Лемматизация оставшихся токенов.
1	Лемматизация I-TERM
2	оставшихся O
3	токенов I-TERM
4	. O

# sent_id = 484
# text =  Для этого корпуса мы обучили word2vec-модель, где для каждого токена выучили эмбеддинг размера 50.
1	Для O
2	этого O
3	корпуса O
4	мы O
5	обучили O
6	word2vec I-TERM
7	- I-TERM
8	модель I-TERM
9	, O
10	где O
11	для O
12	каждого O
13	токена O
14	выучили O
15	эмбеддинг I-TERM
16	размера O
17	50.

# sent_id = 485
# text =  Благодаря богатому набору данных бустинг индивидуально имеет приличное качество.
1	Благодаря O
2	богатому O
3	набору O
4	данных O
5	бустинг I-TERM
6	индивидуально O
7	имеет O
8	приличное O
9	качество O
10	. O

# sent_id = 486
# text =  Одной из первых практических задач было определение авторства политических текстов The Federalist Papers, написанных в США в 1780 годах.
1	Одной O
2	из O
3	первых O
4	практических O
5	задач O
6	было O
7	определение I-TERM
8	авторства I-TERM
9	политических O
10	текстов O
11	The O
12	Federalist O
13	Papers O
14	, O
15	написанных O
16	в O
17	США O
18	в O
19	1780 I-TERM
20	годах I-TERM
21	. O

# sent_id = 487
# text =  Я рассмотрю простейший способ анализа с помощью несложных расчетов и пакета Natural Language Toolkit, что в совокупности с matplotlib позволяет получить интересные результаты буквально в несколько строк кода.
1	Я O
2	рассмотрю O
3	простейший O
4	способ O
5	анализа O
6	с O
7	помощью O
8	несложных O
9	расчетов O
10	и O
11	пакета O
12	Natural I-TERM
13	Language I-TERM
14	Toolkit I-TERM
15	, O
16	что O
17	в O
18	совокупности O
19	с O
20	matplotlib I-TERM
21	позволяет O
22	получить O
23	интересные O
24	результаты O
25	буквально O
26	в O
27	несколько O
28	строк O
29	кода O
30	. O

# sent_id = 488
# text =  К этой группе относятся решения от крупнейших компаний: Amazon Machine Learning, Microsoft Azure Machine Learning и Microsoft Cognitive Services, Google Cloud Prediction API и Google Cloud Machine Learning, IBM Watson Cloud и AlchemyAPI, BigML и другие.
1	К O
2	этой O
3	группе O
4	относятся O
5	решения O
6	от O
7	крупнейших O
8	компаний O
9	: O
10	Amazon I-TERM
11	Machine I-TERM
12	Learning I-TERM
13	, O
14	Microsoft I-TERM
15	Azure I-TERM
16	Machine I-TERM
17	Learning I-TERM
18	и O
19	Microsoft I-TERM
20	Cognitive I-TERM
21	Services I-TERM
22	, O
23	Google I-TERM
24	Cloud I-TERM
25	Prediction I-TERM
26	API I-TERM
27	и O
28	Google I-TERM
29	Cloud I-TERM
30	Machine I-TERM
31	Learning I-TERM
32	, O
33	IBM I-TERM
34	Watson I-TERM
35	Cloud I-TERM
36	и O
37	AlchemyAPI I-TERM
38	, O
39	BigML I-TERM
40	и O
41	другие O
42	. O

# sent_id = 489
# text =  Возможности этого сервиса в области анализа речи и естественного языка пока ограничиваются английским языком, однако многие другие сервисы поддерживают русский язык, например, полностью бесплатный wit.ai, приобретённый Facebook, и его российский конкурент api.ai (понимание текстовых и голосовых команд и вопросов на естественных языках, преобразование речи в текст), IBM AlchemyAPI (анализ тональности текста, выявление сущностей и ключевых слов), Google Natural Language API (классификация текстов, графы связей, извлечение информации из текстов, анализ тональности, намерений, извлечение инсайтов; поддерживает русский язык с помощью технологии машинного перевода Google Translate, использует глубокое обучение и word2vec).
1	Возможности O
2	этого O
3	сервиса O
4	в O
5	области O
6	анализа I-TERM
7	речи I-TERM
8	и O
9	естественного O
10	языка O
11	пока O
12	ограничиваются O
13	английским O
14	языком O
15	, O
16	однако O
17	многие O
18	другие O
19	сервисы O
20	поддерживают O
21	русский O
22	язык O
23	, O
24	например O
25	, O
26	полностью O
27	бесплатный O
28	wit.ai I-TERM
29	, O
30	приобретённый O
31	Facebook I-TERM
32	, O
33	и O
34	его O
35	российский O
36	конкурент O
37	api.ai I-TERM
38	( O
39	понимание I-TERM
40	текстовых I-TERM
41	и I-TERM
42	голосовых I-TERM
43	команд I-TERM
44	и I-TERM
45	вопросов I-TERM
46	на O
47	естественных O
48	языках O
49	, O
50	преобразование I-TERM
51	речи I-TERM
52	в I-TERM
53	текст I-TERM
54	) O
55	, O
56	IBM I-TERM
57	AlchemyAPI I-TERM
58	( O
59	анализ I-TERM
60	тональности I-TERM
61	текста I-TERM
62	, O
63	выявление I-TERM
64	сущностей I-TERM
65	и O
66	ключевых I-TERM
67	слов I-TERM
68	) O
69	, O
70	Google I-TERM
71	Natural I-TERM
72	Language I-TERM
73	API I-TERM
74	( O
75	классификация I-TERM
76	текстов O
77	, O
78	графы I-TERM
79	связей I-TERM
80	, O
81	извлечение I-TERM
82	информации I-TERM
83	из O
84	текстов O
85	, O
86	анализ I-TERM
87	тональности I-TERM
88	, O
89	намерений O
90	, O
91	извлечение O
92	инсайтов O
93	; O
94	поддерживает O
95	русский O
96	язык O
97	с O
98	помощью O
99	технологии O
100	машинного I-TERM
101	перевода I-TERM
102	Google I-TERM
103	Translate I-TERM
104	, O
105	использует O
106	глубокое O
107	обучение O
108	и O
109	word2vec I-TERM
110	) O
111	. O

# sent_id = 490
# text =  Например, IBM Watson предлагает инструмент Personality Insights, позволяющий определять черты личности человека, его потребности и ценности, намерения и другие характеристики по его записям в Твиттере, социальных сетях или по другим текстовым источникам.
1	Например O
2	, O
3	IBM I-TERM
4	Watson I-TERM
5	предлагает O
6	инструмент O
7	Personality I-TERM
8	Insights I-TERM
9	, O
10	позволяющий O
11	определять O
12	черты O
13	личности O
14	человека O
15	, O
16	его O
17	потребности O
18	и O
19	ценности O
20	, O
21	намерения O
22	и O
23	другие O
24	характеристики O
25	по O
26	его O
27	записям O
28	в O
29	Твиттере I-TERM
30	, O
31	социальных O
32	сетях O
33	или O
34	по O
35	другим O
36	текстовым O
37	источникам O
38	. O

# sent_id = 491
# text =  Например, Diffbot позволяет автоматически сканировать страницы сайтов, извлекать из них нужную информацию: тексты, изображения, видео, информацию о продуктах, комментарии и др., в очищенном в структурированном виде, а также позволяет классифицировать страницы.
1	Например O
2	, O
3	Diffbot I-TERM
4	позволяет O
5	автоматически O
6	сканировать I-TERM
7	страницы I-TERM
8	сайтов I-TERM
9	, O
10	извлекать O
11	из O
12	них O
13	нужную O
14	информацию O
15	: O
16	тексты O
17	, O
18	изображения O
19	, O
20	видео O
21	, O
22	информацию O
23	о O
24	продуктах O
25	, O
26	комментарии O
27	и др. O
28	, O
29	в O
30	очищенном O
31	в O
32	структурированном O
33	виде O
34	, O
35	а O
36	также O
37	позволяет O
38	классифицировать I-TERM
39	страницы I-TERM
40	. O

# sent_id = 492
# text =  При этом используются широкий спектр технологий: анализ структуры страниц, машинное обучение, искусственный интеллект, обработка естественных языков и машинное зрение.
1	При O
2	этом O
3	используются O
4	широкий O
5	спектр O
6	технологий O
7	: O
8	анализ I-TERM
9	структуры I-TERM
10	страниц I-TERM
11	, O
12	машинное I-TERM
13	обучение I-TERM
14	, O
15	искусственный I-TERM
16	интеллект I-TERM
17	, O
18	обработка I-TERM
19	естественных I-TERM
20	языков I-TERM
21	и O
22	машинное I-TERM
23	зрение I-TERM
24	. O

# sent_id = 493
# text =  Решения, основанные на Deepomatic, позволяют находить информацию о фильме по его постеру, информацию о картине или скульптуре на выставке по ее фото, сделанному на камеру телефона, позволяют скачивать музыку, сфотографировав обложку альбома на диске и т.п.
1	Решения O
2	, O
3	основанные O
4	на O
5	Deepomatic I-TERM
6	, O
7	позволяют O
8	находить I-TERM
9	информацию I-TERM
10	о I-TERM
11	фильме I-TERM
12	по O
13	его O
14	постеру O
15	, O
16	информацию I-TERM
17	о I-TERM
18	картине I-TERM
19	или O
20	скульптуре O
21	на O
22	выставке O
23	по O
24	ее O
25	фото O
26	, O
27	сделанному O
28	на O
29	камеру O
30	телефона O
31	, O
32	позволяют O
33	скачивать O
34	музыку O
35	, O
36	сфотографировав O
37	обложку O
38	альбома O
39	на O
40	диске O
41	и O
42	т.п. O

# sent_id = 494
# text =  В нашем случае цель была сформулирована как повышение эффективности поиска кандидатов.
1	В O
2	нашем O
3	случае O
4	цель O
5	была O
6	сформулирована O
7	как O
8	повышение I-TERM
9	эффективности I-TERM
10	поиска I-TERM
11	кандидатов I-TERM
12	. O

# sent_id = 495
# text =  Основная задача здесь — найти эффективный способ отображения соответствия кандидатов и навыков.
1	Основная O
2	задача O
3	здесь O
4	— O
5	найти I-TERM
6	эффективный I-TERM
7	способ I-TERM
8	отображения I-TERM
9	соответствия I-TERM
10	кандидатов I-TERM
11	и I-TERM
12	навыков I-TERM
13	. O

# sent_id = 496
# text =  Кодирование в переменные — One-Hot Encoding (OHE) 
1	Кодирование O
2	в O
3	переменные O
4	— O
5	One I-TERM
6	- I-TERM
7	Hot I-TERM
8	Encoding I-TERM
9	( O
10	OHE I-TERM
11	) O

# sent_id = 497
# text =  Для этого используют метод TF-IDF.
1	Для O
2	этого O
3	используют O
4	метод O
5	TF I-TERM
6	- I-TERM
7	IDF I-TERM
8	. O

# sent_id = 498
# text =  Соответственно, можно схлопнуть похожие навыки в некоторые факторы/компоненты/латентные признаки.
1	Соответственно O
2	, O
3	можно O
4	схлопнуть O
5	похожие O
6	навыки O
7	в O
8	некоторые O
9	факторы I-TERM
10	/ O
11	компоненты I-TERM
12	/ O
13	латентные I-TERM
14	признаки I-TERM
15	. O

# sent_id = 499
# text =  Одним из подходов, позволяющих находить такие компоненты, является группа методов матричной факторизации.
1	Одним O
2	из O
3	подходов O
4	, O
5	позволяющих O
6	находить O
7	такие O
8	компоненты I-TERM
9	, O
10	является O
11	группа O
12	методов I-TERM
13	матричной I-TERM
14	факторизации I-TERM
15	. O

# sent_id = 500
# text =  Полученные представления кандидатов и навыков называют эмбедингами.
1	Полученные O
2	представления O
3	кандидатов O
4	и O
5	навыков O
6	называют O
7	эмбедингами I-TERM
8	. O

# sent_id = 501
# text =  При создании нашей системы рекомендации кандидатов на позиции мы использовали нейронную сеть — StarSpace.
1	При O
2	создании O
3	нашей O
4	системы O
5	рекомендации O
6	кандидатов O
7	на O
8	позиции O
9	мы O
10	использовали O
11	нейронную I-TERM
12	сеть I-TERM
13	— O
14	StarSpace I-TERM
15	. O

# sent_id = 502
# text =  Другая группа методов, позволяющая решать задачи репрезентации сущностей — репрезентация графов.
1	Другая O
2	группа O
3	методов O
4	, O
5	позволяющая O
6	решать O
7	задачи I-TERM
8	репрезентации I-TERM
9	сущностей I-TERM
10	— O
11	репрезентация I-TERM
12	графов I-TERM
13	. O

# sent_id = 503
# text =  Но большинство методов графовой репрезентации работает с одномодальными графами, поэтому обычно двухмодальные графы следует трансформировать в граф, где узлы представлены одним видом сущностей.
1	Но O
2	большинство O
3	методов I-TERM
4	графовой I-TERM
5	репрезентации I-TERM
6	работает O
7	с O
8	одномодальными I-TERM
9	графами I-TERM
10	, O
11	поэтому O
12	обычно O
13	двухмодальные I-TERM
14	графы I-TERM
15	следует O
16	трансформировать O
17	в O
18	граф O
19	, O
20	где O
21	узлы O
22	представлены O
23	одним O
24	видом O
25	сущностей O
26	. O

# sent_id = 504
# text =  В первую очередь рассмотрим метод, основанный на графовой факторизации.
1	В O
2	первую O
3	очередь O
4	рассмотрим O
5	метод O
6	, O
7	основанный O
8	на O
9	графовой I-TERM
10	факторизации I-TERM
11	. O

# sent_id = 505
# text =  Это группа методов очень похожа на методы, применяемые для репрезентации текстов — w2v (skip-gram), doc2vec.
1	Это O
2	группа O
3	методов O
4	очень O
5	похожа O
6	на O
7	методы O
8	, O
9	применяемые O
10	для O
11	репрезентации O
12	текстов O
13	— O
14	w2v I-TERM
15	( O
16	skip I-TERM
17	- I-TERM
18	gram I-TERM
19	) O
20	, O
21	doc2vec I-TERM
22	. O

# sent_id = 506
# text =  Почитать подробнее про подобные методы графовой репрезентации можно, например, тут — DeepWalk, Node2vec, Graph2vec.
1	Почитать O
2	подробнее O
3	про O
4	подобные O
5	методы O
6	графовой O
7	репрезентации O
8	можно O
9	, O
10	например O
11	, O
12	тут O
13	— O
14	DeepWalk I-TERM
15	, O
16	Node2vec I-TERM
17	, O
18	Graph2vec I-TERM
19	. O

# sent_id = 507
# text =  Сверточные сети на графах (Graph Convolutional Networks).
1	Сверточные O
2	сети O
3	на O
4	графах O
5	( O
6	Graph I-TERM
7	Convolutional I-TERM
8	Networks I-TERM
9	) O
10	. O

# sent_id = 508
# text =  Для задачи репрезентации графов связей между сущностями мы использовали фреймворк PyTorch BigGraph — это ещё один фреймворк от Facebook Research.
1	Для O
2	задачи O
3	репрезентации I-TERM
4	графов I-TERM
5	связей I-TERM
6	между I-TERM
7	сущностями I-TERM
8	мы O
9	использовали O
10	фреймворк O
11	PyTorch I-TERM
12	BigGraph I-TERM
13	— O
14	это O
15	ещё O
16	один O
17	фреймворк O
18	от O
19	Facebook I-TERM
20	Research I-TERM
21	. O

# sent_id = 509
# text =  Энкодер предложений (sentence encoder) – это модель, которая сопоставляет коротким текстам векторы в многомерном пространстве, причём так, что у текстов, похожих по смыслу, и векторы тоже похожи.
1	Энкодер I-TERM
2	предложений I-TERM
3	( O
4	sentence I-TERM
5	encoder I-TERM
6	) O
7	– O
8	это O
9	модель O
10	, O
11	которая O
12	сопоставляет O
13	коротким O
14	текстам O
15	векторы O
16	в O
17	многомерном O
18	пространстве O
19	, O
20	причём O
21	так O
22	, O
23	что O
24	у O
25	текстов O
26	, O
27	похожих O
28	по O
29	смыслу O
30	, O
31	и O
32	векторы O
33	тоже O
34	похожи O
35	. O

# sent_id = 510
# text =  Обычно для этой цели используются нейросети, а полученные векторы называются эмбеддингами.
1	Обычно O
2	для O
3	этой O
4	цели O
5	используются O
6	нейросети I-TERM
7	, O
8	а O
9	полученные O
10	векторы O
11	называются O
12	эмбеддингами I-TERM
13	. O

# sent_id = 511
# text =  Они полезны для кучи задач, например, few-shot классификации текстов, семантического поиска, или оценки качества перефразирования.
1	Они O
2	полезны O
3	для O
4	кучи O
5	задач O
6	, O
7	например O
8	, O
9	few I-TERM
10	- I-TERM
11	shot I-TERM
12	классификации I-TERM
13	текстов I-TERM
14	, O
15	семантического I-TERM
16	поиска I-TERM
17	, O
18	или O
19	оценки I-TERM
20	качества I-TERM
21	перефразирования I-TERM
22	. O

# sent_id = 512
# text =  Самой качественной моделью оказался mUSE, самой быстрой из предобученных – FastText, а по балансу скорости и качества победил rubert-tiny2.
1	Самой O
2	качественной O
3	моделью O
4	оказался O
5	mUSE I-TERM
6	, O
7	самой O
8	быстрой O
9	из O
10	предобученных O
11	– O
12	FastText I-TERM
13	, O
14	а O
15	по O
16	балансу O
17	скорости O
18	и O
19	качества O
20	победил O
21	rubert I-TERM
22	- I-TERM
23	tiny2 I-TERM
24	. O

# sent_id = 513
# text =  Первой известной попыткой системно сравнить английские эмбеддинги предложений был SentEval, сочетающий чисто лингвистические задачи со вполне прикладными.
1	Первой O
2	известной O
3	попыткой O
4	системно O
5	сравнить O
6	английские O
7	эмбеддинги I-TERM
8	предложений O
9	был O
10	SentEval I-TERM
11	, O
12	сочетающий O
13	чисто O
14	лингвистические O
15	задачи O
16	со O
17	вполне O
18	прикладными O
19	. O

# sent_id = 514
# text =  Для русского языка тоже было создано немало разного рода бенчмарков NLU моделей:RussianSuperGLUE: бенчмарк "сложных" NLP задач; фокус на дообучаемых моделях.
1	Для O
2	русского I-TERM
3	языка O
4	тоже O
5	было O
6	создано O
7	немало O
8	разного O
9	рода O
10	бенчмарков O
11	NLU I-TERM
12	моделей O
13	: O
14	RussianSuperGLUE I-TERM
15	: O
16	бенчмарк O
17	" O
18	сложных O
19	" O
20	NLP I-TERM
21	задач I-TERM
22	; O
23	фокус O
24	на O
25	дообучаемых O
26	моделях O
27	. O

# sent_id = 515
# text = MOROCCO: RussianSuperGLUE + оценка производительности, довольно трудновоспроизводимый бенчмарк.
1	MOROCCO O
2	: O
3	RussianSuperGLUE I-TERM
4	+ O
5	оценка O
6	производительности O
7	, O
8	довольно O
9	трудновоспроизводимый O
10	бенчмарк O
11	. O

# sent_id = 516
# text =  RuSentEval: бенчмарк BERT-подобных энкодеров предложений на лингвистических задачах.
1	RuSentEval I-TERM
2	: O
3	бенчмарк O
4	BERT I-TERM
5	- I-TERM
6	подобных I-TERM
7	энкодеров I-TERM
8	предложений I-TERM
9	на O
10	лингвистических O
11	задачах O
12	. O

# sent_id = 517
# text =  SentEvalRu и deepPavlovEval: два хороших, но давно не обновлявшихся прикладных бенчмарка.
1	SentEvalRu I-TERM
2	и O
3	deepPavlovEval I-TERM
4	: O
5	два O
6	хороших O
7	, O
8	но O
9	давно O
10	не O
11	обновлявшихся O
12	прикладных O
13	бенчмарка O
14	. O

# sent_id = 518
# text =  С тех пор появилось много новых русскоязычных моделей, включая rubert-tiny2, поэтому и бенчмарк пришло время обновить.
1	С O
2	тех O
3	пор O
4	появилось O
5	много O
6	новых O
7	русскоязычных O
8	моделей O
9	, O
10	включая O
11	rubert I-TERM
12	- I-TERM
13	tiny2 I-TERM
14	, O
15	поэтому O
16	и O
17	бенчмарк O
18	пришло O
19	время O
20	обновить O
21	. O

# sent_id = 519
# text =  В основу бенчмарка легли BERT-подобные модели: sbert_large_nlu_ru, sbert_large_mt_nlu_ru, и ruRoberta-large от Сбера; rubert-base-cased-sentence, rubert-base-cased-conversational, distilrubert-tiny-cased-conversational, и distilrubert-base-cased-conversational от DeepPavlov; мои   rubert-tiny и rubert-tiny2; мультиязычные LaBSE (плюс урезанная версия LaBSE-en-ru) и старый добрый bert-base-multilingual-cased.
1	В O
2	основу O
3	бенчмарка O
4	легли O
5	BERT I-TERM
6	- I-TERM
7	подобные I-TERM
8	модели I-TERM
9	: O
10	sbert_large_nlu_ru I-TERM
11	, O
12	sbert_large_mt_nlu_ru I-TERM
13	, O
14	и O
15	ruRoberta I-TERM
16	- I-TERM
17	large I-TERM
18	от O
19	Сбера I-TERM
20	; O
21	rubert I-TERM
22	- I-TERM
23	base I-TERM
24	- I-TERM
25	cased I-TERM
26	- I-TERM
27	sentence I-TERM
28	, O
29	rubert I-TERM
30	- I-TERM
31	base I-TERM
32	- I-TERM
33	cased I-TERM
34	- I-TERM
35	conversational I-TERM
36	, O
37	distilrubert I-TERM
38	- I-TERM
39	tiny I-TERM
40	- I-TERM
41	cased I-TERM
42	- I-TERM
43	conversational I-TERM
44	, O
45	и O
46	distilrubert I-TERM
47	- I-TERM
48	base I-TERM
49	- I-TERM
50	cased I-TERM
51	- I-TERM
52	conversational I-TERM
53	от O
54	DeepPavlov I-TERM
55	; O
56	мои O
57	   O
58	rubert I-TERM
59	- I-TERM
60	tiny I-TERM
61	и O
62	rubert I-TERM
63	- I-TERM
64	tiny2 I-TERM
65	; O
66	мультиязычные O
67	LaBSE I-TERM
68	( O
69	плюс O
70	урезанная O
71	версия O
72	LaBSE I-TERM
73	- I-TERM
74	en I-TERM
75	- I-TERM
76	ru I-TERM
77	) O
78	и O
79	старый O
80	добрый O
81	bert I-TERM
82	- I-TERM
83	base I-TERM
84	- I-TERM
85	multilingual I-TERM
86	- I-TERM
87	cased I-TERM
88	. O

# sent_id = 520
# text =  Кроме этого, я добавил в бенчмарк разные T5 модели, т.к. они тоже должны хорошо понимать тексты: мои rut5-small, rut5-base, rut5-base-multitask, и rut5-base-paraphraser, и Сберовские ruT5-base и ruT5-large.
1	Кроме O
2	этого O
3	, O
4	я O
5	добавил O
6	в O
7	бенчмарк O
8	разные O
9	T5 I-TERM
10	модели I-TERM
11	, O
12	т.к. O
13	они O
14	тоже O
15	должны O
16	хорошо O
17	понимать O
18	тексты O
19	: O
20	мои O
21	rut5-small I-TERM
22	, O
23	rut5-base I-TERM
24	, O
25	rut5-base I-TERM
26	- I-TERM
27	multitask I-TERM
28	, O
29	и O
30	rut5-base I-TERM
31	- I-TERM
32	paraphraser I-TERM
33	, O
34	и O
35	Сберовские I-TERM
36	ruT5-base I-TERM
37	и O
38	ruT5-large I-TERM
39	. O

# sent_id = 521
# text =  Помимо BERTов и T5, я включил в бенчмарк большие мультиязычные модели Laser от FAIR и USE-multilingual-large от Google.
1	Помимо O
2	BERTов O
3	и O
4	T5 O
5	, O
6	я O
7	включил O
8	в O
9	бенчмарк O
10	большие O
11	мультиязычные O
12	модели O
13	Laser I-TERM
14	от O
15	FAIR I-TERM
16	и O
17	USE I-TERM
18	- I-TERM
19	multilingual I-TERM
20	- I-TERM
21	large I-TERM
22	от O
23	Google I-TERM
24	. O

# sent_id = 522
# text =  В качестве быстрого бейзлайна, я добавил FastText, а именно, geowac_tokens_none_fasttextskipgram_300_5_2020  с RusVectores, а также его сжатую версию.
1	В O
2	качестве O
3	быстрого O
4	бейзлайна O
5	, O
6	я O
7	добавил O
8	FastText I-TERM
9	, O
10	а O
11	именно O
12	, O
13	geowac_tokens_none_fasttextskipgram_300_5_2020 I-TERM
14	с O
15	RusVectores I-TERM
16	, O
17	а O
18	также O
19	его O
20	сжатую O
21	версию O
22	. O

# sent_id = 523
# text =  Наконец, я добавил парочку "моделей", которые вообще не выучивают никаких параметров, а просто используют HashingVectorizer для превращения текста в вектор признаков.
1	Наконец O
2	, O
3	я O
4	добавил O
5	парочку O
6	" O
7	моделей O
8	" O
9	, O
10	которые O
11	вообще O
12	не O
13	выучивают O
14	никаких O
15	параметров O
16	, O
17	а O
18	просто O
19	используют O
20	HashingVectorizer I-TERM
21	для O
22	превращения O
23	текста O
24	в O
25	вектор O
26	признаков O
27	. O

# sent_id = 524
# text =  Это доработанная версия rubert-tiny: я расширил словарь модели c 30К до 80К токенов, увеличил максимальную длину текста с 512 до 2048 токенов, и дообучил модель на комбинации задач masked language modelling, natural language inference, и аппроксимации эмбеддингов LaBSE.
1	Это O
2	доработанная O
3	версия O
4	rubert I-TERM
5	- I-TERM
6	tiny I-TERM
7	: O
8	я O
9	расширил O
10	словарь O
11	модели O
12	c O
13	30К O
14	до O
15	80К O
16	токенов O
17	, O
18	увеличил O
19	максимальную O
20	длину O
21	текста O
22	с O
23	512 O
24	до O
25	2048 O
26	токенов O
27	, O
28	и O
29	дообучил O
30	модель O
31	на O
32	комбинации O
33	задач O
34	masked I-TERM
35	language I-TERM
36	modelling I-TERM
37	, O
38	natural I-TERM
39	language I-TERM
40	inference I-TERM
41	, O
42	и O
43	аппроксимации I-TERM
44	эмбеддингов I-TERM
45	LaBSE I-TERM
46	. O

# sent_id = 525
# text =  В новой версии бенчмарка я оставил всё те же 10 задач, что и в прежней, но слегка изменил формат некоторых из них:Semantic text similarity (STS) на основе переведённого датасета STS-B; Paraphrase identification (PI) на основе датасета paraphraser.ru;Natural language inference (NLI) на датасете XNLI; Sentiment analysis (SA) на данных SentiRuEval2016.
1	В O
2	новой O
3	версии O
4	бенчмарка O
5	я O
6	оставил O
7	всё O
8	те O
9	же O
10	10 O
11	задач O
12	, O
13	что O
14	и O
15	в O
16	прежней O
17	, O
18	но O
19	слегка O
20	изменил O
21	формат O
22	некоторых O
23	из O
24	них O
25	: O
26	Semantic I-TERM
27	text I-TERM
28	similarity I-TERM
29	( O
30	STS I-TERM
31	) O
32	на O
33	основе O
34	переведённого O
35	датасета O
36	STS I-TERM
37	- I-TERM
38	B I-TERM
39	; O
40	Paraphrase I-TERM
41	identification I-TERM
42	( O
43	PI I-TERM
44	) O
45	на O
46	основе O
47	датасета O
48	paraphraser.ru I-TERM
49	; O
50	Natural I-TERM
51	language I-TERM
52	inference I-TERM
53	( O
54	NLI I-TERM
55	) O
56	на O
57	датасете O
58	XNLI I-TERM
59	; O
60	Sentiment I-TERM
61	analysis I-TERM
62	( O
63	SA I-TERM
64	) O
65	на O
66	данных O
67	SentiRuEval2016 I-TERM
68	. O

# sent_id = 526
# text =  В прошлой версии бенчмарка я собрал кривые тестовые выборки, поэтому этот датасет я переделал; Toxicity identification (TI) на датасете токсичных комментариев из OKMLCup; Inappropriateness identification (II) на датасете Сколтеха; Intent classification (IC) и её кросс-язычная версия ICX на датасете NLU-evaluation-data, который я автоматически перевёл на русский.
1	В O
2	прошлой O
3	версии O
4	бенчмарка O
5	я O
6	собрал O
7	кривые O
8	тестовые O
9	выборки O
10	, O
11	поэтому O
12	этот O
13	датасет O
14	я O
15	переделал O
16	; O
17	Toxicity I-TERM
18	identification I-TERM
19	( O
20	TI I-TERM
21	) O
22	на O
23	датасете I-TERM
24	токсичных I-TERM
25	комментариев I-TERM
26	из O
27	OKMLCup I-TERM
28	; O
29	Inappropriateness I-TERM
30	identification I-TERM
31	( O
32	II I-TERM
33	) O
34	на O
35	датасете I-TERM
36	Сколтеха I-TERM
37	; O
38	Intent I-TERM
39	classification I-TERM
40	( O
41	IC I-TERM
42	) O
43	и O
44	её O
45	кросс O
46	- O
47	язычная O
48	версия O
49	ICX I-TERM
50	на O
51	датасете O
52	NLU I-TERM
53	- I-TERM
54	evaluation I-TERM
55	- I-TERM
56	data I-TERM
57	, O
58	который O
59	я O
60	автоматически O
61	перевёл O
62	на O
63	русский I-TERM
64	. O

# sent_id = 527
# text =  В IC классификатор обучается на русских данных, а в ICX – на английских, а тестируется в обоих случаях на русских.
1	В O
2	IC I-TERM
3	классификатор I-TERM
4	обучается O
5	на O
6	русских I-TERM
7	данных O
8	, O
9	а O
10	в O
11	ICX I-TERM
12	– O
13	на O
14	английских I-TERM
15	, O
16	а O
17	тестируется O
18	в O
19	обоих O
20	случаях O
21	на O
22	русских O
23	. O

# sent_id = 528
# text =  Распознавание именованных сущностей () на датасетах factRuEval-2016E1) и RuDReC (NE2).
1	Распознавание O
2	именованных O
3	сущностей O
4	( O
5	) O
6	на O
7	датасетах O
8	factRuEval-2016E1 I-TERM
9	) O
10	и O
11	RuDReC I-TERM
12	( O
13	NE2 I-TERM
14	) O
15	. O

# sent_id = 529
# text =  Эти две задачи требуют получать эмбеддинги отдельных токенов, а не целых предложений; поэтому модели USE и Laser, не выдающие эмбеддинги токенов "из коробки", в оценке этих задач не участвовали.
1	Эти O
2	две O
3	задачи O
4	требуют O
5	получать O
6	эмбеддинги O
7	отдельных O
8	токенов O
9	, O
10	а O
11	не O
12	целых O
13	предложений O
14	; O
15	поэтому O
16	модели O
17	USE I-TERM
18	и O
19	Laser I-TERM
20	, O
21	не O
22	выдающие O
23	эмбеддинги O
24	токенов O
25	" O
26	из O
27	коробки O
28	" O
29	, O
30	в O
31	оценке O
32	этих O
33	задач O
34	не O
35	участвовали O
36	. O

# sent_id = 530
# text =  В задачах STS, PI и NLI оценивается степень связи двух текстов.
1	В O
2	задачах O
3	STS I-TERM
4	, O
5	PI I-TERM
6	и O
7	NLI I-TERM
8	оценивается O
9	степень O
10	связи O
11	двух O
12	текстов O
13	. O

# sent_id = 531
# text =  Хороший энкодер предложений должен отражать эту степень в их косинусной близости, поэтому для STS и PI мы измеряем качество как Спирмановскую корреляцию косинусной близости и человеческих оценок сходства.
1	Хороший O
2	энкодер O
3	предложений O
4	должен O
5	отражать O
6	эту O
7	степень O
8	в O
9	их O
10	косинусной I-TERM
11	близости I-TERM
12	, O
13	поэтому O
14	для O
15	STS I-TERM
16	и O
17	PI I-TERM
18	мы O
19	измеряем O
20	качество O
21	как O
22	Спирмановскую I-TERM
23	корреляцию I-TERM
24	косинусной O
25	близости O
26	и O
27	человеческих O
28	оценок O
29	сходства O
30	. O

# sent_id = 532
# text =  Для NLI я обучил трёхклассовую (entail/contradict/neutral) логистическую регрессию поверх косинусной близости, и измеряю её точность (accuracy).
1	Для O
2	NLI I-TERM
3	я O
4	обучил O
5	трёхклассовую O
6	( O
7	entail O
8	/ O
9	contradict O
10	/ O
11	neutral O
12	) O
13	логистическую I-TERM
14	регрессию I-TERM
15	поверх O
16	косинусной I-TERM
17	близости I-TERM
18	, O
19	и O
20	измеряю O
21	её O
22	точность I-TERM
23	( O
24	accuracy I-TERM
25	) O
26	. O

# sent_id = 533
# text =  Для задач бинарной классификации TI и II я измеряю ROC AUC, а в задачах многоклассовой классификации SA, IC и ICX – точность (accuracy).
1	Для O
2	задач O
3	бинарной O
4	классификации O
5	TI I-TERM
6	и O
7	II I-TERM
8	я O
9	измеряю O
10	ROC I-TERM
11	AUC I-TERM
12	, O
13	а O
14	в O
15	задачах O
16	многоклассовой O
17	классификации O
18	SA I-TERM
19	, O
20	IC I-TERM
21	и O
22	ICX I-TERM
23	– O
24	точность I-TERM
25	( O
26	accuracy I-TERM
27	) O
28	. O

# sent_id = 534
# text =  Для всех задач классификации я обучаю логистическую регрессию либо KNN поверх эмбеддингов предложений, и выбираю лучшую модель из двух.
1	Для O
2	всех O
3	задач O
4	классификации I-TERM
5	я O
6	обучаю O
7	логистическую I-TERM
8	регрессию I-TERM
9	либо O
10	KNN I-TERM
11	поверх O
12	эмбеддингов O
13	предложений O
14	, O
15	и O
16	выбираю O
17	лучшую O
18	модель O
19	из O
20	двух O
21	. O

# sent_id = 535
# text =  Для задач NER я классифицировал токены логистической регрессией поверх их эмбеддингов, и измерял macro F1 по всем классам токенов, кроме О. 
1	Для O
2	задач O
3	NER I-TERM
4	я O
5	классифицировал O
6	токены O
7	логистической I-TERM
8	регрессией I-TERM
9	поверх O
10	их O
11	эмбеддингов O
12	, O
13	и O
14	измерял O
15	macro I-TERM
16	F1 I-TERM
17	по O
18	всем O
19	классам O
20	токенов O
21	, O
22	кроме O
23	О O
24	.

# sent_id = 536
# text = Поскольку разные модели токенизируют тексты по-разному, я токенизировал все тексты razdel'ом, и вычислял эмбеддинг слова как средний эмбеддинг его токенов.
1	Поскольку O
2	разные O
3	модели O
4	токенизируют O
5	тексты O
6	по O
7	- O
8	разному O
9	, O
10	я O
11	токенизировал O
12	все O
13	тексты O
14	razdel'ом I-TERM
15	, O
16	и O
17	вычислял O
18	эмбеддинг O
19	слова O
20	как O
21	средний O
22	эмбеддинг O
23	его O
24	токенов O
25	. O

# sent_id = 537
# text =  Единого победителя нет, но MUSE, sbert_large_mt_nlu_ru и rubert-base-cased-sentence взяли по многу призовых мест.
1	Единого O
2	победителя O
3	нет O
4	, O
5	но O
6	MUSE I-TERM
7	, O
8	sbert_large_mt_nlu_ru I-TERM
9	и O
10	rubert I-TERM
11	- I-TERM
12	base I-TERM
13	- I-TERM
14	cased I-TERM
15	- I-TERM
16	sentence I-TERM
17	взяли O
18	по O
19	многу O
20	призовых O
21	мест O
22	. O

# sent_id = 538
# text =  Удивительно, но модели T5 очень хорошо показали себя на задачах NER.
1	Удивительно O
2	, O
3	но O
4	модели O
5	T5 I-TERM
6	очень O
7	хорошо O
8	показали O
9	себя O
10	на O
11	задачах O
12	NER I-TERM
13	. O

# sent_id = 539
# text =  Самыми качественными энкодерами предложений оказались мультиязычные MUSE, LaBSE и Laser.
1	Самыми O
2	качественными O
3	энкодерами O
4	предложений O
5	оказались O
6	мультиязычные O
7	MUSE I-TERM
8	, O
9	LaBSE I-TERM
10	и O
11	Laser I-TERM
12	. O

# sent_id = 540
# text =  Но выбирать стоит из Парето-оптимальных моделей: таких, что ни одна другая модель не превосходит их по всем критериям.
1	Но O
2	выбирать O
3	стоит O
4	из O
5	Парето I-TERM
6	- I-TERM
7	оптимальных I-TERM
8	моделей I-TERM
9	: O
10	таких O
11	, O
12	что O
13	ни O
14	одна O
15	другая O
16	модель O
17	не O
18	превосходит O
19	их O
20	по O
21	всем O
22	критериям O
23	. O

# sent_id = 541
# text =  Из 25 моделей только 12 Парето-оптимальны:MUSE, rubert-tiny2, FT_geowac, Hashing_1000_char и Hashing_1000 обладают самым лучшим качеством для своей скорости на CPU; MUSE, LaBSE, rubert-tiny2, и distilbert-tiny обладают наилучшим качеством для своей скорости на GPU;MUSE, LaBSE, rubert-tiny2, rubert-tiny, FT_geowac_21mb, и Hashing_1000_char обладают наилучшим качеством для своего размера.
1	Из O
2	25 O
3	моделей O
4	только O
5	12 O
6	Парето O
7	- O
8	оптимальны O
9	: O
10	MUSE I-TERM
11	, O
12	rubert I-TERM
13	- I-TERM
14	tiny2 I-TERM
15	, O
16	FT_geowac I-TERM
17	, O
18	Hashing_1000_char I-TERM
19	и O
20	Hashing_1000 I-TERM
21	обладают O
22	самым O
23	лучшим O
24	качеством O
25	для O
26	своей O
27	скорости O
28	на O
29	CPU O
30	; O
31	MUSE I-TERM
32	, O
33	LaBSE I-TERM
34	, O
35	rubert I-TERM
36	- I-TERM
37	tiny2 I-TERM
38	, O
39	и O
40	distilbert I-TERM
41	- I-TERM
42	tiny I-TERM
43	обладают O
44	наилучшим O
45	качеством O
46	для O
47	своей O
48	скорости O
49	на O
50	GPU O
51	; O
52	MUSE I-TERM
53	, O
54	LaBSE I-TERM
55	, O
56	rubert I-TERM
57	- I-TERM
58	tiny2 I-TERM
59	, O
60	rubert I-TERM
61	- I-TERM
62	tiny I-TERM
63	, O
64	FT_geowac_21 I-TERM
65	mb O
66	, O
67	и O
68	Hashing_1000_char I-TERM
69	обладают O
70	наилучшим O
71	качеством O
72	для O
73	своего O
74	размера O
75	. O

# sent_id = 542
# text =  Актуальный лидерборд смотрите в репозитории: https://github.com/avidale/encodechka
1	Актуальный O
2	лидерборд O
3	смотрите O
4	в O
5	репозитории O
6	: O
7	https://github.com/avidale/encodechka I-TERM
8	. O

# sent_id = 543
# text =  Поддержка NlpCraft IDL добавлена в систему начиная с версии 0.7.5.
1	Поддержка O
2	NlpCraft I-TERM
3	IDL I-TERM
4	добавлена O
5	в O
6	систему O
7	начиная O
8	с O
9	версии O
10	0.7.5 O
11	. O

# sent_id = 544
# text =  Новая версия декларативного языка определения интентов, получившая название NlpCraft IDL (NlpCraft Intents Definition Language), значительно упростила процесс работы с интентами в диалоговых и поисковых системах, построенных на базе проекта Apache NlpCraft и вместе с тем расширила возможности системы.
1	Новая O
2	версия O
3	декларативного O
4	языка O
5	определения O
6	интентов O
7	, O
8	получившая O
9	название O
10	NlpCraft I-TERM
11	IDL I-TERM
12	( O
13	NlpCraft I-TERM
14	Intents I-TERM
15	Definition I-TERM
16	Language I-TERM
17	) O
18	, O
19	значительно O
20	упростила O
21	процесс O
22	работы O
23	с O
24	интентами O
25	в O
26	диалоговых O
27	и O
28	поисковых O
29	системах O
30	, O
31	построенных O
32	на O
33	базе O
34	проекта O
35	Apache I-TERM
36	NlpCraft I-TERM
37	и O
38	вместе O
39	с O
40	тем O
41	расширила O
42	возможности O
43	системы O
44	. O

# sent_id = 545
# text =  NlpCraft IDL - это декларативный язык, позволяющий создавать определения интентов для их последующего использования в моделях Apache NlpCraft.
1	NlpCraft I-TERM
2	IDL I-TERM
3	- O
4	это O
5	декларативный O
6	язык O
7	, O
8	позволяющий O
9	создавать O
10	определения O
11	интентов O
12	для O
13	их O
14	последующего O
15	использования O
16	в O
17	моделях O
18	Apache I-TERM
19	NlpCraft I-TERM
20	. O

# sent_id = 546
# text =  Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
1	Чаще O
2	всего O
3	на O
4	практике O
5	в O
6	NLP I-TERM
7	приходится O
8	сталкиваться O
9	с O
10	задачей O
11	построения I-TERM
12	эмбеддингов I-TERM
13	. O

# sent_id = 547
# text =  Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
1	Для O
2	ее O
3	решения O
4	обычно O
5	используют O
6	один O
7	из O
8	следующих O
9	инструментов O
10	: O
11	Готовые O
12	векторы I-TERM
13	/ O
14	эмбеддинги I-TERM
15	слов O
16	[ O
17	6 O
18	] O
19	; O
20	Внутренние O
21	состояния O
22	CNN I-TERM
23	, O
24	натренированных O
25	на O
26	таких O
27	задачах O
28	как O
29	, O
30	как O
31	определение I-TERM
32	фальшивых I-TERM
33	предложений I-TERM
34	/ O
35	языковое I-TERM
36	моделирование I-TERM
37	/ O
38	классификация I-TERM
39	[ O
40	7 O
41	] O
42	; O
43	Комбинация O
44	выше O
45	перечисленных O
46	методов O
47	; O
48	Кроме O
49	того O
50	, O
51	уже O
52	много O
53	раз O
54	было O
55	показано O
56	[ O
57	9 O
58	] O
59	, O
60	что O
61	в O
62	качестве O
63	хорошего O
64	бейслайна O
65	для O
66	эмбеддингов O
67	предложений O
68	можно O
69	взять O
70	и O
71	просто O
72	усредненные O
73	( O
74	с O
75	парой O
76	незначительных O
77	деталей O
78	, O
79	которые O
80	сейчас O
81	опустим O
82	) O
83	векторы O
84	слов O
85	. O

# sent_id = 548
# text =  Если для дальнейшей обработки не важен порядок слов, то текст упаковывают в Мешок слов (Bag-of-words).
1	Если O
2	для O
3	дальнейшей O
4	обработки O
5	не O
6	важен O
7	порядок O
8	слов O
9	, O
10	то O
11	текст O
12	упаковывают O
13	в O
14	Мешок I-TERM
15	слов I-TERM
16	( O
17	Bag I-TERM
18	- I-TERM
19	of I-TERM
20	- I-TERM
21	words I-TERM
22	) O

# sent_id = 549
# text =  В обучающей выборке мы имеем письма с отметками спам/не спам, и скармливаем их в нейросеть: в полносвязную сеть и CNN подаем Bag-of-words, а в RNN уже можно учесть порядок слов, отправив ей Word Vector.
1	В O
2	обучающей O
3	выборке O
4	мы O
5	имеем O
6	письма O
7	с O
8	отметками O
9	спам O
10	/ O
11	не O
12	спам O
13	, O
14	и O
15	скармливаем O
16	их O
17	в O
18	нейросеть O
19	: O
20	в O
21	полносвязную O
22	сеть O
23	и O
24	CNN I-TERM
25	подаем O
26	Bag I-TERM
27	- I-TERM
28	of I-TERM
29	- I-TERM
30	words I-TERM
31	, O
32	а O
33	в O
34	RNN I-TERM
35	уже O
36	можно O
37	учесть O
38	порядок O
39	слов O
40	, O
41	отправив O
42	ей O
43	Word I-TERM
44	Vector I-TERM
45	. O

# sent_id = 550
# text =  Сначала Яндекс распознаёт речь в текст с указанием таймингов и спикеров (за это отвечает голосовая биометрия).
1	Сначала O
2	Яндекс I-TERM
3	распознаёт I-TERM
4	речь I-TERM
5	в O
6	текст O
7	с O
8	указанием O
9	таймингов O
10	и O
11	спикеров O
12	( O
13	за O
14	это O
15	отвечает O
16	голосовая I-TERM
17	биометрия I-TERM
18	) O
19	. O

# sent_id = 551
# text =  Добавили поддержку немецкого, французского и испанского языков.
1	Добавили O
2	поддержку O
3	немецкого I-TERM
4	, O
5	французского I-TERM
6	и O
7	испанского I-TERM
8	языков O
9	. O

# sent_id = 552
# text =  Поэтому всё время существования как Яндекс Браузера, так и Яндекс Переводчика мы стараемся не просто переводить, но и помогать учить язык.
1	Поэтому O
2	всё O
3	время O
4	существования O
5	как O
6	Яндекс I-TERM
7	Браузера I-TERM
8	, O
9	так O
10	и O
11	Яндекс I-TERM
12	Переводчика I-TERM
13	мы O
14	стараемся O
15	не O
16	просто O
17	переводить O
18	, O
19	но O
20	и O
21	помогать O
22	учить O
23	язык O
24	. O

# sent_id = 553
# text =  С 27 по 30 мая в Российском государственном гуманитарном университете (РГГУ) пройдет международная научная конференция по компьютерной лингвистике «Диалог».
1	С O
2	27 O
3	по O
4	30 O
5	мая O
6	в O
7	Российском I-TERM
8	государственном I-TERM
9	гуманитарном I-TERM
10	университете I-TERM
11	( O
12	РГГУ I-TERM
13	) O
14	пройдет O
15	международная O
16	научная O
17	конференция O
18	по O
19	компьютерной I-TERM
20	лингвистике I-TERM
21	« O
22	Диалог O
23	» O
24	. O

# sent_id = 554
# text =  Подробно о том, что такое «Диалог» и почему ABBYY организует эту конференцию, мы писали здесь .
1	Подробно O
2	о O
3	том O
4	, O
5	что O
6	такое O
7	« O
8	Диалог O
9	» O
10	и O
11	почему O
12	ABBYY I-TERM
13	организует O
14	эту O
15	конференцию O
16	, O
17	мы O
18	писали O
19	здесь O
20	. O

# sent_id = 555
# text =  Главной задачей проведенных ранее тестирований был автоматический анализ тональности в целом небольших текстов – отзывов пользователей (о фильмах, книгах, цифровых фотокамерах) или мнений, выраженных в форме прямой или косвенной речи (новости).
1	Главной O
2	задачей O
3	проведенных O
4	ранее O
5	тестирований O
6	был O
7	автоматический I-TERM
8	анализ I-TERM
9	тональности I-TERM
10	в O
11	целом O
12	небольших O
13	текстов O
14	– O
15	отзывов O
16	пользователей O
17	( O
18	о O
19	фильмах O
20	, O
21	книгах O
22	, O
23	цифровых O
24	фотокамерах O
25	) O
26	или O
27	мнений O
28	, O
29	выраженных O
30	в O
31	форме O
32	прямой O
33	или O
34	косвенной O
35	речи O
36	( O
37	новости O
38	) O
39	. O

# sent_id = 556
# text =  Основной целью нового цикла тестирований является автоматическая оценка тональности по отношению к заданному объекту и его конкретным свойствам.
1	Основной O
2	целью O
3	нового O
4	цикла O
5	тестирований O
6	является O
7	автоматическая I-TERM
8	оценка I-TERM
9	тональности I-TERM
10	по O
11	отношению O
12	к O
13	заданному O
14	объекту O
15	и O
16	его O
17	конкретным O
18	свойствам O
19	. O

# sent_id = 557
# text =  Фишинговые электронные письма - это сообщения, которые кажутся очень похожими на настоящие, например, рассылку от вашего любимого интернет-магазина, но при этом они заманивают людей нажимать на прикрепленные вредоносные ссылки или документы.
1	Фишинговые I-TERM
2	электронные I-TERM
3	письма I-TERM
4	- O
5	это O
6	сообщения O
7	, O
8	которые O
9	кажутся O
10	очень O
11	похожими O
12	на O
13	настоящие O
14	, O
15	например O
16	, O
17	рассылку O
18	от O
19	вашего O
20	любимого O
21	интернет O
22	- O
23	магазина O
24	, O
25	но O
26	при O
27	этом O
28	они O
29	заманивают O
30	людей O
31	нажимать O
32	на O
33	прикрепленные O
34	вредоносные O
35	ссылки O
36	или O
37	документы O
38	. O

# sent_id = 558
# text =  Поэтому в статье предлагается способ обнаружения фишинговых сообщений, называемый Federated Phish Bowl (далее FPB), использующий федеративное обучение и рекуррентную нейронную сеть с долгой краткосрочной памятью (LSTM).
1	Поэтому O
2	в O
3	статье O
4	предлагается O
5	способ O
6	обнаружения I-TERM
7	фишинговых I-TERM
8	сообщений I-TERM
9	, O
10	называемый O
11	Federated I-TERM
12	Phish I-TERM
13	Bowl I-TERM
14	( O
15	далее O
16	FPB I-TERM
17	) O
18	, O
19	использующий O
20	федеративное O
21	обучение O
22	и O
23	рекуррентную I-TERM
24	нейронную I-TERM
25	сеть I-TERM
26	с I-TERM
27	долгой I-TERM
28	краткосрочной I-TERM
29	памятью I-TERM
30	( O
31	LSTM I-TERM
32	) O
33	. O

# sent_id = 559
# text =  Для работы с текстовыми последовательностями были придуманы рекуррентные нейронные сети (RNN, их улучшение - LSTM, которая бывает двунаправленной, когда последовательность обрабатывается в двух направлениях).
1	Для O
2	работы O
3	с O
4	текстовыми O
5	последовательностями O
6	были O
7	придуманы O
8	рекуррентные O
9	нейронные O
10	сети O
11	( O
12	RNN I-TERM
13	, O
14	их O
15	улучшение O
16	- O
17	LSTM I-TERM
18	, O
19	которая O
20	бывает O
21	двунаправленной O
22	, O
23	когда O
24	последовательность O
25	обрабатывается O
26	в O
27	двух O
28	направлениях O
29	) O
30	. O

# sent_id = 560
# text =  Например, сеть можно сделать двунаправленной (Bidirectional LSTM).
1	Например O
2	, O
3	сеть O
4	можно O
5	сделать O
6	двунаправленной O
7	( O
8	Bidirectional I-TERM
9	LSTM I-TERM
10	) O
11	. O

# sent_id = 561
# text =  FPB предлагает использовать подход, показанный на следующем изображении: 
1	FPB I-TERM
2	предлагает O
3	использовать O
4	подход O
5	, O
6	показанный O
7	на O
8	следующем O
9	изображении O
10	: O

# sent_id = 562
# text =  Федеративное обучение - это метод машинного обучения, который обучает алгоритм на нескольких децентрализованных устройствах или серверах, содержащих локальные образцы данных, без обмена ими.
1	Федеративное I-TERM
2	обучение I-TERM
3	- O
4	это O
5	метод O
6	машинного O
7	обучения O
8	, O
9	который O
10	обучает O
11	алгоритм O
12	на O
13	нескольких O
14	децентрализованных O
15	устройствах O
16	или O
17	серверах O
18	, O
19	содержащих O
20	локальные O
21	образцы O
22	данных O
23	, O
24	без O
25	обмена O
26	ими O
27	. O

# sent_id = 563
# text =  Для обучения модели FPB с использованием федеративного обучения (FL) сервер параметров (PS) инициализирует глобальную модель (DL) на основе вышеупомянутых двунаправленных нейронных сетей LSTM и отправляет глобальную модель с глобальной матрицей преобразования слов в векторы всем клиентам на первом этапе обучения (см.
1	Для O
2	обучения O
3	модели O
4	FPB I-TERM
5	с O
6	использованием O
7	федеративного I-TERM
8	обучения I-TERM
9	( O
10	FL I-TERM
11	) O
12	сервер I-TERM
13	параметров I-TERM
14	( O
15	PS I-TERM
16	) O
17	инициализирует O
18	глобальную I-TERM
19	модель I-TERM
20	( O
21	DL I-TERM
22	) O
23	на O
24	основе O
25	вышеупомянутых O
26	двунаправленных O
27	нейронных O
28	сетей O
29	LSTM I-TERM
30	и O
31	отправляет O
32	глобальную O
33	модель O
34	с O
35	глобальной O
36	матрицей O
37	преобразования O
38	слов O
39	в O
40	векторы O
41	всем O
42	клиентам O
43	на O
44	первом O
45	этапе O
46	обучения O
47	. O

# sent_id = 564
# text =  В рамках курса вы узнаете: Как латентные переменные применяются в задачах анализа текстов и как строить глубинные генеративные модели с латентными дискретными переменными.
1	В O
2	рамках O
3	курса O
4	вы O
5	узнаете O
6	: O
7	Как O
8	латентные I-TERM
9	переменные I-TERM
10	применяются O
11	в O
12	задачах O
13	анализа I-TERM
14	текстов I-TERM
15	и O
16	как O
17	строить O
18	глубинные I-TERM
19	генеративные I-TERM
20	модели I-TERM
21	с O
22	латентными O
23	дискретными O
24	переменными O
25	. O

# sent_id = 565
# text =  Что такое semantic parsing: как строить формальные представления смысла текста, извлекая при этом неявные значения.
1	Что O
2	такое O
3	semantic I-TERM
4	parsing I-TERM
5	: O
6	как O
7	строить O
8	формальные O
9	представления O
10	смысла O
11	текста O
12	, O
13	извлекая O
14	при O
15	этом O
16	неявные O
17	значения O
18	. O

# sent_id = 566
# text =  Британские ученые обучили ИИ трансформировать устную речь в видео с виртуальным сурдопереводчиком.
1	Британские O
2	ученые O
3	обучили O
4	ИИ O
5	трансформировать O
6	устную I-TERM
7	речь I-TERM
8	в O
9	видео O
10	с O
11	виртуальным O
12	сурдопереводчиком I-TERM
13	. O

# sent_id = 567
# text =  В Университете Суррея разработчики создали алгоритм сурдоперевода нового поколения.
1	В O
2	Университете I-TERM
3	Суррея I-TERM
4	разработчики O
5	создали O
6	алгоритм I-TERM
7	сурдоперевода I-TERM
8	нового O
9	поколения O
10	. O

# sent_id = 568
# text =  После этого последовательность поз подается сверточной нейросети U-Net.
1	После O
2	этого O
3	последовательность O
4	поз O
5	подается O
6	сверточной O
7	нейросети O
8	U I-TERM
9	- I-TERM
10	Net I-TERM
11	. O

# sent_id = 569
# text =  Один из самых известных продуктов — анимированный виртуальный переводчик от IBM.
1	Один O
2	из O
3	самых O
4	известных O
5	продуктов O
6	— O
7	анимированный O
8	виртуальный O
9	переводчик O
10	от O
11	IBM I-TERM
12	. O

# sent_id = 570
# text =  Программа, придуманная учеными из Новосибирского академгородка, распознает речь, анализирует смысл и переводит на жестовый язык.
1	Программа O
2	, O
3	придуманная O
4	учеными O
5	из O
6	Новосибирского I-TERM
7	академгородка I-TERM
8	, O
9	распознает I-TERM
10	речь I-TERM
11	, O
12	анализирует I-TERM
13	смысл I-TERM
14	и O
15	переводит O
16	на O
17	жестовый I-TERM
18	язык I-TERM
19	. O

# sent_id = 571
# text =  В то время считали, что разработка станет такой же популярной, как Google Translator.
1	В O
2	то O
3	время O
4	считали O
5	, O
6	что O
7	разработка O
8	станет O
9	такой O
10	же O
11	популярной O
12	, O
13	как O
14	Google I-TERM
15	Translator I-TERM
16	. O

# sent_id = 572
# text =  Российские ученые из Института проблем управления им. В.А. Трапезникова РАН (ИПУ РАН) несколько лет назад начали разработку подобного ИИ.
1	Российские O
2	ученые O
3	из O
4	Института I-TERM
5	проблем I-TERM
6	управления I-TERM
7	им. I-TERM 
8	В.А. I-TERM
9	Трапезникова I-TERM
10	РАН I-TERM
11	( O
12	ИПУ I-TERM
13	РАН I-TERM
14	) O
15	несколько O
16	лет O
17	назад O
18	начали O
19	разработку O
20	подобного O
21	ИИ O
22	. O

# sent_id = 573
# text =  Она несколько лет развивает сайт «Сурдосервер».
1	Она O
2	несколько O
3	лет O
4	развивает O
5	сайт O
6	« O
7	Сурдосервер I-TERM
8	» O
9	. O

# sent_id = 574
# text =  N-грамм это просто последовательности букв из слова.
1	N I-TERM
2	- I-TERM
3	грамм I-TERM
4	это O
5	просто O
6	последовательности O
7	букв O
8	из O
9	слова O
10	. O

# sent_id = 575
# text =  Создаются лексические и синтаксические признаки токенов текста.
1	Создаются O
2	лексические I-TERM
3	и O
4	синтаксические I-TERM
5	признаки I-TERM
6	токенов I-TERM
7	текста I-TERM
8	. O

# sent_id = 576
# text =  В качестве классификатора намерений применяем Transformer.
1	В 
2	качестве O
3	классификатора I-TERM
4	намерений O
5	применяем O
6	Transformer I-TERM
7	. O

# sent_id = 577
# text = Как отличить хороший ремонт от плохого, или как мы в SRG сделали из Томита-парсера многопоточную Java-библиотеку. 
1	Как O
2	отличить O
3	хороший O
4	ремонт O
5	от O
6	плохого O
7	, O
8	или O
9	как O
10	мы O
11	в O
12	SRG I-TERM
13	сделали O
14	из O
15	Томита I-TERM
16	- I-TERM
17	парсера I-TERM
18	многопоточную O
19	Java I-TERM
20	- I-TERM
21	библиотеку I-TERM
22	. O

# sent_id = 578
# text = В этой статье речь пойдет о том, как мы интегрировали разработанный Яндексом Томита-парсер в нашу систему, превратили его в динамическую библиотеку, подружили с Java, сделали многопоточной и решили с её помощью задачу классификации текста для оценки недвижимости.
1	В O
2	этой O
3	статье O
4	речь O
5	пойдет O
6	о O
7	том O
8	, O
9	как O
10	мы O
11	интегрировали O
12	разработанный O
13	Яндексом I-TERM
14	Томита I-TERM
15	- I-TERM
16	парсер I-TERM
17	в O
18	нашу O
19	систему O
20	, O
21	превратили O
22	его O
23	в O
24	динамическую O
25	библиотеку O
26	, O
27	подружили O
28	с O
29	Java I-TERM
30	, O
31	сделали O
32	многопоточной O
33	и O
34	решили O
35	с O
36	её O
37	помощью O
38	задачу O
39	классификации I-TERM
40	текста O
41	для O
42	оценки I-TERM
43	недвижимости I-TERM
44	. O

# sent_id = 579
# text =  Итак, у нас есть текст объявления, который необходимо классифицировать в одну из категорий согласно состоянию ремонта в квартире (без отделки, чистовой, средний, хороший, отличный, эксклюзивный).
1	Итак O
2	, O
3	у O
4	нас O
5	есть O
6	текст I-TERM
7	объявления I-TERM
8	, O
9	который O
10	необходимо O
11	классифицировать O
12	в O
13	одну O
14	из O
15	категорий O
16	согласно O
17	состоянию O
18	ремонта O
19	в O
20	квартире O
21	( O
22	без O
23	отделки O
24	, O
25	чистовой O
26	, O
27	средний O
28	, O
29	хороший O
30	, O
31	отличный O
32	, O
33	эксклюзивный O
34	) O
35	. O

# sent_id = 580
# text =  Таким образом, по мере решения сформировалась вторая большая и интересная задача — научиться извлекать всю достаточную и необходимую информацию о ремонте из объявления, а именно обеспечить быстрый синтаксический и морфологический анализ текста, который сможет работать параллельно под нагрузкой в режиме библиотеки.
1	Таким O
2	образом O
3	, O
4	по O
5	мере O
6	решения O
7	сформировалась O
8	вторая O
9	большая O
10	и O
11	интересная O
12	задача O
13	— O
14	научиться O
15	извлекать I-TERM
16	всю I-TERM
17	достаточную I-TERM
18	и I-TERM
19	необходимую I-TERM
20	информацию I-TERM
21	о O
22	ремонте O
23	из O
24	объявления O
25	, O
26	а O
27	именно O
28	обеспечить O
29	быстрый O
30	синтаксический O
31	и O
32	морфологический I-TERM
33	анализ I-TERM
34	текста I-TERM
35	, O
36	который O
37	сможет O
38	работать O
39	параллельно O
40	под O
41	нагрузкой O
42	в O
43	режиме O
44	библиотеки O
45	. O

# sent_id = 581
# text =  Из доступных средств для извлечения фактов из текста на основе контекстно-свободных грамматик, способных работать с русским языком, наше внимание привлекли Томита-парсер и библиотека Yagry на питоне.
1	Из O
2	доступных O
3	средств O
4	для O
5	извлечения I-TERM
6	фактов I-TERM
7	из O
8	текста I-TERM
9	на O
10	основе O
11	контекстно I-TERM
12	- I-TERM
13	свободных I-TERM
14	грамматик I-TERM
15	, O
16	способных O
17	работать O
18	с O
19	русским O
20	языком O
21	, O
22	наше O
23	внимание O
24	привлекли O
25	Томита I-TERM
26	- I-TERM
27	парсер I-TERM
28	и O
29	библиотека O
30	Yagry I-TERM
31	на O
32	питоне O
33	. O

# sent_id = 582
# text =  Многопоточный вариант Томиты — TomitaPooledParser использует для парсинга пул объектов TomitaParser, одинаковым образом сконфигурированных.
1	Многопоточный O
2	вариант O
3	Томиты I-TERM
4	— O
5	TomitaPooledParser I-TERM
6	использует O
7	для O
8	парсинга I-TERM
9	пул O
10	объектов O
11	TomitaParser I-TERM
12	, O
13	одинаковым O
14	образом O
15	сконфигурированных O
16	. O

# sent_id = 583
# text =  Приведу только показатели качества классификации, которые были нами получены на тестах: Accuracy = 95% F1 score = 93%
1	Приведу O
2	только O
3	показатели O
4	качества O
5	классификации I-TERM
6	, O
7	которые O
8	были O
9	нами O
10	получены O
11	на O
12	тестах O
13	: O
14	Accuracy I-TERM
15	= O
16	95 I-TERM
17	% I-TERM
18	F1 I-TERM
19	score I-TERM
20	= O
21	93 I-TERM
22	% I-TERM

# sent_id = 584
# text = JavaScript-библиотека для обработки текстов на русском языке
1	JavaScript I-TERM
2	- I-TERM
3	библиотека I-TERM
4	для O
5	обработки I-TERM
6	текстов I-TERM
7	на O
8	русском O
9	языке O

# sent_id = 585
# text =  Бессвязность текстов в нынешней версии «Генератора» вызвана тем, что на самом деле никакого анализа он производить не умеет.
1	Бессвязность O
2	текстов O
3	в O
4	нынешней O
5	версии O
6	« O
7	Генератора I-TERM
8	» O
9	вызвана O
10	тем O
11	, O
12	что O
13	на O
14	самом O
15	деле O
16	никакого O
17	анализа O
18	он O
19	производить O
20	не O
21	умеет O
22	. O

# sent_id = 586
# text =  На данный момент библиотека умеет две вещи: токенизацию и анализ морфологии.
1	На O
2	данный O
3	момент O
4	библиотека O
5	умеет O
6	две O
7	вещи O
8	: O
9	токенизацию I-TERM
10	и O
11	анализ I-TERM
12	морфологии I-TERM
13	. O

# sent_id = 587
# text =  Полный список граммем можно найти на странице проекта OpenCorpora.
1	Полный O
2	список O
3	граммем O
4	можно O
5	найти O
6	на O
7	странице O
8	проекта O
9	OpenCorpora I-TERM
10	. O

# sent_id = 588
# text =  Кроме того, для анализа используется словарь OpenCorpora, упакованный в специальном формате, но об этом ниже.
1	Кроме O
2	того O
3	, O
4	для O
5	анализа I-TERM
6	используется O
7	словарь O
8	OpenCorpora I-TERM
9	, O
10	упакованный O
11	в O
12	специальном O
13	формате O
14	, O
15	но O
16	об O
17	этом O
18	ниже O
19	. O

# sent_id = 589
# text =  Вообще создатели проекта OpenCorpora большие молодцы и я вам рекомендую не только ознакомиться с ним, но и принять участие в коллаборативной разметке корпуса — это также поможет и другим опенсорсным проектам.
1	Вообще O
2	создатели O
3	проекта O
4	OpenCorpora I-TERM
5	большие O
6	молодцы O
7	и O
8	я O
9	вам O
10	рекомендую O
11	не O
12	только O
13	ознакомиться O
14	с O
15	ним O
16	, O
17	но O
18	и O
19	принять O
20	участие O
21	в O
22	коллаборативной O
23	разметке I-TERM
24	корпуса I-TERM
25	— O
26	это O
27	также O
28	поможет O
29	и O
30	другим O
31	опенсорсным O
32	проектам O
33	. O

# sent_id = 590
# text =  По сути эта часть библиотеки — порт замечательного морфологического анализатора pymorphy2 за авторством kmike (на Хабре была пара статей об этой библиотеке).
1	По O
2	сути O
3	эта O
4	часть O
5	библиотеки O
6	— O
7	порт O
8	замечательного O
9	морфологического I-TERM
10	анализатора I-TERM
11	pymorphy2 I-TERM
12	за O
13	авторством O
14	kmike I-TERM
15	( O
16	на O
17	Хабре I-TERM
18	была O
19	пара O
20	статей O
21	об O
22	этой O
23	библиотеке O
24	) O
25	. O

# sent_id = 591
# text =  Анализируем тональность текстов с помощью Fast.ai
1	Анализируем O
2	тональность O
3	текстов O
4	с O
5	помощью O
6	Fast.ai I-TERM

# sent_id = 592
# text = В статье пойдет речь о классификации тональности текстовых сообщений на русском языке (а по сути любой классификации текстов, используя те же технологии).
1	В O
2	статье O
3	пойдет O
4	речь O
5	о O
6	классификации I-TERM
7	тональности I-TERM
8	текстовых I-TERM
9	сообщений I-TERM
10	на O
11	русском I-TERM
12	языке O
13	( O
14	а O
15	по O
16	сути O
17	любой O
18	классификации O
19	текстов O
20	, O
21	используя O
22	те O
23	же O
24	технологии O
25	) O
26	. O

# sent_id = 593
# text =  За основу возьмем данную статью, в которой была рассмотрена классификация тональности на архитектуре CNN с использованием Word2vec модели.
1	За O
2	основу O
3	возьмем O
4	данную O
5	статью O
6	, O
7	в O
8	которой O
9	была O
10	рассмотрена O
11	классификация I-TERM
12	тональности I-TERM
13	на O
14	архитектуре O
15	CNN I-TERM
16	с O
17	использованием O
18	Word2vec I-TERM
19	модели O
20	. O

# sent_id = 594
# text =  В нашем примере будем решать ту же самую задачу разделения твитов на позитивные и негативные на том же самом датасете с использованием модели ULMFit.
1	В O
2	нашем O
3	примере O
4	будем O
5	решать O
6	ту O
7	же O
8	самую O
9	задачу O
10	разделения I-TERM
11	твитов I-TERM
12	на O
13	позитивные I-TERM
14	и O
15	негативные I-TERM
16	на O
17	том O
18	же O
19	самом O
20	датасете O
21	с O
22	использованием O
23	модели O
24	ULMFit I-TERM
25	. O

# sent_id = 595
# text =  Результат из статьи (average F1-score = 0.78142) примем в качестве baseline.
1	Результат O
2	из O
3	статьи O
4	( O
5	average O
6	F1-score I-TERM
7	= O
8	0.78142 I-TERM
9	) O
10	примем O
11	в O
12	качестве O
13	baseline O
14	. O

# sent_id = 596
# text =  Модель ULMFIT была представлена разработчиками fast.ai (Jeremy Howard, Sebastian Ruder) в 2018 году.
1	Модель O
2	ULMFIT I-TERM
3	была O
4	представлена O
5	разработчиками O
6	fast.ai I-TERM
7	( O
8	Jeremy I-TERM
9	Howard I-TERM
10	, O
11	Sebastian I-TERM
12	Ruder I-TERM
13	) O
14	в O
15	2018 I-TERM
16	году O
17	. O

# sent_id = 597
# text =  Суть подхода состоит в использовании transfer learning в задачах NLP, когда вы используете предобученные модели, сокращая время на обучение своих моделей и снижая требования к размерам размеченной тестовой выборки.
1	Суть O
2	подхода O
3	состоит O
4	в O
5	использовании O
6	transfer I-TERM
7	learning I-TERM
8	в O
9	задачах O
10	NLP I-TERM
11	, O
12	когда O
13	вы O
14	используете O
15	предобученные O
16	модели O
17	, O
18	сокращая O
19	время O
20	на O
21	обучение O
22	своих O
23	моделей O
24	и O
25	снижая O
26	требования O
27	к O
28	размерам O
29	размеченной O
30	тестовой O
31	выборки O
32	. O

# sent_id = 598
# text =  Для задачи моделирования языка ULMFit использует архитектуру AWD-LSTM, которая предполагает активное использование dropout везде, где только можно и имеет смысл.
1	Для O
2	задачи O
3	моделирования I-TERM
4	языка O
5	ULMFit I-TERM
6	использует O
7	архитектуру O
8	AWD I-TERM
9	- I-TERM
10	LSTM I-TERM
11	, O
12	которая O
13	предполагает O
14	активное O
15	использование O
16	dropout O
17	везде O
18	, O
19	где O
20	только O
21	можно O
22	и O
23	имеет O
24	смысл O
25	. O

# sent_id = 599
# text =  Результат, показанный на тестовой выборке average F1-score = 0,80064.
1	Результат O
2	, O
3	показанный O
4	на O
5	тестовой O
6	выборке O
7	average O
8	F1-score I-TERM
9	= O
10	0,80064 I-TERM
11	. O

# sent_id = 600
# text =  Добавьте возможности IBM Watson платформы в ваши приложения, разработанные на платформе IBM Cloud, или в сторонние приложения!
1	Добавьте O
2	возможности O
3	IBM I-TERM
4	Watson I-TERM
5	платформы O
6	в O
7	ваши O
8	приложения O
9	, O
10	разработанные O
11	на O
12	платформе O
13	IBM I-TERM
14	Cloud I-TERM
15	, O
16	или O
17	в O
18	сторонние O
19	приложения O
20	! O

# sent_id = 601
# text =  IBM Automation Platform для цифрового бизнеса — это интегрированная платформа с пятью возможностями автоматизации, которая помогает бизнесу быстро и масштабно управлять практически всеми типами проектов автоматизации — от повторяющихся и административных до работы на уровне экспертов.
1	IBM I-TERM
2	Automation I-TERM
3	Platform I-TERM
4	для O
5	цифрового I-TERM
6	бизнеса I-TERM
7	— O
8	это O
9	интегрированная O
10	платформа O
11	с O
12	пятью O
13	возможностями O
14	автоматизации O
15	, O
16	которая O
17	помогает O
18	бизнесу O
19	быстро O
20	и O
21	масштабно O
22	управлять O
23	практически O
24	всеми O
25	типами O
26	проектов O
27	автоматизации O
28	— O
29	от O
30	повторяющихся O
31	и O
32	административных O
33	до O
34	работы O
35	на O
36	уровне O
37	экспертов O
38	. O

# sent_id = 602
# text =  HuggingArtists | Генерируем текст песен с трансформером за 5 минут 
1	HuggingArtists I-TERM
2	| O
3	Генерируем I-TERM
4	текст I-TERM
5	песен I-TERM
6	с O
7	трансформером O
8	за O
9	5 O
10	минут O

# sent_id = 603
# text =  В HuggingArtists, мы можем создавать тексты песен на основе конкретного исполнителя.
1	В O
2	HuggingArtists I-TERM
3	, O
4	мы O
5	можем O
6	создавать I-TERM
7	тексты I-TERM
8	песен O
9	на O
10	основе O
11	конкретного O
12	исполнителя O
13	. O

# sent_id = 604
# text =  Это было сделано путем fine-tune (точной настройки) предварительно обученного трансформера HuggingFace  на собранных данных Genius.
1	Это O
2	было O
3	сделано O
4	путем O
5	fine I-TERM
6	- I-TERM
7	tune I-TERM
8	( O
9	точной I-TERM
10	настройки I-TERM
11	) O
12	предварительно O
13	обученного O
14	трансформера O
15	HuggingFace I-TERM
16	  O
17	на O
18	собранных O
19	данных O
20	Genius I-TERM
21	. O

# sent_id = 605
# text =  Кроме того, мы используем интеграцию Weights & Biases для автоматического учета производительности и прогнозов модели.
1	Кроме O
2	того O
3	, O
4	мы O
5	используем O
6	интеграцию O
7	Weights I-TERM
8	& I-TERM
9	Biases I-TERM
10	для O
11	автоматического O
12	учета O
13	производительности O
14	и O
15	прогнозов O
16	модели O
17	. O

# sent_id = 606
# text = Анализ тональности текста с использованием фреймворка Lightautoml 
1	Анализ I-TERM
2	тональности I-TERM
3	текста I-TERM
4	с O
5	использованием O
6	фреймворка O
7	Lightautoml I-TERM

# sent_id = 607
# text = Сентиментный анализ (анализ тональности) – это область компьютерной лингвистики, занимающаяся изучением эмоций в текстовых документах, в основе которой лежит машинное обучение.
1	Сентиментный I-TERM
2	анализ I-TERM
3	( O
4	анализ I-TERM
5	тональности I-TERM
6	) O
7	– O
8	это O
9	область O
10	компьютерной O
11	лингвистики O
12	, O
13	занимающаяся O
14	изучением O
15	эмоций O
16	в O
17	текстовых O
18	документах O
19	, O
20	в O
21	основе O
22	которой O
23	лежит O
24	машинное O
25	обучение O
26	. O

# sent_id = 608
# text = В этой статье я покажу, как мы использовали для этих целей внутреннюю разработку компании – фреймворк LightAutoML, в котором имеется всё для решения поставленной задачи – предобученные готовые векторные представления слов FastText и готовые текстовые пресеты, в которых необходимо только указать гиперпараметры.
1	В O
2	этой O
3	статье O
4	я O
5	покажу O
6	, O
7	как O
8	мы O
9	использовали O
10	для O
11	этих O
12	целей O
13	внутреннюю O
14	разработку O
15	компании O
16	– O
17	фреймворк O
18	LightAutoML I-TERM
19	, O
20	в O
21	котором O
22	имеется O
23	всё O
24	для O
25	решения O
26	поставленной O
27	задачи O
28	– O
29	предобученные I-TERM
30	готовые I-TERM
31	векторные I-TERM
32	представления I-TERM
33	слов I-TERM
34	FastText I-TERM
35	и O
36	готовые O
37	текстовые O
38	пресеты O
39	, O
40	в O
41	которых O
42	необходимо O
43	только O
44	указать O
45	гиперпараметры O
46	. O

# sent_id = 609
# text =  При обучении модели значение метрики F1-score достигло 0.894, соответственно можно сделать вывод о том, что модель хорошо справляется с задачей определения нейтральных и негативных обращений.
1	При O
2	обучении O
3	модели O
4	значение O
5	метрики O
6	F1-score I-TERM
7	достигло O
8	0.894 I-TERM
9	, O
10	соответственно O
11	можно O
12	сделать O
13	вывод O
14	о O
15	том O
16	, O
17	что O
18	модель O
19	хорошо O
20	справляется O
21	с O
22	задачей O
23	определения I-TERM
24	нейтральных I-TERM
25	и I-TERM
26	негативных I-TERM
27	обращений I-TERM
28	. O

# sent_id = 610
# text =  Также одним из способов оценить работу модели в целом можно по кривой ROC-AUC, которая описывает площадь под кривой (Area Under Curve – Receiver Operating Characteristic).
1	Также O
2	одним O
3	из O
4	способов O
5	оценить O
6	работу O
7	модели O
8	в O
9	целом O
10	можно O
11	по O
12	кривой O
13	ROC I-TERM
14	- I-TERM
15	AUC I-TERM
16	, O
17	которая O
18	описывает O
19	площадь O
20	под O
21	кривой O
22	( O
23	Area I-TERM
24	Under I-TERM
25	Curve I-TERM
26	– I-TERM
27	Receiver I-TERM
28	Operating I-TERM
29	Characteristic I-TERM
30	) O
31	. O

# sent_id = 611
# text =  В качестве подтверждения вышесказанного можно привести работу встроенного в LAMA модуля – LIME, который раскрывает работу модели окрашивая слова в тот или иной цвет, в зависимости от их эмоционального окраса.
1	В O
2	качестве O
3	подтверждения O
4	вышесказанного O
5	можно O
6	привести O
7	работу O
8	встроенного O
9	в O
10	LAMA I-TERM
11	модуля O
12	– O
13	LIME I-TERM
14	, O
15	который O
16	раскрывает O
17	работу O
18	модели O
19	окрашивая O
20	слова O
21	в O
22	тот O
23	или O
24	иной O
25	цвет O
26	, O
27	в O
28	зависимости O
29	от O
30	их O
31	эмоционального O
32	окраса O
33	. O

# sent_id = 612
# text =  Также фреймворк может решать задачи регрессионного анализа, целью которого является определение зависимости между переменными и оценкой функции регрессии.
1	Также O
2	фреймворк O
3	может O
4	решать O
5	задачи O
6	регрессионного I-TERM
7	анализа I-TERM
8	, O
9	целью O
10	которого O
11	является O
12	определение I-TERM
13	зависимости I-TERM
14	между I-TERM
15	переменными I-TERM
16	и O
17	оценкой I-TERM
18	функции I-TERM
19	регрессии I-TERM
20	. O

# sent_id = 613
# text =  .Работа с текстомВ LightAutoML имеется большое количество вариантов разработки той или иной модели, работающей с текстом.
1	.Работа O
2	с O
3	текстомВ O
4	LightAutoML I-TERM
5	имеется O
6	большое O
7	количество O
8	вариантов O
9	разработки O
10	той O
11	или O
12	иной O
13	модели O
14	, O
15	работающей O
16	с O
17	текстом O
18	. O

# sent_id = 614
# text =  Библиотека предоставляет не только получение стандартных признаков на основе TF-IDF, но и на основе эмбеддингов:1) На основе встроенного FastText, который можно тренировать на том или ином корпусе2) Предобученных моделей Gensim3) Любой другой объект, который имеет вид словаря, где на вход подается слово, а на выходе его эмбеддинги
1	Библиотека O
2	предоставляет O
3	не O
4	только O
5	получение O
6	стандартных O
7	признаков O
8	на O
9	основе O
10	TF I-TERM
11	- I-TERM
12	IDF I-TERM
13	, O
14	но O
15	и O
16	на O
17	основе O
18	эмбеддингов O
19	: O
20	1 O
21	) O
22	На O
23	основе O
24	встроенного O
25	FastText I-TERM
26	, O
27	который O
28	можно O
29	тренировать O
30	на O
31	том O
32	или O
33	ином O
34	корпусе O
35	2 O
36	) O
37	Предобученных O
38	моделей O
39	Gensim3 I-TERM
40	) O
41	Любой O
42	другой O
43	объект O
44	, O
45	который O
46	имеет O
47	вид O
48	словаря O
49	, O
50	где O
51	на O
52	вход O
53	подается O
54	слово O
55	, O
56	а O
57	на O
58	выходе O
59	его O
60	эмбеддинги O

# sent_id = 615
# text = Среди используемых стратегий извлечения представлений текстов из эмбеддингов слов, можно выделить:1) Weighted Average Transformer (WAT) – взвешивается каждое слово с некоторым весом
1	Среди O
2	используемых O
3	стратегий O
4	извлечения O
5	представлений O
6	текстов O
7	из O
8	эмбеддингов O
9	слов O
10	, O
11	можно O
12	выделить:1 O
13	) O
14	Weighted I-TERM
15	Average I-TERM
16	Transformer I-TERM
17	( O
18	WAT I-TERM
19	) O
20	– O
21	взвешивается O
22	каждое O
23	слово O
24	с O
25	некоторым O
26	весом O

# sent_id = 616
# text = Bag of Random Embedding Projections (BOREP) – строится линейная модель со случайными весами  
1	Bag I-TERM
2	of I-TERM
3	Random I-TERM
4	Embedding I-TERM
5	Projections I-TERM
6	( O
7	BOREP O
8	) O
9	– O
10	строится O
11	линейная O
12	модель O
13	со O
14	случайными O
15	весами O

# sent_id = 617
# text = Bert Pooling – получение эмбеддинга с последнего выхода модели Transformer  
1	Bert I-TERM
2	Pooling I-TERM
3	– O
4	получение O
5	эмбеддинга O
6	с O
7	последнего O
8	выхода O
9	модели O
10	Transformer I-TERM

# sent_id = 618
# text = За препроцессинг текста отвечает класс токенайзера, по умолчанию применяется только для TF-IDF.
1	За O
2	препроцессинг O
3	текста O
4	отвечает O
5	класс O
6	токенайзера O
7	, O
8	по O
9	умолчанию O
10	применяется O
11	только O
12	для O
13	TF I-TERM
14	- I-TERM
15	IDF I-TERM
16	. O

# sent_id = 619
# text = Подводя итоги стоит сказать, что LightAutoML благодаря встроенному инструментарию способен показывать достаточно хорошие результаты в задачах бинарной или мультиклассовой классификации и регрессии.
1	Подводя O
2	итоги O
3	стоит O
4	сказать O
5	, O
6	что O
7	LightAutoML O
8	благодаря O
9	встроенному O
10	инструментарию O
11	способен O
12	показывать O
13	достаточно O
14	хорошие O
15	результаты O
16	в O
17	задачах O
18	бинарной O
19	или O
20	мультиклассовой I-TERM
21	классификации I-TERM
22	и O
23	регрессии O
24	. O

# sent_id = 620
# text = Конкретно в нашем случае нам удалось создать модель сентиментного анализа, которая с 89% точностью определяет эмоциональный окрас обращения и слова, которые оказывают на это наибольшее влияние.
1	Конкретно O
2	в O
3	нашем O
4	случае O
5	нам O
6	удалось O
7	создать O
8	модель O
9	сентиментного O
10	анализа O
11	, O
12	которая O
13	с O
14	89 I-TERM
15	% I-TERM
16	точностью I-TERM
17	определяет O
18	эмоциональный O
19	окрас O
20	обращения O
21	и O
22	слова O
23	, O
24	которые O
25	оказывают O
26	на O
27	это O
28	наибольшее O
29	влияние O
30	. O

# sent_id = 621
# text =  Яндекс открывает датасеты Беспилотных автомобилей, Погоды и Переводчика, чтобы помочь решить проблему сдвига данных в ML       
1	Яндекс I-TERM
2	открывает O
3	датасеты O
4	Беспилотных O
5	автомобилей O
6	, O
7	Погоды I-TERM
8	и O
9	Переводчика I-TERM
10	, O
11	чтобы O
12	помочь O
13	решить O
14	проблему O
15	сдвига I-TERM
16	данных I-TERM
17	в O
18	ML I-TERM

# sent_id = 622
# text =  Для современных моделей, которые используются в машинном переводе, такой язык представляет серьезную проблему, так как большинство переводчиков обучаются на чуть более формальном языке: классической литературе, юридических документах или статьях Википедии.
1	Для O
2	современных O
3	моделей O
4	, O
5	которые O
6	используются O
7	в O
8	машинном O
9	переводе O
10	, O
11	такой O
12	язык O
13	представляет O
14	серьезную O
15	проблему O
16	, O
17	так O
18	как O
19	большинство O
20	переводчиков O
21	обучаются O
22	на O
23	чуть O
24	более O
25	формальном O
26	языке O
27	: O
28	классической I-TERM
29	литературе I-TERM
30	, O
31	юридических I-TERM
32	документах I-TERM
33	или O
34	статьях I-TERM
35	Википедии I-TERM
36	. O

# sent_id = 623
# text =  В треке перевода мы использовали для обучения англо-русский корпус WMT’20, который в основном состоит из государственных и новостных текстов.
1	В O
2	треке O
3	перевода O
4	мы O
5	использовали O
6	для O
7	обучения O
8	англо O
9	- O
10	русский O
11	корпус O
12	WMT’20 I-TERM
13	, O
14	который O
15	в O
16	основном O
17	состоит O
18	из O
19	государственных O
20	и O
21	новостных I-TERM
22	текстов I-TERM
23	. O

# sent_id = 624
# text =  Данные без сдвига взяты из англо-русского корпуса Newstest’19, а также из корпуса новостных текстов, собранных службой Global Voices и переведенных Яндексом.
1	Данные O
2	без O
3	сдвига O
4	взяты O
5	из O
6	англо I-TERM
7	- O
8	русского I-TERM
9	корпуса O
10	Newstest’19 I-TERM
11	, O
12	а O
13	также O
14	из O
15	корпуса O
16	новостных O
17	текстов O
18	, O
19	собранных O
20	службой O
21	Global I-TERM
22	Voices I-TERM
23	и O
24	переведенных O
25	Яндексом I-TERM
26	. O

# sent_id = 625
# text =  Данные со сдвигом для отладки взяты из подготовленного для WMT Robustness Challenge корпуса Reddit и также переведены Яндексом.
1	Данные O
2	со O
3	сдвигом O
4	для O
5	отладки O
6	взяты O
7	из O
8	подготовленного O
9	для O
10	WMT O
11	Robustness O
12	Challenge O
13	корпуса O
14	Reddit I-TERM
15	и O
16	также O
17	переведены O
18	Яндексом I-TERM
19	. O

# sent_id = 626
# text =  Для проверки модели на данных со сдвигом мы также собрали, перевели и разметили дополнительные данные с Reddit.
1	Для O
2	проверки O
3	модели O
4	на O
5	данных O
6	со O
7	сдвигом O
8	мы O
9	также O
10	собрали O
11	, O
12	перевели O
13	и O
14	разметили O
15	дополнительные O
16	данные O
17	с O
18	Reddit I-TERM
19	. O

# sent_id = 627
# text =  Парсить комментарии мы будем с помощью официального API ВКонтакте для Python
1	Парсить O
2	комментарии O
3	мы O
4	будем O
5	с O
6	помощью O
7	официального O
8	API I-TERM
9	ВКонтакте I-TERM
10	для O
11	Python I-TERM

# sent_id = 628
# text =  Необходимо убрать из комментария направление, чтобы при поиске расстояния Левенштейна меньше ошибаться.
1	Необходимо O
2	убрать O
3	из O
4	комментария O
5	направление O
6	, O
7	чтобы O
8	при O
9	поиске O
10	расстояния I-TERM
11	Левенштейна I-TERM
12	меньше O
13	ошибаться O
14	. O

# sent_id = 629
# text =  Небольшая справка: расстояние Левенштейна — минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую.
1	Небольшая O
2	справка O
3	: O
4	расстояние I-TERM
5	Левенштейна I-TERM
6	— O
7	минимальное O
8	количество O
9	операций O
10	вставки O
11	одного O
12	символа O
13	, O
14	удаления O
15	одного O
16	символа O
17	и O
18	замены O
19	одного O
20	символа O
21	на O
22	другой O
23	, O
24	необходимых O
25	для O
26	превращения O
27	одной O
28	строки O
29	в O
30	другую O
31	. O

# sent_id = 630
# text =  Его мы будем находить с помощью библиотеки fuzzywuzzy.
1	Его O
2	мы O
3	будем O
4	находить O
5	с O
6	помощью O
7	библиотеки O
8	fuzzywuzzy I-TERM
9	. O

# sent_id = 631
# text =  Для ускорения работы авторы библиотеки советуют также установить библиотеку python-Levenshtein.
1	Для O
2	ускорения O
3	работы O
4	авторы O
5	библиотеки O
6	советуют O
7	также O
8	установить O
9	библиотеку O
10	python I-TERM
11	- I-TERM
12	Levenshtein I-TERM
13	. O

# sent_id = 632
# text =  Его мне любезно предоставил разработчик приложения GoTrans, Александр Козлов.
1	Его O
2	мне O
3	любезно O
4	предоставил O
5	разработчик O
6	приложения O
7	GoTrans I-TERM
8	, O
9	Александр I-TERM
10	Козлов I-TERM
11	. O

# sent_id = 633
# text =  Самый сложный кроссворд, составленный компьютером
1	Самый O
2	сложный O
3	кроссворд I-TERM
4	, O
5	составленный O
6	компьютером O

# sent_id = 634
# text =  Пример Deep Blue показывает, что программы ИИ могут участвовать в викторинах и обыгрывать людей.
1	Пример O
2	Deep I-TERM
3	Blue I-TERM
4	показывает O
5	, O
6	что O
7	программы O
8	ИИ O
9	могут O
10	участвовать O
11	в O
12	викторинах I-TERM
13	и O
14	обыгрывать O
15	людей O
16	. O

# sent_id = 635
# text =  Американский разработчик Мэтью Гинсберг (Matthew Ginsberg) создал программу под названием Dr Fill, которая справляется с кроссвордами гораздо лучше, чем абсолютное большинство людей, пишет New Scientist.
1	Американский O
2	разработчик O
3	Мэтью I-TERM
4	Гинсберг I-TERM
5	( O
6	Matthew I-TERM
7	Ginsberg I-TERM
8	) O
9	создал O
10	программу O
11	под O
12	названием O
13	Dr I-TERM
14	Fill I-TERM
15	, O
16	которая O
17	справляется O
18	с O
19	кроссвордами I-TERM
20	гораздо O
21	лучше O
22	, O
23	чем O
24	абсолютное O
25	большинство O
26	людей O
27	, O
28	пишет O
29	New I-TERM
30	Scientist I-TERM
31	. O

# sent_id = 636
# text =  Анализ тональности текстов с помощью сверточных нейронных сетей 
1	Анализ I-TERM
2	тональности I-TERM
3	текстов I-TERM
4	с O
5	помощью O
6	сверточных I-TERM
7	нейронных I-TERM
8	сетей I-TERM

# sent_id = 637
# text =  Есть много способов решать такую задачу, и один из них — свёрточные нейронные сети (Convolutional Neural Networks).
1	Есть O
2	много O
3	способов O
4	решать O
5	такую O
6	задачу O
7	, O
8	и O
9	один O
10	из O
11	них O
12	— O
13	свёрточные I-TERM
14	нейронные I-TERM
15	сети I-TERM
16	( O
17	Convolutional I-TERM
18	Neural I-TERM
19	Networks I-TERM
20	) O
21	. O

# sent_id = 638
# text =  CNN изначально были разработаны для обработки изображений, однако они успешно справляются с решением задач в сфере автоматической обработки текстов.
1	CNN I-TERM
2	изначально O
3	были O
4	разработаны O
5	для O
6	обработки O
7	изображений O
8	, O
9	однако O
10	они O
11	успешно O
12	справляются O
13	с O
14	решением O
15	задач O
16	в O
17	сфере O
18	автоматической I-TERM
19	обработки I-TERM
20	текстов I-TERM
21	. O

# sent_id = 639
# text =  Я познакомлю вас с бинарным анализом тональности русскоязычных текстов с помощью свёрточной нейронной сети, для которой векторные представления слов были сформированы на основе обученной Word2Vec модели.
1	Я O
2	познакомлю O
3	вас O
4	с O
5	бинарным O
6	анализом I-TERM
7	тональности I-TERM
8	русскоязычных I-TERM
9	текстов I-TERM
10	с O
11	помощью O
12	свёрточной I-TERM
13	нейронной I-TERM
14	сети I-TERM
15	, O
16	для O
17	которой O
18	векторные I-TERM
19	представления I-TERM
20	слов I-TERM
21	были O
22	сформированы O
23	на O
24	основе O
25	обученной O
26	Word2Vec I-TERM
27	модели O
28	. O

# sent_id = 640
# text =  Для обучения я выбрал корпус коротких текстов Юлии Рубцовой, сформированный на основе русскоязычных сообщений из Twitter [4].
1	Для O
2	обучения O
3	я O
4	выбрал O
5	корпус I-TERM
6	коротких I-TERM
7	текстов I-TERM
8	Юлии I-TERM
9	Рубцовой I-TERM
10	, O
11	сформированный O
12	на O
13	основе O
14	русскоязычных I-TERM
15	сообщений I-TERM
16	из O
17	Twitter I-TERM
18	[ O
19	4 O
20	] O
21	. O

# sent_id = 641
# text =  Визуализация кластеров похожих слов с использование t-SNE.
1	Визуализация O
2	кластеров I-TERM
3	похожих O
4	слов O
5	с O
6	использование O
7	t I-TERM
8	- I-TERM
9	SNE I-TERM
10	. O

# sent_id = 642
# text =  На следующем этапе каждый текст был отображен в массив идентификаторов токенов.
1	На O
2	следующем O
3	этапе O
4	каждый O
5	текст I-TERM
6	был O
7	отображен O
8	в O
9	массив O
10	идентификаторов O
11	токенов I-TERM
12	. O

# sent_id = 643
# text =  Вот пусть комментаторы поправят, но кроме модуля LanguageTool для Open Office (о нём мы ещё поговорим) даже в голову ничего не приходит.
1	Вот O
2	пусть O
3	комментаторы O
4	поправят O
5	, O
6	но O
7	кроме O
8	модуля O
9	LanguageTool I-TERM
10	для O
11	Open I-TERM
12	Office I-TERM
13	( O
14	о O
15	нём O
16	мы O
17	ещё O
18	поговорим O
19	) O
20	даже O
21	в O
22	голову O
23	ничего O
24	не O
25	приходит O
26	. O

# sent_id = 644
# text =  Было бы здорово составить базу с инструкциями не для людей, а для роботов, подумали инженеры из Института искусственного интеллекта при Бременском университете (Германия), авторы проекта RoboHow.
1	Было O
2	бы O
3	здорово O
4	составить O
5	базу O
6	с O
7	инструкциями I-TERM
8	не O
9	для O
10	людей O
11	, O
12	а O
13	для O
14	роботов O
15	, O
16	подумали O
17	инженеры O
18	из O
19	Института I-TERM
20	искусственного I-TERM
21	интеллекта I-TERM
22	при O
23	Бременском I-TERM
24	университете I-TERM
25	( O
26	Германия O
27	) O
28	, O
29	авторы O
30	проекта O
31	RoboHow I-TERM
32	. O

# sent_id = 645
# text =  С такой базой wiki-инструкций роботы смогут передавать информацию друг другу.
1	С O
2	такой O
3	базой O
4	wiki I-TERM
5	- I-TERM
6	инструкций I-TERM
7	роботы O
8	смогут O
9	передавать O
10	информацию O
11	друг O
12	другу O
13	. O

# sent_id = 646
# text =  Созданный в Бременском университете робот PR2 (на фото вверху) учится понимать и выполнять «человеческие» инструкции из базы WikiHow.
1	Созданный O
2	в O
3	Бременском I-TERM
4	университете I-TERM
5	робот O
6	PR2 O
7	( O
8	на O
9	фото O
10	вверху O
11	) O
12	учится O
13	понимать O
14	и O
15	выполнять O
16	« O
17	человеческие O
18	» O
19	инструкции I-TERM
20	из O
21	базы O
22	WikiHow I-TERM
23	. O

# sent_id = 647
# text =  Успешно выполнив задачу, то есть усвоив урок, робот делится приобретёнными знаниями со своими собратьями через онлайновую базу Open Ease.
1	Успешно O
2	выполнив O
3	задачу O
4	, O
5	то O
6	есть O
7	усвоив O
8	урок O
9	, O
10	робот O
11	делится O
12	приобретёнными O
13	знаниями O
14	со O
15	своими O
16	собратьями O
17	через O
18	онлайновую O
19	базу O
20	Open I-TERM
21	Ease I-TERM
22	. O

# sent_id = 648
# text =  Здесь инструкции записаны в машиночитаемом виде, на языке, похожем на язык Семантической сети.
1	Здесь O
2	инструкции I-TERM
3	записаны O
4	в O
5	машиночитаемом O
6	виде O
7	, O
8	на O
9	языке O
10	, O
11	похожем O
12	на O
13	язык O
14	Семантической I-TERM
15	сети I-TERM
16	. O

# sent_id = 649
# text =  Это очень сложная задача, которая сочетает в себе тесную интеграцию распознавания речи, интерпретации команд на естественном языке, машинного зрения и планирования сложных действий через алгоритмы осуществления отдельных манипуляций.
1	Это O
2	очень O
3	сложная O
4	задача O
5	, O
6	которая O
7	сочетает O
8	в O
9	себе O
10	тесную O
11	интеграцию O
12	распознавания I-TERM
13	речи I-TERM
14	, O
15	интерпретации I-TERM
16	команд I-TERM
17	на I-TERM
18	естественном I-TERM
19	языке I-TERM
20	, O
21	машинного O
22	зрения O
23	и O
24	планирования O
25	сложных O
26	действий O
27	через O
28	алгоритмы O
29	осуществления O
30	отдельных O
31	манипуляций O
32	. O

# sent_id = 650
# text =  «М.видео-Эльдорадо» внедряет нейросеть для ответов на вопросы покупателей 
1	« O
2	М.видео I-TERM
3	- I-TERM
4	Эльдорадо I-TERM
5	» O
6	внедряет O
7	нейросеть O
8	для O
9	ответов I-TERM
10	на I-TERM
11	вопросы I-TERM
12	покупателей O

# sent_id = 651
# text =  Президент Ассоциации больших данных Анна Серебряникова отметила, что ИИ в ретейле может применяться для прогнозирования открытия новых торговых точек, а также для персонализации предложений для клиентов и создания чат-ботов для службы поддержки.
1	Президент O
2	Ассоциации I-TERM
3	больших I-TERM
4	данных I-TERM
5	Анна I-TERM
6	Серебряникова I-TERM
7	отметила O
8	, O
9	что O
10	ИИ O
11	в O
12	ретейле O
13	может O
14	применяться O
15	для O
16	прогнозирования O
17	открытия O
18	новых O
19	торговых O
20	точек O
21	, O
22	а O
23	также O
24	для O
25	персонализации I-TERM
26	предложений I-TERM
27	для O
28	клиентов O
29	и O
30	создания I-TERM
31	чат I-TERM
32	- I-TERM
33	ботов I-TERM
34	для O
35	службы O
36	поддержки O
37	. O

# sent_id = 652
# text =  В Facebook AI продемонстрировали прямой машинный перевод с одного языка на другой
1	В O
2	Facebook I-TERM
3	AI I-TERM
4	продемонстрировали O
5	прямой O
6	машинный O
7	перевод O
8	с O
9	одного O
10	языка O
11	на O
12	другой O

# sent_id = 653
# text = Facebook AI представила новую систему машинного перевода M2M-100 с 15 млрд параметров.
1	Facebook I-TERM
2	AI I-TERM
3	представила O
4	новую O
5	систему I-TERM
6	машинного I-TERM
7	перевода I-TERM
8	M2M-100 I-TERM
9	с O
10	15 O
11	млрд O
12	параметров O
13	. O

# sent_id = 654
# text =  Она способна переводить с одного языка на другой напрямую, не используя английский в качестве промежуточного.
1	Она O
2	способна O
3	переводить I-TERM
4	с I-TERM
5	одного I-TERM
6	языка I-TERM
7	на I-TERM
8	другой I-TERM
9	напрямую O
10	, O
11	не O
12	используя O
13	английский O
14	в O
15	качестве O
16	промежуточного O
17	. O

# sent_id = 655
# text =  Она способна осуществлять переводы между парами из ста языков.
1	Она O
2	способна O
3	осуществлять O
4	переводы I-TERM
5	между O
6	парами O
7	из O
8	ста O
9	языков O
10	. O

# sent_id = 656
# text =  Модель обучали на наборе данных из более чем 7,5 млрд предложений как из базы Facebook, так и из других источников.
1	Модель O
2	обучали O
3	на O
4	наборе O
5	данных O
6	из O
7	более O
8	чем O
9	7,5 O
10	млрд O
11	предложений O
12	как O
13	из O
14	базы O
15	Facebook I-TERM
16	, O
17	так O
18	и O
19	из O
20	других O
21	источников O
22	. O

# sent_id = 657
# text =  При разработке использовали инструмент CommonCrawl, который поддерживает открытый репозиторий данных веб-сканирования, и систему классификации текстов FastText, которую в Facebook представили несколько лет назад.
1	При O
2	разработке O
3	использовали O
4	инструмент O
5	CommonCrawl I-TERM
6	, O
7	который O
8	поддерживает O
9	открытый O
10	репозиторий O
11	данных O
12	веб O
13	- O
14	сканирования O
15	, O
16	и O
17	систему O
18	классификации O
19	текстов O
20	FastText I-TERM
21	, O
22	которую O
23	в O
24	Facebook I-TERM
25	представили O
26	несколько O
27	лет O
28	назад O
29	. O

# sent_id = 658
# text =  Согласно метрикам BLEU, M2M-100 на 10 баллов опережает предшественника, где английский язык был промежуточным.
1	Согласно O
2	метрикам O
3	BLEU I-TERM
4	, O
5	M2M-100 I-TERM
6	на O
7	10 O
8	баллов O
9	опережает O
10	предшественника O
11	, O
12	где O
13	английский O
14	язык O
15	был O
16	промежуточным O
17	. O

# sent_id = 659
# text =  Facebook AI отмечает, что эта модель может быть полезной не только при машинном переводе, но и при изучении языков.
1	Facebook I-TERM
2	AI I-TERM
3	отмечает O
4	, O
5	что O
6	эта O
7	модель O
8	может O
9	быть O
10	полезной O
11	не O
12	только O
13	при O
14	машинном I-TERM
15	переводе I-TERM
16	, O
17	но O
18	и O
19	при O
20	изучении O
21	языков O
22	. O

# sent_id = 660
# text =  Я тестировала Google Translate на одних и тех же текстах в марте и декабре 2011, январе 2016 и декабре 2017 года.
1	Я O
2	тестировала O
3	Google I-TERM
4	Translate I-TERM
5	на O
6	одних O
7	и O
8	тех O
9	же O
10	текстах O
11	в O
12	марте O
13	и O
14	декабре O
15	2011 O
16	, O
17	январе O
18	2016 O
19	и O
20	декабре O
21	2017 O
22	года O
23	. O

# sent_id = 661
# text =  Брала одни и те же отрывки на английском, русском, немецком, французском, украинском и польском языках и переводила каждый на остальные пять языков из выборки.
1	Брала O
2	одни O
3	и O
4	те O
5	же O
6	отрывки O
7	на O
8	английском I-TERM
9	, O
10	русском I-TERM
11	, O
12	немецком I-TERM
13	, O
14	французском I-TERM
15	, O
16	украинском I-TERM
17	и O
18	польском I-TERM
19	языках O
20	и O
21	переводила O
22	каждый O
23	на O
24	остальные O
25	пять O
26	языков O
27	из O
28	выборки O
29	. O

# sent_id = 662
# text =  Результаты cross-verification в целом совпали с тенденциями в первоначальной выборке.
1	Результаты O
2	cross I-TERM
3	- I-TERM
4	verification I-TERM
5	в O
6	целом O
7	совпали O
8	с O
9	тенденциями O
10	в O
11	первоначальной O
12	выборке O
13	. O

# sent_id = 663
# text =  С марта 2017 года нейросеть стали использовать для перевода на русский.
1	С O
2	марта O
3	2017 I-TERM
4	года O
5	нейросеть O
6	стали O
7	использовать O
8	для O
9	перевода I-TERM
10	на O
11	русский I-TERM
12	. O

# sent_id = 664
# text =  Сервис не переводит дословно, результат стал более свободным: адекватная перефразировка, перегруппировка слов, перестановка слов из начала в конец предложения, если того требуют правила языка (в немецком это реализовано великолепно).
1	Сервис O
2	не O
3	переводит O
4	дословно O
5	, O
6	результат O
7	стал O
8	более O
9	свободным O
10	: O
11	адекватная O
12	перефразировка I-TERM
13	, O
14	перегруппировка I-TERM
15	слов I-TERM
16	, O
17	перестановка I-TERM
18	слов I-TERM
19	из O
20	начала O
21	в O
22	конец O
23	предложения O
24	, O
25	если O
26	того O
27	требуют O
28	правила O
29	языка O
30	( O
31	в O
32	немецком I-TERM
33	это O
34	реализовано O
35	великолепно O
36	) O
37	. O

# sent_id = 665
# text =  В отличие от предыдущего уровня (phrase-based translation– однократное нахождение соответствий отдельных слов и фраз), нейронный переводчик в какой-то степени трансформирует предложения, анализирует их как единое целое и устанавливает соответствия «из конца в конец» в несколько стадий(end-to-end mapping – сквозное преобразование, полного цикла, непрерывная трансформация многообразия данных со входа на выход).
1	В O
2	отличие O
3	от O
4	предыдущего O
5	уровня O
6	( O
7	phrase I-TERM
8	- I-TERM
9	based I-TERM
10	translation I-TERM
11	– O
12	однократное I-TERM
13	нахождение I-TERM
14	соответствий I-TERM
15	отдельных I-TERM
16	слов I-TERM
17	и O
18	фраз O
19	) O
20	, O
21	нейронный O
22	переводчик O
23	в O
24	какой O
25	- O
26	то O
27	степени O
28	трансформирует O
29	предложения O
30	, O
31	анализирует O
32	их O
33	как O
34	единое O
35	целое O
36	и O
37	устанавливает O
38	соответствия O
39	« O
40	из O
41	конца O
42	в O
43	конец O
44	» O
45	в O
46	несколько O
47	стадий O
48	( O
49	end I-TERM
50	- I-TERM
51	to I-TERM
52	- I-TERM
53	end I-TERM
54	mapping I-TERM
55	– O
56	сквозное I-TERM
57	преобразование I-TERM
58	, O
59	полного O
60	цикла O
61	, O
62	непрерывная O
63	трансформация I-TERM
64	многообразия I-TERM
65	данных I-TERM
66	со O
67	входа O
68	на O
69	выход O
70	) O
71	. O

# sent_id = 666
# text =  Сейчас в Яндексе мой основной проект это Алиса, голосовой помощник, который Яндекс запустил в октябре прошлого года, и моя группа отвечает за то, что можно условно назвать мозгами Алисы.
1	Сейчас O
2	в O
3	Яндексе I-TERM
4	мой O
5	основной O
6	проект O
7	это O
8	Алиса I-TERM
9	, O
10	голосовой I-TERM
11	помощник I-TERM
12	, O
13	который O
14	Яндекс I-TERM
15	запустил O
16	в O
17	октябре O
18	прошлого O
19	года O
20	, O
21	и O
22	моя O
23	группа O
24	отвечает O
25	за O
26	то O
27	, O
28	что O
29	можно O
30	условно O
31	назвать O
32	мозгами O
33	Алисы I-TERM
34	. O

# sent_id = 667
# text =  Мы интерпретируем то, что сказал пользователь на естественном языке и превращаем это в некоторое структурированное представление.
1	Мы O
2	интерпретируем O
3	то O
4	, O
5	что O
6	сказал O
7	пользователь O
8	на O
9	естественном I-TERM
10	языке I-TERM
11	и O
12	превращаем O
13	это O
14	в O
15	некоторое O
16	структурированное I-TERM
17	представление I-TERM
18	. O

# sent_id = 668
# text =  Есть Siri, единственный голосовой помощник, который тоже понимает русский язык, но он работает только на iOS и MacOS, это как бы не самая популярная платформа в России, и к Siri как к продукту тоже есть определенные вопросы.
1	Есть O
2	Siri I-TERM
3	, O
4	единственный O
5	голосовой I-TERM
6	помощник I-TERM
7	, O
8	который O
9	тоже O
10	понимает O
11	русский I-TERM
12	язык O
13	, O
14	но O
15	он O
16	работает O
17	только O
18	на O
19	iOS I-TERM
20	и O
21	MacOS I-TERM
22	, O
23	это O
24	как O
25	бы O
26	не O
27	самая O
28	популярная O
29	платформа O
30	в O
31	России O
32	, O
33	и O
34	к O
35	Siri I-TERM
36	как O
37	к O
38	продукту O
39	тоже O
40	есть O
41	определенные O
42	вопросы O
43	. O

# sent_id = 669
# text =  На самом деле у нас уже есть модель которая оценивает градацию этой оскорбительности, и если бы возникла продуктовая необходимость, мы уже могли бы сделать такой ползунок который делает ответы более или менее дерзкими.
1	На O
2	самом O
3	деле O
4	у O
5	нас O
6	уже O
7	есть O
8	модель O
9	которая O
10	оценивает I-TERM
11	градацию I-TERM
12	этой O
13	оскорбительности I-TERM
14	, O
15	и O
16	если O
17	бы O
18	возникла O
19	продуктовая O
20	необходимость O
21	, O
22	мы O
23	уже O
24	могли O
25	бы O
26	сделать O
27	такой O
28	ползунок O
29	который O
30	делает O
31	ответы O
32	более O
33	или O
34	менее O
35	дерзкими O
36	. O

# sent_id = 670
# text =  Это генеративная нейронная сеть, способная решать множество задач по обработке естествнного языка (NLP).
1	Это O
2	генеративная I-TERM
3	нейронная I-TERM
4	сеть I-TERM
5	, O
6	способная O
7	решать O
8	множество O
9	задач O
10	по O
11	обработке I-TERM
12	естествнного I-TERM
13	языка I-TERM
14	( O
15	NLP I-TERM
16	) O
17	. O

# sent_id = 671
# text =  Это такие задачи как суммаризация (сделать из большого текста его резюме), понимание текста (NLU), вопросно-ответные системы, генерация (например, стихов, — на Хабре была хорошая статья) и другие.
1	Это O
2	такие O
3	задачи O
4	как O
5	суммаризация I-TERM
6	( O
7	сделать O
8	из O
9	большого O
10	текста O
11	его O
12	резюме O
13	) O
14	, O
15	понимание I-TERM
16	текста I-TERM
17	( O
18	NLU I-TERM
19	) O
20	, O
21	вопросно I-TERM
22	- I-TERM
23	ответные I-TERM
24	системы I-TERM
25	, O
26	генерация I-TERM
27	( O
28	например O
29	, O
30	стихов O
31	, O
32	— O
33	на O
34	Хабре I-TERM
35	была O
36	хорошая O
37	статья O
38	) O
39	и O
40	другие O
41	. O

# sent_id = 672
# text =  В Яндекс.Браузер внедрили машинный перевод видеороликов 
1	В O
2	Яндекс I-TERM
3	. I-TERM
4	Браузер I-TERM
5	внедрили O
6	машинный O
7	перевод O
8	видеороликов O

# sent_id = 673
# text =  Алгоритм отслеживает темп речи говорящего, за счет чего переводчик делает паузы, замедляет или ускоряет речь, чтобы закадровый голос совпадал с картинкой.Перевод доступен в Яндекс.Браузере для Windows и macOS.
1	Алгоритм O
2	отслеживает O
3	темп I-TERM
4	речи I-TERM
5	говорящего O
6	, O
7	за O
8	счет O
9	чего O
10	переводчик O
11	делает O
12	паузы I-TERM
13	, O
14	замедляет O
15	или O
16	ускоряет O
17	речь I-TERM
18	, O
19	чтобы O
20	закадровый O
21	голос I-TERM
22	совпадал O
23	с O
24	картинкой O
25	. O
26	Перевод O
27	доступен O
28	в O
29	Яндекс I-TERM
30	. O
31	Браузере O
32	для O
33	Windows I-TERM
34	и O
35	macOS I-TERM
36	. O

# sent_id = 674
# text =  Тогда к статистической модели, которая была в «Переводчике» с момента запуска, добавили технологию перевода с помощью нейросети.
1	Тогда O
2	к O
3	статистической O
4	модели O
5	, O
6	которая O
7	была O
8	в O
9	« O
10	Переводчике I-TERM
11	» O
12	с O
13	момента O
14	запуска O
15	, O
16	добавили O
17	технологию O
18	перевода I-TERM
19	с O
20	помощью O
21	нейросети O
22	. O

# sent_id = 675
# text =  Компания объясняла, что ее технология не разбивает текст на отдельные слова, а рассматривает его целиком, чтобы лучше передать смысл.В июне Яндекс открыл доступ к нейросети «Балабоба» для всех пользователей.
1	Компания O
2	объясняла O
3	, O
4	что O
5	ее O
6	технология O
7	не O
8	разбивает O
9	текст I-TERM
10	на O
11	отдельные O
12	слова I-TERM
13	, O
14	а O
15	рассматривает O
16	его O
17	целиком O
18	, O
19	чтобы O
20	лучше O
21	передать O
22	смысл O
23	. O
24	В O
25	июне O
26	Яндекс I-TERM
27	открыл O
28	доступ O
29	к O
30	нейросети O
31	« O
32	Балабоба I-TERM
33	» O
34	для O
35	всех O
36	пользователей O
37	. O

# sent_id = 676
# text =  Она работает на языковой модели из семейства YaLM (Yet another Language Model).
1	Она O
2	работает O
3	на O
4	языковой O
5	модели O
6	из O
7	семейства O
8	YaLM I-TERM
9	( O
10	Yet I-TERM
11	another I-TERM
12	Language I-TERM
13	Model I-TERM
14	) O
15	. O

# sent_id = 677
# text =  Эта модель помогает нейросети запоминать правила языка, выбирать подходящие слова и связывать их по смыслу.
1	Эта O
2	модель O
3	помогает O
4	нейросети O
5	запоминать I-TERM
6	правила I-TERM
7	языка I-TERM
8	, O
9	выбирать I-TERM
10	подходящие I-TERM
11	слова I-TERM
12	и O
13	связывать O
14	их O
15	по O
16	смыслу O
17	. O

# sent_id = 678
# text =  У «Балабобы» нет своего мнения, она выдает случайные продолжения и может закончить историю, придумать подпись или написать небольшой рассказ.
1	У O
2	« O
3	Балабобы O
4	» O
5	нет O
6	своего O
7	мнения O
8	, O
9	она O
10	выдает O
11	случайные O
12	продолжения O
13	и O
14	может O
15	закончить O
16	историю O
17	, O
18	придумать O
19	подпись O
20	или O
21	написать O
22	небольшой O
23	рассказ O
24	. O

# sent_id = 679
# text =  AntiToxicBot — бот, распознающий токсичных пользователей в телеграм чатах.
1	AntiToxicBot O
2	— O
3	бот O
4	, O
5	распознающий I-TERM
6	токсичных I-TERM
7	пользователей I-TERM
8	в O
9	телеграм O
10	чатах O
11	. O

# sent_id = 680
# text =  Почему же выбрано CNN+GRU, а не просто GRU или CNN?
1	Почему O
2	же O
3	выбрано O
4	CNN+GRU I-TERM
5	, O
6	а O
7	не O
8	просто O
9	GRU I-TERM
10	или O
11	CNN I-TERM
12	? O

# sent_id = 681
# text =  Нейросеть состоит из 3-х основных частей(CNN, GRU, Linear).
1	Нейросеть O
2	состоит O
3	из O
4	3-х O
5	основных O
6	частей O
7	( O
8	CNN I-TERM
9	, O
10	GRU I-TERM
11	, O
12	Linear I-TERM
13	) O
14	. O

# sent_id = 682
# text =  Как и в классификации картинок, свёрточный слой выделяет “признаки”, но в нашем случае векторизированный текст.
1	Как O
2	и O
3	в O
4	классификации O
5	картинок O
6	, O
7	свёрточный O
8	слой O
9	выделяет O
10	“ O
11	признаки O
12	” O
13	, O
14	но O
15	в O
16	нашем O
17	случае O
18	векторизированный I-TERM
19	текст I-TERM
20	. O

# sent_id = 683
# text =  То-есть данная часть сети учится выделять признаки токсичных и позитивных сообщений.
1	То O
2	- O
3	есть O
4	данная O
5	часть O
6	сети O
7	учится O
8	выделять I-TERM
9	признаки I-TERM
10	токсичных I-TERM
11	и I-TERM
12	позитивных I-TERM
13	сообщений I-TERM
14	. O

# sent_id = 684
# text =   GRU - Recurrent Neural Network
1	GRU I-TERM
2	- O
3	Recurrent I-TERM
4	Neural I-TERM
5	Network

# sent_id = 685
# text =  Чтобы обрабатывать последовательности произвольной длины, используют рекуррентные слои.
1	Чтобы O
2	обрабатывать I-TERM
3	последовательности I-TERM
4	произвольной I-TERM
5	длины I-TERM
6	, O
7	используют O
8	рекуррентные O
9	слои O
10	. O

# sent_id = 686
# text =  В архитектуре используется рекуррентный слой GRU.
1	В O
2	архитектуре O
3	используется O
4	рекуррентный O
5	слой O
6	GRU I-TERM
7	. O

# sent_id = 687
# text = Данный слой учится делать заключительное решение по определению тональности текста на основе предыдущих слоёв.
1	Данный O
2	слой O
3	учится O
4	делать O
5	заключительное O
6	решение O
7	по O
8	определению I-TERM
9	тональности I-TERM
10	текста I-TERM
11	на O
12	основе O
13	предыдущих O
14	слоёв O
15	. O

# sent_id = 688
# text =  Датасет был взят с сайта kaggle.
1	Датасет O
2	был O
3	взят O
4	с O
5	сайта O
6	kaggle I-TERM
7	. O

# sent_id = 689
# text =  Около 14000 комментариев с разметкой токсичное сообщение или нет.
1	Около O
2	14000 O
3	комментариев O
4	с O
5	разметкой I-TERM
6	токсичное I-TERM
7	сообщение I-TERM
8	или O
9	нет O
10	. O

# sent_id = 690
# text =  Для решения данной проблемы была использована библиотека Yandex Speller, которая исправляет орфографические ошибки.
1	Для O
2	решения O
3	данной O
4	проблемы O
5	была O
6	использована O
7	библиотека O
8	Yandex I-TERM
9	Speller I-TERM
10	, O
11	которая O
12	исправляет I-TERM
13	орфографические I-TERM
14	ошибки I-TERM
15	. O

# sent_id = 691
# text =  Можно было обучить собственный Word2Vec на основе данного набора данных, но лучше взять уже обученный.
1	Можно O
2	было O
3	обучить O
4	собственный O
5	Word2Vec I-TERM
6	на O
7	основе O
8	данного O
9	набора O
10	данных O
11	, O
12	но O
13	лучше O
14	взять O
15	уже O
16	обученный O
17	. O

# sent_id = 692
# text =  Например: Navec.
1	Например O
2	: O
3	Navec I-TERM
4	. O

# sent_id = 693
# text =  Модель обучали на русской литературе (~150gb), что говорит о качественной векторизации текста.
1	Модель O
2	обучали O
3	на O
4	русской I-TERM
5	литературе I-TERM
6	( O
7	~150 O
8	gb O
9	) O
10	, O
11	что O
12	говорит O
13	о O
14	качественной O
15	векторизации O
16	текста O
17	. O

# sent_id = 694
# text = Для классификации используется обыкновенная функция потерь – кросс энтропия.
1	Для O
2	классификации I-TERM
3	используется O
4	обыкновенная O
5	функция I-TERM
6	потерь I-TERM
7	– I-TERM
8	кросс I-TERM
9	энтропия I-TERM
10	. O

# sent_id = 695
# text = При обучении сети надо обращать внимание на основные параметры такие, как loss, precision и accuracy.
1	При O
2	обучении O
3	сети O
4	надо O
5	обращать O
6	внимание O
7	на O
8	основные O
9	параметры O
10	такие O
11	, O
12	как O
13	loss I-TERM
14	, O
15	precision I-TERM
16	и O
17	accuracy I-TERM
18	. O

# sent_id = 696
# text =  В ~80% случаев нейросеть классифицирует тональность текста правильно.
1	В O
2	~80 I-TERM
3	% O
4	случаев O
5	нейросеть O
6	классифицирует O
7	тональность O
8	текста O
9	правильно O
10	. O

# sent_id = 697
# text =  Теперь нейронная сеть указала конкретные сцены, написанные не Шекспиром, и определила, кто на самом деле их написал.
1	Теперь O
2	нейронная I-TERM
3	сеть I-TERM
4	указала O
5	конкретные O
6	сцены O
7	, O
8	написанные O
9	не O
10	Шекспиром O
11	, O
12	и O
13	определила O
14	, O
15	кто O
16	на O
17	самом O
18	деле O
19	их O
20	написал O
21	. O

# sent_id = 698
# text =  Плехач обучил алгоритм распознавать стиль Шекспира на пьесах «Кориолан», «Цимбелин», «Зимняя сказка» и «Буря».
1	Плехач I-TERM
2	обучил O
3	алгоритм O
4	распознавать I-TERM
5	стиль I-TERM
6	Шекспира O
7	на O
8	пьесах I-TERM
9	« O
10	Кориолан O
11	» O
12	, O
13	« O
14	Цимбелин O
15	» O
16	, O
17	« O
18	Зимняя O
19	сказка O
20	» O
21	и O
22	« O
23	Буря O
24	» O
25	. O

# sent_id = 699
# text =  В результате искусственный интеллект согласился с анализом Спеддинга.
1	В O
2	результате O
3	искусственный O
4	интеллект O
5	согласился O
6	с O
7	анализом I-TERM
8	Спеддинга I-TERM
9	. O

# sent_id = 700
# text =  В прошлом году учёные из Университета Торонто, Мельбурнского Университета и подразделения IBM в Австралии научили искусственный интеллект генерировать сонеты в шекспировском стиле.
1	В O
2	прошлом O
3	году O
4	учёные O
5	из O
6	Университета I-TERM
7	Торонто I-TERM
8	, O
9	Мельбурнского I-TERM
10	Университета I-TERM
11	и O
12	подразделения O
13	IBM I-TERM
14	в O
15	Австралии O
16	научили O
17	искусственный O
18	интеллект O
19	генерировать O
20	сонеты O
21	в O
22	шекспировском O
23	стиле O
24	. O

# sent_id = 701
# text =  Алгоритм под названием Deepspeare обучали на 2,7 тыс. сонетов Шекспира, после чего он научился писать собственные, придерживаясь похожего стиля.
1	Алгоритм O
2	под O
3	названием O
4	Deepspeare I-TERM
5	обучали O
6	на O
7	2,7 O
8	тыс. O 
9	сонетов I-TERM
10	Шекспира O
11	, O
12	после O
13	чего O
14	он O
15	научился O
16	писать O
17	собственные O
18	, O
19	придерживаясь O
20	похожего O
21	стиля O
22	. O

# sent_id = 702
# text =  Как научить свою нейросеть генерировать стихи
1	Как O
2	научить O
3	свою O
4	нейросеть O
5	генерировать I-TERM
6	стихи I-TERM

# sent_id = 703
# text =  Языковые модели определяют вероятность появления последовательности слов  в данном языке: .
1	Языковые O
2	модели O
3	определяют I-TERM
4	вероятность I-TERM
5	появления I-TERM
6	последовательности I-TERM
7	слов I-TERM
8	в O
9	данном O
10	языке I-TERM
11	: O
12	. O

# sent_id = 704
# text =  Кажется, самым простым способом построить такую модель является использование N-граммной статистики.
1	Кажется O
2	, O
3	самым O
4	простым O
5	способом O
6	построить O
7	такую O
8	модель O
9	является O
10	использование O
11	N I-TERM
12	- I-TERM
13	граммной I-TERM
14	статистики I-TERM
15	. O

# sent_id = 705
# text =  Для решения такой проблемы используют обычно сглаживание Kneser–Ney или Katz’s backing-off.
1	Для O
2	решения O
3	такой O
4	проблемы O
5	используют O
6	обычно O
7	сглаживание I-TERM
8	Kneser I-TERM
9	– I-TERM
10	Ney I-TERM
11	или O
12	Katz I-TERM
13	’s I-TERM
14	backing I-TERM
15	- I-TERM
16	off I-TERM
17	. O

# sent_id = 706
# text =  За более подробной информацией про методы сглаживания N-грамм стоит обратиться к известной книге Кристофера Маннинга “Foundations of Statistical Natural Language Processing”.
1	За O
2	более O
3	подробной O
4	информацией O
5	про O
6	методы O
7	сглаживания I-TERM
8	N I-TERM
9	- I-TERM
10	грамм I-TERM
11	стоит O
12	обратиться O
13	к O
14	известной O
15	книге O
16	Кристофера I-TERM
17	Маннинга I-TERM
18	“ O
19	Foundations I-TERM
20	of I-TERM
21	Statistical I-TERM
22	Natural I-TERM
23	Language I-TERM
24	Processing I-TERM
25	” O
26	. O

# sent_id = 707
# text =  Хочу заметить, что 5-граммы слов я назвал не просто так: именно их (со сглаживанием, конечно) Google демонстрирует в статье “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling” — и показывает результаты, весьма сопоставимые с результатами у рекуррентных нейронных сетей — о которых, собственно, и пойдет далее речь.
1	Хочу O
2	заметить O
3	, O
4	что O
5	5-граммы O
6	слов O
7	я O
8	назвал O
9	не O
10	просто O
11	так O
12	: O
13	именно O
14	их O
15	( O
16	со O
17	сглаживанием O
18	, O
19	конечно O
20	) O
21	Google I-TERM
22	демонстрирует O
23	в O
24	статье O
25	“ I-TERM
26	One I-TERM
27	Billion I-TERM
28	Word I-TERM
29	Benchmark I-TERM
30	for I-TERM
31	Measuring I-TERM
32	Progress I-TERM
33	in I-TERM
34	Statistical I-TERM
35	Language I-TERM
36	Modeling I-TERM
37	” I-TERM
38	— O
39	и O
40	показывает O
41	результаты O
42	, O
43	весьма O
44	сопоставимые O
45	с O
46	результатами O
47	у O
48	рекуррентных I-TERM
49	нейронных I-TERM
50	сетей I-TERM
51	— O
52	о O
53	которых O
54	, O
55	собственно O
56	, O
57	и O
58	пойдет O
59	далее O
60	речь O
61	. O

# sent_id = 708
# text =  Преимущество рекуррентных нейронных сетей — в возможности использовать неограниченно длинный контекст.
1	Преимущество O
2	рекуррентных I-TERM
3	нейронных I-TERM
4	сетей I-TERM
5	— O
6	в O
7	возможности O
8	использовать O
9	неограниченно O
10	длинный O
11	контекст O
12	. O

# sent_id = 709
# text =  На практике классические RNN страдают от затухания градиента — по сути, отсутствия возможности помнить контекст дальше, чем на несколько слов.
1	На O
2	практике O
3	классические O
4	RNN I-TERM
5	страдают O
6	от O
7	затухания O
8	градиента O
9	— O
10	по O
11	сути O
12	, O
13	отсутствия O
14	возможности O
15	помнить O
16	контекст O
17	дальше O
18	, O
19	чем O
20	на O
21	несколько O
22	слов O
23	. O

# sent_id = 710
# text =  Самыми популярными являются LSTM и GRU.
1	Самыми O
2	популярными O
3	являются O
4	LSTM I-TERM
5	и O
6	GRU I-TERM
7	. O

# sent_id = 711
# text =  В дальнейшем, говоря о рекуррентном слое, я всегда буду подразумевать LSTM.
1	В O
2	дальнейшем O
3	, O
4	говоря O
5	о O
6	рекуррентном O
7	слое O
8	, O
9	я O
10	всегда O
11	буду O
12	подразумевать O
13	LSTM I-TERM
14	. O

# sent_id = 712
# text =  Вспомним теперь, что для нашей задачи языковая модель нужна для выбора наиболее подходящего следующего слова по уже сгенерированной последовательности.
1	Вспомним O
2	теперь O
3	, O
4	что O
5	для O
6	нашей O
7	задачи O
8	языковая O
9	модель O
10	нужна O
11	для O
12	выбора I-TERM
13	наиболее I-TERM
14	подходящего I-TERM
15	следующего I-TERM
16	слова I-TERM
17	по O
18	уже O
19	сгенерированной I-TERM
20	последовательности I-TERM
21	. O

# sent_id = 713
# text =  Метрические правила определяют последовательность ударных и безударных слогов в строке.
1	Метрические I-TERM
2	правила I-TERM
3	определяют O
4	последовательность O
5	ударных O
6	и O
7	безударных O
8	слогов I-TERM
9	в O
10	строке O
11	. O

# sent_id = 714
# text =  Для решения этой проблемы мы делаем лучевой поиск (beam search), выбирая на каждом шаге вместо одного сразу N путей с наивысшими вероятностями.
1	Для O
2	решения O
3	этой O
4	проблемы O
5	мы O
6	делаем O
7	лучевой I-TERM
8	поиск I-TERM
9	( O
10	beam I-TERM
11	search I-TERM
12	) O
13	, O
14	выбирая O
15	на O
16	каждом O
17	шаге O
18	вместо O
19	одного O
20	сразу O
21	N O
22	путей O
23	с O
24	наивысшими O
25	вероятностями O
26	. O

# sent_id = 715
# text =  Автоматическое определение эмоций в текстовых беседах с использованием нейронных сетей
1	Автоматическое I-TERM
2	определение I-TERM
3	эмоций I-TERM
4	в O
5	текстовых I-TERM
6	беседах I-TERM
7	с O
8	использованием O
9	нейронных I-TERM
10	сетей I-TERM

# sent_id = 716
# text = Одна из основных задач диалоговых систем состоит не только в предоставлении нужной пользователю информации, но и в генерации как можно более человеческих -TERM
1	Одна O
2	из O
3	основных O
4	задач O
5	диалоговых O
6	систем O
7	состоит O
8	не O
9	только O
10	в O
11	предоставлении I-TERM
12	нужной I-TERM
13	пользователю I-TERM
14	информации I-TERM
15	, O
16	но O
17	и O
18	в O
19	генерации I-TERM
20	как O
21	можно O
22	более O
23	человеческих O
24	ответов I-TERM
25	. O

# sent_id = 717
# text =  В этой статье мы рассмотрим архитектуру рекуррентной нейросети для определения эмоций в текстовых беседах, которая принимала участие в SemEval-2019 Task 3 “EmoContext”, ежегодном соревновании по компьютерной лингвистике.
1	В O
2	этой O
3	статье O
4	мы O
5	рассмотрим O
6	архитектуру O
7	рекуррентной I-TERM
8	нейросети I-TERM
9	для O
10	определения I-TERM
11	эмоций I-TERM
12	в O
13	текстовых I-TERM
14	беседах I-TERM
15	, O
16	которая O
17	принимала O
18	участие O
19	в O
20	SemEval-2019 O
21	Task O
22	3 O
23	“ O
24	EmoContext O
25	” O
26	, O
27	ежегодном O
28	соревновании O
29	по O
30	компьютерной I-TERM
31	лингвистике I-TERM
32	. O

# sent_id = 718
# text =  Задача состояла в классификации эмоций (“happy”, “sad”, “angry” и “others”) в беседе из трех реплик, в которой участвовали чат-бот и человек.
1	Задача O
2	состояла O
3	в O
4	классификации I-TERM
5	эмоций I-TERM
6	( O
7	“ O
8	happy I-TERM
9	” O
10	, O
11	“ O
12	sad I-TERM
13	” O
14	, O
15	“ O
16	angry I-TERM
17	” O
18	и O
19	“ O
20	others I-TERM
21	” O
22	) O
23	в O
24	беседе O
25	из O
26	трех O
27	реплик O
28	, O
29	в O
30	которой O
31	участвовали O
32	чат O
33	- O
34	бот O
35	и O
36	человек O
37	. O

# sent_id = 719
# text =  В четвёртой части мы опишем архитектуру LSTM, которую мы использовали в соревновании.
1	В O
2	четвёртой O
3	части O
4	мы O
5	опишем O
6	архитектуру O
7	LSTM I-TERM
8	, O
9	которую O
10	мы O
11	использовали O
12	в O
13	соревновании O
14	. O

# sent_id = 720
# text =  Код написан на языке Python с использованием библиотеки Keras.
1	Код O
2	написан O
3	на O
4	языке O
5	Python I-TERM
6	с O
7	использованием O
8	библиотеки O
9	Keras I-TERM
10	. O

# sent_id = 721
# text =  Подробное описание представлено здесь: (Chatterjee et al., 2019).
1	Подробное O
2	описание O
3	представлено O
4	здесь O
5	: O
6	( O
7	Chatterjee I-TERM
8	et I-TERM
9	al I-TERM
10	. I-TERM
11	, I-TERM
12	2019 I-TERM
13	) O
14	. O

# sent_id = 722
# text =  Примеры из датасета EmoContext (Chatterjee et al., 2019)
1	Примеры O
2	из O
3	датасета O
4	EmoContext I-TERM
5	( O
6	Chatterjee I-TERM
7	et I-TERM
8	al I-TERM
9	. I-TERM
10	, I-TERM
11	2019 I-TERM
12	) O

# sent_id = 723
# text =  Данные предоставлены Microsoft, скачать их можно в официальной группе в LinkedIn.
1	Данные O
2	предоставлены O
3	Microsoft I-TERM
4	, O
5	скачать O
6	их O
7	можно O
8	в O
9	официальной O
10	группе O
11	в O
12	LinkedIn I-TERM
13	. O

# sent_id = 724
# text =  В дополнение к этим данным мы собрали 900 тыс. англоязычных сообщений из Twitter, чтобы создать Distant-датасет (300 тыс. твитов на каждую эмоцию).
1	В O
2	дополнение O
3	к O
4	этим O
5	данным O
6	мы O
7	собрали O
8	900 O
9	тыс. O
10	англоязычных O
11	сообщений O
12	из O
13	Twitter I-TERM
14	, O
15	чтобы O
16	создать O
17	Distant I-TERM
18	- O
19	датасет O
20	( O
21	300 O
22	тыс. O
23	твитов O
24	на O
25	каждую O
26	эмоцию O
27	) O
28	. O

# sent_id = 725
# text =  При его создании мы придерживались стратегии Go et al. (2009), в рамках которой просто ассоциировали сообщения с наличием относящихся к эмоциям слов, таких как #angry, #annoyed, #happy, #sad, #surprised и так далее.
1	При O
2	его O
3	создании O
4	мы O
5	придерживались O
6	стратегии O
7	Go I-TERM
8	et I-TERM
9	al I-TERM
10	. I-TERM
11	( I-TERM
12	2009 I-TERM
13	) I-TERM
14	, O
15	в O
16	рамках O
17	которой O
18	просто O
19	ассоциировали I-TERM
20	сообщения I-TERM
21	с I-TERM
22	наличием I-TERM
23	относящихся I-TERM
24	к I-TERM
25	эмоциям I-TERM
26	слов I-TERM
27	, O
28	таких O
29	как O
30	# O
31	angry I-TERM
32	, O
33	# O
34	annoyed I-TERM
35	, O
36	# O
37	happy I-TERM
38	, O
39	# O
40	sad I-TERM
41	, O
42	# O
43	surprised I-TERM
44	и O
45	так O
46	далее O
47	. O

# sent_id = 726
# text =  Список терминов основан на терминах из SemEval-2018 AIT DISC (Duppada et al., 2018).
1	Список O
2	терминов O
3	основан O
4	на O
5	терминах O
6	из O
7	SemEval-2018 I-TERM
8	AIT I-TERM
9	DISC I-TERM
10	( O
11	Duppada I-TERM
12	et I-TERM
13	al I-TERM
14	. I-TERM
15	, I-TERM
16	2018 I-TERM
17	) O
18	. O

# sent_id = 727
# text =  Главной метрикой качества в соревновании EmoContext является усредненная F1-мера для трёх классов эмоций, то есть для классов «happy», «sad» и «angry».
1	Главной O
2	метрикой O
3	качества O
4	в O
5	соревновании O
6	EmoContext O
7	является O
8	усредненная O
9	F1-мера I-TERM
10	для O
11	трёх O
12	классов O
13	эмоций O
14	, O
15	то O
16	есть O
17	для O
18	классов O
19	« O
20	happy I-TERM
21	» O
22	, O
23	« O
24	sad I-TERM
25	» O
26	и O
27	« O
28	angry I-TERM
29	» O
30	. O

# sent_id = 728
# text =  Перед обучением мы предварительно обработали тексты с помощью инструмента Ekphrasis (Baziotis et al., 2017).
1	Перед O
2	обучением O
3	мы O
4	предварительно O
5	обработали O
6	тексты I-TERM
7	с O
8	помощью O
9	инструмента O
10	Ekphrasis I-TERM
11	( O
12	Baziotis I-TERM
13	et I-TERM
14	al I-TERM
15	. I-TERM
16	, I-TERM
17	2017 I-TERM
18	) O
19	. O

# sent_id = 729
# text =  Он помогает исправить орфографию, нормализовать слова, сегментировать, а также определить, какие токены следует отбросить, нормализовать или аннотировать с помощью специальных тегов.
1	Он O
2	помогает O
3	исправить I-TERM
4	орфографию I-TERM
5	, O
6	нормализовать I-TERM
7	слова I-TERM
8	, O
9	сегментировать I-TERM
10	, O
11	а O
12	также O
13	определить O
14	, O
15	какие O
16	токены O
17	следует O
18	отбросить O
19	, O
20	нормализовать O
21	или O
22	аннотировать O
23	с O
24	помощью O
25	специальных O
26	тегов I-TERM
27	. O

# sent_id = 730
# text =  Кроме того, Emphasis содержит токенизатор, который может идентифицировать большинство эмодзи, эмотиконов и сложных выражений, а также даты, время, валюты и акронимы.
1	Кроме O
2	того O
3	, O
4	Emphasis I-TERM
5	содержит O
6	токенизатор I-TERM
7	, O
8	который O
9	может O
10	идентифицировать I-TERM
11	большинство I-TERM
12	эмодзи I-TERM
13	, O
14	эмотиконов I-TERM
15	и O
16	сложных I-TERM
17	выражений I-TERM
18	, O
19	а O
20	также O
21	даты I-TERM
22	, O
23	время I-TERM
24	, O
25	валюты I-TERM
26	и O
27	акронимы I-TERM
28	. O

