# text =  Чатботы и искусственный интеллект для понимания естественного языка (NLU – Natural Language Understanding) тема достаточно горячая, про нее не раз говорилось на Хабре.
Чатботы O
и O
искусственный B-TERM
интеллект I-TERM
для O
понимания B-TERM
естественного I-TERM
языка I-TERM
( O
NLU B-TERM
– O
Natural B-TERM
Language I-TERM
Understanding I-TERM
) O
тема O
достаточно O
горячая O
, O
про O
нее O
не O
раз O
говорилось O
на O
Хабре O
. O

# text =  Хотя AI — это достаточно широкая область, включающая в себя машинное зрение, предиктивный анализ, машинный перевод и другие области – понимание естественного языка (NLU) и его генерация (NLG) является значительной и быстрорастущей его частью.
Хотя O
AI B-TERM
— O
это O
достаточно O
широкая O
область O
, O
включающая O
в O
себя O
машинное B-TERM
зрение I-TERM
, O
предиктивный B-TERM
анализ I-TERM
, O
машинный B-TERM
перевод I-TERM
и O
другие O
области O
– O
понимание B-TERM
естественного I-TERM
языка I-TERM
( O
NLU B-TERM
) O
и O
его O
генерация B-TERM
( O
NLG B-TERM
) O
является O
значительной O
и O
быстрорастущей O
его O
частью O
. O

# text =  Опуская историю, начавшуюся еще в 50-е годы с Алана Тьюринга и программы Элиза в 60-е годы, а также научные исследования в области лингвистики и машинного обучения 90-х годов, значимым событием более новой истории стало появление языка разметки AIML (Artificial Intelligence Markup Language), разработанной в 2001-м году Ричардом Уэлсом (Richard Wallace) и созданным на его основе чатботом A.L.I.C.E.
Опуская O
историю O
, O
начавшуюся O
еще O
в O
50-е O
годы O
с O
Алана B-TERM
Тьюринга I-TERM
и O
программы O
Элиза B-TERM
в O
60-е O
годы O
, O
а O
также O
научные O
исследования O
в O
области O
лингвистики O
и O
машинного B-TERM
обучения I-TERM
90-х O
годов O
, O
значимым O
событием O
более O
новой O
истории O
стало O
появление O
языка O
разметки O
AIML B-TERM
( O
Artificial B-TERM
Intelligence I-TERM
Markup I-TERM
Language I-TERM
) O
, O
разработанной O
в O
2001-м B-TERM
году O
Ричардом B-TERM
Уэлсом I-TERM
( O
Richard I-TERM
Wallace I-TERM
) O
и O
созданным O
на O
его O
основе O
чатботом O
A.L.I.C.E. B-TERM

# text =  В течение последующих десяти лет подходы к написанию чатботов во многом представляли из себя переработки или улучшения этой методологии, получившей название «rule-based подход» или «подход на основе формальных правил».
В O
течение O
последующих O
десяти O
лет O
подходы O
к O
написанию O
чатботов O
во O
многом O
представляли O
из O
себя O
переработки O
или O
улучшения O
этой O
методологии O
, O
получившей O
название O
« O
rule B-TERM
- I-TERM
based I-TERM
подход O
» O
или O
« O
подход O
на O
основе O
формальных B-TERM
правил I-TERM
» O
. O

# text =  Именно эти технологии, вместе с заметным продвижением в области технологий синтеза и распознавания речи, а также распространением мессенджеров и вебчатов – обусловили стремительный рост количества внедрений NLU-технологий в 2015-2018-м годах.
Именно O
эти O
технологии O
, O
вместе O
с O
заметным O
продвижением O
в O
области O
технологий O
синтеза B-TERM
и I-TERM
распознавания I-TERM
речи I-TERM
, O
а O
также O
распространением O
мессенджеров O
и O
вебчатов O
– O
обусловили O
стремительный O
рост O
количества O
внедрений O
NLU B-TERM
- I-TERM
технологий I-TERM
в O
2015 B-TERM
- I-TERM
2018-м I-TERM
годах O
. O

# text =  Голосовые ассистенты (IVA): Alexa от Amazon, Google Assistant от Google, Siri от Apple, Cortana от Microsoft, Алиса от Яндекса – они определяют интенты (намерения) пользователей и исполняют команды.
Голосовые O
ассистенты O
( O
IVA O
) O
: O
Alexa B-TERM
от O
Amazon B-TERM
, O
Google B-TERM
Assistant I-TERM
от O
Google B-TERM
, O
Siri B-TERM
от O
Apple B-TERM
, O
Cortana B-TERM
от O
Microsoft B-TERM
, O
Алиса B-TERM
от O
Яндекса B-TERM
– O
они O
определяют O
интенты B-TERM
( O
намерения B-TERM
) O
пользователей O
и O
исполняют O
команды O
. O

# text =  В качестве каналов могут выступать умные устройства, ассистенты, встроенные в устройства или мобильные телефоны, привычный звонок на номер телефона, мессенджеры или вебчаты, подобные популярным в России Livetex, Jivosite или Webim.
В O
качестве O
каналов O
могут O
выступать O
умные O
устройства O
, O
ассистенты O
, O
встроенные O
в O
устройства O
или O
мобильные O
телефоны O
, O
привычный O
звонок O
на O
номер O
телефона O
, O
мессенджеры O
или O
вебчаты O
, O
подобные O
популярным O
в O
России O
Livetex B-TERM
, O
Jivosite B-TERM
или O
Webim B-TERM
. O

# text =  За эту конвертацию отвечают платформы ASR (распознавание речи), TTS (синтез речи), системы интеграции с телефонией.
За O
эту O
конвертацию O
отвечают O
платформы O
ASR B-TERM
( O
распознавание B-TERM
речи I-TERM
) O
, O
TTS B-TERM
( O
синтез B-TERM
речи I-TERM
) O
, O
системы O
интеграции O
с O
телефонией O
. O

# text =  Наличие развитого rule-based синтаксиса может ускорить разработку чатботов в разы.
Наличие O
развитого O
rule B-TERM
- I-TERM
based I-TERM
синтаксиса O
может O
ускорить O
разработку O
чатботов O
в O
разы O
. O

# text =  Анализ эмоций, богатая и глубокая аналитика, специальные фильтры (например, на использование ненормативной лексики), языковая поддержка, хранение контекста, как и собственно, точность работы используемых нейросетевых алгоритмов, а также производительность, масштабируемость и стабильность – все это также важные, хотя и не всегда очевидные со стороны, особенности диалоговых платформ.
Анализ B-TERM
эмоций I-TERM
, O
богатая O
и O
глубокая B-TERM
аналитика I-TERM
, O
специальные O
фильтры O
( O
например O
, O
на O
использование O
ненормативной O
лексики O
) O
, O
языковая O
поддержка O
, O
хранение O
контекста O
, O
как O
и O
собственно O
, O
точность O
работы O
используемых O
нейросетевых O
алгоритмов O
, O
а O
также O
производительность O
, O
масштабируемость O
и O
стабильность O
– O
все O
это O
также O
важные O
, O
хотя O
и O
не O
всегда O
очевидные O
со O
стороны O
, O
особенности O
диалоговых B-TERM
платформ I-TERM
. O

# text =  Алгоритм понимания естественного языка (Natural Language Understanding, NLU) Microsoft DeBERTa превзошел человеческие возможности в одном из самых сложных тестов для подобных алгоритмов SuperGLUE.
Алгоритм O
понимания O
естественного O
языка O
( O
Natural B-TERM
Language I-TERM
Understanding I-TERM
, O
NLU B-TERM
) O
Microsoft B-TERM
DeBERTa B-TERM
превзошел O
человеческие O
возможности O
в O
одном O
из O
самых O
сложных O
тестов O
для O
подобных O
алгоритмов O
SuperGLUE B-TERM
. O

# text =  На данный момент модель занимает первое место в рейтинге с показателем в 90,3, в то время как среднее значение человеческих возможностей составляет 89,8 баллов.
На O
данный O
момент O
модель O
занимает O
первое O
место O
в O
рейтинге O
с O
показателем O
в O
90,3 B-TERM
, O
в O
то O
время O
как O
среднее O
значение O
человеческих O
возможностей O
составляет O
89,8 B-TERM
баллов O
. O

# text = Тест SuperGLUE включает в себя ряд задач, которые разработаны для оценки способности ИИ-моделей распознавать и понимать естественный язык, например, дать правильный ответ на вопрос на базе прочитанного абзаца, определить, правильно ли используется многозначное слово в определенном контексте и т.д.
Тест O
SuperGLUE B-TERM
включает O
в O
себя O
ряд O
задач O
, O
которые O
разработаны O
для O
оценки O
способности O
ИИ O
- O
моделей O
распознавать O
и O
понимать O
естественный O
язык O
, O
например O
, O
дать B-TERM
правильный I-TERM
ответ I-TERM
на I-TERM
вопрос I-TERM
на I-TERM
базе I-TERM
прочитанного I-TERM
абзаца I-TERM
, O
определить O
, O
правильно O
ли O
используется O
многозначное B-TERM
слово I-TERM
в O
определенном O
контексте O
и O
т.д. O

# text =  Тест был разработан группой исследователей в 2019 году.
Тест O
был O
разработан O
группой O
исследователей O
в O
2019 B-TERM
году O
. O

# text = Для того чтобы добиться текущего результата в 90,3 балла, DeBERTa получила масштабное обновление архитектуры: теперь она состоит из 48 слоев и имеет 1,5 млрд параметров.
Для O
того O
чтобы O
добиться O
текущего O
результата O
в O
90,3 B-TERM
балла O
, O
DeBERTa B-TERM
получила O
масштабное O
обновление O
архитектуры O
: O
теперь O
она O
состоит O
из O
48 B-TERM
слоев O
и O
имеет O
1,5 O
млрд O
параметров O
. O

# text =  Кроме того, DeBERTa будет интегрирована в следующую версию Тьюринговой модели Microsoft Turing (Turing NLRv4).
Кроме O
того O
, O
DeBERTa B-TERM
будет O
интегрирована O
в O
следующую O
версию O
Тьюринговой B-TERM
модели I-TERM
Microsoft I-TERM
Turing I-TERM
( O
Turing B-TERM
NLRv4 I-TERM
) O
. O

# text =  Тьюринговые модели используются в таких продуктах Microsoft, как Bing, Office, Dynamics и Azure Cognitive Services, чтобы совершенствовать, к примеру, взаимодействие с чат-ботами, предоставление рекомендаций и ответов на вопросы, поиск, автоматизацию поддержки клиентов, создание контента и решение многих других задач на пользу сотен миллионов пользователей.
Тьюринговые B-TERM
модели I-TERM
используются O
в O
таких O
продуктах O
Microsoft B-TERM
, O
как O
Bing B-TERM
, O
Office B-TERM
, O
Dynamics B-TERM
и O
Azure B-TERM
Cognitive I-TERM
Services I-TERM
, O
чтобы O
совершенствовать O
, O
к O
примеру O
, O
взаимодействие O
с O
чат B-TERM
- I-TERM
ботами I-TERM
, O
предоставление O
рекомендаций O
и O
ответов O
на O
вопросы O
, O
поиск O
, O
автоматизацию 
поддержки O
клиентов O
, O
создание B-TERM
контента I-TERM
и O
решение O
многих O
других O
задач O
на O
пользу O
сотен O
миллионов O
пользователей O
. O

# text =  В отличии от машин, люди хорошо умеют использовать знания, ранее полученные при выполнении различных задач, для решения новых – это называется композиционным обобщением (англ. compositional generalization).
В O
отличии O
от O
машин O
, O
люди O
хорошо O
умеют O
использовать O
знания O
, O
ранее O
полученные O
при O
выполнении O
различных O
задач O
, O
для O
решения O
новых O
– O
это O
называется O
композиционным B-TERM
обобщением I-TERM
( O
англ O
. O
compositional B-TERM
generalization I-TERM
) O
. O

# text =  Зайдя на несколько из них я увидел что большая половина типа Wix используют технологию Искусственного Интеллекта, чтобы создать шаблон разметки страницы и далее её уже заполнить.
Зайдя O
на O
несколько O
из O
них O
я O
увидел O
что O
большая O
половина O
типа O
Wix B-TERM
используют O
технологию O
Искусственного B-TERM
Интеллекта I-TERM
, O
чтобы O
создать O
шаблон O
разметки O
страницы O
и O
далее O
её O
уже O
заполнить O
. O

# text =  В финале я могу его редактировать путем Drag & Drop.
В O
финале O
я O
могу O
его O
редактировать O
путем O
Drag B-TERM
& I-TERM
Drop I-TERM
. O

# text =  Типичным методом обучения без учителя является кластеризация, благодаря которому обучающая выборка разбивается на устойчивые группы или кластеры.
Типичным O
методом O
обучения O
без O
учителя O
является O
кластеризация B-TERM
, O
благодаря O
которому O
обучающая O
выборка B-TERM
разбивается O
на O
устойчивые O
группы O
или O
кластеры B-TERM
. O

# text =  Другой подход обучения без учителя для текстов называется тематическим моделированием (topic modeling), позволяющим выявить в неразмеченных текстах основные тематики.
Другой O
подход O
обучения O
без O
учителя O
для O
текстов O
называется O
тематическим B-TERM
моделированием I-TERM
( O
topic B-TERM
modeling I-TERM
) O
, O
позволяющим O
выявить O
в O
неразмеченных O
текстах O
основные O
тематики O
. O

# text =  Если отказываемся от методов unsupervised learning, то логично обратиться к методам обучения с учителем (supervised learning) и в частности к классификации.
Если O
отказываемся O
от O
методов O
unsupervised B-TERM
learning I-TERM
, O
то O
логично O
обратиться O
к O
методам B-TERM
обучения I-TERM
с I-TERM
учителем I-TERM
( O
supervised B-TERM
learning I-TERM
) O
и O
в O
частности O
к O
классификации B-TERM
. O

# text =  Результатом работы языковой модели являются эмбеддинги — это отображение из пространства слов в пространство векторов конкретной фиксированной длины, причем векторы, соответствующие близким по смыслу словам, будут расположены в новом пространстве рядом, а далекие по смыслу — далеко.
Результатом O
работы O
языковой O
модели O
являются O
эмбеддинги B-TERM
— O
это O
отображение O
из O
пространства O
слов B-TERM
в O
пространство O
векторов O
конкретной O
фиксированной O
длины O
, O
причем O
векторы O
, O
соответствующие O
близким O
по O
смыслу B-TERM
словам I-TERM
, O
будут O
расположены O
в O
новом O
пространстве O
рядом O
, O
а O
далекие O
по O
смыслу O
— O
далеко O
. O

# text =  При использовании TF-IDF (например, вот) подхода с фильтром по частотам и логистической регрессии уже можно получить прекрасные результаты: изначально в краулер отправлялись очень разные тексты, и модель прекрасно справляется.
При O
использовании O
TF B-TERM
- I-TERM
IDF I-TERM
( O
например O
, O
вот O
) O
подхода O
с O
фильтром O
по O
частотам O
и O
логистической B-TERM
регрессии I-TERM
уже O
можно O
получить O
прекрасные O
результаты O
: O
изначально O
в O
краулер O
отправлялись O
очень O
разные O
тексты O
, O
и O
модель O
прекрасно O
справляется O
. O

# text =  Для каждой из популяций рассчитаем word2vec расстояние до центра положительной обучающей выборки.
Для O
каждой O
из O
популяций O
рассчитаем O
word2vec B-TERM
расстояние O
до O
центра O
положительной O
обучающей O
выборки B-TERM
. O

# text =  Распределения можно разделить, и для оценки расстояния между распределениями в первую очередь логично обратиться к Дивергенции Кульбака-Лейблера (ДКЛ).
Распределения O
можно O
разделить O
, O
и O
для O
оценки O
расстояния O
между O
распределениями O
в O
первую O
очередь O
логично O
обратиться O
к O
Дивергенции B-TERM
Кульбака I-TERM
- I-TERM
Лейблера I-TERM
( O
ДКЛ B-TERM
) O
. O

# text =  Основатель компании Imagination Engines, Stephen L. Thaler продвигает свою нейронную сеть по имени DABUS (Device for the Autonomous Boot-strapping of Unified Sentience), указывая ее в качестве автора изобретения в заявках на патенты на разные изобретения, сгенерированные этой сетью.
Основатель O
компании O
Imagination B-TERM
Engines I-TERM
, O
Stephen B-TERM
L. I-TERM
Thaler I-TERM
продвигает O
свою O
нейронную O
сеть O
по O
имени O
DABUS B-TERM
( O
Device B-TERM
for I-TERM
the I-TERM
Autonomous I-TERM
Boot I-TERM
- I-TERM
strapping I-TERM
of I-TERM
Unified I-TERM
Sentience I-TERM
) O
, O
указывая O
ее O
в O
качестве O
автора O
изобретения O
в O
заявках O
на O
патенты O
на O
разные O
изобретения O
, O
сгенерированные O
этой O
сетью O
. O

# text =  1 January 2010 at 07:59 Заметки об NLP (часть 2) Artificial Intelligence Natural Language Processing.
1 O
January O
2010 O
at O
07:59 O
Заметки O
об O
NLP B-TERM
( O
часть O
2 O
) O
Artificial B-TERM
Intelligence I-TERM
Natural B-TERM
Language I-TERM
Processing I-TERM
. O


# text =  Хотя в первой части я и говорил, что не собираюсь останавливаться на морфологии, видимо, совсем без неё не получится
Хотя O
в O
первой O
части O
я O
и O
говорил O
, O
что O
не O
собираюсь O
останавливаться O
на O
морфологии B-TERM
, O
видимо O
, O
совсем O
без O
неё O
не O
получится O
. O

# text =  Всё-таки обработка предложений сильно завязана на предшествующий морфологический анализ.
Всё O
- O
таки O
обработка O
предложений O
сильно O
завязана O
на O
предшествующий O
морфологический B-TERM
анализ I-TERM
. O

# text =  Наш с вами родной русский язык очень хорош (для нас) и труден (для иностранцев) богатой фонетикой и разнообразием грамматических средств.
Наш O
с O
вами O
родной O
русский O
язык O
очень O
хорош O
( O
для O
нас O
) O
и O
труден O
( O
для O
иностранцев O
) O
богатой O
фонетикой B-TERM
и O
разнообразием O
грамматических B-TERM
средств I-TERM
. O


# text =  Во-первых, в них не так много незнакомых нам фонем.
Во O
- O
первых O
, O
в O
них O
не O
так O
много O
незнакомых O
нам O
фонем B-TERM
. O


# text =  Во-вторых, обилие грамматических явлений редко сталкивает нас с чем-либо непонятным.
Во O
- O
вторых O
, O
обилие O
грамматических B-TERM
явлений I-TERM
редко O
сталкивает O
нас O
с O
чем O
- O
либо O
непонятным O
. O

# text =  А для американца, например, само понятие рода или падежа совершенно неочевидно.
А O
для O
американца O
, O
например O
, O
само O
понятие O
рода B-TERM
или O
падежа B-TERM
совершенно O
неочевидно O
. O

# text =  Теперь о морфологии.
Теперь O
о O
морфологии B-TERM
. O


# text =  Автоматические морфологические анализаторы работают хорошо.
Автоматические B-TERM
морфологические I-TERM
анализаторы I-TERM
работают O
хорошо O
. O



# text =  Если кому интересно посмотреть, как работает автоматический анализатор — можно поэкспериментировать на сайте С.А. Старостина.
Если O
кому O
интересно O
посмотреть O
, O
как O
работает O
автоматический B-TERM
анализатор B-TERM
— O
можно O
поэкспериментировать O
на O
сайте O
С.А. B-TERM
Старостина I-TERM
. O

# text =  Смею предположить, что едва ли не все морфологические анализаторы русского так или иначе опираются на Грамматический словарь Зализняка.
Смею O
предположить O
, O
что O
едва O
ли O
не O
все O
морфологические O
анализаторы O
русского O
так O
или O
иначе O
опираются O
на O
Грамматический B-TERM
словарь I-TERM
Зализняка I-TERM
. O

# text =  Сам я пользуюсь разработками Алексея Сокирко, «обёрнутыми» в удобный интерфейс на сайте Lemmatizer.
Сам O
я O
пользуюсь O
разработками O
Алексея B-TERM
Сокирко I-TERM
, O
« O
обёрнутыми O
» O
в O
удобный O
интерфейс O
на O
сайте O
Lemmatizer B-TERM
. O


# text =  Судите сами: упомянутый русский морфологический анализатор Алексея Сокирко оперирует базой данных в 18,5 мегабайт.
Судите O
сами O
: O
упомянутый O
русский B-TERM
морфологический I-TERM
анализатор I-TERM
Алексея I-TERM
Сокирко I-TERM
оперирует O
базой O
данных O
в O
18,5 O
мегабайт O
. O

# text =  На Грамоте предлагают относить их к «предикативам», но общепринятого подхода нет.
На O
Грамоте B-TERM
предлагают O
относить O
их O
к O
« O
предикативам O
» O
, O
но O
общепринятого O
подхода O
нет O
. O

# text =  Например, ещё одна «фича» анализатора Сокирко: он называет глаголы в личной форме («бегаю») глаголами, а в начальной форме («бегать») — инфинитивами.
Например O
, O
ещё O
одна O
« O
фича O
» O
анализатора B-TERM
Сокирко I-TERM
: O
он O
называет O
глаголы O
в O
личной O
форме O
( O
« O
бегаю O
» O
) O
глаголами O
, O
а O
в O
начальной O
форме O
( O
« O
бегать O
» O
) O
— O
инфинитивами O
. O



# text =  Tags: NLP, обработка текстовб, компьютерная лингвистика.
Tags O
: O
NLP B-TERM
, O
обработка B-TERM
текстов
, O
компьютерная B-TERM
лингвистика I-TERM


# text =  Туториал по фреймворку для программирования датасетов MTS AI corporate blog.
Туториал O
по O
фреймворку O
для O
программирования O
датасетов O
MTS B-TERM
AI I-TERM
corporate B-TERM
blog I-TERM


# text =  Я Игорь Буянов, старший разработчик группы разметки данных MTS AI.
Я O
Игорь B-TERM
Буянов I-TERM
, O
старший O
разработчик O
группы O
разметки O
данных O
MTS B-TERM
AI I-TERM
. O

# text =  Недавно рассказывал о том, как делать иерархически датасет из Википедии.
Недавно O
рассказывал O
о O
том O
, O
как O
делать O
иерархически O
датасет O
из O
Википедии B-TERM
. O

# text =  В этом посте хочу рассказать вам о Сноркеле - фреймворке для программирования данных (data programming).
В O
этом O
посте O
хочу O
рассказать O
вам O
о O
Сноркеле B-TERM
- O
фреймворке B-TERM
для I-TERM
программирования I-TERM
данных I-TERM
( O
data B-TERM
programming I-TERM
) O
. O


# text =  Проект стартовал в Стэнфорде как инструмент для помощи в разметке датасетов для задачи information extraction, а сейчас разработчики делают платформу для пользования внешними заказчиками. 

Проект O
стартовал O
в O
Стэнфорде B-TERM
как O
инструмент O
для O
помощи O
в O
разметке O
датасетов O
для O
задачи O
information B-TERM
extraction I-TERM
, O
а O
сейчас O
разработчики O
делают O
платформу O
для O
пользования O
внешними O
заказчиками O
. O


# text =  В разметочные функции (labeling functions) закодированы все возможные правила, по которым можно поставить какую-либо метку каждому примеру из набора данных.
В O
разметочные B-TERM
функции I-TERM
( O
labeling B-TERM
functions I-TERM
) O
закодированы O
все O
возможные O
правила O
, O
по O
которым O
можно O
поставить O
какую O
- O
либо O
метку O
каждому O
примеру O
из O
набора O
данных O
. O

# text =  В качестве основы для таких функций используются:внешние базы данных, такие как WordNet или WikiBase.
В O
качестве O
основы O
для O
таких O
функций O
используются O
: O
внешние O
базы O
данных O
, O
такие O
как O
WordNet B-TERM
или O
WikiBase B-TERM
. O

# text =  Генеративная модель, являющаяся сердцем Сноркеля, попытается учесть недостатки отдельных функций.
Генеративная B-TERM
модель I-TERM
, O
являющаяся O
сердцем O
Сноркеля B-TERM
, O
попытается O
учесть O
недостатки O
отдельных O
функций O
. O

# text =  Для наглядности оставляю здесь иллюстрацию с последовательностью работы со Снокрелем для задачи information extraction из оригинальной статьи.
Для O
наглядности O
оставляю O
здесь O
иллюстрацию O
с O
последовательностью O
работы O
со O
Снокрелем B-TERM
для O
задачи O
information B-TERM
extraction I-TERM
из O
оригинальной O
статьи O
. O

# text =  Авторы оригинальной статьи представляют ее как факторный граф, или графическую вероятностную модель.
Авторы O
оригинальной O
статьи O
представляют O
ее O
как O
факторный B-TERM
граф I-TERM
, O
или O
графическую B-TERM
вероятностную I-TERM
модель I-TERM
. O

# text =  Тогда модель определяется так, чтобы обучить эту модель без доступа к истинным меткам, это нужно обучаться с помощью логарифмического негативного маргинализированного правдоподобия, зная матрицу Оптимизацию авторы проводили с помощью SGD с семплированием Гиббса.
Тогда O
модель O
определяется O
так
, O
чтобы O
обучить O
эту O
модель O
без O
доступа O
к O
истинным O
меткам O
, O
это O
нужно O
обучаться O
с O
помощью O
логарифмического B-TERM
негативного I-TERM
маргинализированного I-TERM
правдоподобия I-TERM
, O
зная O
матрицу B-TERM
Оптимизацию I-TERM
авторы O
проводили O
с O
помощью O
SGD B-TERM
с O
семплированием B-TERM
Гиббса I-TERM
. O

# text =  Загрузим заранее обученную модель fastText, чей выбор объясняется наличием огромного количества опечаток в текстах.
Загрузим O
заранее O
обученную O
модель O
fastText B-TERM
, O
чей O
выбор O
объясняется O
наличием O
огромного O
количества O
опечаток O
в O
текстах O
. O

# text =  Таким образом мы получили опорный вектор для класса "диарея".
Таким O
образом O
мы O
получили O
опорный B-TERM
вектор I-TERM
для O
класса O
" O
диарея O
" O
. O
# text =  Речь шла о морфологической разметке (part of speech tagging) современных текстов на русском языке.
Речь O
шла O
о O
морфологической B-TERM
разметке I-TERM
( O
part B-TERM
of I-TERM
speech I-TERM
tagging I-TERM
) O
современных O
текстов O
на O
русском B-TERM
языке I-TERM
. O

# text =  Как обычно, результат разметки будет опубликован на условиях лицензии Creative Commons.
Как O
обычно O
, O
результат O
разметки B-TERM
будет O
опубликован O
на O
условиях O
лицензии O
Creative B-TERM
Commons I-TERM
. O

# text =  Извлечение именованных сущностей из текста — одна из востребованных функций текстовой аналитики.
Извлечение B-TERM
именованных I-TERM
сущностей I-TERM
из O
текста O
— O
одна O
из O
востребованных O
функций O
текстовой O
аналитики O
. O

# text =  Специалисты Data Science часто применяют различные методы получения датасетов.
Специалисты O
Data B-TERM
Science I-TERM
часто O
применяют O
различные O
методы O
получения O
датасетов O
. O

# text =  Цель этой статьи — представить краткий обзор трех разных методов извлечения данных с использованием языка Python.
Цель O
этой O
статьи O
— O
представить O
краткий O
обзор O
трех O
разных O
методов O
извлечения O
данных O
с O
использованием O
языка O
Python B-TERM
. O

# text =  Я расскажу, как делать это с помощью Jupyter Notebook.
Я O
расскажу O
, O
как O
делать O
это O
с O
помощью O
Jupyter B-TERM
Notebook I-TERM
. O

# text =  Библиотека SQLAlchemy позволит связать ваш код в ноутбуке с наиболее распространенными типами баз данных.
Библиотека O
SQLAlchemy B-TERM
позволит O
связать O
ваш O
код O
в O
ноутбуке O
с O
наиболее O
распространенными O
типами O
баз O
данных O
. O


# text =  Мы собираемся применить Beautiful Soup и библиотеку urllib, чтобы соскрапить названия отелей и цены на них с веб-сайта TripAdvisor.
Мы O
собираемся O
применить O
Beautiful B-TERM
Soup I-TERM
и O
библиотеку O
urllib B-TERM
, O
чтобы O
соскрапить O
названия O
отелей O
и O
цены O
на O
них O
с O
веб O
- O
сайта O
TripAdvisor B-TERM
. O

# text =  Я приведу простой пример извлечения данных о погоде с общедоступного API Dark Sky.
Я O
приведу O
простой O
пример O
извлечения O
данных O
о O
погоде O
с O
общедоступного O
API B-TERM
Dark I-TERM
Sky I-TERM
. O

# text =  Для доступа к данным из Dark Sky я воспользуюсь библиотекой requests.
Для O
доступа O
к O
данным O
из O
Dark B-TERM
Sky I-TERM
я O
воспользуюсь O
библиотекой O
requests B-TERM
. O

# text =  Известный учёный Алан Тьюринг в 1950 году усомнился в том, что машина не может мыслить, и для проверки предложил свой знаменитый тест.
Известный O
учёный O
Алан B-TERM
Тьюринг I-TERM
в O
1950 B-TERM
году O
усомнился O
в O
том O
, O
что O
машина O
не O
может O
мыслить O
, O
и O
для O
проверки O
предложил O
свой O
знаменитый O
тест B-TERM
. O

# text = В 1954 году прошёл Джорджтаунский эксперимент.
В O
1954 B-TERM
году O
прошёл O
Джорджтаунский B-TERM
эксперимент I-TERM
. O

# text =  В его рамках демонстрировалась система, которая автоматически перевела 60 предложений с русского языка на французский.
В O
его O
рамках O
демонстрировалась O
система B-TERM
, O
которая O
автоматически O
перевела O
60 O
предложений B-TERM
с O
русского B-TERM
языка B-TERM
на O
французский B-TERM
. O

# text =  В 1960-е годы появились первые чат-боты, очень примитивные: в основном они перефразировали то, что говорил им собеседник-человек.
В O
1960-е B-TERM
годы O
появились O
первые O
чат B-TERM
- I-TERM
боты I-TERM
, O
очень O
примитивные O
: O
в O
основном O
они O
перефразировали O
то O
, O
что O
говорил O
им O
собеседник O
- O
человек O
. O

# text =  Даже знаменитый чат-бот Женя Густман, который, как считается, прошёл одну из версий теста Тьюринга, сделал это не благодаря хитрым алгоритмам.
Даже O
знаменитый O
чат B-TERM
- I-TERM
бот I-TERM
Женя I-TERM
Густман I-TERM
, O
который O
, O
как O
считается O
, O
прошёл O
одну O
из O
версий O
теста B-TERM
Тьюринга I-TERM
, O
сделал O
это O
не O
благодаря O
хитрым O
алгоритмам O
. O

# text =  Учёные пытались всё формализовать, построить формальную модель, онтологию, понятия, связи, общие правила синтаксического разбора и универсальную грамматику.
Учёные O
пытались O
всё O
формализовать O
, O
построить O
формальную B-TERM
модель I-TERM
, O
онтологию B-TERM
, O
понятия B-TERM
, O
связи B-TERM
, O
общие O
правила O
синтаксического B-TERM
разбора I-TERM
и O
универсальную B-TERM
грамматику I-TERM
. O

# text =  Тогда возникла теория грамматик Хомского.
Тогда O
возникла O
теория B-TERM
грамматик I-TERM
Хомского I-TERM
. O

# text =  Поэтому в 1980-е годы внимание переключилось на систему другого класса: на алгоритмы машинного обучения и так называемую корпусную лингвистику.
Поэтому O
в O
1980-е B-TERM
годы O
внимание O
переключилось O
на O
систему O
другого O
класса B-TERM
: O
на O
алгоритмы B-TERM
машинного I-TERM
обучения I-TERM
и O
так O
называемую O
корпусную B-TERM
лингвистику I-TERM
. O

# text =  В 1990-е годы эта область получила очень мощный толчок благодаря развитию Всемирной паутины с большим количеством слабоструктурированного текста, по которому нужно было искать, его требовалось каталогизировать.
В O
1990-е B-TERM
годы O
эта O
область O
получила O
очень O
мощный O
толчок O
благодаря O
развитию O
Всемирной B-TERM
паутины I-TERM
с O
большим O
количеством O
слабоструктурированного B-TERM
текста I-TERM
, O
по O
которому O
нужно O
было O
искать O
, O
его O
требовалось O
каталогизировать O
. O

# text =  В 2000-е анализ естественных языков начал применяться уже не только для поиска в Интернете, но и для решения разнообразных задач.
В O
2000-е O
анализ B-TERM
естественных I-TERM
языков I-TERM
начал O
применяться O
уже O
не O
только O
для O
поиска O
в O
Интернете B-TERM
, O
но O
и O
для O
решения O
разнообразных O
задач O
. O

# text =  Возникли модели, основанные на краудсорсинге: мы не только пытаемся что-то понять с помощью машины, а подключаем людей, которые за небольшую плату определяют, на каком языке написан текст.
Возникли O
модели O
, O
основанные O
на O
краудсорсинге B-TERM
: O
мы O
не O
только O
пытаемся O
что O
- O
то O
понять O
с O
помощью O
машины O
, O
а O
подключаем O
людей O
, O
которые O
за O
небольшую O
плату O
определяют O
, O
на O
каком O
языке O
написан O
текст O
. O

# text =  В некотором смысле начали возрождаться идеи использования формальных онтологий, но теперь онтологии крутятся вокруг краудсорсинговых баз знаний, в частности баз на основе Linked Open Data.
В O
некотором O
смысле O
начали O
возрождаться O
идеи O
использования O
формальных B-TERM
онтологий I-TERM
, O
но O
теперь O
онтологии B-TERM
крутятся O
вокруг O
краудсорсинговых O
баз O
знаний O
, O
в O
частности O
баз O
на O
основе O
Linked B-TERM
Open I-TERM
Data I-TERM
. O

# text =  Это целый набор баз знаний, его центр — машиночитаемый вариант «Википедии» DBpedia, который тоже наполняется по краудсорсинговой модели.
Это O
целый O
набор O
баз O
знаний O
, O
его O
центр O
— O
машиночитаемый O
вариант O
« O
Википедии B-TERM
» O
DBpedia B-TERM
, O
который O
тоже O
наполняется O
по O
краудсорсинговой B-TERM
модели I-TERM
. O

# text =  В частности, семантический анализ (о чём документ?), генерация автоматической аннотации и автоматического summary, перевод и создание документов.
В O
частности O
, O
семантический B-TERM
анализ I-TERM
( O
о O
чём O
документ O
? O
) O 
, O
генерация B-TERM
автоматической I-TERM
аннотации I-TERM
и O
автоматического B-TERM
summary I-TERM
, O
перевод B-TERM
и O
создание B-TERM
документов I-TERM
. O

# text =  Все наверняка слышали об известном генераторе научных статей SCIgen, который создал статью «Корчеватель: Алгоритм типичной унификации точек доступа и избыточности».
Все O
наверняка O
слышали O
об O
известном O
генераторе O
научных O
статей O
SCIgen B-TERM
, O
который O
создал O
статью O
« O
Корчеватель O
: O
Алгоритм O
типичной O
унификации O
точек O
доступа O
и O
избыточности O
» O
. O

# text =  Но в случае с лентой такие рекомендации работают плохо: здесь постоянно возникает ситуация холодного старта.
Но O
в O
случае O
с O
лентой O
такие O
рекомендации O
работают O
плохо O
: O
здесь O
постоянно O
возникает O
ситуация B-TERM
холодного I-TERM
старта I-TERM
. O

# text =  Поэтому применим классический воркэраунд для задачи холодного старта и построим систему контентных рекомендаций: попробуем научить машину понимать, о чём написан пост.
Поэтому O
применим O
классический O
воркэраунд B-TERM
для O
задачи O
холодного B-TERM
старта I-TERM
и O
построим O
систему O
контентных O
рекомендаций O
: O
попробуем O
научить O
машину O
понимать O
, O
о O
чём O
написан O
пост O
. O

# text =  Соответственно, требуется метод семантического анализа.
Соответственно O
, O
требуется O
метод B-TERM
семантического I-TERM
анализа I-TERM
. O

# text =  Тут поможет анализ эмоциональной окраски.
Тут O
поможет O
анализ B-TERM
эмоциональной I-TERM
окраски I-TERM
. O

# text =  В частности, это Apache Tika, японская библиотека language-detection и одна из последних разработок — питоновский пакет Ldig, который как раз работает на инфинитиграммах.
В O
частности O
, O
это O
Apache B-TERM
Tika I-TERM
, O
японская O
библиотека O
language B-TERM
- I-TERM
detection I-TERM
и O
одна O
из O
последних O
разработок O
— O
питоновский O
пакет O
Ldig B-TERM
, O
который O
как O
раз O
работает O
на O
инфинитиграммах O
. O

# text =  Но если текст короткий, из одного предложения или нескольких слов, то классический подход, основанный на триграммах, очень часто ошибается.
Но O
если O
текст B-TERM
короткий O
, O
из O
одного O
предложения B-TERM
или O
нескольких O
слов O
, O
то O
классический O
подход O
, O
основанный O
на O
триграммах B-TERM
, O
очень O
часто O
ошибается O
. O

# text =  Исправить ситуацию могут инфинитиграммы, но это новая область, далеко не для всех языков уже есть обученные и готовые классификаторы.
Исправить O
ситуацию O
могут O
инфинитиграммы B-TERM
, O
но O
это O
новая O
область O
, O
далеко O
не O
для O
всех O
языков O
уже O
есть O
обученные O
и O
готовые O
классификаторы O
. O

# text =  Первый основан на так называемом фонетическом матчинге.
Первый O
основан O
на O
так O
называемом O
фонетическом B-TERM
матчинге I-TERM
. O

# text =  Альтернативный подход — так называемое редакционное расстояние, с помощью которого мы ищем в словаре максимально похожие слова-аналоги.
Альтернативный O
подход O
— O
так O
называемое O
редакционное B-TERM
расстояние I-TERM
, O
с O
помощью O
которого O
мы O
ищем O
в O
словаре O
максимально O
похожие O
слова O
- O
аналоги O
. O

# text =  Первая концепция — стемминг, мы пытаемся найти основу слова.
Первая O
концепция O
— O
стемминг B-TERM
, O
мы O
пытаемся O
найти O
основу B-TERM
слова I-TERM
. O

# text =  Здесь используется подход affix stripping.
Здесь O
используется O
подход O
affix B-TERM
stripping I-TERM
. O

# text =  Есть известная реализация, так называемый стеммер Портера, или проект Snowball.
Есть O
известная O
реализация O
, O
так O
называемый O
стеммер B-TERM
Портера I-TERM
, O
или O
проект O
Snowball I-TERM
. O

# text =  Самый распространённый, наверное, инструмент — реализация в пакете Apache Lucene.
Самый O
распространённый O
, O
наверное O
, O
инструмент O
— O
реализация O
в O
пакете O
Apache B-TERM
Lucene I-TERM
. O

# text =  Вторая концепция, альтернатива стемминга — лемматизация.
Вторая O
концепция O
, O
альтернатива O
стемминга B-TERM
— O
лемматизация B-TERM
. O

# text =  Она пытается привести слово не к основе или корню, а к базовой, словарной форме — т. е. лемме.
Она O
пытается O
привести O
слово B-TERM
не O
к O
основе B-TERM
или O
корню B-TERM
, O
а O
к O
базовой O
, O
словарной B-TERM
форме I-TERM
— O
т O
. O
  O
е O
. O
лемме B-TERM
. O

# text =  Существует множество реализаций, и тема очень хорошо проработана именно для user generated текстов, пользовательски зашумлённых текстов.
Существует O
множество O
реализаций O
, O
и O
тема B-TERM
очень O
хорошо O
проработана O
именно O
для O
user B-TERM
generated I-TERM
текстов I-TERM
, O
пользовательски O
зашумлённых O
текстов B-TERM
. O

# text =  Теперь отобразим это в векторном пространстве, потому что почти все математические модели работают в векторных пространствах больших размерностей.
Теперь O
отобразим O
это O
в O
векторном B-TERM
пространстве I-TERM
, O
потому O
что O
почти O
все O
математические B-TERM
модели I-TERM
работают O
в O
векторных B-TERM
пространствах I-TERM
больших O
размерностей O
. O

# text =  Базовый подход, который используют многие модели, — метод "мешка слов".
Базовый O
подход O
, O
который O
используют O
многие O
модели O
, O
— O
метод B-TERM
" I-TERM
мешка I-TERM
слов I-TERM
" I-TERM
. O

# text =  Доминирует так называемый TF-IDF.
Доминирует O
так O
называемый O
TF B-TERM
- I-TERM
IDF I-TERM
. O

# text =  Частоту слова (term frequency, TF) определяют по-разному.
Частоту B-TERM
слова I-TERM
( O
term B-TERM
frequency I-TERM
, O
TF B-TERM
) O
определяют O
по O
- O
разному O
. O

# text =  Определив TF в документе, мы перемножаем её с обратной частотой документа (inverse document frequency, IDF).
Определив O
TF B-TERM
в O
документе O
, O
мы O
перемножаем O
её O
с O
обратной B-TERM
частотой I-TERM
документа I-TERM
( O
inverse B-TERM
document I-TERM
frequency I-TERM
, O
IDF B-TERM
) O
. O

# text =  IDF обычно вычисляют как логарифм от числа документов в корпусе, разделённый на количество документов, где это слово представлено.
IDF B-TERM
обычно O
вычисляют O
как O
логарифм B-TERM
от O
числа O
документов O
в O
корпусе B-TERM
, O
разделённый O
на O
количество O
документов O
, O
где O
это O
слово B-TERM
представлено O
. O

# text =  Например, при анализе эмоциональной окраски очень важно, к чему относилось, условно говоря, слово «хороший» или «нет».
Например O
, O
при O
анализе B-TERM
эмоциональной I-TERM
окраски I-TERM
очень O
важно O
, O
к O
чему O
относилось O
, O
условно O
говоря O
, O
слово B-TERM
« O
хороший O
» O
или O
« O
нет O
» O
. O

# text =  Тогда наряду с мешком слов поможет мешок N-грамм: мы добавляем в словарь не только слова, но и словосочетания.
Тогда O
наряду O
с O
мешком B-TERM
слов Ш-TERM
поможет O
мешок B-TERM
N I-TERM
- I-TERM
грамм I-TERM
: O
мы O
добавляем O
в O
словарь O
не O
только O
слова O
, O
но O
и O
словосочетания B-TERM
. O

# text =  Мы не будем вносить все словосочетания, потому что это приведёт к комбинаторному взрыву, но часто используемые статистически значимые пары или пары, соответствующие именованным сущностям, можно добавить, и это повысит качество работы итоговой модели.
Мы O
не O
будем O
вносить O
все O
словосочетания B-TERM
, O
потому O
что O
это O
приведёт O
к O
комбинаторному O
взрыву O
, O
но O
часто O
используемые O
статистически O
значимые O
пары O
или O
пары O
, O
соответствующие O
именованным B-TERM
сущностям I-TERM
, O
можно O
добавить O
, O
и O
это O
повысит O
качество O
работы O
итоговой O
модели B-TERM
. O

# text =  Отчасти эти ситуации позволяют обработать методы построения "векторных представлений слов", например, знаменитый word2vec или более модные skip-gramm.
Отчасти O
эти O
ситуации O
позволяют O
обработать O
методы O
построения O
" O
векторных B-TERM
представлений B-TERM
слов B-TERM
" O
, O
например O
, O
знаменитый O
word2vec B-TERM
или O
более O
модные O
skip B-TERM
- I-TERM
gramm I-TERM
. O

# text =  Стандартные хеш-функции равномерно размазывают данные по пространству хешей.
Стандартные O
хеш B-TERM
- I-TERM
функции I-TERM
равномерно O
размазывают O
данные O
по O
пространству O
хешей O
. O

# text =  Локально-чувствительный хеш похожие объекты поместит в пространстве объектов близко.
Локально B-TERM
- I-TERM
чувствительный I-TERM
хеш I-TERM
похожие O
объекты O
поместит O
в O
пространстве O
объектов O
близко O
. O

# text =  Мы выбираем случайный базис из случайных векторов.
Мы O
выбираем O
случайный O
базис B-TERM
из O
случайных B-TERM
векторов I-TERM
. O

# text =  Задача семантического анализа достаточно старая.
Задача O
семантического B-TERM
анализа I-TERM
достаточно O
старая O
. O

# text =  Современный подход — анализ семантики без учителя, поэтому его называют анализом скрытой (латентной) семантики.
Современный O
подход O
— O
анализ B-TERM
семантики I-TERM
без I-TERM
учителя I-TERM
, O
поэтому O
его O
называют O
анализом B-TERM
скрытой I-TERM
( I-TERM
латентной I-TERM
) I-TERM
семантики I-TERM
. O

# text =  Исторически первый подход к латентно-семантическому анализу — это латентно-семантическое индексирование.
Исторически O
первый O
подход O
к O
латентно B-TERM
- I-TERM
семантическому I-TERM
анализу I-TERM
— O
это O
латентно B-TERM
- I-TERM
семантическое I-TERM
индексирование I-TERM
. O

# text =  Мы уже использовали для решения задач коллаборативных рекомендаций хорошо зарекомендовавшие себя техники факторизации матриц.
Мы O
уже O
использовали O
для O
решения O
задач B-TERM
коллаборативных I-TERM
рекомендаций I-TERM
хорошо O
зарекомендовавшие O
себя O
техники O
факторизации O
матриц O
. O

# text =  В чём суть факторизации?
В O
чём O
суть O
факторизации B-TERM
? O

# text =  Одной из альтернатив стал так называемый вероятностный латентно-семантический индекс.
Одной O
из O
альтернатив O
стал O
так O
называемый O
вероятностный B-TERM
латентно I-TERM
- I-TERM
семантический I-TERM
индекс I-TERM
. O

# text =  Важно понять, что техника вероятностного латентно-семантического индекса — это техника факторизации матрицы.
Важно O
понять O
, O
что O
техника B-TERM
вероятностного I-TERM
латентно I-TERM
- I-TERM
семантического I-TERM
индекса I-TERM
— O
это O
техника I-TERM
факторизации I-TERM
матрицы I-TERM
. O

# text =  По сравнению с классической факторизацией на основе сингулярного разложения у вероятностной генерирующей модели есть важное преимущество.
По O
сравнению O
с O
классической B-TERM
факторизацией I-TERM
на O
основе O
сингулярного B-TERM
разложения I-TERM
у O
вероятностной O
генерирующей O
модели O
есть O
важное O
преимущество O
. O

# text =  Для этого используется перплексия.
Для O
этого O
используется O
перплексия B-TERM
. O

# text =  Есть так называемый EM-алгоритм.
Есть O
так O
называемый O
EM B-TERM
- I-TERM
алгоритм I-TERM
. O

# text = Как поясняет сам Томас Димсон, This Word Does Not Exist является вариацией нейросети GPT-2.
Как O
поясняет O
сам O
Томас B-TERM
Димсон I-TERM
, O
This B-TERM
Word I-TERM
Does I-TERM
Not I-TERM
Exist I-TERM
является O
вариацией O
нейросети O
GPT-2 B-TERM
. O

# text =  Существует также твиттер-бот проекта.
Существует O
также O
твиттер B-TERM
- I-TERM
бот I-TERM
проекта O
. O

# text =  Чтобы натренировать свою нейросеть на основе загруженных файлов, Димсон рекомендует воспользоваться контентом Apple Dictionary или Urban Dictionary.
Чтобы O
натренировать O
свою O
нейросеть O
на O
основе O
загруженных O
файлов O
, O
Димсон B-TERM
рекомендует O
воспользоваться O
контентом O
Apple B-TERM
Dictionary I-TERM
или O
Urban B-TERM
Dictionary I-TERM
. O

# text = Правда, пользователи YCombinator уже заметили, что This Word Does Not Exist иногда предлагает уже существующие слова — например, refactoring.
Правда O
, O
пользователи O
YCombinator B-TERM
уже O
заметили O
, O
что O
This B-TERM
Word I-TERM
Does I-TERM
Not I-TERM
Exist I-TERM
иногда O
предлагает O
уже O
существующие O
слова O
— O
например O
, O
refactoring O
. O

# text =  Он опубликовал программу (репозиторий на гитхабе), которая делает именно это: генерирует политические речи, удивительно похожие на настоящие.
Он O
опубликовал O
программу O
( O
репозиторий O
на O
гитхабе O
) O
, O
которая O
делает O
именно O
это O
: O
генерирует B-TERM
политические I-TERM
речи I-TERM
, O
удивительно O
похожие O
на O
настоящие O
. O
# text = Ученые Новосибирского государственного технического университета НЭТИ завершают разработку системы распознавания русского жестового языка.
Ученые O
Новосибирского B-TERM
государственного I-TERM
технического I-TERM
университета I-TERM
НЭТИ B-TERM
завершают O
разработку O
системы O
распознавания O
русского B-TERM
жестового B-TERM
языка I-TERM
. O

# text =  Точность распознавания составляет 92%.
Точность B-TERM
распознавания O
составляет O
92 B-TERM
% I-TERM
. O

# text =  «Мы также вели работу над выделением эпентезы (межжестовое движение).
« O
Мы O
также O
вели O
работу O
над O
выделением B-TERM
эпентезы I-TERM
( O
межжестовое B-TERM
движение I-TERM
) O
. O

# text =  Сейчас точность выделения жестов в видеопотоке составляет 85—90%.
Сейчас O
точность B-TERM
выделения B-TERM
жестов I-TERM
в O
видеопотоке O
составляет O
85—90 B-TERM
% I-TERM
. O

# text =  После выхода учебника я читал курс на его основе в УрФУ, ШАДе, ИТМО и СПбГУ и убедился, что наличие перевода очень помогает.
После O
выхода O
учебника O
я O
читал O
курс O
на O
его O
основе O
в O
УрФУ B-TERM
, O
ШАДе B-TERM
, O
ИТМО B-TERM
и O
СПбГУ B-TERM
и O
убедился O
, O
что O
наличие O
перевода O
очень O
помогает O
. O

# text =  В случае NLP потребность в «локализованных» учебных материалах еще заметнее, чем в информационном поиске.
В O
случае O
NLP B-TERM
потребность O
в O
« O
локализованных O
» O
учебных O
материалах O
еще O
заметнее O
, O
чем O
в O
информационном B-TERM
поиске I-TERM
. O

# text =  Слушателям предлагается самостоятельно реализовать методы морфологического анализа, определения тональности текста, автоматического реферирования документов, извлечения именованных сущностей и машинного перевода.
Слушателям O
предлагается O
самостоятельно O
реализовать O
методы O
морфологического B-TERM
анализа I-TERM
, O
определения O
тональности B-TERM
текста I-TERM
, O
автоматического B-TERM
реферирования I-TERM
документов I-TERM
, O
извлечения B-TERM
именованных I-TERM
сущностей I-TERM
и O
машинного B-TERM
перевода I-TERM
. O

# text =  Желательно, чтобы слушатели обладали базовыми знаниями линейной алгебры, теории вероятностей, математической статистики и машинного обучения, а также навыками программирования (необходимы для решения практических заданий).
Желательно O
, O
чтобы O
слушатели O
обладали O
базовыми O
знаниями O
линейной B-TERM
алгебры I-TERM
, O
теории B-TERM
вероятностей I-TERM
, O
математической B-TERM
статистики I-TERM
и O
машинного B-TERM
обучения I-TERM
, O
а O
также O
навыками O
программирования B-TERM
( O
необходимы O
для O
решения O
практических O
заданий O
) O
. O

# text =  Прорывы #DeepPavlov в 2019 году: обзор и итоги года Московский физико-технический институт (МФТИ).
Прорывы O
# O
DeepPavlov B-TERM
в O
2019 O
году O
: O
обзор O
и O
итоги O
года O
Московский B-TERM
физико I-TERM
- I-TERM
технический I-TERM
институт I-TERM
( O
МФТИ B-TERM
) O
. O

# text =  Библиотеке #DeepPavlov, на минуточку, уже два года, и мы рады, что наше сообщество с каждым днем растет.
Библиотеке O
# O
DeepPavlov B-TERM
, O
на O
минуточку O
, O
уже O
два O
года O
, O
и O
мы O
рады O
, O
что O
наше O
сообщество O
с O
каждым O
днем O
растет O
. O

# text =  Увеличилось количество коммерческих решений за счет state-of-art технологий, реализованных в DeepPavlov, в разных отраслях от ритейла до промышленности.
Увеличилось O
количество O
коммерческих O
решений O
за O
счет O
state B-TERM
- I-TERM
of I-TERM
- I-TERM
art I-TERM
технологий O
, O
реализованных O
в O
DeepPavlov B-TERM
, O
в O
разных O
отраслях O
от O
ритейла O
до O
промышленности O
. O

# text =  Вышел первый релиз DeepPavlov Agent.
Вышел O
первый O
релиз O
DeepPavlov B-TERM
Agent I-TERM
. O

# text =  DeepPavlov решает проблемы такие как: классификация текста, исправление опечаток, распознавание именованных сущностей, ответы на вопросы по базе знаний и многие другие.
DeepPavlov B-TERM
решает O
проблемы O
такие O
как O
: O
классификация B-TERM
текста I-TERM
, O
исправление B-TERM
опечаток I-TERM
, O
распознавание B-TERM
именованных I-TERM
сущностей I-TERM
, O
ответы B-TERM
на I-TERM
вопросы I-TERM
по O
базе O
знаний O
и O
многие O
другие O
. O

# text =  Библиотека поддерживает платформы Linux и Windows.
Библиотека O
поддерживает O
платформы O
Linux B-TERM
и O
Windows B-TERM
. O

# text =  В настоящее время современные результаты во многих задачах были достигнуты благодаря применению моделей на основе BERT.
В O
настоящее O
время O
современные O
результаты O
во O
многих O
задачах O
были O
достигнуты O
благодаря O
применению O
моделей O
на O
основе O
BERT B-TERM
. O

# text =  Команда DeepPavlov интегрировала BERT в три последующие задачи: классификация текста, распознавание именованных сущностей и ответы на вопросы.
Команда O
DeepPavlov B-TERM
интегрировала O
BERT B-TERM
в O
три O
последующие O
задачи O
: O
классификация B-TERM
текста I-TERM
, O
распознавание B-TERM
именованных I-TERM
сущностей I-TERM
и O
ответы B-TERM
на I-TERM
вопросы I-TERM
. O

# text =  Модель классификации текста на основе BERT DeepPavlov служит, например, для решения проблемы обнаружения оскорблений.
Модель O
классификации O
текста O
на O
основе O
BERT B-TERM
DeepPavlov B-TERM
служит O
, O
например O
, O
для O
решения O
проблемы O
обнаружения B-TERM
оскорблений I-TERM
. O

# text =  В дополнение к моделям классификации текста DeepPavlov содержит модель на основе BERT для распознавания именованных сущностей (NER).
В O
дополнение O
к O
моделям O
классификации B-TERM
текста I-TERM
DeepPavlov B-TERM
содержит O
модель O
на O
основе O
BERT B-TERM
для O
распознавания B-TERM
именованных I-TERM
сущностей I-TERM
( O
NER B-TERM
) O
. O

# text =  Например, модель может извлечь важную информацию из резюме, чтобы облегчить работу специалистов по кадрам.
Например O
, O
модель O
может O
извлечь B-TERM
важную I-TERM
информацию I-TERM
из O
резюме O
, O
чтобы O
облегчить O
работу O
специалистов O
по O
кадрам O
. O

# text =  Кроме того, NER может использоваться для идентификации соответствующих объектов в запросах клиентов, таких как спецификации продуктов, названия компаний или данные о филиалах компании.
Кроме O
того O
, O
NER B-TERM
может O
использоваться O
для O
идентификации B-TERM
соответствующих I-TERM
объектов I-TERM
в O
запросах O
клиентов O
, O
таких O
как O
спецификации O
продуктов O
, O
названия O
компаний O
или O
данные O
о O
филиалах O
компании O
. O

# text =  Команда DeepPavlov обучила модель NER на англоязычном корпусе OntoNotes, который имеет 19 типов разметки, включая PER (человек), LOC (местоположение), ORG (организация) и многие другие.
Команда O
DeepPavlov B-TERM
обучила O
модель B-TERM
NER I-TERM
на O
англоязычном O
корпусе O
OntoNotes B-TERM
, O
который O
имеет O
19 O
типов O
разметки B-TERM
, O
включая O
PER B-TERM
( O
человек O
) O
, O
LOC B-TERM
( O
местоположение O
) O
, O
ORG B-TERM
( O
организация O
) O
и O
многие O
другие O
. O

# text =  Одним из основных переломных моментов в этой области стал выпуск Стэнфордского набора данных для ответов на вопросы (SQuAD).
Одним O
из O
основных O
переломных O
моментов O
в O
этой O
области O
стал O
выпуск O
Стэнфордского B-TERM
набора I-TERM
данных I-TERM
для I-TERM
ответов I-TERM
на I-TERM
вопросы I-TERM
( O
SQuAD B-TERM
) O
. O

# text =  Набор данных SQuAD привел к появлению бесчисленных подходов к решению задачи вопросно-ответных систем.
Набор O
данных O
SQuAD B-TERM
привел O
к O
появлению O
бесчисленных O
подходов O
к O
решению O
задачи O
вопросно B-TERM
- I-TERM
ответных I-TERM
систем I-TERM
. O

# text =  Одной из наиболее успешных является модель DeepPavlov BERT.
Одной O
из O
наиболее O
успешных O
является O
модель O
DeepPavlov B-TERM
BERT I-TERM
. O

# text =  Чтобы использовать модель QA на основе BERT с DeepPavlov, необходимо следующее.
Чтобы O
использовать O
модель O
QA B-TERM
на O
основе O
BERT B-TERM
с O
DeepPavlov B-TERM
, O
необходимо O
следующее O
. O

# text =  DeepPavlov Agent — платформа для создания многозадачных чат-ботов.
DeepPavlov B-TERM
Agent I-TERM
— O
платформа O
для O
создания O
многозадачных O
чат O
- O
ботов O
. O

# text =  При разработке разговорных агентов в основном применяется модульная архитектура для целенаправленного диалога, при котором разворачивается сценарий.
При O
разработке O
разговорных B-TERM
агентов I-TERM
в O
основном O
применяется O
модульная B-TERM
архитектура I-TERM
для O
целенаправленного B-TERM
диалога I-TERM
, O
при O
котором O
разворачивается O
сценарий O
. O

# text =  Для решения этой задачи в октябре 2019 года вышел первый релиз DeepPavlov Agent 1.0 — платформы для создания многозадачных чат-ботов.
Для O
решения O
этой O
задачи O
в O
октябре O
2019 B-TERM
года I-TERM
вышел O
первый O
релиз O
DeepPavlov B-TERM
Agent I-TERM
1.0 I-TERM
— O
платформы O
для O
создания O
многозадачных O
чат O
- O
ботов O
. O

# text =  Агент помогает разработчикам производственных чатботов организовать несколько NLP моделей в одном конвейере.
Агент O
помогает O
разработчикам O
производственных O
чатботов O
организовать B-TERM
несколько I-TERM
NLP I-TERM
моделей I-TERM
в O
одном O
конвейере O
. O

# text =  Чтобы упростить работу с предобученными NLP моделями из DeepPavlov, в сентябрь 2019 года был запущен SaaS сервис.
Чтобы O
упростить O
работу O
с O
предобученными O
NLP O
моделями O
из O
DeepPavlov B-TERM
, O
в O
сентябрь O
2019 B-TERM
года I-TERM
был O
запущен O
SaaS B-TERM
сервис I-TERM
. O

# text =  DeepPavlov Cloud позволяет анализировать текст, а также хранить документы в облачном хранилище.
DeepPavlov B-TERM
Cloud I-TERM
позволяет O
анализировать B-TERM
текст I-TERM
, O
а O
также O
хранить B-TERM
документы I-TERM
в O
облачном O
хранилище O
. O

# text =  Оценка состояния диалога (DST — Dialogue State Traking) является основным компонентом в таких диалоговых системах.
Оценка B-TERM
состояния I-TERM
диалога I-TERM
( O
DST B-TERM
— O
Dialogue B-TERM
State I-TERM
Traking I-TERM
) O
является O
основным O
компонентом O
в O
таких O
диалоговых O
системах O
. O

# text =  DST отвечает за перевод высказываний на человеческом языке в семантическое представление языка, в частности, за извлечение намерений (intets) и пар слот-значение (slot, value), соответствующих цели пользователя.
DST B-TERM
отвечает O
за O
перевод B-TERM
высказываний I-TERM
на O
человеческом O
языке O
в O
семантическое O
представление O
языка O
, O
в O
частности O
, O
за O
извлечение O
намерений O
( O
intets O
) O
и O
пар O
слот O
- O
значение O
( O
slot O
, O
value O
) O
, O
соответствующих O
цели O
пользователя O
. O

# text =  В ходе участия команды в DSTC8 была разработана модель GOLOMB (GOaL-Oriented Multi-task BERT-based dialogue state tracker) — целеориентированная мультизадачная модель на базе BERT для отслеживания состояния диалога.
В O
ходе O
участия O
команды O
в O
DSTC8 O
была O
разработана O
модель O
GOLOMB B-TERM
( O
GOaL B-TERM
- I-TERM
Oriented I-TERM
Multi I-TERM
- I-TERM
task I-TERM
BERT I-TERM
- I-TERM
based I-TERM
dialogue I-TERM
state I-TERM
tracker I-TERM
) O
— O
целеориентированная O
мультизадачная O
модель O
на O
базе O
BERT B-TERM
для O
отслеживания B-TERM
состояния I-TERM
диалога I-TERM
. O

# text =  Для предсказания состояния диалога модель решает несколько классификационных задач и задачу поиска подстроки.
Для O
предсказания O
состояния O
диалога O
модель O
решает O
несколько O
классификационных B-TERM
задач I-TERM
и O
задачу B-TERM
поиска I-TERM
подстроки I-TERM
. O

# text =  В скором времени данная модель появится библиотеке DeepPavlov.
В O
скором O
времени O
данная O
модель O
появится O
библиотеке O
DeepPavlov B-TERM
. O

# text =  Как было сказано ранее, DeepPavlov поставляется с несколькими предобученными компонентами, работающими на TensorFlow и Keras.
Как O
было O
сказано O
ранее O
, O
DeepPavlov B-TERM
поставляется O
с O
несколькими O
предобученными O
компонентами O
, O
работающими O
на O
TensorFlow B-TERM
и O
Keras B-TERM
. O

# text =  На основании триггера на определенные ключевые слова она сможет определять, к примеру, признаки обмана, мошенничества.
На O
основании O
триггера O
на O
определенные O
ключевые B-TERM
слова I-TERM
она O
сможет O
определять O
, O
к O
примеру O
, O
признаки O
обмана O
, O
мошенничества O
. O

# text =  То есть, сформировав некоторый корпус слов-триггеров, вполне возможно классифицировать сайты по их текстовому содержанию.
То O
есть O
, O
сформировав O
некоторый O
корпус B-TERM
слов I-TERM
- I-TERM
триггеров I-TERM
, O
вполне O
возможно O
классифицировать O
сайты O
по O
их O
текстовому O
содержанию O
. O

# text =  Задача распознавания текста относится к сфере обработки естественного языка или NLP (natural language processing).
Задача B-TERM
распознавания I-TERM
текста I-TERM
относится O
к O
сфере O
обработки B-TERM
естественного I-TERM
языка I-TERM
или O
NLP B-TERM
( O
natural B-TERM
language I-TERM
processing I-TERM
) O
. O

# text =  NLP — направление искусственного интеллекта, нацеленное на обработку и анализ данных на естественном языке и обучение машин взаимодействию с людьми [1].
NLP B-TERM
— O
направление O
искусственного I-TERM
интеллекта I-TERM
, O
нацеленное O
на O
обработку O
и O
анализ I-TERM
данных I-TERM
на O
естественном O
языке O
и O
обучение O
машин O
взаимодействию O
с O
людьми O
[ O
1 O
] O
. O

# text =  Такой подход называется методом вложения слов (word embedding).
Такой O
подход O
называется O
методом B-TERM
вложения I-TERM
слов I-TERM
( O
word B-TERM
embedding I-TERM
) O
. O

# text =  Используя данные, состоящие из таких векторов, мы можем применять различные методы Machine Learning.
Используя O
данные O
, O
состоящие O
из O
таких O
векторов O
, O
мы O
можем O
применять O
различные O
методы O
Machine B-TERM
Learning I-TERM
. O

# text =  И поскольку искусственные нейронные сети лучшим образом справляются с векторно-матричными вычислениями, то выбор в их пользу становиться очевидным.
И O
поскольку O
искусственные B-TERM
нейронные I-TERM
сети I-TERM
лучшим O
образом O
справляются O
с O
векторно B-TERM
- I-TERM
матричными I-TERM
вычислениями I-TERM
, O
то O
выбор O
в O
их O
пользу O
становиться O
очевидным O
. O

# text =  Искусственная нейронная сеть — это математическая модель, а также ее программное или аппаратное воплощение, построенные по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.
Искусственная B-TERM
нейронная I-TERM
сеть I-TERM
— O
это O
математическая B-TERM
модель I-TERM
, O
а O
также O
ее O
программное O
или O
аппаратное O
воплощение O
, O
построенные O
по O
принципу O
организации O
и O
функционирования O
биологических O
нейронных O
сетей O
— O
сетей O
нервных O
клеток O
живого O
организма O
. O

# text =  Современные модели представляют собой так называемые глубокие модели.
Современные O
модели O
представляют O
собой O
так O
называемые O
глубокие B-TERM
модели I-TERM
. O

# text =  И в ее решении наилучшие метрики точности были достигнуты рекуррентными нейронными сетями, LSTM (сети с долгой краткосрочной памятью).
И O
в O
ее O
решении O
наилучшие O
метрики O
точности B-TERM
были O
достигнуты O
рекуррентными B-TERM
нейронными I-TERM
сетями I-TERM
, O
LSTM B-TERM
( O
сети B-TERM
с I-TERM
долгой I-TERM
краткосрочной I-TERM
памятью I-TERM
) O
. O

# text =  Позже свое превосходство в этой нише обрели NLP-модели-трансформеры.
Позже O
свое O
превосходство O
в O
этой O
нише O
обрели O
NLP B-TERM
- I-TERM
модели I-TERM
- I-TERM
трансформеры I-TERM
. O

# text = Описание упомянутых рекуррентных нейросетей (RNN), LSTM и GRU выходит за рамки темы статьи.
Описание O
упомянутых O
рекуррентных B-TERM
нейросетей I-TERM
( O
RNN B-TERM
) O
, O
LSTM B-TERM
и O
GRU B-TERM
выходит O
за O
рамки O
темы O
статьи O
. O

# text =  Однако RNN способны фиксировать зависимости только в одном направлении языка.
Однако O
RNN B-TERM
способны O
фиксировать B-TERM
зависимости I-TERM
только O
в O
одном O
направлении O
языка O
. O

# text =  Кроме этого, RNN не очень хороши в захвате долгосрочных зависимостей.
Кроме O
этого O
, O
RNN B-TERM
не O
очень O
хороши O
в O
захвате B-TERM
долгосрочных I-TERM
зависимостей I-TERM
. O

# text = LSTM избегают проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.
LSTM B-TERM
избегают O
проблемы O
долговременной O
зависимости O
, O
запоминая O
значения O
как O
на O
короткие O
, O
так O
и O
на O
длинные O
промежутки O
времени O
. O

# text =  Это объясняется тем, что LSTM не использует функцию активации внутри своих рекуррентных компонентов.
Это O
объясняется O
тем O
, O
что O
LSTM B-TERM
не O
использует O
функцию B-TERM
активации I-TERM
внутри O
своих O
рекуррентных O
компонентов O
. O

# text =  LSTM часто используются в машинном переводе и в задачах генерирования текстов на естественном языке.
LSTM B-TERM
часто O
используются O
в O
машинном B-TERM
переводе I-TERM
и O
в O
задачах O
генерирования B-TERM
текстов I-TERM
на O
естественном O
языке O
. O

# text =  Прежде чем использовать такой мощный и в то же время сложный инструмент, наша команда протестировала и более простые NLP-методы машинного обучения, в том числе «наивный байесовский классификатор», алгоритмы, использующие bag-of-words («мешок слов» — метод представления слов) и tf-idf (метрика определения частоты вхождения слов), а также простейшие модели нейронных сетей, состоящие из небольшого количества скрытых слоев.
Прежде O
чем O
использовать O
такой O
мощный O
и O
в O
то O
же O
время O
сложный O
инструмент O
, O
наша O
команда O
протестировала O
и O
более O
простые O
NLP O
- O
методы O
машинного O
обучения O
, O
в O
том O
числе O
« O
наивный B-TERM
байесовский I-TERM
классификатор I-TERM
» O
, O
алгоритмы O
, O
использующие O
bag B-TERM
- I-TERM
of I-TERM
- I-TERM
words I-TERM
( O
« O
мешок B-TERM
слов I-TERM
» O
— O
метод B-TERM
представления I-TERM
слов O
) O
и O
tf B-TERM
- I-TERM
idf I-TERM
( O
метрика B-TERM
определения I-TERM
частоты I-TERM
вхождения I-TERM
слов O
) O
, O
а O
также O
простейшие O
модели O
нейронных O
сетей O
, O
состоящие O
из O
небольшого O
количества O
скрытых O
слоев O
. O

# text =  BERT, или Bidirectional Encoder Representations from Transformers, — нейросетевая модель-трансформер от Google, на которой сегодня строится большинство инструментов автоматической обработки языка.
BERT B-TERM
, O
или O
Bidirectional B-TERM
Encoder I-TERM
Representations I-TERM
from I-TERM
Transformers I-TERM
, O
— O
нейросетевая O
модель O
- O
трансформер O
от O
Google B-TERM
, O
на O
которой O
сегодня O
строится O
большинство O
инструментов O
автоматической O
обработки O
языка O
. O

# text = Релиз BERT в 2018 году стал некоторой переломной точкой в развитии NLP-моделей.
Релиз O
BERT B-TERM
в O
2018 B-TERM
году I-TERM
стал O
некоторой O
переломной O
точкой O
в O
развитии O
NLP B-TERM
- I-TERM
моделей I-TERM
. O

# text =  Его появлению предшествовал ряд недавних разработок в области обработки естественного языка (BERT, ELMO и Ко в картинках — как в NLP пришло трансферное обучение): Semi-supervised Sequence learning (Andrew Dai и Quoc Le), ELMo (Matthew Peters и исследователи из AI2 и UW CSE), ULMFiT (Jeremy Howard и Sebastian Ruder), OpenAI Transformer (исследователи OpenAI Radford, Narasimhan, Salimans, и Sutskever) и Трансформер (Vaswani et al).
Его O
появлению O
предшествовал O
ряд O
недавних O
разработок O
в O
области O
обработки O
естественного O
языка O
( O
BERT B-TERM
, O
ELMO B-TERM
и O
Ко B-TERM
в O
картинках O
— O
как O
в O
NLP B-TERM
пришло O
трансферное B-TERM
обучение I-TERM
): O
Semi B-TERM
- I-TERM
supervised I-TERM
Sequence I-TERM
learning I-TERM
( O
Andrew B-TERM
Dai I-TERM
и O
Quoc B-TERM
Le I-TERM
) O
, O
ELMo B-TERM
( O
Matthew B-TERM
Peters I-TERM
и O
исследователи O
из O
AI2 B-TERM
и O
UW B-TERM
CSE B-TERM
) O
, O
ULMFiT B-TERM
( O
Jeremy B-TERM
Howard I-TERM
и O
Sebastian B-TERM
Ruder I-TERM
) O
, O
OpenAI B-TERM
Transformer I-TERM
( O
исследователи O
OpenAI B-TERM
Radford I-TERM
, O
Narasimhan B-TERM
, O
Salimans B-TERM
, O
и O
Sutskever B-TERM
) O
и O
Трансформер B-TERM
( O
Vaswani B-TERM
et O
al O
)O 
. O

# text = Трансформеры в машинном обучении — это семейство архитектур нейронных сетей, общая идея которых основана на так называемом «самовнимании» (self-attention).
Трансформеры B-TERM
в O
машинном B-TERM
обучении I-TERM
— O
это O
семейство O
архитектур O
нейронных O
сетей O
, O
общая O
идея O
которых O
основана O
на O
так O
называемом O
« O
самовнимании B-TERM
» O
( O
self B-TERM
- I-TERM
attention I-TERM
) O
. O

# text =  Однако алгоритм Self-attention не сразу поймет смысл предложения.
Однако O
алгоритм O
Self B-TERM
- I-TERM
attention I-TERM
не O
сразу O
поймет O
смысл O
предложения O
. O

# text =  Потом результаты сетей объединяется.По своей сути BERT — это обученный стек энкодеров Трансформера.
Потом O
результаты O
сетей O
объединяется O
. O
По O
своей O
сути O
BERT B-TERM
— O
это O
обученный O
стек O
энкодеров B-TERM
Трансформера I-TERM
. O

# text =  Разработкой и обучением модели BERT занималась целая группа исследователей Google AI Language на многомиллионном наборе слов на разных языках (более 100).
Разработкой O
и O
обучением O
модели O
BERT B-TERM
занималась O
целая O
группа O
исследователей O
Google B-TERM
AI I-TERM
Language I-TERM
на O
многомиллионном O
наборе O
слов O
на O
разных O
языках O
( O
более O
100 O
) O
. O

# text =  И мы дообучили BERT распознавать текстовое содержимое сайтов по 63 категориям (медицина, здоровье, видео, интернет-магазины, юмористические сайты, эротика, оружие, секты, криминал и пр.).
И O
мы O
дообучили O
BERT B-TERM
распознавать O
текстовое O
содержимое O
сайтов O
по O
63 O
категориям O
( O
медицина B-TERM
, O
здоровье O
, O
видео O
, O
интернет O
- O
магазины O
, O
юмористические O
сайты O
, O
эротика O
, O
оружие O
, O
секты O
, O
криминал O
и пр. O
) O
. O

# text =  Smart-Cat на первом этапе проводит их предобработку.
Smart B-TERM
- I-TERM
Cat I-TERM
на O
первом O
этапе O
проводит O
их O
предобработку O
. O

# text =  Для удобства работы с категоризатором Smart-Cat мы создали специальный Telegram-бот.
Для O
удобства O
работы O
с O
категоризатором O
Smart B-TERM
- I-TERM
Cat I-TERM
мы O
создали O
специальный O
Telegram B-TERM
- I-TERM
бот I-TERM
. O

# text =  После своей работы BERT-bot отправит CSV-таблицу.
После O
своей O
работы O
BERT B-TERM
- I-TERM
bot I-TERM
отправит O
CSV O
- O
таблицу O
. O

# text =  Но, как я уже сказал ранее, такие модели могут применяться не только в задачах классификации текста.
Но O
, O
как O
я O
уже O
сказал O
ранее O
, O
такие O
модели O
могут O
применяться O
не O
только O
в O
задачах O
классификации B-TERM
текста I-TERM
. O

# text =  С моделью от OpenAI связано сразу несколько новостей — хорошая и не очень.
С O
моделью O
от O
OpenAI B-TERM
связано O
сразу O
несколько O
новостей O
— O
хорошая O
и O
не O
очень O
. O

# text =  Сделка OpenAI и Microsoft. Начать придется с менее приятной — компания Майкрософт завладела эксклюзивными правами на GPT-3.
Сделка O
OpenAI B-TERM
и O
Microsoft B-TERM
. O
Начать O
придется O
с O
менее O
приятной O
— O
компания O
Майкрософт B-TERM
завладела O
эксклюзивными O
правами O
на O
GPT-3 B-TERM
. O

# text =  Сделка предсказуемо вызвала негодование — Элон Маск, основатель OpenAI, а ныне бывший член совета директоров компании, заявил, что Майкрософт по сути захватили OpenAI.
Сделка O
предсказуемо O
вызвала O
негодование O
— O
Элон B-TERM
Маск I-TERM
, O
основатель O
OpenAI B-TERM
, O
а O
ныне O
бывший O
член O
совета O
директоров O
компании O
, O
заявил O
, O
что O
Майкрософт B-TERM
по O
сути O
захватили O
OpenAI B-TERM
. O

# text =  Дело в том, что OpenAI изначально создавалась как некоммерческая организация с высокой миссией — не позволить искусственному интеллекту оказаться в руках отдельного государства или корпорации.
Дело O
в O
том O
, O
что O
OpenAI B-TERM
изначально O
создавалась O
как O
некоммерческая O
организация O
с O
высокой O
миссией O
— O
не O
позволить O
искусственному B-TERM
интеллекту I-TERM
оказаться O
в O
руках O
отдельного O
государства O
или O
корпорации O
. O

# text =  ruGPT3 от Сбера. Теперь к более приятной новости — исследователи из Сбера выложили в открытый доступ модель, которая повторяет архитектуру GPT-3 и основана на коде GPT-2 и, самое главное, обучена на русскоязычном корпусе.
ruGPT3 B-TERM
от O
Сбера B-TERM
. O
Теперь O
к O
более O
приятной O
новости O
— O
исследователи O
из O
Сбера B-TERM
выложили O
в O
открытый O
доступ O
модель O
, O
которая O
повторяет O
архитектуру O
GPT-3 B-TERM
и O
основана O
на O
коде O
GPT-2 B-TERM
и O
, O
самое O
главное O
, O
обучена O
на O
русскоязычном O
корпусе B-TERM
. O

# text =  Если коммерческие организации можно оправдать тем, что код часто вплетен в инфраструктуру проектов, то что говорить про исследовательские институты и некоммерческие компании вроде DeepMind и OpenAI?
Если O
коммерческие O
организации O
можно O
оправдать O
тем O
, O
что O
код O
часто O
вплетен O
в O
инфраструктуру O
проектов O
, O
то O
что O
говорить O
про O
исследовательские O
институты O
и O
некоммерческие O
компании O
вроде O
DeepMind B-TERM
и O
OpenAI B-TERM
? O

# text =  Платформа для видеозвонков Maxine объединяет в себе целый зоопарк ML-алгоритмов.
Платформа O
для O
видеозвонков O
Maxine B-TERM
объединяет O
в O
себе O
целый O
зоопарк O
ML O
- O
алгоритмов O
. O

# text =  Google Meet поделились кейсом создания своего алгоритма для качественного удаления фона на основе фреймворка от Mediapipe (который умеет отслеживание движение глаз, головы и рук).
Google B-TERM
Meet I-TERM
поделились O
кейсом O
создания O
своего O
алгоритма O
для O
качественного O
удаления O
фона O
на O
основе O
фреймворка O
от O
Mediapipe B-TERM
( O
который O
умеет O
отслеживание O
движение O
глаз O
, O
головы O
и O
рук O
) O
. O

# text =  Google также запустил новую функцию для сервиса YouTube Stories на iOS, который позволяет улучшать качество речи.
Google B-TERM
также O
запустил O
новую O
функцию O
для O
сервиса O
YouTube B-TERM
Stories I-TERM
на O
iOS B-TERM
, O
который O
позволяет O
улучшать O
качество O
речи O
. O

# text =  В современном мире всё большую популярность приобретает методика под названием customer development для тестирования идей и гипотез о будущем продукте.
В O
современном O
мире O
всё O
большую O
популярность O
приобретает O
методика O
под O
названием O
customer B-TERM
development I-TERM
для O
тестирования O
идей O
и O
гипотез O
о O
будущем O
продукте O
. O

# text =  В данном решении была использована готовая нейросеть от сервиса RusVectores, обученная на корпусе НКРЯ с использованием алгоритма word2vec CBOW с длиной вектора 300.
В O
данном O
решении O
была O
использована O
готовая O
нейросеть O
от O
сервиса O
RusVectores B-TERM
, O
обученная O
на O
корпусе O
НКРЯ B-TERM
с O
использованием O
алгоритма O
word2vec B-TERM
CBOW B-TERM
с O
длиной O
вектора O
300.

# text = НКРЯ – это совокупность русскоязычных текстов, Национальный Корпус Русского Языка в полном объёме.
НКРЯ B-TERM
– O
это O
совокупность O
русскоязычных O
текстов O
, O
Национальный B-TERM
Корпус I-TERM
Русского I-TERM
Языка I-TERM
в O
полном O
объёме O
. O

# text = Word2vec CBOW — алгоритм, благодаря которому слово на естественном языке представляется в виде числового вектора.
Word2vec B-TERM
CBOW B-TERM
— O
алгоритм O
, O
благодаря O
которому O
слово O
на O
естественном O
языке O
представляется O
в O
виде O
числового O
вектора O
. O

# text =  CBOW – это аббревиатура Continuous Bag of Words.
CBOW B-TERM
– O
это O
аббревиатура O
Continuous B-TERM
Bag I-TERM
of I-TERM
Words I-TERM
. O

# text =  Она обозначает алгоритм, который есть в word2vec.
Она O
обозначает O
алгоритм O
, O
который O
есть O
в O
word2vec B-TERM
. O

# text =  Данный алгоритм называют моделью «мешка слов», он предсказывает слово по контексту.
Данный O
алгоритм O
называют O
моделью O
« O
мешка B-TERM
слов I-TERM
» O
, O
он O
предсказывает B-TERM
слово I-TERM
по I-TERM
контексту I-TERM
. O

# text =  Ещё один алгоритм в word2vec - Skip-gram предсказывает контекст по слову.
Ещё O
один O
алгоритм O
в O
word2vec B-TERM
- O
Skip B-TERM
- I-TERM
gram I-TERM
предсказывает O
контекст O
по O
слову O
. O

# text = С помощью данных алгоритмов генерируют близкие по смыслу слова при запросе в поисковой системе, сравнивают документы по смыслу, определяют смысловую близость слов и предложений.
С O
помощью O
данных O
алгоритмов O
генерируют O
близкие O
по O
смыслу O
слова O
при O
запросе O
в O
поисковой O
системе O
, O
сравнивают B-TERM
документы I-TERM
по I-TERM
смыслу I-TERM
, O
определяют B-TERM
смысловую I-TERM
близость I-TERM
слов I-TERM
и O
предложений O
. O

# text = Более подробно о word2vec можно почитать в статье "Немного про word2vec: полезная теория".
Более O
подробно O
о O
word2vec B-TERM
можно O
почитать O
в O
статье O
" O
Немного O
про O
word2vec O
: O
полезная O
теория" O
. O

# text = О векторном представлении слов (эмбеддинге) хорошо и с примерами описано в статье "Что такое эмбеддинги и как они помогают машинам понимать тексты".
О O
векторном B-TERM
представлении I-TERM
слов I-TERM
( O
эмбеддинге B-TERM
) O
хорошо O
и O
с O
примерами O
описано O
в O
статье O
" O
Что O
такое O
эмбеддинги O
и O
как O
они O
помогают O
машинам O
понимать O
тексты" O
. O

# text =  Т.к. у меня таких мощностей нет, я воспользовался доступным онлайн сервисом RusVectores.
Т.к O
. O
у O
меня O
таких O
мощностей O
нет O
, O
я O
воспользовался O
доступным O
онлайн O
сервисом O
RusVectores B-TERM
. O

# text =  Эти модели всегда ищут синонимы — даже для устоявшихся словосочетаний.
Эти O
модели O
всегда O
ищут B-TERM
синонимы I-TERM
— O
даже O
для O
устоявшихся O
словосочетаний B-TERM
. O
# text =  Вчера OpenAI выпустили Whisper.
Вчера O
OpenAI B-TERM
выпустили O
Whisper B-TERM
. O

# text =  По сути авторы попытались: Исключить транскрипты других систем ASR из датасета; Привести пунктуации к некому стандарту.
По O
сути O
авторы O
попытались O
: O
Исключить O
транскрипты O
других O
систем O
ASR B-TERM
из O
датасета; O
Привести O
пунктуации O
к O
некому O
стандарту O
. O

# text =  Серьезной нормализации или денормализации текста не делалось.
Серьезной O
нормализации B-TERM
или O
денормализации B-TERM
текста O
не O
делалось O
. O

# text =  Под капотом же seq2seq модель, глядишь сама всё и так выучит.
Под O
капотом O
же O
seq2seq B-TERM
модель O
, O
глядишь O
сама O
всё O
и O
так O
выучит O
. O

# text =  Ведь исходя из названий FAIR, OpenAI и прочие же FOSS - альтруисты, борющиеся за наше будущее, они же выложили код для тренировки (а повторить смогут лишь GAFA компании) и все датасеты, не так ли?
Ведь O
исходя O
из O
названий O
FAIR B-TERM
, O
OpenAI B-TERM
и O
прочие O
же O
FOSS B-TERM
- O
альтруисты O
, O
борющиеся O
за O
наше O
будущее O
, O
они O
же O
выложили O
код O
для O
тренировки O
( O
а O
повторить O
смогут O
лишь O
GAFA B-TERM
компании O
) O
и O
все O
датасеты O
, O
не O
так O
ли O
? O

# text =  На практике OpenAI уже давно не Open, а недавняя история с DALLE-2 / Midjourney / Stable Diffusion скорее иллюстрируют тренд.
На O
практике O
OpenAI B-TERM
уже O
давно O
не O
Open O
, O
а O
недавняя O
история O
с O
DALLE-2 B-TERM
/ O
Midjourney B-TERM
/ O
Stable B-TERM
Diffusion I-TERM
скорее O
иллюстрируют O
тренд O
. O

# text =  Наверное стоит только сказать, что это sequence-to-sequence encoder-decoder трансформерная модель, без особого снижения длины инпута с довольном стандартным окном в 25 миллисекунд и шагом в 10 миллисекунд, работающая на аудио в 16 килогерц.
Наверное O
стоит O
только O
сказать O
, O
что O
это O
sequence B-TERM
- I-TERM
to I-TERM
- I-TERM
sequence I-TERM
encoder I-TERM
- I-TERM
decoder I-TERM
трансформерная O
модель O
, O
без O
особого O
снижения O
длины O
инпута O
с O
довольном O
стандартным O
окном O
в O
25 O
миллисекунд O
и O
шагом O
в O
10 O
миллисекунд O
, O
работающая O
на O
аудио O
в O
16 O
килогерц O
. O

# text =  Решать конечно вам для вашего конкретного приложения, но если сравнивать только саму модель распознавания, а не весь обвес в виде сервиса (понятно, что тут VAD и детектор языка запихали тоже в модель), например с древними бенчмарками из silero-models, то самые маленькие модели на CPU в расчете на 1 ядро (1 ядро = 2 потока) отличаются по скорости … на два порядка.
Решать O
конечно O
вам O
для O
вашего O
конкретного O
приложения O
, O
но O
если O
сравнивать O
только O
саму O
модель O
распознавания O
, O
а O
не O
весь O
обвес O
в O
виде O
сервиса O
( O
понятно O
, O
что O
тут O
VAD B-TERM
и O
детектор O
языка O
запихали O
тоже O
в O
модель O
) O
, O
например O
с O
древними O
бенчмарками O
из O
silero B-TERM
- I-TERM
models I-TERM
, O
то O
самые O
маленькие O
модели O
на O
CPU O
в O
расчете O
на O
1 O
ядро O
( O
1 O
ядро O
= O
2 O
потока O
) O
отличаются O
по O
скорости O
… O
на O
два O
порядка O
. O

# text =  Для наших моделей из прошлого релиза, многие из этих датасетов тоже как бы "zero-shot" (то есть у нас нет соответствующего большого тренировочного датасета).
Для O
наших O
моделей O
из O
прошлого O
релиза O
, O
многие O
из O
этих O
датасетов O
тоже O
как O
бы O
" O
zero B-TERM
- I-TERM
shot I-TERM
" O
( O
то O
есть O
у O
нас O
нет O
соответствующего O
большого O
тренировочного O
датасета O
) O
. O

# text =  В течение четырех лет вышло несколько версий модели, способных транскрибировать лекции, телефонные разговоры, телевизионные программы, радиошоу и другие прямые трансляции с «человеческой точностью».
В O
течение O
четырех O
лет O
вышло O
несколько O
версий O
модели O
, O
способных O
транскрибировать B-TERM
лекции I-TERM
, O
телефонные O
разговоры O
, O
телевизионные O
программы O
, O
радиошоу O
и O
другие O
прямые O
трансляции O
с O
« O
человеческой O
точностью O
» O
. O

# text =  Модель DeepSpeech представляет собой сквозную обучаемую архитектуру на уровне символов, которая может транскрибировать аудио на различных языках.
Модель O
DeepSpeech B-TERM
представляет O
собой O
сквозную O
обучаемую O
архитектуру O
на O
уровне O
символов O
, O
которая O
может O
транскрибировать B-TERM
аудио I-TERM
на O
различных O
языках O
. O

# text =  Вдохновленная этими усилиями по сбору данных, исследовательская группа Mozilla в сотрудничестве с группой открытых инноваций запустила проект Common Voice, цель которого заключалась в сборе и проверке речевых данных.
Вдохновленная O
этими O
усилиями O
по O
сбору O
данных O
, O
исследовательская O
группа O
Mozilla B-TERM
в O
сотрудничестве O
с O
группой O
открытых O
инноваций O
запустила O
проект O
Common B-TERM
Voice I-TERM
, O
цель O
которого O
заключалась O
в O
сборе O
и O
проверке B-TERM
речевых I-TERM
данных I-TERM
. O

# text =  Common Voice включает не только речевые записи, но и из добровольно предоставленные метаданные, такие как возраст, пол и акцент говорящего.
Common B-TERM
Voice I-TERM
включает O
не O
только O
речевые B-TERM
записи I-TERM
, O
но O
и O
из O
добровольно O
предоставленные O
метаданные B-TERM
, O
такие O
как O
возраст O
, O
пол O
и O
акцент O
говорящего O
. O

# text =  Сегодня Common Voice является одним из крупнейших в мире мультиязычных корпусов, являющихся общественным достоянием, с более чем 9 тысячами часов голосовых данных на 60 различных языках, включая такие редкие языки, как валлийский и киньяруанда.
Сегодня O
Common B-TERM
Voice I-TERM
является O
одним O
из O
крупнейших O
в O
мире O
мультиязычных B-TERM
корпусов I-TERM
, O
являющихся O
общественным O
достоянием O
, O
с O
более O
чем O
9 O
тысячами O
часов O
голосовых O
данных O
на O
60 O
различных O
языках O
, O
включая O
такие O
редкие O
языки O
, O
как O
валлийский B-TERM
и O
киньяруанда B-TERM
. O

# text = В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3.
В O
этой O
статье O
мы O
научим O
вас O
генерировать B-TERM
текст I-TERM
с O
помощью O
предварительно O
обученного O
GPT-2 B-TERM
— O
более O
легкого O
предшественника O
GPT-3 B-TERM
. O

# text =  Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface.
Мы O
будем O
использовать O
именитую O
библиотеку O
Transformers B-TERM
, O
разработанную O
Huggingface B-TERM
. O

# text =  Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.
Модель O
по O
умолчанию O
для O
конвейера O
генерации O
текста O
— O
GPT-2 B-TERM
, O
самая O
популярная O
модель O
декодирующего O
трансформера B-TERM
для O
генерации B-TERM
языка I-TERM
. O

# text =  Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.
Эта O
модель O
GPT2 B-TERM
от O
CKIPLab B-TERM
предварительно O
обучена O
на O
китайском B-TERM
корпусе I-TERM
, O
поэтому O
мы O
можем O
использовать O
их O
модель O
без O
необходимости O
заниматься O
настройкой O
самостоятельно O
. O

# text =  Перед переходом к самим метрикам необходимо ввести важную концепцию для описания этих метрик в терминах ошибок классификации — confusion matrix (матрица ошибок).
Перед O
переходом O
к O
самим O
метрикам O
необходимо O
ввести O
важную O
концепцию O
для O
описания O
этих O
метрик O
в O
терминах O
ошибок O
классификации O
— O
confusion B-Method
matrix I-Method
( O
матрица B-Method
ошибок I-Method
) O
. O

# text =  Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма.
Интуитивно O
понятной O
, O
очевидной O
и O
почти O
неиспользуемой O
метрикой O
является O
accuracy B-Metric
— O
доля O
правильных O
ответов O
алгоритма O
. O

# text =  Для оценки качества работы алгоритма на каждом из классов по отдельности введем метрики precision (точность) и recall (полнота).
Для O
оценки O
качества O
работы O
алгоритма O
на O
каждом O
из O
классов O
по O
отдельности O
введем O
метрики O
precision B-Metric
( O
точность B-Metric
) O
и O
recall B-Metric
( O
полнота B-Metric
) O
. O

# text =  Precision можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными, а recall показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.
Precision B-Metric
можно O
интерпретировать O
как O
долю O
объектов O
, O
названных O
классификатором O
положительными O
и O
при O
этом O
действительно O
являющимися O
положительными O
, O
а O
recall B-Metric
показывает O
, O
какую O
долю O
объектов O
положительного O
класса O
из O
всех O
объектов O
положительного O
класса O
нашел O
алгоритм O
. O

# text =  F-мера достигает максимума при полноте и точности, равными единице, и близка к нулю, если один из аргументов близок к нулю.
F B-Metric
- I-Metric
мера I-Metric
достигает O
максимума O
при O
полноте B-Metric
и O
точности B-Metric
, O
равными O
единице O
, O
и O
близка O
к O
нулю O
, O
если O
один O
из O
аргументов O
близок O
к O
нулю O
. O

# text =  Одним из способов оценить модель в целом, не привязываясь к конкретному порогу, является AUC-ROC (или ROC AUC) — площадь (Area Under Curve) под кривой ошибок (Receiver Operating Characteristic curve ).
Одним O
из O
способов O
оценить O
модель O
в O
целом O
, O
не O
привязываясь O
к O
конкретному O
порогу O
, O
является O
AUC B-Metric
- I-Metric
ROC I-Metric
( O
или O
ROC B-Metric
AUC I-Metric
) O
— O
площадь B-Metric
( O
Area B-Metric
Under I-Metric
Curve I-Metric
) O
под O
кривой O
ошибок O
( O
Receiver B-Metric
Operating I-Metric
Characteristic I-Metric
curve I-Metric
) O
. O

# text =  Интуитивно можно представить минимизацию logloss как задачу максимизации accuracy путем штрафа за неверные предсказания.
Интуитивно O
можно O
представить O
минимизацию O
logloss B-Metric
как O
задачу B-Task
максимизации I-Task
accuracy I-Task
путем O
штрафа O
за O
неверные O
предсказания O
. O

# text =  Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком.
Однажды O
нам O
понадобилось O
выбрать O
синтаксический B-TERM
парсер I-TERM
для O
работы O
с O
русским O
языком O
. O

# text =  Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение.
Для O
этого O
мы O
углубились O
в O
дебри O
морфологии B-TERM
и O
токенизации B-TERM
, O
протестировали O
разные O
варианты O
и O
оценили O
их O
применение O
. O

# text =  В первой строке предложение разобрано в рамках грамматики зависимостей.
В O
первой O
строке O
предложение O
разобрано O
в O
рамках O
грамматики B-TERM
зависимостей I-TERM
. O

# text =  Во второй строке разбор идет в соответствии с грамматикой непосредственно составляющих.
Во O
второй O
строке O
разбор O
идет O
в O
соответствии O
с O
  O
грамматикой B-TERM
непосредственно I-TERM
составляющих I-TERM
. O

# text =  Поэтому в автоматическом парсинге русского языка принято работать исходя из грамматики зависимостей.
Поэтому O
в O
автоматическом B-TERM
парсинге I-TERM
русского B-TERM
языка O
принято O
работать O
исходя O
из O
грамматики O
зависимостей O
. O

# text =  Чтобы облегчить себе выбор парсера, мы обратили свой взгляд на проект Universal Dependencies и недавно прошедшее в его рамках соревнование CoNLL Shared Task.
Чтобы O
облегчить O
себе O
выбор O
парсера O
, O
мы O
обратили O
свой O
взгляд O
на O
проект O
Universal B-TERM
Dependencies I-TERM
и O
недавно O
прошедшее O
в O
его O
рамках O
соревнование O
CoNLL O
Shared O
Task O
. O

# text =  Universal Dependencies — это проект по унификации разметки синтаксических корпусов (трибанков) в рамках грамматики зависимостей.
Universal B-TERM
Dependencies I-TERM
— O
это O
проект O
по O
унификации B-TERM
разметки I-TERM
синтаксических B-TERM
корпусов I-TERM
( O
трибанков B-TERM
) O
в O
рамках O
грамматики B-TERM
зависимостей I-TERM
. O

# text =  Мы можем оценивать, правильно ли нашли вершину слова — метрика UAS (Unlabeled attachment score).
Мы O
можем O
оценивать O
, O
правильно O
ли O
нашли O
вершину O
слова O
— O
метрика O
UAS B-TERM
( O
Unlabeled B-TERM
attachment I-TERM
score I-TERM
) O
. O

# text =  Или оценивать, правильно ли найдена как вершина, так и тип зависимости — метрика LAS (Labeled attachment score).
Или O
оценивать O
, O
правильно O
ли O
найдена O
как O
вершина O
, O
так O
и O
тип O
зависимости O
— O
метрика O
LAS B-TERM
( O
Labeled B-TERM
attachment I-TERM
score I-TERM
) O
. O

# text =  Казалось бы, здесь напрашивается оценка точности (accuracy) — считаем, сколько раз мы попали из общего количества случаев.
Казалось O
бы O
, O
здесь O
напрашивается O
оценка O
точности B-TERM
( O
accuracy B-TERM
) O
— O
считаем O
, O
сколько O
раз O
мы O
попали O
из O
общего O
количества O
случаев O
. O

# text =  Разработчики, решающие задачи автоматического парсинга, часто берут на вход сырой текст, который в соответствии с пирамидой анализа проходит этапы токенизации и морфологического анализа.
Разработчики O
, O
решающие O
задачи O
автоматического B-TERM
парсинга I-TERM
, O
часто O
берут O
на O
вход O
сырой O
текст O
, O
который O
в O
соответствии O
с O
пирамидой O
анализа O
проходит O
этапы O
токенизации B-TERM
и O
морфологического B-TERM
анализа I-TERM
. O

# text =  Поэтому формулой оценки в данном случае является ф-мера, где точность (precision) — доля точных попаданий относительно общего числа предсказаний, а полнота — доля точных попаданий относительно числа связей в размеченных данных.
Поэтому O
формулой O
оценки O
в O
данном O
случае O
является O
ф B-TERM
- I-TERM
мера I-TERM
, O
где O
точность B-TERM
( O
precision B-TERM
) O
— O
доля O
точных O
попаданий O
относительно O
общего O
числа O
предсказаний O
, O
а O
полнота B-TERM
— O
доля O
точных O
попаданий O
относительно O
числа O
связей O
в O
размеченных O
данных O
. O

# text =  Очевидно, что все эксперименты проводятся на SynTagRus (разработка ИППИ РАН), в котором более миллиона токенов.
Очевидно O
, O
что O
все O
эксперименты O
проводятся O
на O
SynTagRus B-TERM
( O
разработка O
ИППИ B-TERM
РАН I-TERM
) O
, O
в O
котором O
более O
миллиона O
токенов B-TERM
. O

# text =  По итогам соревнования прошлого года модели, которые обучались на одном и том же SynTagRus, достигли следующих показателей LAS:
По O
итогам O
соревнования O
прошлого O
года O
модели O
, O
которые O
обучались O
на O
одном O
и O
том O
же O
SynTagRus B-TERM
, O
достигли O
следующих O
показателей O
LAS B-TERM
: O

# text =  Забегая вперед, заметим, что новая версия UDPipe (Future) оказалась еще выше в этом году.
Забегая O
вперед O
, O
заметим O
, O
что O
новая O
версия O
UDPipe B-TERM
( O
Future B-TERM
) O
оказалась O
еще O
выше O
в O
этом O
году O
. O

# text =  В список не вошел Syntaxnet — парсер Google.
В O
список O
не O
вошел O
Syntaxnet B-TERM
— O
парсер O
Google B-TERM
. O

# text =  Ответ прост: Syntaxnet начинался лишь с этапа морфологического анализа.
Ответ O
прост O
: O
Syntaxnet B-TERM
начинался O
лишь O
с O
этапа O
морфологического B-TERM
анализа I-TERM
. O

# text =  В качестве начальных данных у нас есть табличка выше с лидирующим Syntaxnet и с UDPipe 2.0 где-то на 7 месте.
В O
качестве O
начальных O
данных O
у O
нас O
есть O
табличка O
выше O
с O
лидирующим O
Syntaxnet B-TERM
и O
с O
UDPipe B-TERM
2.0 I-TERM
где O
- O
то O
на O
7 O
месте O
. O

# text =  Синтаксис, разумеется, далеко не единственный модуль «под капотом» real-time системы, поэтому тратить на него больше десятка миллисекунд не стоит.
Синтаксис B-TERM
, O
разумеется O
, O
далеко O
не O
единственный O
модуль O
« O
под O
капотом O
» O
real B-TERM
- I-TERM
time I-TERM
системы O
, O
поэтому O
тратить O
на O
него O
больше O
десятка O
миллисекунд O
не O
стоит O
. O

# text =  Для русского языка у нас есть достаточно хорошие морфологические анализаторы, которые могут встроиться в нашу пирамиду.
Для O
русского O
языка O
у O
нас O
есть O
достаточно O
хорошие O
морфологические B-TERM
анализаторы I-TERM
, O
которые O
могут O
встроиться O
в O
нашу O
пирамиду O
. O

# text =  Затем начинает работу теггер — штука, которая предсказывает морфологические свойства токена: в каком падеже слово стоит, в каком числе.
Затем O
начинает O
работу O
теггер B-TERM
— O
штука O
, O
которая O
предсказывает O
морфологические B-TERM
свойства I-TERM
токена O
: O
в O
каком O
падеже O
слово O
стоит O
, O
в O
каком O
числе O
. O

# text =  В UDPipe есть еще лемматизатор, который подбирает для слов начальную форму.
В O
UDPipe B-TERM
есть O
еще O
лемматизатор B-TERM
, O
который O
подбирает O
для O
слов O
начальную B-TERM
форму I-TERM
. O

# text =  UDPipe — это transition-based архитектура: она работает быстро, за линейное время проходя по всем токенам один раз.
UDPipe B-TERM
— O
это O
transition B-TERM
- I-TERM
based I-TERM
архитектура O
: O
она O
работает O
быстро O
, O
за O
линейное O
время O
проходя O
по O
всем O
токенам B-TERM
один O
раз O
. O

# text =  RightArc — то же самое, но зависимость строится в другую сторону, и отбрасывается верхушка.
RightArc B-TERM
— O
то O
же O
самое O
, O
но O
зависимость O
строится O
в O
другую O
сторону O
, O
и O
отбрасывается O
верхушка O
. O

# text =  У классических transition-based parser возможны три операции, перечисленные выше: стрелка в одну сторону, стрелка в другую сторону и шифт.
У O
классических O
transition B-TERM
- I-TERM
based I-TERM
parser I-TERM
возможны O
три O
операции O
, O
перечисленные O
выше O
: O
стрелка O
в O
одну O
сторону O
, O
стрелка O
в O
другую O
сторону O
и O
шифт O
. O

# text =  Анализатор Mystem (разработка яндекса) в определении частей речи достигает лучших результатов, чем UDPipe.
Анализатор O
Mystem B-TERM
( O
разработка O
яндекса B-TERM
) O
в O
определении O
частей O
речи O
достигает O
лучших O
результатов O
, O
чем O
UDPipe B-TERM
. O

# text =  Многие знают, что Mystem не полностью понимает морфологическую омонимию.
Многие O
знают O
, O
что O
Mystem B-TERM
не O
полностью O
понимает O
морфологическую B-TERM
омонимию I-TERM
. O

# text =  При помощи анализатора RNNMorph.
При O
помощи O
анализатора O
RNNMorph B-TERM
. O

# text =  Про него мало кто слышал, но в прошлом году он выиграл соревнование среди морфологических анализаторов, проводившееся в рамках конференции «Диалог».
Про O
него O
мало O
кто O
слышал O
, O
но O
в O
прошлом O
году O
он O
выиграл O
соревнование O
среди O
морфологических B-TERM
анализаторов I-TERMS
, O
проводившееся O
в O
рамках O
конференции O
« O
Диалог O
» O
. O

# text =  Хотя если сравнивать их чисто на уровне качества морфологической разметки (данные с MorphoRuEval-2017), то проигрыш получается значительный — порядка 15%, если считать accuracy по словам.
Хотя O
если O
сравнивать O
их O
чисто O
на O
уровне O
качества O
морфологической B-TERM
разметки I-TERM
( O
данные O
с O
MorphoRuEval-2017 O
) O
, O
то O
проигрыш O
получается O
значительный O
— O
порядка O
15 O
% O
, O
если O
считать O
accuracy B-TERM
по O
словам O
. O

# text =  Дальше буду сравнивать нас с Syntaxnet и остальными алгоритмами.
Дальше O
буду O
сравнивать O
нас O
с O
Syntaxnet B-TERM
и O
остальными O
алгоритмами O
. O

# text =  Интересно, что мы почти дотянулись по метрике LAS до версии Syntaxnet.
Интересно O
, O
что O
мы O
почти O
дотянулись O
по O
метрике O
LAS B-TERM
до O
версии O
Syntaxnet B-TERM
. O

# text =  В архитектуре стенфордского парсера и Syntaxnet заложена другая концепия: сначала они генерируют полный ориентированный граф, и дальше работа алгоритма состоит в том, чтобы оставить тот скелет (минимальное остовное дерево), который будет наиболее вероятным.
В O
архитектуре O
стенфордского O
парсера O
и O
Syntaxnet B-TERM
заложена O
другая O
концепия O
: O
сначала O
они O
генерируют O
полный O
ориентированный B-TERM
граф I-TERM
, O
и O
дальше O
работа O
алгоритма O
состоит O
в O
том O
, O
чтобы O
оставить O
тот O
скелет O
( O
минимальное B-TERM
остовное I-TERM
дерево I-TERM
) O
, O
который O
будет O
наиболее O
вероятным O
. O

# text =  Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
Чаще O
всего O
на O
практике O
в O
NLP B-TERM
приходится O
сталкиваться O
с O
задачей O
построения B-TERM
эмбеддингов I-TERM
. O

# text =  Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
Для O
ее O
решения O
обычно O
используют O
один O
из O
следующих O
инструментов O
: O
Готовые O
векторы B-TERM
/ O
эмбеддинги B-TERM
слов I-TERM
[ O
6 O
] O
; O
Внутренние O
состояния O
CNN B-TERM
, O
натренированных O
на O
таких O
задачах O
как O
, O
как O
определение B-TERM
фальшивых I-TERM
предложений I-TERM
/ O
языковое B-TERM
моделирование I-TERM
/ O
классификация B-TERM
[ O
7 O
] O
; O
Комбинация O
выше O
перечисленных O
методов O
; O
Кроме O
того O
, O
уже O
много O
раз O
было O
показано O
[ O
9 O
] O
, O
что O
в O
качестве O
хорошего O
бейслайна O
для O
эмбеддингов O
предложений O
можно O
взять O
и O
просто O
усредненные O
( O
с O
парой O
незначительных O
деталей O
, O
которые O
сейчас O
опустим O
) O
векторы O
слов O
. O

# text =  Выраженная в тексте эмоциональная оценка называется тональностью или сентиментом (от англ. sentiment — чувство; мнение, настроение) текста.
Выраженная O
в O
тексте O
эмоциональная B-TERM
оценка I-TERM
называется O
тональностью B-TERM
или O
сентиментом B-TERM
( O
от O
англ O
. O 
sentiment O
— O
чувство O
; O
мнение O
, O
настроение O
) O
текста O
. O

# text =  Исторически сложилось так, что традиционный подход к сентимент анализу представляет собой задачу классификации текста (части текста) на две-три категории (негативный, позитивный, нейтральный или просто: негативный или позитивный) [Pang & Lee; Turney ].
Исторически O
сложилось O
так O
, O
что O
традиционный O
подход O
к O
сентимент B-TERM
анализу I-TERM
представляет O
собой O
задачу O
классификации B-TERM
текста O
( O
части O
текста O
) O
на O
две O
- O
три O
категории O
( O
негативный O
, O
позитивный O
, O
нейтральный O
или O
просто O
: O
негативный O
или O
позитивный O
) O
[ O
Pang B-TERM
& O
Lee B-TERM
; O
Turney B-TERM
] O
. O

# text =  Такой вид сентимент анализа называется объектной тональностью (object-based).
Такой O
вид O
сентимент B-TERM
анализа I-TERM
называется O
объектной B-TERM
тональностью I-TERM
( O
object B-TERM
- I-TERM
based I-TERM
) O
. O

# text =  Таким образом, тональность высказывания определяется тремя компонентами: субъектом тональности (кто высказал оценку), объектом тональности (о ком или о чём высказана оценка) и собственно тональной оценкой (как оценили).
Таким O
образом O
, O
тональность B-TERM
высказывания I-TERM
определяется O
тремя O
компонентами O
: O
субъектом B-TERM
тональности I-TERM
( O
кто O
высказал O
оценку O
) O
, O
объектом B-TERM
тональности I-TERM
( O
о O
ком O
или O
о O
чём O
высказана O
оценка O
) O
и O
собственно O
тональной B-TERM
оценкой I-TERM
( O
как O
оценили O
) O
. O

# text =  Еще одним направлением сентимент анализа является выявление негативности / позитивности атрибутов объекта тональности (feature-based / aspect-based sentiment analysis).
Еще O
одним O
направлением O
сентимент O
анализа O
является O
выявление O
негативности B-TERM
/ I-TERM
позитивности I-TERM
атрибутов I-TERM
объекта I-TERM
тональности I-TERM
( O
feature B-TERM
- I-TERM
based I-TERM
/ I-TERM
aspect I-TERM
- I-TERM
based I-TERM
sentiment I-TERM
analysis I-TERM
) O
. O

# text =  При статистическом подходе для решения задачи общей классификации текстов на классы тональности широко используют метод опорных векторов (SVM), Байесовы модели, различного рода регрессии [Chetviorkin & Loukachevitch — описание соревнования ROMIP-2011 по сентимент анализу данных, практически все участники использовали SVM или Байес].
При O
статистическом O
подходе O
для O
решения O
задачи O
общей O
классификации B-TERM
текстов O
на O
классы O
тональности O
широко O
используют O
метод B-TERM
опорных I-TERM
векторов I-TERM
( O
SVM B-TERM
) O
, O
Байесовы B-TERM
модели I-TERM
, O
различного O
рода O
регрессии B-TERM
[ O
Chetviorkin B-TERM
& O
Loukachevitch B-TERM
— O
описание O
соревнования O
ROMIP-2011 O
по O
сентимент B-TERM
анализу I-TERM
данных O
, O
практически O
все O
участники O
использовали O
SVM B-TERM
или O
Байес B-TERM
] O
. O

# text =  Если же целью является определение тональности у определенного, заранее заданного объекта (нескольких объектов), то применяют более сложные статистические алгоритмы, такие как CRF [Антонова и Соловьев], алгоритмы семантической близости (например, латентно-семантический анализ – LSA, латентное размещение Дирихле — LDA) и др., а также методы, основанные на правилах [Пазельская и Соловьев].
Если O
же O
целью O
является O
определение B-TERM
тональности I-TERM
у O
определенного O
, O
заранее O
заданного O
объекта O
( O
нескольких O
объектов O
) O
, O
то O
применяют O
более O
сложные O
статистические B-TERM
алгоритмы I-TERM
, O
такие O
как O
CRF B-TERM
[ O
Антонова B-TERM
и O
Соловьев B-TERM
] O
, O
алгоритмы B-TERM
семантической I-TERM
близости I-TERM
( O
например O
, O
латентно B-TERM
- I-TERM
семантический I-TERM
анализ I-TERM
– O
LSA B-TERM
, O
латентное B-TERM
размещение I-TERM
Дирихле I-TERM
— O
LDA B-TERM
) O
и др. O
, O
а O
также O
методы O
, O
основанные O
на O
правилах O
[ O
Пазельская B-TERM
и O
Соловьев B-TERM
] O
. O

# text =  Удалось найти лишь это упоминание про систему Deepgram.
Удалось O
найти O
лишь O
это O
упоминание O
про O
систему O
Deepgram B-TERM
. O

# text =  Также очень похожая функциональность есть у Microsoft в Streams, но нигде не нашел упоминания про поддержку русского языка, судя по всему, ее тоже нет.
Также O
очень O
похожая O
функциональность O
есть O
у O
Microsoft B-TERM
в O
Streams B-TERM
, O
но O
нигде O
не O
нашел O
упоминания O
про O
поддержку O
русского O
языка O
, O
судя O
по O
всему O
, O
ее O
тоже O
нет O
. O

# text =  Нейросети, которые могут преобразовывать речь в текст называются (сюрприз-сюрприз), speech-to-text.
Нейросети O
, O
которые O
могут O
преобразовывать B-TERM
речь I-TERM
в I-TERM
текст I-TERM
называются O
( O
сюрприз O
- O
сюрприз O
) O
, O
speech B-TERM
- I-TERM
to I-TERM
- I-TERM
text I-TERM
. O

# text =  Если получится найти публичный сервис speech-to-text, то его можно использовать, чтобы «оцифровать» речь во всех вебинарах, а сделать потом нечеткий поиск по тексту – более простая задача.
Если O
получится O
найти O
публичный O
сервис O
speech B-TERM
- I-TERM
to I-TERM
- I-TERM
text I-TERM
, O
то O
его O
можно O
использовать O
, O
чтобы O
« O
оцифровать O
» O
речь O
во O
всех O
вебинарах O
, O
а O
сделать O
потом O
нечеткий O
поиск O
по O
тексту O
– O
более O
простая O
задача O
. O

# text =  Поиск сервисов, способных делать speech-to-text показал, что таких систем масса, в том числе и разработанных в России, есть среди них также глобальные облачные провайдеры вроде Google, Amazon, MS Azure.
Поиск O
сервисов O
, O
способных O
делать O
speech B-TERM
- I-TERM
to I-TERM
- I-TERM
text I-TERM
показал O
, O
что O
таких O
систем O
масса O
, O
в O
том O
числе O
и O
разработанных O
в O
России O
, O
есть O
среди O
них O
также O
глобальные O
облачные O
провайдеры O
вроде O
Google B-TERM
, O
Amazon B-TERM
, O
MS B-TERM
Azure I-TERM
. O

# text =  Custom Vocabularies – позволяет создать «словарь» из тех, слов, которые должна «выучить» нейросеть перед тем, как приступить к распознаванию.
Custom B-TERM
Vocabularies I-TERM
– O
позволяет O
создать O
« O
словарь O
» O
из O
тех O
, O
слов O
, O
которые O
должна O
« O
выучить O
» O
нейросеть O
перед O
тем O
, O
как O
приступить O
к O
распознаванию B-TERM
. O

# text =  Можно попробовать прикрутить к итоговому набору текстов алгоритм BERT (Bi-directional Encoder Representation from Transformer), описание есть тут.
Можно O
попробовать O
прикрутить O
к O
итоговому O
набору O
текстов O
алгоритм O
BERT B-TERM
( O
Bi B-TERM
- I-TERM
directional I-TERM
Encoder I-TERM
Representation I-TERM
from I-TERM
Transformer I-TERM
) O
, O
описание O
есть O
тут O
. O

# text =  Некоторое время назад к нам обратился заказчик с не совсем обычной задачей — воспроизвести сервис IBM Watson Personality Insights, который анализировал текст, написанный человеком и определял по нему ряд личностных характеристик.
Некоторое O
время O
назад O
к O
нам O
обратился O
заказчик O
с O
не O
совсем O
обычной O
задачей O
— O
воспроизвести O
сервис O
IBM B-TERM
Watson I-TERM
Personality I-TERM
Insights I-TERM
, O
который O
анализировал B-TERM
текст I-TERM
, O
написанный O
человеком O
и O
определял O
по O
нему O
ряд O
личностных B-TERM
характеристик I-TERM
. O

# text =  Основная идея данного сервиса состояла в том, что он получает на вход текст написанный определенным человеком и определяет по этому тексту четыре группы характеристик личности.
Основная O
идея O
данного O
сервиса O
состояла O
в O
том O
, O
что O
он O
получает O
на O
вход O
текст O
написанный O
определенным O
человеком O
и O
определяет B-TERM
по O
этому O
тексту O
четыре O
группы I-TERM
характеристик I-TERM
личности I-TERM
. O

# text =  Например, Personality Insights использовался в психотерапии для оценки состояния пациентов [5], в искусстве (оценка личности персонажей пьес Шекспира) [6], определении спама [7] а также в научных исследованиях.
Например O
, O
Personality B-TERM
Insights I-TERM
использовался O
в O
психотерапии B-TERM
для O
оценки B-TERM
состояния I-TERM
пациентов I-TERM
[ O
5 O
] O
, O
в O
искусстве O
( O
оценка B-TERM
личности I-TERM
персонажей I-TERM
пьес I-TERM
Шекспира I-TERM
) O
[ O
6 O
] O
, O
определении B-TERM
спама I-TERM
[ O
7 O
] O
а O
также O
в O
научных O
исследованиях O
. O

# text =  На сайте Personality Insights качество моделей Watson оценивалось с помощью двух показателей — средней абсолютной ошибки (MAE) и коэффициента корреляции.
На O
сайте O
Personality B-TERM
Insights I-TERM
качество O
моделей O
Watson B-TERM
оценивалось O
с O
помощью O
двух O
показателей O
— O
средней B-TERM
абсолютной I-TERM
ошибки I-TERM
( O
MAE B-TERM
) O
и O
коэффициента B-TERM
корреляции I-TERM
. O

# text =  В литературе для предсказания характеристик Big 5 использовались различные методы линейная регрессия с использованием признаков полученных латентным семантическим анализом [11], ридж-регрессия по большому набору собранных вручную признаков [12], SVM с признаками TF/IDF [13], word2vec и doc2vec [14].
В O
литературе O
для O
предсказания O
характеристик O
Big O
5 O
использовались O
различные O
методы O
линейная B-TERM
регрессия I-TERM
с O
использованием O
признаков O
полученных O
латентным O
семантическим O
анализом O
[ O
11 O
] O
, O
ридж B-TERM
- I-TERM
регрессия I-TERM
по O
большому O
набору O
собранных O
вручную O
признаков O
[ O
12 O
] O
, O
SVM B-TERM
с O
признаками O
TF B-TERM
/ O
IDF B-TERM
[ O
13 O
] O
, O
word2vec B-TERM
и O
doc2vec B-TERM
[ O
14 O
] O
. O

# text =  В более современных работах присутствуют сверточные нейронные сети [15, 16], а также предобученные модели BERT [17]
В O
более O
современных O
работах O
присутствуют O
сверточные B-TERM
нейронные I-TERM
сети I-TERM
[ O
15 O
, O
16 O
] O
, O
а O
также O
предобученные O
модели O
BERT B-TERM
[ O
17 O
] O


# text = Модель, которую построил заказчик использовала вектора слов word2vec и рекуррентную нейронную сеть на базе GRU (gated recurrent unit) (Рис 1а).
Модель O
, O
которую O
построил O
заказчик O
использовала O
вектора O
слов O
word2vec B-TERM
и O
рекуррентную O
нейронную O
сеть O
на O
базе O
GRU B-TERM
( O
gated B-TERM
recurrent I-TERM
unit I-TERM
) O
( O
Рис O
1а O
) O
. O

# text =  Обучалась модель с функцией ошибки MSE (среднеквадратичное отклонение).
Обучалась O
модель O
с O
функцией O
ошибки O
MSE B-TERM
( O
среднеквадратичное B-TERM
отклонение I-TERM
) O
. O

# text =  Сигмоидная функция активации обычно не очень хорошо подходит для задачи регрессии.
Сигмоидная B-TERM
функция I-TERM
активации I-TERM
обычно O
не O
очень O
хорошо O
подходит O
для O
задачи O
регрессии B-TERM
. O

# text =  В литературе для регрессии рекомендуют использовать линейную активацию или RelU.
В O
литературе O
для O
регрессии B-TERM
рекомендуют O
использовать O
линейную B-TERM
активацию I-TERM
или O
RelU B-TERM
. O

# text =  Вычислив MAE отдельно для характеристики личности и отдельно для потребительских предпочтений получили значения 0.11 и 0.148 соответственно, т. е. потребительские предпочтения сильно портят общую картину.
Вычислив O
MAE B-TERM
отдельно O
для O
характеристики O
личности O
и O
отдельно O
для O
потребительских O
предпочтений O
получили O
значения O
0.11 B-TERM
и O
0.148 B-TERM
соответственно O
, O
т O
. O
  O
е O
. O
потребительские O
предпочтения O
сильно O
портят O
общую O
картину O
. O

# text =  Замена BERT на более современную модель XLM RoBERTa large позволило улучшить результаты (эта модель более ресурсозатратная и медленная, но заказчик сказал, что скорость работы не критична).
Замена O
BERT O
на O
более O
современную O
модель O
XLM B-TERM
RoBERTa I-TERM
large I-TERM
позволило O
улучшить O
результаты O
( O
эта O
модель O
более O
ресурсозатратная O
и O
медленная O
, O
но O
заказчик O
сказал O
, O
что O
скорость O
работы O
не O
критична O
) O
. O

# text =  Итоговый MAE составил 0.073 для характеристик личности и 0.098 для потребительских предпочтений.
Итоговый O
MAE B-TERM
составил O
0.073 B-TERM
для O
характеристик O
личности O
и O
0.098 B-TERM
для O
потребительских O
предпочтений O
. O

# text =  Получились немного разные цифры, но средний коэффициент корреляции по всем параметрам составил 0.68, что говорит о том, что характеристики, выдаваемые с разных переводов одного текста должны быть весьма похожи.
Получились O
немного O
разные O
цифры O
, O
но O
средний O
коэффициент B-TERM
корреляции I-TERM
по O
всем O
параметрам O
составил O
0.68 B-TERM
, O
что O
говорит O
о O
том O
, O
что O
характеристики O
, O
выдаваемые O
с O
разных O
переводов O
одного O
текста O
должны O
быть O
весьма O
похожи O
. O

# text =  Надо сказать, что признаки, формируемые верхними слоями подобных моделей не всегда являются самыми лучшими, точнее даже сказать, как правило, не являются — в классических задачах, таких как поиск именованных сущностей или ответы на вопросы по тексту, признаки верхних уровней работают хуже, чем признаки промежуточных [18].
Надо O
сказать O
, O
что O
признаки O
, O
формируемые O
верхними O
слоями O
подобных O
моделей O
не O
всегда O
являются O
самыми O
лучшими O
, O
точнее O
даже O
сказать O
, O
как O
правило O
, O
не O
являются O
— O
в O
классических O
задачах O
, O
таких O
как O
поиск B-TERM
именованных I-TERM
сущностей I-TERM
или O
ответы O
на O
вопросы O
по O
тексту O
, O
признаки O
верхних O
уровней O
работают O
хуже O
, O
чем O
признаки O
промежуточных O
[ O
18 O
] O
. O

# text =  Veridical Data Science — программная статья о методологии верификации моделей.
Veridical B-TERM
Data I-TERM
Science I-TERM
— O
программная O
статья O
о O
методологии O
верификации B-TERM
моделей I-TERM
. O

# text = Чтобы обеспечить надежную проверку и разработать механизмы проверки и пополнения знаний, нужны специалисты смежных областей, одновременно обладающие компетенциями в ML и в предметной области (медицине, лингвистике, нейробиологии, образовании и т.д.).
Чтобы O
обеспечить O
надежную O
проверку O
и O
разработать O
механизмы O
проверки O
и O
пополнения O
знаний O
, O
нужны O
специалисты O
смежных O
областей O
, O
одновременно O
обладающие O
компетенциями O
в O
ML O
и O
в O
предметной O
области O
( O
медицине B-TERM
, O
лингвистике B-TERM
, O
нейробиологии B-TERM
, O
образовании B-TERM
и O
т.д. O
) O
. O

# text =  В частности, развивается causal inference и commonsense reasoning.
В O
частности O
, O
развивается O
causal B-TERM
inference I-TERM
и O
commonsense B-TERM
reasoning I-TERM
. O

# text =  Часть докладов посвящена мета-обучению (о том, как учиться учиться) и соединению DL-технологий с логикой 1 и 2 порядка — термин Artificial General Intelligence (AGI) становится обычным термином в выступлениях спикеров.
Часть O
докладов O
посвящена O
мета O
- O
обучению O
( O
о O
том O
, O
как O
учиться O
учиться O
) O
и O
соединению O
DL B-TERM
- I-TERM
технологий I-TERMS
с O
логикой O
1 O
и O
2 O
порядка O
— O
термин O
Artificial B-TERM
General I-TERM
Intelligence I-TERM
( O
AGI B-TERM
) O
становится O
обычным O
термином O
в O
выступлениях O
спикеров O
. O

# text =  Google запускает Coral ai – аналог raspberry pi, мини-компьютер для внедрения нейросетей в экспериментальные установки.
Google B-TERM
запускает O
Coral B-TERM
ai I-TERM
– O
аналог O
raspberry B-TERM
pi I-TERM
, O
мини O
- O
компьютер O
для O
внедрения O
нейросетей O
в O
экспериментальные O
установки O
. O

# text =  Federated learning – направление ML, в котором отдельные модели учатся независимо друг от друга, а затем объединяются в единую модель (без централизации исходных данных), с поправками на редкие события, аномалии, персонализацию и т.д.
Federated B-TERM
learning I-TERM
– O
направление O
ML B-TERM
, O
в O
котором O
отдельные O
модели O
учатся O
независимо O
друг O
от O
друга O
, O
а O
затем O
объединяются O
в O
единую O
модель O
( O
без O
централизации O
исходных O
данных O
) O
, O
с O
поправками O
на O
редкие O
события O
, O
аномалии O
, O
персонализацию O
и O
т.д. O

# text =  Генеративные модели на основании federated learning – будущее перспективное направление по мнению Google, которое находится “в ранних стадиях экспоненциального роста”.
Генеративные B-TERM
модели I-TERM
на O
основании O
federated B-TERM
learning I-TERM
– O
будущее O
перспективное O
направление O
по O
мнению O
Google B-TERM
, O
которое O
находится O
“ O
в O
ранних O
стадиях O
экспоненциального O
роста O
” O
. O

# text =  МТИ Technology Review протестировали два инструмента — MyInterview и Curious Thing.
МТИ B-TERM
Technology I-TERM
Review I-TERM
протестировали O
два O
инструмента O
— O
MyInterview B-TERM
и O
Curious B-TERM
Thing I-TERM
. O

# text =  Потом появилась нейросеть GPT-2, которая была как минимум в 10 раз мощнее и была способна обрабатывать 1,5 миллиарда параметров (переменных, определяющих возможности машинного обучения).
Потом O
появилась O
нейросеть O
GPT-2 B-TERM
, O
которая O
была O
как O
минимум O
в O
10 O
раз O
мощнее O
и O
была O
способна O
обрабатывать O
1,5 O
миллиарда O
параметров O
( O
переменных O
, O
определяющих O
возможности O
машинного B-TERM
обучения I-TERM
) O
. O

# text =  Используя новый алгоритм упаковки, в Graphcore ускорили обработку естественного языка более чем в 2 раза при обучении BERT-Large.
Используя O
новый O
алгоритм B-TERM
упаковки I-TERM
, O
в O
Graphcore B-TERM
ускорили O
обработку O
естественного O
языка O
более O
чем O
в O
2 O
раза O
при O
обучении O
BERT B-TERM
- I-TERM
Large I-TERM
. O

# text =  В Graphcore предполагают, что алгоритм упаковки также может применяться в геномике, в моделях фолдинга белков и других моделях с перекошенным распределением длины, оказывая гораздо более широкое влияние на различные отрасли и приложения.
В O
Graphcore B-TERM
предполагают O
, O
что O
алгоритм B-TERM
упаковки I-TERM
также O
может O
применяться O
в O
геномике B-TERM
, O
в O
моделях B-TERM
фолдинга I-TERM
белков O
и O
других O
моделях O
с O
перекошенным O
распределением O
длины O
, O
оказывая O
гораздо O
более O
широкое O
влияние O
на O
различные O
отрасли O
и O
приложения O
. O

# text =  В новой работе Graphcore представили высокоэффективный алгоритм гистограммной упаковки с неотрицательными наименьшими квадратами (или NNLSHP), а также алгоритм BERT, применяемый к упакованным последовательностям.
В O
новой O
работе O
Graphcore B-TERM
представили O
высокоэффективный O
алгоритм B-TERM
гистограммной I-TERM
упаковки I-TERM
с O
неотрицательными O
наименьшими O
квадратами O
( O
или O
NNLSHP B-TERM) O
, O
а O
также O
алгоритм O
BERT B-TERM
, O
применяемый O
к O
упакованным O
последовательностям O
. O

# text =  Сбер создал и опубликовал в открытом доступе программную библиотеку PyTorch-LifeStream, содержащую несколько алгоритмов построения эмбеддингов событийных данных.
Сбер B-TERM
создал O
и O
опубликовал O
в O
открытом O
доступе O
программную O
библиотеку O
PyTorch B-TERM
- I-TERM
LifeStream I-TERM
, O
содержащую O
несколько O
алгоритмов O
построения O
эмбеддингов O
событийных O
данных O
. O

# text =  Эмбеддинг (Embedding) – преобразования сложноструктурированных данных,  например слов, текстов, атрибутов событий, событий и их последовательностей, в машинно-читаемый набор чисел – числовой вектор.Событийные данные – разные последовательности.
Эмбеддинг B-TERM
( O
Embedding B-TERM
) O
– O
преобразования O
сложноструктурированных O
данных O
, O
   O
например O
слов O
, O
текстов O
, O
атрибутов O
событий O
, O
событий O
и O
их O
последовательностей O
, O
в O
машинно O
- O
читаемый O
набор O
чисел O
– O
числовой O
вектор O
. O
Событийные O
данные O
– O
разные O
последовательности O
. O

# text =  Самой популярной из трёх задач соревнования стала главная – Matching.
Самой O
популярной O
из O
трёх O
задач O
соревнования O
стала O
главная O
– O
Matching B-TERM
. O

# text =  Стоит отметить, что и для них всё было непросто – конкурсная задача матчинга позволила удачно применить разработанный в Лаборатории ИИ метод генерации эмбеддингов транзакционных данных одновременно для двух разных доменов событийных данных (транзакции и кликстрим – атрибуты посещения веб-страниц). 
Стоит O
отметить O
, O
что O
и O
для O
них O
всё O
было O
непросто O
– O
конкурсная O
задача O
матчинга O
позволила O
удачно O
применить O
разработанный O
в O
Лаборатории B-TERM
ИИ I-TERM
метод B-TERM
генерации I-TERM
эмбеддингов I-TERM
транзакционных I-TERM
данных I-TERM
одновременно O
для O
двух O
разных O
доменов O
событийных O
данных O
( O
транзакции B-TERM
и O
кликстрим B-TERM
– O
атрибуты O
посещения O
веб O
- O
страниц O
) O
. O

# text =  Победители создали лучшее решение благодаря применению собственной библиотеки PyTorch-LifeStream, которая позволила ускорить разработку решения, так как содержит много готовых инструментов для работы с событийными данными, и дала возможность стать фаворитом престижного конкурса. 
Победители O
создали O
лучшее O
решение O
благодаря O
применению O
собственной O
библиотеки O
PyTorch B-TERM
- I-TERM
LifeStream I-TERM
, O
которая O
позволила O
ускорить O
разработку O
решения O
, O
так O
как O
содержит O
много O
готовых O
инструментов O
для O
работы O
с O
событийными O
данными O
, O
и O
дала O
возможность O
стать O
фаворитом O
престижного O
конкурса O

# text =  Фичи для транзакций и кликов объединялись и подавались в алгоритм бустинга.
Фичи O
для O
транзакций O
и O
кликов O
объединялись O
и O
подавались O
в O
алгоритм O
бустинга B-TERM
. O

# text =  Алгоритм обучался как задача бинарной классификации.
Алгоритм O
обучался O
как O
задача B-TERM
бинарной I-TERM
классификации I-TERM
. O

# text =  Команда решила использовать схему обучения, похожую на сиамскую сеть.
Команда O
решила O
использовать O
схему O
обучения O
, O
похожую O
на O
сиамскую B-TERM
сеть I-TERM
. O

# text =  Метки использованы для выборки положительных и отрицательных образцов для функции потерь Softmax Loss.
Метки O
использованы O
для O
выборки O
положительных O
и O
отрицательных O
образцов O
для O
функции O
потерь O
Softmax B-TERM
Loss I-TERM
. O

# text =  SequenceEncoder – рекурентно-нейронная сеть (RNN), совместно используемая для транзакций и кликов.
SequenceEncoder B-TERM
– O
рекурентно B-TERM
- I-TERM
нейронная I-TERM
сеть I-TERM
( O
RNN B-TERM
) O
, O
совместно O
используемая O
для O
транзакций O
и O
кликов O
. O

# text =  В итоге это дало самый большой прирост качества: для ансамбля из пяти моделей метрика качества R1 выросла с 0.2819 до 0.2949.
В O
итоге O
это O
дало O
самый O
большой O
прирост O
качества O
: O
для O
ансамбля O
из O
пяти O
моделей O
метрика O
качества O
R1 B-TERM
выросла O
с O
0.2819 B-TERM
до O
0.2949 B-TERM
. O

# text =  Решать задачу будем с использованием нейронных сетей, но оптимизируемых генетическим алгоритмом (ГА) – такой процесс называют нейроэволюцией.
Решать O
задачу O
будем O
с O
использованием O
нейронных O
сетей O
, O
но O
оптимизируемых O
генетическим B-TERM
алгоритмом I-TERM
( O
ГА B-TERM
) O
– O
такой O
процесс O
называют O
нейроэволюцией O
. O

# text =  Мы воспользовались методом NEAT (NeuroEvolution of Augmenting Topologies), изобретенным Кеннетом Стенли и Ристо Мииккулайненом в начале века [1]: во-первых, он хорошо зарекомендовал себя в важных для народного хозяйства проблемах, во-вторых, к началу работы над проектом у нас уже был свой фреймворк, реализующий NEAT.
Мы O
воспользовались O
методом O
NEAT B-TERM
( O
NeuroEvolution B-TERM
of I-TERM
Augmenting I-TERM
Topologies I-TERM
) O
, O
изобретенным O
Кеннетом B-TERM
Стенли I-TERM
и O
Ристо B-TERM
Мииккулайненом I-TERM
в O
начале O
века O
[ O
1 O
] O
: O
во O
- O
первых O
, O
он O
хорошо O
зарекомендовал O
себя O
в O
важных O
для O
народного O
хозяйства O
проблемах O
, O
во O
- O
вторых O
, O
к O
началу O
работы O
над O
проектом O
у O
нас O
уже O
был O
свой O
фреймворк O
, O
реализующий O
NEAT B-TERM
. O

# text =  Строго говоря, нам нужно получить параллельный корпус из двух текстов.
Строго O
говоря O
, O
нам O
нужно O
получить O
параллельный B-TERM
корпус I-TERM
из O
двух O
текстов O
. O

# text =  Для выравнивания воспользуемся библиотекой lingtrain-aligner, над которой я работаю около года и которая родилась из кучи скриптов на python, часть из которых еще ждет своего часа.
Для O
выравнивания O
воспользуемся O
библиотекой O
lingtrain B-TERM
- I-TERM
aligner I-TERM
, O
над O
которой O
я O
работаю O
около O
года O
и O
которая O
родилась O
из O
кучи O
скриптов O
на O
python O
, O
часть O
из O
которых O
еще O
ждет O
своего O
часа O
. O

# text =  Хорошим решением мне видится регрессия на координаты строк при выравнивании батча и сдвиг окна на конец потока при выравнивании следующего.
Хорошим O
решением O
мне O
видится O
регрессия B-TERM
на O
координаты O
строк O
при O
выравнивании B-TERM
батча O
и O
сдвиг O
окна O
на O
конец O
потока O
при O
выравнивании O
следующего O
. O

# text =  Facebook представила систему распознавания речи wav2vec-U.
Facebook B-TERM
представила O
систему O
распознавания B-TERM
речи I-TERM
wav2vec B-TERM
- O
U O

# text =  Система разбивает запись на речевые единицы, которые приблизительно соответствуют отдельным звукам.
Система O
разбивает O
запись O
на O
речевые B-TERM
единицы I-TERM
, O
которые O
приблизительно O
соответствуют O
отдельным O
звукам O
. O

# text =  Чтобы научиться распознавать слова в аудиозаписи, Facebook обучила генеративную состязательную сеть (GAN).
Чтобы O
научиться O
распознавать O
слова O
в O
аудиозаписи O
, O
Facebook B-TERM
обучила O
генеративную B-TERM
состязательную I-TERM
сеть I-TERM
( O
GAN B-TERM
) O
. O

# text =  Генератор берет каждый аудиосегмент и предсказывает фонему, соответствующую звуку на языке.
Генератор O
берет O
каждый O
аудиосегмент O
и O
предсказывает O
фонему B-TERM
, O
соответствующую O
звуку O
на O
языке O
. O

# text =  Новая модель распознавания речи Facebook AI — это последняя разработка за несколько лет работы над моделями распознавания речи.
Новая O
модель O
распознавания B-TERM
речи I-TERM
Facebook B-TERM
AI I-TERM
— O
это O
последняя O
разработка O
за O
несколько O
лет O
работы O
над O
моделями O
распознавания O
речи O
. O

# text =  Ее предшественниками стали wav2letter, wav2vec, Librilight, wav2vec 2.0, XLSR и wav2vec 2.0.
ее O
предшественниками O
стали O
wav2letter B-TERM
, O
wav2vec B-TERM
, O
Librilight B-TERM
, O
wav2vec B-TERM
2.0 I-TERM
, O
XLSR B-TERM
и O
wav2vec B-TERM
2.0 I-TERM
. O

# text =  В полку LLM прибыло: недавно специалисты из Французского национального центра научных исследований (French National Center for Scientific Research) объявили о релизе новой большой языковой модели под названием BLOOM (расшифровывается как BigScience Large Open-science Open-access Multilingual Language Model).
В O
полку O
LLM B-TERM
прибыло O
: O
недавно O
специалисты O
из O
Французского B-TERM
национального I-TERM
центра I-TERM
научных I-TERM
исследований I-TERM
( O
French B-TERM
National I-TERM
Center I-TERM
for I-TERM
Scientific I-TERM
Research I-TERM
) O
объявили O
о O
релизе O
новой O
большой O
языковой O
модели O
под O
названием O
BLOOM B-TERM
( O
расшифровывается O
как O
BigScience B-TERM
Large I-TERM
Open I-TERM
- I-TERM
science I-TERM
Open I-TERM
- I-TERM
access I-TERM
Multilingual I-TERM
Language I-TERM
Model I-TERM
) O
. O

# text =  Большие языковые модели или LLM (Large Language Models) — это алгоритмы глубокого обучения, которые обучаются на огромных объемах данных.
Большие O
языковые O
модели O
или O
LLM B-TERM
( O
Large B-TERM
Language I-TERM
Models I-TERM
) O
— O
это O
алгоритмы O
глубокого O
обучения O
, O
которые O
обучаются O
на O
огромных O
объемах O
данных O
. O

# text =  Их можно использовать в качестве чат-ботов, для поиска информации, модерации онлайн-контента, анализа литературы или для создания совершенно новых фрагментов текста на основе подсказок (чем занимается, например, «Порфирьевич», который способен генерировать весьма забавные короткие рассказы).
Их O
можно O
использовать O
в O
качестве O
чат B-TERM
- I-TERM
ботов I-TERM
, O
для O
поиска B-TERM
информации I-TERM
, O
модерации B-TERM
онлайн I-TERM
- I-TERM
контента I-TERM
, O
анализа B-TERM
литературы I-TERM
или O
для O
создания B-TERM
совершенно I-TERM
новых I-TERM
фрагментов I-TERM
текста I-TERM
на O
основе O
подсказок O
( O
чем O
занимается O
, O
например O
, O
« O
Порфирьевич B-TERM
» O
, O
который O
способен O
генерировать O
весьма O
забавные O
короткие O
рассказы O
) O
. O

# text =  Новая LLM с открытым исходным кодом в отличие от таких известных LLM, как GPT-3 от OpenAI и LaMDA от Google, BLOOM является открытой языковой моделью, а исследователи охотно делятся подробностями о тех данных, на которых она обучалась, рассказывают о проблемах в ее разработке и о том, как они оценивали производительность BLOOM.
Новая O
LLM B-TERM
с O
открытым O
исходным O
кодом O
в O
отличие O
от O
таких O
известных O
LLM B-TERM
, O
как O
GPT-3 B-TERM
от O
OpenAI B-TERM
и O
LaMDA B-TERM
от O
Google B-TERM
, O
BLOOM B-TERM
является O
открытой O
языковой O
моделью O
, O
а O
исследователи O
охотно O
делятся O
подробностями O
о O
тех O
данных O
, O
на O
которых O
она O
обучалась O
, O
рассказывают O
о O
проблемах O
в O
ее O
разработке O
и O
о O
том O
, O
как O
они O
оценивали O
производительность O
BLOOM B-TERM
. O

# text =  OpenAI и Google не делились своим кодом и не делали свои модели общедоступными.
OpenAI B-TERM
и O
Google B-TERM
не O
делились O
своим O
кодом O
и O
не O
делали O
свои O
модели O
общедоступными O
. O

# text =  И уже сейчас над BLOOM работают более тысячи исследователей-добровольцев в рамках проекта под названием BigScience, который координирует стартап Hugging Face, существующий за счет финансовой поддержки французского правительства.
И O
уже O
сейчас O
над O
BLOOM B-TERM
работают O
более O
тысячи O
исследователей O
- O
добровольцев O
в O
рамках O
проекта O
под O
названием O
BigScience B-TERM
, O
который O
координирует O
стартап O
Hugging B-TERM
Face I-TERM
, O
существующий O
за O
счет O
финансовой O
поддержки O
французского O
правительства O
. O

# text =  BLOOM может обрабатывать 46 языков, включая французский, испанский, арабский, вьетнамский, китайский, индонезийский, каталанский, целых 13 языков Индии (хинди, бенгали, маратхи и ряд других) и аж 20 африканских.
BLOOM B-TERM
может O
обрабатывать O
46 O
языков O
, O
включая O
французский B-TERM
, O
испанский B-TERM
, O
арабский B-TERM
, O
вьетнамский B-TERM
, O
китайский B-TERM
, O
индонезийский B-TERM
, O
каталанский B-TERM
, O
целых O
13 O
языков O
Индии O
( O
хинди B-TERM
, O
бенгали B-TERM
, O
маратхи B-TERM
и O
ряд O
других O
) O
и O
аж O
20 O
африканских O
. O

# text =  На русском BLOOM тоже пишет, но пока довольно вяло.
На O
русском B-TERM
тоже O
пишет O
, O
но O
пока O
довольно O
вяло O
. O

# text =  Почти треть обучающих данных была введена в модель BLOOM на английском языке: следствие того, что именно английский является наиболее часто используемым языком в интернете.
Почти O
треть O
обучающих O
данных O
была O
введена O
в O
модель O
BLOOM B-TERM
на O
английском B-TERM
языке O
: O
следствие O
того O
, O
что O
именно O
английский O
является O
наиболее O
часто O
используемым O
языком O
в O
интернете O
. O

# text =  В июне текущего года инженер Google Блейк Лемуан (он на фото выше) ошарашил мировую общественность заявлением, что LLM LaMDA, над которой он работал вместе с другими программистами, может обладать некоторым подобием разума.
В O
июне O
текущего O
года O
инженер O
Google B-TERM
Блейк B-TERM
Лемуан I-TERM
( O
он O
на O
фото O
выше O
) O
ошарашил O
мировую O
общественность O
заявлением O
, O
что O
LLM B-TERM
LaMDA I-TERM
, O
над O
которой O
он O
работал O
вместе O
с O
другими O
программистами O
, O
может O
обладать O
некоторым O
подобием O
разума O
. O

# text =  Американский ученый и исследователь ИИ Гэри Маркус еще до появления в сети откровений Лемуана опубликовал на портале Scientific American материал под названием «Общий ИИ не так неизбежен, как вы думаете».
Американский O
ученый O
и O
исследователь O
ИИ B-TERM
Гэри B-TERM
Маркус I-TERM
еще O
до O
появления O
в O
сети O
откровений O
Лемуана B-TERM
опубликовал O
на O
портале O
Scientific B-TERM
American I-TERM
материал O
под O
названием O
« B-TERM
Общий I-TERM
ИИ I-TERM
не I-TERM
так I-TERM
неизбежен I-TERM
, I-TERM
как I-TERM
вы I-TERM
думаете I-TERM
» I-TERM
. O

# text =  В частности, DALL-E 2 от OpenAI провалил тест на различение изображений астронавтов, едущих на лошадях, перепутав их с лошадьми, оседлавшими астронавтов.
В O
частности O
, O
DALL B-TERM
- I-TERM
E I-TERM
2 I-TERM
от O
OpenAI B-TERM
провалил O
тест O
на O
различение O
изображений O
астронавтов O
, O
едущих O
на O
лошадях O
, O
перепутав O
их O
с O
лошадьми O
, O
оседлавшими O
астронавтов O
. O

# text =  «Сбер» представил mGPT — версию нейросети GPT-3, способную генерировать тексты на 61 языке Open source *Machine learning *Artificial Intelligence IT-companies       
« O
Сбер B-TERM
» O
представил O
mGPT B-TERM
— O
версию O
нейросети O
GPT-3 B-TERM
, O
способную O
генерировать O
тексты O
на O
61 O
языке O

# text =  21 апреля 2022 года команда разработчиков SberDevices представила многоязычную версию нейросети GPT-3 под названием mGPT.
21 B-TERM
апреля I-TERM
2022 I-TERM
года O
команда O
разработчиков O
SberDevices B-TERM
представила O
многоязычную O
версию O
нейросети O
GPT-3 B-TERM
под O
названием O
mGPT B-TERM
. O

# text =  «Сбер» рассказал, что модель mGPT может использоваться как просто для генерации текста, так и для решения различных задач в области обработки естественного языка на одном из поддерживаемых языков путем дообучения или в составе ансамблей моделей.
« O
Сбер B-TERM
» O
рассказал O
, O
что O
модель O
mGPT B-TERM
может O
использоваться O
как O
просто O
для O
генерации B-TERM
текста I-TERM
, O
так O
и O
для O
решения O
различных O
задач O
в O
области O
обработки B-TERM
естественного I-TERM
языка I-TERM
на O
одном O
из O
поддерживаемых O
языков O
путем O
дообучения O
или O
в O
составе O
ансамблей B-TERM
моделей I-TERM
. O

# text =  Разработчики уточнили, что модель mGPT показывает выдающиеся результаты на многих задачах few-shot и zero-shot learning: в этой области машинного обучения не требуется отдельно доучивать модель, достаточно сформулировать задачу текстом и привести несколько примеров, после чего mGPT научится выполнять новую задачу.
Разработчики O
уточнили O
, O
что O
модель O
mGPT B-TERM
показывает O
выдающиеся O
результаты O
на O
многих O
задачах O
few B-TERM
- I-TERM
shot I-TERM
и O
zero B-TERM
- I-TERM
shot I-TERM
learning I-TERM
: O
в O
этой O
области O
машинного O
обучения O
не O
требуется O
отдельно O
доучивать O
модель O
, O
достаточно O
сформулировать O
задачу O
текстом O
и O
привести O
несколько O
примеров O
, O
после O
чего O
mGPT B-TERM
научится O
выполнять O
новую O
задачу O
. O

# text =  Это может использоваться для того, чтобы научить автоматизированную систему отвечать на вопросы, определять эмоциональную окраску текста, извлекать из текста имена, фамилии, названия компаний и тому подобное.
Это O
может O
использоваться O
для O
того O
, O
чтобы O
научить O
автоматизированную O
систему O
отвечать O
на O
вопросы O
, O
определять B-TERM
эмоциональную I-TERM
окраску I-TERM
текста I-TERM
, O
извлекать O
из O
текста O
имена O
, O
фамилии O
, O
названия O
компаний O
и O
тому O
подобное O
. O

# text =  «Сбер» раскрыл, что модель mGPT может также использоваться как компонент различных речевых технологий — например, для улучшения качества распознавания речи, генерации сценариев диалоговых систем и других задачах.
« O
Сбер B-TERM
» O
раскрыл O
, O
что O
модель O
mGPT B-TERM
может O
также O
использоваться O
как O
компонент O
различных O
речевых B-TERM
технологий I-TERM
— O
например O
, O
для O
улучшения B-TERM
качества I-TERM
распознавания I-TERM
речи I-TERM
, O
генерации B-TERM
сценариев I-TERM
диалоговых I-TERM
систем I-TERM
и O
других O
задачах O
. O

# text =  Полный перечень языков, доступный в модели mGPT: азербайджанский, английский, арабский, армянский, африкаанс, баскский, башкирский, белорусский, бенгали, бирманский, болгарский, бурятский, венгерский, вьетнамский, голландский, греческий, грузинский, датский, иврит, индонезийский, испанский, итальянский, йоруба, казахский, калмыцкий, киргизский, китайский, корейский, латышский, литовский, малайский, малаялам, маратхи, молдавский, монгольский, немецкий, осетинский, персидский, польский, португальский, румынский, русский, суахили, таджикский, тайский, тамильский, татарский, телугу, тувинский, турецкий, туркменский, узбекский, украинский, урду, финский, французский, хинди, чувашский, шведский, якутский, японский.
Полный O
перечень O
языков O
, O
доступный O
в O
модели O
mGPT B-TERM
: O
азербайджанский B-TERM
, O
английский B-TERM
, O
арабский B-TERM
, O
армянский B-TERM
, O
африкаанс B-TERM
, O
баскский B-TERM
, O
башкирский B-TERM
, O
белорусский B-TERM
, O
бенгали B-TERM
, O
бирманский B-TERM
, O
болгарский B-TERM
, O
бурятский B-TERM
, O
венгерский B-TERM
, O
вьетнамский B-TERM
, O
голландский B-TERM
, O
греческий B-TERM
, O
грузинский B-TERM
, O
датский B-TERM
, O
иврит B-TERM
, O
индонезийский B-TERM
, O
испанский B-TERM
, O
итальянский B-TERM
, O
йоруба B-TERM
, O
казахский B-TERM
, O
калмыцкий B-TERM
, O
киргизский B-TERM
, O
китайский B-TERM
, O
корейский B-TERM
, O
латышский B-TERM
, O
литовский B-TERM
, O
малайский B-TERM
, O
малаялам B-TERM
, O
маратхи B-TERM
, O
молдавский B-TERM
, O
монгольский B-TERM
, O
немецкий B-TERM
, O
осетинский B-TERM
, O
персидский B-TERM
, O
польский B-TERM
, O
португальский B-TERM
, O
румынский B-TERM
, O
русский B-TERM
, O
суахили B-TERM
, O
таджикский B-TERM
, O
тайский B-TERM
, O
тамильский B-TERM
, O
татарский B-TERM
, O
телугу B-TERM
, O
тувинский B-TERM
, O
турецкий B-TERM
, O
туркменский B-TERM
, O
узбекский B-TERM
, O
украинский B-TERM
, O
урду B-TERM
, O
финский B-TERM
, O
французский B-TERM
, O
хинди B-TERM
, O
чувашский B-TERM
, O
шведский B-TERM
, O
якутский B-TERM
, O
японский B-TERM
. O

# text =  В 2020 году «Сбер» представил русскоязычную версию нейросети GPT-3, именно она используется в двух виртуальных ассистентах семейства «Салют» от «Сбера».
В O
2020 B-TERM
году O
« O
Сбер B-TERM
» O
представил O
русскоязычную B-TERM
версию O
нейросети O
GPT-3 B-TERM
, O
именно O
она O
используется O
в O
двух O
виртуальных O
ассистентах O
семейства O
« O
Салют B-TERM
» O
от O
« O
Сбера B-TERM
» O
. O

# text =  Русскоязычная версия GPT-3, разработанная «Сбером», доступна на платформе SmartMarket.
Русскоязычная O
версия O
GPT-3 B-TERM
, O
разработанная O
« O
Сбером B-TERM
» O
, O
доступна O
на O
платформе O
SmartMarket B-TERM
. O

# text =  В ноябре 2021 года «Сбер» обучил нейросеть ruGPT-3 автоматически писать код и назвал эту функцию JARVIS.
В O
ноябре O
2021 B-TERM
года O
« O
Сбер B-TERM
» O
обучил O
нейросеть O
ruGPT-3 B-TERM
автоматически O
писать O
код O
и O
назвал O
эту O
функцию O
JARVIS B-TERM
. O

# text =  Не заблокированы: Sber AI — на GitHub; ruDALL-E — на GitHub; Russian GPT-3 models — GitHub.
Не O
заблокированы O
: O

Sber B-TERM
AI I-TERM
— O
на O
GitHub B-TERM
; O
ruDALL B-TERM
- I-TERM
E I-TERM
— O
на O
GitHub B-TERM
; O
Russian B-TERM
GPT-3 I-TERM
models I-TERM
— O
GitHub B-TERM
. O

# text =  Заблокированы: большая часть ссылок на открытом портале Open Source от разработчиков «Сбера»; SberDevices; Sberbank AI Lab; Open source software developed by Sberbank-Technology.
Заблокированы O
: O
большая O
часть O
ссылок O
на O
открытом O
портале O
Open B-TERM
Source I-TERM
от O
разработчиков O
« O
Сбера B-TERM
» O
; O
SberDevices B-TERM
; O
Sberbank B-TERM
AI I-TERM
Lab I-TERM
; O
Open B-TERM
source I-TERM
software I-TERM
developed I-TERM
by I-TERM
Sberbank I-TERM
- I-TERM
Technology I-TERMO
. O

# text =  В этой статье мы расскажем о методе Propensity Score Adjustment, который применим для коррекции смещений и улучшения данных, полученных на онлайн-панелях.
В O
этой O
статье O
мы O
расскажем O
о O
методе B-TERM
Propensity I-TERM
Score I-TERM
Adjustment I-TERM
, O
который O
применим O
для O
коррекции O
смещений O
и O
улучшения O
данных O
, O
полученных O
на O
онлайн O
- O
панелях O
. O

# text =  Итоговые коэффициенты, корректирующие смещение онлайн-выборки, можно рассчитать по методу Хорвица-Томпсона.
Итоговые O
коэффициенты O
, O
корректирующие O
смещение O
онлайн O
- O
выборки O
, O
можно O
рассчитать O
по O
методу B-TERM
Хорвица I-TERM
- I-TERM
Томпсона I-TERM
. O

# text =  Взвешивание (Weighting) - метод предназначен для коррекции известных перекосов выборок по социально-демографическим атрибутам.
Взвешивание B-TERM
( O
Weighting B-TERM
) O
метод O
предназначен O
для O
коррекции O
известных O
перекосов O
выборок O
по O
социально O
- O
демографическим O
атрибутам O
. O

# text =  Авторы оригинального исследования Pew Research рекомендуют использовать для корректировки онлайн-опросов модели случайных лесов (random forest).
Авторы O
оригинального O
исследования O
Pew B-TERM
Research I-TERM
рекомендуют O
использовать O
для O
корректировки O
онлайн O
- O
опросов O
модели O
случайных B-TERM
лесов I-TERM
( O
random B-TERM
forest I-TERM
) O
. O


# text =  Сейчас стандарт коррекции онлайн-выборок находится на стадии обсуждения и разработки и метод Propensity Score Adjustment, который мы рассмотрели, может стать общепринятым способом коррекции онлайн-панелей.
Сейчас O
стандарт O
коррекции O
онлайн O
- O
выборок O
находится O
на O
стадии O
обсуждения O
и O
разработки O
и O
метод B-TERM
Propensity I-TERM
Score I-TERM
Adjustment I-TERM
, O
который O
мы O
рассмотрели O
, O
может O
стать O
общепринятым O
способом O
коррекции O
онлайн O
- O
панелей O
. O

# text =  В данной статье мы будем использовать модель трансформера для бинарной классификации текста.
В O
данной O
статье O
мы O
будем O
использовать O
модель B-TERM
трансформера I-TERM
для O
бинарной B-TERM
классификации I-TERM
текста I-TERM
. O

# text =  Самая простая и популярная связка – TF-IDF + линейная модель.
Самая O
простая O
и O
популярная O
связка O
– O
TF B-TERM
- I-TERM
IDF I-TERM
+ O
линейная B-TERM
модель I-TERM
. O

# text =  В случае с BERT можно (даже нужно) опустить препроцессинг и сразу перейти к токенизации и обучению.
В O
случае O
с O
BERT B-TERM
можно O
( O
даже O
нужно O
) O
опустить O
препроцессинг B-TERM
и O
сразу O
перейти O
к O
токенизации B-TERM
и O
обучению O
. O

# text =  Необходимо обучить модель находить обращения с жалобой на сотрудника или другими словами – бинарная классификация.
Необходимо O
обучить O
модель O
находить O
обращения O
с O
жалобой O
на O
сотрудника O
или O
другими O
словами O
– O
бинарная B-TERM
классификация I-TERM
. O

# text = Для решения описанной задачи используется модель от DeepPavlov rubert-base-cased-sentence.
Для O
решения O
описанной O
задачи O
используется O
модель O
от O
DeepPavlov B-TERM
rubert I-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
- I-TERM
sentence I-TERM
. O

# text =  На выходе мы получаем метрику f1 = 0.91 Посмотрим, как модель классифицировала данные показанные в начале статьи.
На O
выходе O
мы O
получаем O
метрику O
f1 B-TERM
= O
0.91 B-TERM
Посмотрим O
, O
как O
модель O
классифицировала O
данные O
показанные O
в O
начале O
статьи O
. O

# text =  Обученные модели можно найти на сайтах HuggingFace и DeepPavlov.
Обученные O
модели O
можно O
найти O
на O
сайтах O
HuggingFace B-TERM
и O
DeepPavlov B-TERM
. O

# text =  Соответственно, мы приходим к стандартной задаче Machine Learning (ML) – «многоклассовая классификация».
Соответственно O
, O
мы O
приходим O
к O
стандартной O
задаче O
Machine B-TERM
Learning I-TERM
( O
ML B-TERM
) O
– O
« O
многоклассовая B-TERM
классификация I-TERM
» O
. O

# text =  В результате данного анализа решается задача — сбор сводной аналитики по организации.
В O
результате O
данного O
анализа O
решается O
задача O
— O
сбор B-TERM
сводной I-TERM
аналитики I-TERM
по I-TERM
организации I-TERM
. O

# text =  В случае многоклассовой классификации число классов должно быть более 2 и может достигать даже многих тысяч.
В O
случае O
многоклассовой B-TERM
классификации I-TERM
число O
классов O
должно O
быть O
более O
2 O
и O
может O
достигать O
даже O
многих O
тысяч O
. O

# text =  Во-вторых, такого разброса тематик, связанных с техническими текстами, у нас еще не было: нейросети, переиспользование контента, автоматическое тестирование документации, встраивание текста в интерфейс, мастерство технических коммуникаций, построение процессов перевода и принципы написания документов с расчетом на их последующую локализацию.
Во O
- O
вторых O
, O
такого O
разброса O
тематик O
, O
связанных O
с O
техническими B-TERM
текстами I-TERM
, O
у O
нас O
еще O
не O
было O
: O
нейросети O
, O
переиспользование B-TERM
контента I-TERM
, O
автоматическое B-TERM
тестирование I-TERM
документации I-TERM
, O
встраивание B-TERM
текста I-TERM
в I-TERM
интерфейс I-TERM
, O
мастерство B-TERM
технических I-TERM
коммуникаций I-TERM
, O
построение B-TERM
процессов I-TERM
перевода I-TERM
и O
принципы B-TERM
написания I-TERM
документов I-TERM
с O
расчетом O
на O
их O
последующую O
локализацию O
. O

# text =  Зачастую суммаризация предполагает работу с большими генеративными текстовыми моделями, куда надо «положить» все отзывы.
Зачастую O
суммаризация B-TERM
предполагает O
работу O
с O
большими O
генеративными B-TERM
текстовыми I-TERM
моделями I-TERM
, O
куда O
надо O
« O
положить O
» O
все O
отзывы O
. O

# text =  То есть какие аспекты искать; выделять эти аспекты в отзывах; оценивать тональность высказываний.
То O
есть O
какие O
аспекты O
искать O
; O
выделять O
эти O
аспекты O
в O
отзывах O
; O
оценивать O
тональность B-TERM
высказываний I-TERM
. O

# text =  Мы начали со внутреннего инструмента Яндекса — библиотеки регулярных выражений под названием Remorph.
Мы O
начали O
со O
внутреннего O
инструмента O
Яндекса B-TERM
— O
библиотеки B-TERM
регулярных I-TERM
выражений I-TERM
под O
названием O
Remorph B-TERM
. O

# text =  Исследователи Массачусетского технологического университета разработали систему искусственного интеллекта, которая способна переписывать устаревшие предложения в статьях «Википедии».
Исследователи O
Массачусетского B-TERM
технологического I-TERM
университета I-TERM
разработали O
систему B-TERM
искусственного I-TERM
интеллекта I-TERM
, O
которая O
способна O
переписывать O
устаревшие O
предложения O
в O
статьях O
« O
Википедии B-TERM
» O
. O

# text =  Расширение статей, серьезные переписывания или другие рутинные изменения, такие как обновление номеров, дат, имен и местоположений в настоящее время добровольно выполняются пользователями из разных стран.
Расширение B-TERM
статей I-TERM
, O
серьезные O
переписывания O
или O
другие O
рутинные O
изменения O
, O
такие O
как O
обновление O
номеров O
, O
дат O
, O
имен O
и O
местоположений O
в O
настоящее O
время O
добровольно O
выполняются O
пользователями O
из O
разных O
стран O
. O

# text =  Если она видит какие-либо противоречия между этими двумя высказываниями, то использует «маску нейтральности», чтобы определить те противоречивые слова, которые нужно удалить, и те, которые обязательно нужно сохранить.
Если O
она O
видит O
какие O
- O
либо O
противоречия O
между O
этими O
двумя O
высказываниями O
, O
то O
использует O
« O
маску B-TERM
нейтральности I-TERM
» O
, O
чтобы O
определить O
те O
противоречивые O
слова O
, O
которые O
нужно O
удалить O
, O
и O
те O
, O
которые O
обязательно O
нужно O
сохранить O
. O

# text =  Отмечается, что систему также можно использовать для дополнения наборов данных, предназначенных для обучения детекторов фейкньюс, что потенциально снижает предвзятость и повышает точность информации.
Отмечается O
, O
что O
систему O
также O
можно O
использовать O
для O
дополнения B-TERM
наборов I-TERM
данных I-TERM
, O
предназначенных O
для O
обучения B-TERM
детекторов I-TERM
фейкньюс I-TERM
, O
что O
потенциально O
снижает O
предвзятость O
и O
повышает O
точность O
информации O
. O

# text =  Наше выработанное решение – обучить нейронную сеть, которая способна по тексту обращения автоматически распознавать заранее ранжированные по классам проблемы, извлекать сущность (номер заказа и телефон клиента) и по определённым классам сделать автоматизацию решения.
Наше O
выработанное O
решение O
– O
обучить O
нейронную O
сеть O
, O
которая O
способна O
по O
тексту O
обращения O
автоматически B-TERM
распознавать I-TERM
заранее I-TERM
ранжированные I-TERM
по I-TERM
классам I-TERM
проблемы I-TERM
, O
извлекать O
сущность O
( O
номер O
заказа O
и O
телефон O
клиента O
) O
и O
по O
определённым O
классам O
сделать O
автоматизацию O
решения O
. O

# text =  На самом деле уже существуют продвинутые и проверенные методы ее обработки, использующие нейронные сети, с распознаванием смысла и контекста – BERT (Bidirectional Encoder Representations from Transformers).
На O
самом O
деле O
уже O
существуют O
продвинутые O
и O
проверенные O
методы O
ее O
обработки O
, O
использующие O
нейронные B-Method
сети I-Method
, O
с O
распознаванием O
смысла O
и O
контекста O
– O
BERT B-TERM
( O
Bidirectional B-TERM
Encoder I-TERM
Representations I-TERM
from I-TERM
Transformers I-TERM
) O
. O

# text =  Перед тем как выбрать нейронные сети, мы протестировали несколько более стандартных архитектур, случайные леса и бустинг.
Архитектура O
модели O
Перед O
тем O
как O
выбрать O
нейронные O
сети O
, O
мы O
протестировали O
несколько O
более O
стандартных O
архитектур O
, O
случайные B-TERM
леса I-TERM
и O
бустинг B-TERM
. O

# text =  Эта модель была обучена на огромном корпусе русскоязычного текста с двумя задачами – предсказать замаскированное слово в предложениях и предсказать, если одно из предложений следует по смыслу за вторым.
Эта O
модель O
была O
обучена O
на O
огромном O
корпусе O
русскоязычного O
текста O
с O
двумя O
задачами O
– O
предсказать B-TERM
замаскированное I-TERM
слово I-TERM
в I-TERM
предложениях I-TERM
и O
предсказать O
, O
если O
одно O
из O
предложений O
следует O
по O
смыслу O
за O
вторым O
. O

# text =  Наша задача – дообучить эту языковую модель для нашего приложения (одна модель для классификации и одна – для извлечения сущности).
Наша O
задача O
– O
дообучить O
эту O
языковую O
модель O
для O
нашего O
приложения O
( O
одна O
модель O
для O
классификации B-TERM
и O
одна O
– O
для O
извлечения B-TERM
сущности I-TERM
) O
. O

# text =  результат первой модели – точность 77%
результат O
первой O
модели O
– O
точность B-TERM
77% B-TERM

# text =  Чтобы определить, какие ещё есть потенциальные классы, мы повели так называемое тематическое моделирование, используя несколько подходов: начиная от пробалистических моделей (латентное распределение Дирихле, ARTM) и всё те же нейронные сети (BERT).
Чтобы O
определить O
, O
какие O
ещё O
есть O
потенциальные O
классы O
, O
мы O
повели O
так O
называемое O
тематическое B-TERM
моделирование I-TERM
, O
используя O
несколько O
подходов O
: O
начиная O
от O
пробалистических B-TERM
моделей I-TERM
( O
латентное B-TERM
распределение I-TERM
Дирихле I-TERM
, O
ARTM B-TERM
) O
и O
всё O
те O
же O
нейронные B-TERM
сети I-TERM
( O
BERT B-TERM
) O
. O

# text =  Теперь нам нужно было использовать некоторые технические способы, чтобы сделать максимально высоким качество модели, которая на новых классах давала точность 72%.
Теперь O
нам O
нужно O
было O
использовать O
некоторые O
технические O
способы O
, O
чтобы O
сделать O
максимально O
высоким O
качество O
модели O
, O
которая O
на O
новых O
классах O
давала O
точность B-TERM
72 B-TERM
% I-TERM
. O

# text =  Второе, мы стандартно провели экстенсивный тюнинг гиперпараметров и изменили нашу метрику с точности на F1, чтобы ставить больше акцента на точность по каждому классу, так как общая точность предвзято относится к доминирующим классам.
Второе O
, O
мы O
стандартно O
провели O
экстенсивный B-TERM
тюнинг I-TERM
гиперпараметров I-TERM
и O
изменили O
нашу O
метрику O
с O
точности B-TERM
на O
F1 B-TERM
, O
чтобы O
ставить O
больше O
акцента O
на O
точность B-TERM
по O
каждому O
классу O
, O
так O
как O
общая O
точность B-TERM
предвзято O
относится O
к O
доминирующим O
классам O
. O

# text =  Изменение оптимизирующей метрики на F1 позволило алгоритму обучения дольше обучаться, так как почти на каждом этапе происходило улучшение по F1, когда метрика была точность, мы достигали плато гораздо быстрее.
Изменение O
оптимизирующей O
метрики O
на O
F1 B-TERM
позволило O
алгоритму O
обучения O
дольше O
обучаться O
, O
так O
как O
почти O
на O
каждом O
этапе O
происходило O
улучшение O
по O
F1 B-TERM
, O
когда O
метрика O
была O
точность B-TERM
, O
мы O
достигали O
плато O
гораздо O
быстрее O
. O

# text =  Изначально на этапе MVP (minimum viable product) мы применяли регулярные выражения для извлечения сущности.
Изначально O
на O
этапе O
MVP B-TERM
( O
minimum B-TERM
viable I-TERM
product I-TERM
) O
мы O
применяли O
регулярные B-TERM
выражения I-TERM
для O
извлечения B-TERM
сущности I-TERM
. O

# text =  Протестировав поведение модели на продовских данных, мы обнаружили, что точность извлечения была около 50%.
Протестировав O
поведение O
модели O
на O
продовских O
данных O
, O
мы O
обнаружили O
, O
что O
точность B-TERM
извлечения O
была O
около O
50 B-TERM
% I-TERM
. O

# text =  Мы поняли, что даже извлечение сущности зависит от контекста, и решили использовать BERT.
Мы O
поняли O
, O
что O
даже O
извлечение B-TERM
сущности I-TERM
зависит O
от O
контекста O
, O
и O
решили O
использовать O
BERT B-TERM
. O

# text =  На вход необходимо представить размеченные данные с маркировкой BIO (beginning, intermediate, O – пустота).
На O
вход O
необходимо O
представить O
размеченные O
данные O
с O
маркировкой B-TERM
BIO B-TERM
( O
beginning O
, O
intermediate O
, O
O O
– O
пустота O
) O
. O

# text =  Мы производили разметку 800 обращений на DataTurcks: Точность подхода BERT – 94% на этапе обучения, она валидирована на тестовых данных.
Мы O
производили O
разметку B-TERM
800 O
обращений O
на O
DataTurcks B-TERM
: O
Точность B-TERM
подхода O
BERT O
– O
94 B-TERM
% I-TERM
на O
этапе O
обучения O
, O
она O
валидирована O
на O
тестовых O
данных O
. O

# text =  Это постобработка увеличила точность до 98%.
Это O
постобработка O
увеличила O
точность B-TERM
до O
98% B-TERM

# text =  В иностранной литературе можно встретить термин Continuous Learning (CL), который объединяет различные методы использования новых данных для поддержания эффективности моделей.
В O
иностранной O
литературе O
можно O
встретить O
термин O
Continuous B-TERM
Learning I-TERM
( O
CL B-TERM
) O
, O
который O
объединяет O
различные O
методы O
использования O
новых O
данных O
для O
поддержания O
эффективности O
моделей O
. O

# text =  Методы CL были положены в основу пайплайна переобучения.
Методы O
CL B-TERM
были O
положены O
в O
основу O
пайплайна O
переобучения O
. O

# text =  Для решения этой проблемы требуется архитектура, которая позволяет GPT-3 анализировать содержание письма и оценивать, какая информация актуальна для ответа.
Для O
решения O
этой O
проблемы O
требуется O
архитектура O
, O
которая O
позволяет O
GPT-3 B-TERM
анализировать B-TERM
содержание I-TERM
письма I-TERM
и O
оценивать B-TERM
, I-TERM
какая I-TERM
информация I-TERM
актуальна I-TERM
для O
ответа O
. O

# text =  OpenAI представила модель машинного обучения GPT-3, обученную на 175 млрд параметров, в июне 2020 года.
OpenAI B-TERM
представила O
модель O
машинного O
обучения O
GPT-3 B-TERM
, O
обученную O
на O
175 O
млрд O
параметров O
, O
в O
июне B-TERM
2020 I-TERM
года I-TERM
. O

# text =  В отличие от предшественников GPT-2 и GPT-1 ее исходный код или обучающий набор данных решили не открывать.
В O
отличие O
от O
предшественников O
GPT-2 B-TERM
и O
GPT-1 B-TERM
ее O
исходный O
код O
или O
обучающий O
набор O
данных O
решили O
не O
открывать O
. O

# text =  Модель уже попытались применить в медицинской сфере для общения с пациентами, но результаты эксперимента оказались неутешительными.
Модель O
уже O
попытались O
применить O
в O
медицинской B-TERM
сфере I-TERM
для O
общения O
с O
пациентами O
, O
но O
результаты O
эксперимента O
оказались O
неутешительными O
. O

# text = Между тем создатели проекта GPT-Neo от EleutherAI решили воссоздать аналог GPT-3, но с открытым исходным кодом.
Между O
тем O
создатели O
проекта O
GPT B-TERM
- B-TERM
Neo B-TERM
от O
EleutherAI B-TERM
решили O
воссоздать O
аналог O
GPT-3 B-TERM
, O
но O
с O
открытым O
исходным O
кодом O
. O

# text =  Однако только сейчас мы немного приблизились к сюжетам фантастических фильмов: можем попросить Алису убавить громкость, Google Assistant — заказать такси или Siri — завести будильник.
Однако O
только O
сейчас O
мы O
немного O
приблизились O
к O
сюжетам O
фантастических O
фильмов O
: O
можем O
попросить O
Алису B-TERM
убавить B-TERM
громкость I-TERM
, O
Google B-TERM
Assistant I-TERM
— O
заказать B-TERM
такси I-TERM
или O
Siri B-TERM
— O
завести B-TERM
будильник I-TERM
. O

# text =  Технологии языкового процессинга востребованы в разработках, связанных с построением искусственного интеллекта: в поисковых системах, для извлечения фактов, оценки тональности текста, машинного перевода и диалога.
Технологии B-TERM
языкового I-TERM
процессинга I-TERM
востребованы O
в O
разработках O
, O
связанных O
с O
построением O
искусственного O
интеллекта O
: O
в O
поисковых B-TERM
системах I-TERM
, O
для O
извлечения B-TERM
фактов I-TERM
, O
оценки B-TERM
тональности I-TERM
текста I-TERM
, O
машинного B-TERM
перевода I-TERM
и O
диалога O
. O

# text =  Первые разговоры об обработке естественного языка компьютером начались еще в 30-е годы XX-го века с философских рассуждений Айера — он предлагал отличать разумного человека от глупой машины с помощью эмпирического теста.
Первые O
разговоры O
об O
обработке B-TERM
естественного I-TERM
языка I-TERM
компьютером O
начались O
еще O
в O
30-е B-TERM
годы I-TERM
XX I-TERM
- I-TERM
го I-TERM
века I-TERM
с O
философских O
рассуждений O
Айера B-TERM
— O
он O
предлагал O
отличать O
разумного O
человека O
от O
глупой O
машины O
с O
помощью O
эмпирического O
теста O
. O

# text =  В 1950 году Алан Тьюринг в философском журнале Mind предложил такой тест, где судья должен определить, с кем он ведет диалог: с человеком или компьютером.
В O
1950 B-TERM
году O
Алан B-TERM
Тьюринг I-TERM
в O
философском O
журнале O
Mind B-TERM
предложил O
такой O
тест B-TERM
, O
где O
судья O
должен O
определить O
, O
с O
кем O
он O
ведет O
диалог O
: O
с O
человеком O
или O
компьютером O
. O

# text =  В 1954 году Джорджтаунский университет совместно с компанией IBM продемонстрировали программу машинного перевода с русского на английский, которая работала на базе словаря из 250 слов и набора из 6 грамматических правил.
В O
1954 B-TERM
году O
Джорджтаунский B-TERM
университет I-TERM
совместно O
с O
компанией O
IBM B-TERM
продемонстрировали O
программу B-TERM
машинного I-TERM
перевода I-TERM
с O
русского B-TERM
на O
английский B-TERM
, O
которая O
работала O
на O
базе O
словаря O
из O
250 O
слов O
и O
набора O
из O
6 O
грамматических O
правил O
. O

# text =  Параллельно с попытками научить компьютер переводить текст, ученые и целые университеты думали над созданием робота, способного имитировать речевое поведение человека.
Параллельно O
с O
попытками O
научить O
компьютер O
переводить O
текст O
, O
ученые O
и O
целые O
университеты O
думали O
над O
созданием O
робота O
, O
способного O
имитировать B-TERM
речевое I-TERM
поведение I-TERM
человека I-TERM
. O

# text =  Первой успешной реализацией чат-бота стал виртуальный собеседник ELIZA, написанный в 1966 году Джозефом Вейценбаумом.
Первой O
успешной O
реализацией O
чат O
- O
бота O
стал O
виртуальный B-TERM
собеседник I-TERM
ELIZA I-TERM
, O
написанный O
в O
1966 B-TERM
году I-TERM
Джозефом B-TERM
Вейценбаумом I-TERM
. O

# text =  Элиза пародировала поведение психотерапевта, выделяя значимые слова из фразы собеседника и задавая встречный вопрос.
Элиза B-TERM
пародировала O
поведение O
психотерапевта O
, O
выделяя O
значимые O
слова O
из O
фразы O
собеседника O
и O
задавая O
встречный O
вопрос O
. O

# text =  Можно считать, что это был первый чат-бот, построенный на правилах (rule-based bot), и он положил начало целому классу таких систем.
Можно O
считать O
, O
что O
это O
был O
первый O
чат O
- O
бот O
, O
построенный O
на O
правилах O
( O
rule B-TERM
- I-TERM
based I-TERM
bot I-TERM
) O
, O
и O
он O
положил O
начало O
целому O
классу O
таких O
систем O
. O

# text =  Без Элизы не появились бы такие программы-собеседники, как Cleverbot, WeChat Xiaoice, Eugene Goostman — формально прошедший тест Тьюринга в 2014 году, — и даже Siri, Jarvis и Alexa.
Без O
Элизы B-TERM
не O
появились O
бы O
такие O
программы O
- O
собеседники O
, O
как O
Cleverbot B-TERM
, O
WeChat B-TERM
Xiaoice I-TERM
, O
Eugene B-TERM
Goostman I-TERM
— O
формально O
прошедший O
тест B-TERM
Тьюринга I-TERM
в O
2014 B-TERM
году O
, O
— O
и O
даже O
Siri B-TERM
, O
Jarvis B-TERM
и O
Alexa B-TERM
. O

# text =  В 1968 году Терри Виноградом на языке LISP была разработана программа SHRDLU.
В O
1968 B-TERM
году O
Терри B-TERM
Виноградом I-TERM
на O
языке O
LISP B-TERM
была O
разработана O
программа O
SHRDLU B-TERM
. O

# text =  Следующим шагом в развитии чат-ботов стала программа A.L.I.C.E., для которой Ричард Уоллес разработал специальный язык разметки — AIML (англ. Artificial Intelligence Markup Language).
Следующим O
шагом O
в O
развитии O
чат O
- O
ботов O
стала O
программа O
A.L.I.C.E. B-TERM
, O
для O
которой O
Ричард B-TERM
Уоллес I-TERM
разработал O
специальный O
язык O
разметки B-TERM
— O
AIML B-TERM
( O
англ O
. O
Artificial B-TERM
Intelligence I-TERM
Markup I-TERM
Language I-TERM
) O
. O

# text =  Разговоры о нейронных сетях и глубоком обучении ходили уже в 90-е годы, а первый нейрокомпьютер «Марк-1» появился вообще в 1958 году.
Разговоры O
о O
нейронных B-TERM
сетях I-TERM
и O
глубоком O
обучении O
ходили O
уже O
в O
90-е O
годы O
, O
а O
первый O
нейрокомпьютер B-TERM
« O
Марк-1 B-TERM
» O
появился O
вообще O
в O
1958 B-TERM
году I-TERM
. O
# text =  1970 г. Машинный перевод на основе правил (англ. RBMT) был первой попыткой научить машину переводить.
1970 O
г. O
Машинный B-TERM
перевод I-TERM
на I-TERM
основе I-TERM
правил I-TERM
( O
англ O
RBMT B-TERM
. O
) O
был O
первой O
попыткой O
научить O
машину O
переводить O
. O

# text =  1984 г. Машинный перевод на основе примеров (англ. EBMT) был способен переводить даже совсем не похожие друг на друга языки, где задавать какие-то правила было бесполезно.
1984 B-TERM
г. I-TERM
Машинный B-TERM
перевод I-TERM
на I-TERM
основе I-TERM
примеров I-TERM
( O
англ O
. O
EBMT B-TERM
) O
был O
способен O
переводить O
даже O
совсем O
не O
похожие O
друг O
на O
друга O
языки O
, O
где O
задавать O
какие O
- O
то O
правила O
было O
бесполезно O
. O

# text =  1990 г. Статистический машинный перевод (англ. SMT) в эпоху развития интернета позволил использовать не только готовые языковые корпуса, но даже книги и вольно переведенные статьи.
1990 B-TERM
г. O
Статистический B-TERM
машинный I-TERM
перевод I-TERM
( O
англ I-TERM
. O 
SMT B-TERM
) O
в O
эпоху O
развития O
интернета O
позволил O
использовать O
не O
только O
готовые O
языковые O
корпуса O
, O
но O
даже O
книги O
и O
вольно O
переведенные O
статьи O
. O

# text =  Статистические методы и сейчас активно используются в языковом процессинге.
Статистические B-TERM
методы I-TERM
и O
сейчас O
активно O
используются O
в O
языковом B-TERM
процессинге I-TERM
. O

# text =  По мере развития обработки естественного языка множество задач решалось классическими статистическими методами и множеством правил, однако проблему нечеткости и неоднозначности в языке это не решало.
По O
мере O
развития O
обработки B-TERM
естественного I-TERM
языка I-TERM
множество O
задач O
решалось O
классическими O
статистическими B-TERM
методами I-TERM
и O
множеством O
правил O
, O
однако O
проблему B-TERM
нечеткости I-TERM
и I-TERM
неоднозначности I-TERM
в O
языке O
это O
не O
решало O
. O

# text =  Так родился статистический метод анализа текста word2vec (англ. Word to vector).
Так O
родился O
статистический B-TERM
метод I-TERM
анализа I-TERM
текста I-TERM
word2vec B-TERM
( O
англ O
. O
Word B-TERM
to I-TERM
vector I-TERM
) O
. O

# text =  Под эти критерии отлично подходит рекуррентная нейронная сеть (RNN), однако по мере увеличения расстояния между связанными частями текста необходимо увеличивать и размер RNN, из-за чего падает качество обработки информации.
Под O
эти O
критерии O
отлично O
подходит O
рекуррентная B-TERM
нейронная I-TERM
сеть I-TERM
( O
RNN B-TERM
) O
, O
однако O
по O
мере O
увеличения O
расстояния O
между O
связанными O
частями O
текста O
необходимо O
увеличивать O
и O
размер O
RNN B-TERM
, O
из O
- O
за O
чего O
падает O
качество O
обработки O
информации O
. O

# text =  Эту проблему решает сеть LSTM (англ. Long short-term memory).
Эту O
проблему O
решает O
сеть O
LSTM B-TERM
( O
англ O
. O 
Long B-TERM
short I-TERM
- I-TERM
term I-TERM
memory I-TERM
) O
. O

# text =  Если говорить о языке Python, который часто используется для анализа данных, то это NLTK и Spacy.
Если O
говорить O
о O
языке O
Python B-TERM
, O
который O
часто O
используется O
для O
анализа B-TERM
данных I-TERM
, O
то O
это O
NLTK B-TERM
и O
Spacy B-TERM
. O

# text =  Крупные компании также принимают участие в разработке библиотек для NLP, как например NLP Architect от Intel или PyTorch от исследователей из Facebook и Uber.
Крупные O
компании O
также O
принимают O
участие O
в O
разработке O
библиотек O
для O
NLP B-TERM
, O
как O
например O
NLP B-TERM
Architect I-TERM
от O
Intel B-TERM
или O
PyTorch B-TERM
от O
исследователей O
из O
Facebook B-TERM
и O
Uber B-TERM
. O

# text =  Направление b2c не единственное, где можно применять чат-ботов.
Направление O
b2c B-TERM
не O
единственное O
, O
где O
можно O
применять O
чат O
- O
ботов O
. O

# text =  Участникам предлагалось определить потенциальные заболевания коров по реальным жалобам людей из открытых источников, а также научиться выделять из текстов симптомы заболеваний (NER - Named Entity Recognition).
Участникам O
предлагалось O
определить O
потенциальные O
заболевания O
коров O
по O
реальным O
жалобам O
людей O
из O
открытых O
источников O
, O
а O
также O
научиться O
выделять O
из O
текстов O
симптомы O
заболеваний O
( O
NER B-TERM
- O
Named B-TERM
Entity I-TERM
Recognition I-TERM
) O
. O

# text =  Эта статья будет интересна не только тем, кто специализируется в NLP (Natural Language Processing), но и начинающим исследователям данных.
Эта O
статья O
будет O
интересна O
не O
только O
тем O
, O
кто O
специализируется O
в O
NLP B-TERM
( O
Natural B-TERM
Language I-TERM
Processing I-TERM
) O
, O
но O
и O
начинающим O
исследователям O
данных O
. O

# text =  Спаны - это участки текста, которые содержат в себе определенный смысл.
Спаны B-TERM
  O
- O
  O
это O
участки O
текста B-TERM
, O
которые O
содержат O
в O
себе O
определенный O
смысл O
. O

# text =  Программа для разметки YEDDA и процесс разметки.
Программа O
для O
разметки B-TERM
YEDDA B-TERM
и O
процесс O
разметки O
. O

# text =  Так как задача является составной, то и метрика состояла из двух компонентов с весом 0.8 для задачи классификации и 0.2 для задачи NER.
Так O
как O
задача O
является O
составной O
, O
то O
и O
метрика O
состояла O
из O
двух O
компонентов O
с O
весом O
0.8 B-TERM
для O
задачи O
классификации B-TERM
и O
0.2 B-TERM
для O
задачи O
NER B-TERM
. O

# text =  В задаче классификации использовался logloss, вычисляемый как среднее значение метрики sklearn.metrics.log_loss по классам болезней.
В O
задаче O
классификации B-TERM
использовался O
logloss B-TERM
, O
вычисляемый O
как O
среднее O
значение O
метрики O
sklearn.metrics.log_loss B-TERM
по O
классам O
болезней O
. O

# text = В задаче NER использовался span-based F1-score, рассчитываемый следующим образом: для каждого текста берутся предсказанные индексы начала и конца размеченных признаков болезни, по ним выделяются из текста токены (отдельные слова, разделенные пробелом) и сравниваются с истинной (экспертной) разметкой.
В O
задаче O
NER B-TERM
использовался O
span B-TERM
- I-TERM
based I-TERM
F1-score I-TERM
, O
рассчитываемый O
следующим O
образом O
: O
для O
каждого O
текста B-TERM
берутся O
предсказанные O
индексы B-TERM
начала O
и O
конца O
размеченных O
признаков O
болезни O
, O
по O
ним O
выделяются O
из O
текста O
токены B-TERM
( O
отдельные O
слова B-TERM
, O
разделенные O
пробелом O
) O
и O
сравниваются O
с O
истинной O
( O
экспертной O
) O
разметкой B-TERM
. O

# text =  Код для подсчета метрики span-based F1-score.
Код O
для O
подсчета O
метрики O
span B-TERM
- I-TERM
based I-TERM
F1-score I-TERM
.

# text =  Этим решением стало использование классификатора CatBoost, который прямо из коробки может обрабатывать текстовые фичи.
Этим O
решением O
стало O
использование O
классификатора O
CatBoost B-TERM
, O
который O
прямо O
из O
коробки O
может O
обрабатывать O
текстовые O
фичи O
. O

# text =  Решение для задачи распознавания симптомов мы давать не стали, чтобы участники Data Science чемпионата могли покреативить.
Решение O
для O
задачи B-TERM
распознавания I-TERM
симптомов I-TERM
мы O
давать O
не O
стали O
, O
чтобы O
участники O
Data B-TERM
Science I-TERM
чемпионата O
могли O
покреативить O
. O

# text =  Во-первых, конкретно для этого соревнования наиболее эффективный подход - это доразметка спанов тренировочных данных для задачи NER.
Во O
- O
первых O
, O
конкретно O
для O
этого O
соревнования O
наиболее O
эффективный O
подход O
- O
это O
доразметка B-TERM
спанов I-TERM
тренировочных I-TERM
данных I-TERM
для O
задачи O
NER B-TERM
. O

# text =  Во-вторых, участники использовали базовые подходы для NLP-задач: удаление стоп-слов и знаков пунктуации, приведение к нижнему регистру, стемминг и лемматизация.
Во O
- O
вторых O
, O
участники O
использовали O
базовые O
подходы O
для O
NLP B-TERM
- O
задач O
: O
удаление O
стоп O
- O
слов O
и O
знаков O
пунктуации O
, O
приведение O
к O
нижнему O
регистру O
, O
стемминг B-TERM
и O
лемматизация B-TERM
. O

# text = Более же продвинутым подходом является аугментация данных.
Более O
же O
продвинутым O
подходом O
является O
аугментация B-TERM
данных I-TERM
. O

# text =  Один из возможных способов аугментации текста - перифраз текста.
Один O
из O
возможных O
способов O
аугментации B-TERM
текста I-TERM
- O
перифраз B-TERM
текста I-TERM
. O

# text =  Примером данного решения является использование парафрайзера на основе “rut5-base-paraphraser” из библиотеки huggingface.
Примером O
данного O
решения O
является O
использование O
парафрайзера B-TERM
на O
основе O
“ O
rut5-base B-TERM
- I-TERM
paraphraser I-TERM
” O
из O
библиотеки O
huggingface B-TERM
. O

# text =  Реализуется данный метод аналогично с предыдущим, как модель можно использовать “LaBSE-en-ru”.
Реализуется O
данный O
метод O
аналогично O
с O
предыдущим O
, O
как O
модель O
можно O
использовать O
“ O
LaBSE B-TERM
- I-TERM
en I-TERM
- I-TERM
ru I-TERM
” O
. O

# text =  Сначала решается задача выделения симптомов (NER), после чего в текстах убираются все слова, не являющиеся симптомами.
Сначала O
решается O
задача B-TERM
выделения I-TERM
симптомов I-TERM
( O
NER B-TERM
) O
, O
после O
чего O
в O
текстах B-TERM
убираются O
все O
слова B-TERM
, O
не O
являющиеся O
симптомами O
. O

# text =  Базовым вариантом эмбеддингов является TF-IDF, который зависит от частоты употребления слова в документе.
Базовым O
вариантом O
эмбеддингов B-TERM
является O
TF B-TERM
- I-TERM
IDF I-TERM
, O
который O
зависит O
от O
частоты B-TERM
употребления I-TERM
слова I-TERM
в O
документе O
. O

# text =  И чтобы его улучшить, можно использовать эмбеддинги предобученных моделей, таких как Word2Vec, FastText и тд.
И O
чтобы O
его O
улучшить O
, O
можно O
использовать O
эмбеддинги B-TERM
предобученных I-TERM
моделей I-TERM
, O
таких O
как O
Word2Vec B-TERM
, O
FastText B-TERM
и O
тд O
. O

# text =  В частности, в одном из лучших решений использовался необычный FastText, предобученный на корпусе текстов RuDReC, который содержит отзывы потребителей на русском языке о фармацевтической продукции.
В O
частности O
, O
в O
одном O
из O
лучших O
решений O
использовался O
необычный O
FastText B-TERM
, O
предобученный O
на O
корпусе O
текстов O
RuDReC B-TERM
, O
который O
содержит O
отзывы O
потребителей O
на O
русском B-TERM
языке I-TERM
о O
фармацевтической O
продукции O
. O

# text =  Напомним, что алгоритм работы с трансформерами можно представить следующим образом: сначала тексты преобразовываются токенизатором, далее обучается модель-трансформер.
Напомним O
, O
что O
алгоритм O
работы O
с O
трансформерами B-TERM
можно O
представить O
следующим O
образом O
: O
сначала O
тексты O
преобразовываются O
токенизатором B-TERM
, O
далее O
обучается O
модель B-TERM
- I-TERM
трансформер I-TERM
. O

# text =  Если же говорить о выборе моделей, то наилучшие результаты были получены следующими из них: RuBERT-base, RuBERT-large, LaBSE-en-ru.
Если O
же O
говорить O
о O
выборе O
моделей O
, O
то O
наилучшие O
результаты O
были O
получены O
следующими O
из O
них O
: O
RuBERT B-TERM
- I-TERM
base I-TERM
, O
RuBERT B-TERM
- I-TERM
large I-TERM
, O
LaBSE B-TERM
- I-TERM
en I-TERM
- I-TERM
ru I-TERM
. O

# text =  Предположим, что вы и так слышали о моделях семейства BERT (в предыдущей статье мы описывали, как применяем BERT в других задачах), а вот LaBSE - выбор совершенно неочевидный.
Предположим O
, O
что O
вы O
и O
так O
слышали O
о O
моделях O
семейства O
BERT B-TERM
( O
в O
предыдущей O
статье O
мы O
описывали O
, O
как O
применяем O
BERT B-TERM
в O
других O
задачах O
) O
, O
а O
вот O
LaBSE B-TERM
- O
выбор O
совершенно O
неочевидный O
. O

# text = Далее слова в тестовом наборе текстов также приводятся к векторам и сравниваются со словами из тренировочной разметки при помощи косинусной близости.
Далее O
слова O
в O
тестовом O
наборе O
текстов O
также O
приводятся O
к O
векторам O
и O
сравниваются O
со O
словами O
из O
тренировочной O
разметки O
при O
помощи O
косинусной B-TERM
близости I-TERM
. O

# text =  Архитектура в свою очередь может содержать LSTM, BiLSTM, RNN или GRU слои.
Архитектура O
в O
свою O
очередь O
может O
содержать O
LSTM B-TERM
, O
BiLSTM B-TERM
, O
RNN B-TERM
или O
GRU B-TERM
слои O
. O

# text =  Из интересных решений один из участников представил BiLSTM-сеть с CRF слоем.
Из O
интересных O
решений O
один O
из O
участников O
представил O
BiLSTM B-TERM
- I-TERM
сеть I-TERM
с O
CRF B-TERM
слоем O
. O

# text =  Используются те же модели, поэтому расскажем о различии в подготовке данных для моделей.Для задачи NER тексты преобразовываются с помощью токенизатора и теггинга.
Используются O
те O
же O
модели O
, O
поэтому O
расскажем O
о O
различии O
в O
подготовке O
данных O
для O
моделей O
. O
Для O
задачи O
NER B-TERM
тексты O
преобразовываются O
с O
помощью O
токенизатора B-TERM
и O
теггинга B-TERM
. O

# text =  Сначала тексты при помощи токенизатора переводятся в вектора - это то, на чем обучается модель.
Сначала O
тексты O
при O
помощи O
токенизатора B-TERM
переводятся O
в O
вектора B-TERM
- O
это O
то O
, O
на O
чем O
обучается O
модель O
. O

# text =  Далее создаются таргеты при помощи теггинга.
Далее O
создаются O
таргеты B-TERM
при O
помощи O
теггинга B-TERM
. O

# text =  Самым распространенным алгоритмом теггинга является “Inside–outside–beginning”.
Самым O
распространенным O
алгоритмом O
теггинга O
является O
“ O
Inside B-TERM
– I-TERM
outside I-TERM
– I-TERM
beginning I-TERM
” 
. O

# text =  Тег указывает на то, что слово находится внутри спана.
Тег B-TERM
указывает O
на O
то O
, O
что O
слово O
находится O
внутри O
спана B-TERM
. O

# text =  Среди решений были как кастомный код для обучения и инференса, так и код от huggingface, который можно использовать из коробки.
Среди O
решений O
были O
как O
кастомный O
код O
для O
обучения O
и O
инференса O
, O
так O
и O
код O
от O
huggingface B-TERM
, O
который O
можно O
использовать O
из O
коробки O
. O

# text =  Безусловно, основной метрикой оценивания являлся лидерборд.
Безусловно O
, O
основной O
метрикой O
оценивания O
являлся O
лидерборд B-TERM
. O

# text =  Для решения ситуации мы можем искусственно сгенерировать данные с помощью языка программирования.
Для O
решения O
ситуации O
мы O
можем O
искусственно O
сгенерировать B-TERM
данные I-TERM
с O
помощью O
языка O
программирования O
. O

# text =  Пересмотрев множество примеров и статей, была найдена англоязычная статья, в которой рассмотрены три самых интересных, в плане функциональности и простоты использования, способа генерации синтетических данных с помощью пакетов Python.
Пересмотрев O
множество O
примеров O
и O
статей O
, O
была O
найдена O
англоязычная O
статья O
, O
в O
которой O
  O
рассмотрены O
три O
самых O
интересных O
, O
в O
плане O
функциональности O
и O
простоты O
использования O
, O
способа O
генерации B-TERM
синтетических I-TERM
данных I-TERM
с O
помощью O
пакетов O
  O
Python B-TERM
. O

# text =  Faker - это пакет Python, разработанный для упрощения генерации синтетических данных.
Faker B-TERM
- O
это O
пакет O
Python B-TERM
, O
разработанный O
для O
упрощения O
генерации B-TERM
синтетических I-TERM
данных I-TERM
. O

# text =  SDV или Synthetic Data Vault - это пакет Python для генерации синтетических данных на основе предоставленного набора данных.
SDV B-TERM
или O
Synthetic B-TERM
Data I-TERM
Vault I-TERM
- O
это O
пакет O
Python B-TERM
для O
генерации B-TERM
синтетических I-TERM
данных I-TERM
на O
основе O
предоставленного O
набора O
данных O
. O

# text =  SDV генерирует данные, применяя математические методы и модели машинного обучения.
SDV B-TERM
генерирует O
данные O
, O
применяя O
математические B-TERM
методы I-TERM
и O
модели B-TERM
машинного I-TERM
обучения I-TERM
. O

# text =  С помощью SVD можно обработать данные, даже если они содержат несколько типов данных и отсутствующие значения.
С O
помощью O
SVD B-TERM
можно O
обработать B-TERM
данные I-TERM
, O
даже O
если O
они O
содержат O
несколько O
типов O
данных O
и O
отсутствующие O
значения O
. O

# text =  Используем для этого одну из доступных моделей SVD Singular Table GaussianCopula.
Используем O
для O
этого O
одну O
из O
доступных O
моделей O
SVD B-TERM
Singular B-TERM
Table I-TERM
GaussianCopula I-TERM
. O

# text =  Воспользуемся функцией evaluate из SDV.
Воспользуемся O
функцией O
evaluate B-TERM
из O
SDV B-TERM
. O

# text =  Возьмем для примера статистические метрики (критерии Колмогорова–Смирнова и Хи-квадрат) и метрику обнаружения, основанную на классификаторе логистической регрессии.
Возьмем O
для O
примера O
статистические B-TERM
метрики I-TERM
( O
критерии B-TERM
Колмогорова I-TERM
– I-TERM
Смирнова I-TERM
и O
Хи B-TERM
- I-TERM
квадрат I-TERM
) O
и O
метрику B-TERM
обнаружения I-TERM
, O
основанную O
на O
классификаторе B-TERM
логистической I-TERM
регрессии I-TERM
. O

# text =  KSTest используется для сравнения столбцов с непрерывными данными, а CSTest с дискретными данными.
KSTest B-TERM
используется O
для O
сравнения O
столбцов O
с O
непрерывными O
данными O
, O
а O
CSTest B-TERM
с O
дискретными O
данными O
. O

# text =  Метрика LogisticDetection при помощи машинного обучения позволяет оценить насколько сложно отличить синтетические данные от исходных.
Метрика O
LogisticDetection B-TERM
при O
помощи O
машинного O
обучения O
позволяет O
оценить O
насколько O
сложно O
отличить B-TERM
синтетические I-TERM
данные I-TERM
от I-TERM
исходных I-TERM
. O

# text =  Gretel или Gretel Synthetics – это пакет Python с открытым исходным кодом, основанный на рекуррентной нейронной сети для создания структурированных и не структурированных данных.
Gretel B-TERM
или O
Gretel B-TERM
Synthetics I-TERM
– O
это O
пакет O
Python B-TERM
с O
открытым O
исходным O
кодом O
, O
основанный O
на O
рекуррентной B-TERM
нейронной I-TERM
сети I-TERM
для O
создания O
структурированных O
и O
не O
структурированных O
данных O
. O

# text =  Этот модуль работает непосредственно с датафреймами данных Pandas и позволяет автоматически разбивать датафрейм на более мелкие датафреймы (по кластерам столбцов), выполнять обучение модели и генерацию для каждого фрейма независимо.
Этот O
модуль O
работает O
  O
непосредственно O
с O
датафреймами O
данных O
Pandas B-TERM
и O
позволяет O
автоматически O
разбивать O
датафрейм O
на O
более O
мелкие O
датафреймы O
( O
по O
кластерам O
столбцов O
) O
, O
выполнять O
обучение O
модели O
и O
генерацию O
для O
каждого O
фрейма O
независимо O
. O

# text =  Теперь с помощью пакета Gretel cгенерируем синтетические данные для Stroke Prediction Dataset и проанализируем их относительно данных полученных с помощью пакета SVD из пункта 2.
Теперь O
с O
помощью O
пакета O
Gretel B-TERM
cгенерируем O
синтетические O
данные O
для O
Stroke B-TERM
Prediction I-TERM
Dataset I-TERM
и O
проанализируем O
их O
относительно O
данных O
полученных O
с O
помощью O
пакета O
SVD B-TERM
из O
пункта O
2 O
. O

# text =  Метрикой оценки качества является ROC-AUC.
Метрикой O
оценки O
качества O
является O
ROC B-TERM
- I-TERM
AUC I-TERM
. O

# text =  Разработанный подход для решения задачи кредитного скоринга в дальнейшем легко переносим и на прочие банковские задачи: модели склонности, оттока и дохода.
Разработанный O
подход O
для O
решения O
задачи B-TERM
кредитного I-TERM
скоринга I-TERM
в O
дальнейшем O
легко O
переносим O
и O
на O
прочие O
банковские O
задачи O
: O
модели O
склонности O
, O
оттока O
и O
дохода O
. O

# text =  Токены, относящиеся к ФИО, мы выделяем с помощью клиентской базы и проверки с помощью библиотек для морфологического анализа.
Токены B-TERM
, O
относящиеся O
к O
ФИО O
, O
мы O
выделяем O
с O
помощью O
клиентской O
базы O
и O
проверки O
с O
помощью O
библиотек O
для O
морфологического B-TERM
анализа I-TERM
. O

# text = Лемматизация оставшихся токенов.
Лемматизация B-TERM
оставшихся O
токенов B-TERM
. O

# text =  Для этого корпуса мы обучили word2vec-модель, где для каждого токена выучили эмбеддинг размера 50.
Для O
этого O
корпуса O
мы O
обучили O
word2vec B-TERM
- I-TERM
модель I-TERM
, O
где O
для O
каждого O
токена O
выучили O
эмбеддинг B-TERM
размера O
50.

# text =  Благодаря богатому набору данных бустинг индивидуально имеет приличное качество.
Благодаря O
богатому O
набору O
данных O
бустинг B-TERM
индивидуально O
имеет O
приличное O
качество O
. O

# text =  Одной из первых практических задач было определение авторства политических текстов The Federalist Papers, написанных в США в 1780 годах.
Одной O
из O
первых O
практических O
задач O
было O
определение B-TERM
авторства I-TERM
политических O
текстов O
The O
Federalist O
Papers O
, O
написанных O
в O
США BO
в O
1780 B-TERM
годах I-TERM
. O

# text =  Я рассмотрю простейший способ анализа с помощью несложных расчетов и пакета Natural Language Toolkit, что в совокупности с matplotlib позволяет получить интересные результаты буквально в несколько строк кода.
Я O
рассмотрю O
простейший O
способ O
анализа O
с O
помощью O
несложных O
расчетов O
и O
пакета O
Natural B-TERM
Language I-TERM
Toolkit I-TERM
, O
что O
в O
совокупности O
с O
matplotlib B-TERM
позволяет O
получить O
интересные O
результаты O
буквально O
в O
несколько O
строк O
кода O
. O

# text =  К этой группе относятся решения от крупнейших компаний: Amazon Machine Learning, Microsoft Azure Machine Learning и Microsoft Cognitive Services, Google Cloud Prediction API и Google Cloud Machine Learning, IBM Watson Cloud и AlchemyAPI, BigML и другие.
К O
этой O
группе O
относятся O
решения O
от O
крупнейших O
компаний O
: O
Amazon B-TERM
Machine I-TERM
Learning I-TERM
, O
Microsoft B-TERM
Azure I-TERM
Machine I-TERM
Learning I-TERM
и O
Microsoft B-TERM
Cognitive I-TERM
Services I-TERM
, O
Google B-TERM
Cloud I-TERM
Prediction I-TERM
API I-TERM
и O
Google B-TERM
Cloud I-TERM
Machine I-TERM
Learning I-TERM
, O
IBM B-TERM
Watson I-TERM
Cloud I-TERM
и O
AlchemyAPI B-TERM
, O
BigML B-TERM
и O
другие O
. O

# text =  Возможности этого сервиса в области анализа речи и естественного языка пока ограничиваются английским языком, однако многие другие сервисы поддерживают русский язык, например, полностью бесплатный wit.ai, приобретённый Facebook, и его российский конкурент api.ai (понимание текстовых и голосовых команд и вопросов на естественных языках, преобразование речи в текст), IBM AlchemyAPI (анализ тональности текста, выявление сущностей и ключевых слов), Google Natural Language API (классификация текстов, графы связей, извлечение информации из текстов, анализ тональности, намерений, извлечение инсайтов; поддерживает русский язык с помощью технологии машинного перевода Google Translate, использует глубокое обучение и word2vec).
Возможности O
этого O
сервиса O
в O
области O
анализа B-Task
речи I-Task
и O
естественного O
языка O
пока O
ограничиваются O
английским O
языком O
, O
однако O
многие O
другие O
сервисы O
поддерживают O
русский O
язык O
, O
например O
, O
полностью O
бесплатный O
wit.ai B-TERM
, O
приобретённый O
Facebook B-TERM
, O
и O
его O
российский O
конкурент O
api.ai B-TERM
( O
понимание B-TERM
текстовых I-TERM
и I-TERM
голосовых I-TERM
команд I-TERM
и I-TERM
вопросов I-TERM
на O
естественных O
языках O
, O
преобразование B-TERM
речи I-TERM
в I-TERM
текст I-TERM
) O
, O
IBM B-TERM
AlchemyAPI I-TERM
( O
анализ B-TERM
тональности I-TERM
текста I-TERM
, O
выявление B-TERM
сущностей I-TERM
и O
ключевых B-TERM
слов I-TERM
) O
, O
Google B-TERM
Natural I-TERM
Language I-TERM
API I-TERM
( O
классификация B-TERM
текстов O
, O
графы B-TERM
связей I-TERM
, O
извлечение B-TERM
информации I-TERM
из O
текстов O
, O
анализ B-TERM
тональности I-TERM
, O
намерений O
, O
извлечение O
инсайтов O
; O
поддерживает O
русский O
язык O
с O
помощью O
технологии O
машинного B-TERM
перевода I-TERM
Google B-TERM
Translate I-TERM
, O
использует O
глубокое O
обучение O
и O
word2vec B-TERM
) O
. O

# text =  Например, IBM Watson предлагает инструмент Personality Insights, позволяющий определять черты личности человека, его потребности и ценности, намерения и другие характеристики по его записям в Твиттере, социальных сетях или по другим текстовым источникам.
Например O
, O
IBM B-TERM
Watson I-TERM
предлагает O
инструмент O
Personality B-TERM
Insights I-TERM
, O
позволяющий O
определять O
черты O
личности O
человека O
, O
его O
потребности O
и O
ценности O
, O
намерения O
и O
другие O
характеристики O
по O
его O
записям O
в O
Твиттере B-TERM
, O
социальных O
сетях O
или O
по O
другим O
текстовым O
источникам O
. O

# text =  Например, Diffbot позволяет автоматически сканировать страницы сайтов, извлекать из них нужную информацию: тексты, изображения, видео, информацию о продуктах, комментарии и др., в очищенном в структурированном виде, а также позволяет классифицировать страницы.
Например O
, O
Diffbot B-TERM
позволяет O
автоматически O
сканировать B-TERM
страницы I-TERM
сайтов I-TERM
, O
извлекать O
из O
них O
нужную O
информацию O
: O
тексты O
, O
изображения O
, O
видео O
, O
информацию O
о O
продуктах O
, O
комментарии O
и др. O
, O
в O
очищенном O
в O
структурированном O
виде O
, O
а O
также O
позволяет O
классифицировать B-TERM
страницы I-TERM
. O

# text =  При этом используются широкий спектр технологий: анализ структуры страниц, машинное обучение, искусственный интеллект, обработка естественных языков и машинное зрение.
При O
этом O
используются O
широкий O
спектр O
технологий O
: O
анализ B-TERM
структуры I-TERM
страниц I-TERM
, O
машинное B-TERM
обучение I-TERM
, O
искусственный B-TERM
интеллект I-TERM
, O
обработка B-TERM
естественных I-TERM
языков I-TERM
и O
машинное B-TERM
зрение I-TERM
. O

# text =  Решения, основанные на Deepomatic, позволяют находить информацию о фильме по его постеру, информацию о картине или скульптуре на выставке по ее фото, сделанному на камеру телефона, позволяют скачивать музыку, сфотографировав обложку альбома на диске и т.п.
Решения O
, O
основанные O
на O
Deepomatic B-TERM
, O
позволяют O
находить B-TERM
информацию I-TERM
о I-TERM
фильме I-TERM
по O
его O
постеру O
, O
информацию B-TERM
о I-TERM
картине I-TERM
или O
скульптуре O
на O
выставке O
по O
ее O
фото O
, O
сделанному O
на O
камеру O
телефона O
, O
позволяют O
скачивать O
музыку O
, O
сфотографировав O
обложку O
альбома O
на O
диске O
и O
т.п. O

# text =  В нашем случае цель была сформулирована как повышение эффективности поиска кандидатов.
В O
нашем O
случае O
цель O
была O
сформулирована O
как O
повышение B-TERM
эффективности I-TERM
поиска I-TERM
кандидатов I-TERM
. O

# text =  Основная задача здесь — найти эффективный способ отображения соответствия кандидатов и навыков.
Основная O
задача O
здесь O
— O
найти B-TERM
эффективный I-TERM
способ I-TERM
отображения I-TERM
соответствия I-TERM
кандидатов I-TERM
и I-TERM
навыков I-TERM
. O

# text =  Кодирование в переменные — One-Hot Encoding (OHE) 
Кодирование O
в O
переменные O
— O
One B-TERM
- I-TERM
Hot I-TERM
Encoding I-TERM
( O
OHE B-TERM
) O

# text =  Для этого используют метод TF-IDF.
Для O
этого O
используют O
метод O
TF B-TERM
- I-TERM
IDF I-TERM
. O

# text =  Соответственно, можно схлопнуть похожие навыки в некоторые факторы/компоненты/латентные признаки.
Соответственно O
, O
можно O
схлопнуть O
похожие O
навыки O
в O
некоторые O
факторы B-TERM
/ O
компоненты B-TERM
/ O
латентные B-TERM
признаки I-TERM
. O

# text =  Одним из подходов, позволяющих находить такие компоненты, является группа методов матричной факторизации.
Одним O
из O
подходов O
, O
позволяющих O
находить O
такие O
компоненты B-TERM
, O
является O
группа O
методов B-TERM
матричной I-TERM
факторизации I-TERM
. O

# text =  Полученные представления кандидатов и навыков называют эмбедингами.
Полученные O
представления O
кандидатов O
и O
навыков O
называют O
эмбедингами B-TERM
. O

# text =  При создании нашей системы рекомендации кандидатов на позиции мы использовали нейронную сеть — StarSpace.
При O
создании O
нашей O
системы O
рекомендации O
кандидатов O
на O
позиции O
мы O
использовали O
нейронную B-TERM
сеть I-TERM
— O
StarSpace B-TERM
. O

# text =  Другая группа методов, позволяющая решать задачи репрезентации сущностей — репрезентация графов.
Другая O
группа O
методов O
, O
позволяющая O
решать O
задачи B-TERM
репрезентации I-TERM
сущностей I-TERM
— O
репрезентация B-TERM
графов I-TERM
. O

# text =  Но большинство методов графовой репрезентации работает с одномодальными графами, поэтому обычно двухмодальные графы следует трансформировать в граф, где узлы представлены одним видом сущностей.
Но O
большинство O
методов B-TERM
графовой I-TERM
репрезентации I-TERM
работает O
с O
одномодальными B-TERM
графами I-TERM
, O
поэтому O
обычно O
двухмодальные B-TERM
графы I-TERM
следует O
трансформировать O
в O
граф O
, O
где O
узлы O
представлены O
одним O
видом O
сущностей O
. O

# text =  В первую очередь рассмотрим метод, основанный на графовой факторизации.
В O
первую O
очередь O
рассмотрим O
метод O
, O
основанный O
на O
графовой B-TERM
факторизации I-TERM
. O

# text =  Это группа методов очень похожа на методы, применяемые для репрезентации текстов — w2v (skip-gram), doc2vec.
Это O
группа O
методов O
очень O
похожа O
на O
методы O
, O
применяемые O
для O
репрезентации O
текстов O
— O
w2v B-TERM 
( O
skip B-TERM
- I-TERM
gram I-TERM
) O
, O
doc2vec B-TERM
. O

# text =  Почитать подробнее про подобные методы графовой репрезентации можно, например, тут — DeepWalk, Node2vec, Graph2vec.
Почитать O
подробнее O
про O
подобные O
методы O
графовой O
репрезентации O
можно O
, O
например O
, O
тут O
— O
DeepWalk B-TERM
, O
Node2vec B-TERM
, O
Graph2vec B-TERM
. O

# text =  Сверточные сети на графах (Graph Convolutional Networks).
Сверточные O
сети O
на O
графах O
( O
Graph B-TERM
Convolutional I-TERM
Networks I-TERM
) O
. O

# text =  Для задачи репрезентации графов связей между сущностями мы использовали фреймворк PyTorch BigGraph — это ещё один фреймворк от Facebook Research.
Для O
задачи O
репрезентации B-TERM
графов I-TERM
связей I-TERM
между I-TERM
сущностями I-TERM
мы O
использовали O
фреймворк O
PyTorch B-TERM
BigGraph I-TERM
— O
это O
ещё O
один O
фреймворк O
от O
Facebook B-TERM
Research I-TERM
. O

# text =  Энкодер предложений (sentence encoder) – это модель, которая сопоставляет коротким текстам векторы в многомерном пространстве, причём так, что у текстов, похожих по смыслу, и векторы тоже похожи.
Энкодер B-TERM
предложений I-TERM
( O
sentence B-TERM
encoder I-TERM
) O
– O
это O
модель O
, O
которая O
сопоставляет O
коротким O
текстам O
векторы O
в O
многомерном O
пространстве O
, O
причём O
так O
, O
что O
у O
текстов O
, O
похожих O
по O
смыслу O
, O
и O
векторы O
тоже O
похожи O
. O

# text =  Обычно для этой цели используются нейросети, а полученные векторы называются эмбеддингами.
Обычно O
для O
этой O
цели O
используются O
нейросети B-TERM
, O
а O
полученные O
векторы O
называются O
эмбеддингами B-TERM
. O

# text =  Они полезны для кучи задач, например, few-shot классификации текстов, семантического поиска, или оценки качества перефразирования.
Они O
полезны O
для O
кучи O
задач O
, O
например O
, O
few B-TERM
- I-TERM
shot I-TERM
классификации I-TERM
текстов I-TERM
, O
семантического B-TERM
поиска I-TERM
, O
или O
оценки B-TERM
качества I-TERM
перефразирования I-TERM
. O

# text =  Самой качественной моделью оказался mUSE, самой быстрой из предобученных – FastText, а по балансу скорости и качества победил rubert-tiny2.
Самой O
качественной O
моделью O
оказался O
mUSE B-TERM
, O
самой O
быстрой O
из O
предобученных O
– O
FastText B-TERM
, O
а O
по O
балансу O
скорости O
и O
качества O
победил O
rubert B-TERM
- I-TERM
tiny2 I-TERM
. O

# text =  Первой известной попыткой системно сравнить английские эмбеддинги предложений был SentEval, сочетающий чисто лингвистические задачи со вполне прикладными.
Первой O
известной O
попыткой O
системно O
сравнить O
английские O
эмбеддинги B-TERM
предложений O
был O
SentEval B-TERM
, O
сочетающий O
чисто O
лингвистические O
задачи O
со O
вполне O
прикладными O
. O

# text =  Для русского языка тоже было создано немало разного рода бенчмарков NLU моделей:RussianSuperGLUE: бенчмарк "сложных" NLP задач; фокус на дообучаемых моделях.
Для O
русского B-TERM
языка O
тоже O
было O
создано O
немало O
разного O
рода O
бенчмарков O
NLU B-TERM
моделей O
: O
RussianSuperGLUE B-TERM
: O
бенчмарк O
" O
сложных O
" O
NLP B-TERM
задач I-TERM
; O
фокус O
на O
дообучаемых O
моделях O
. O

# text = MOROCCO: RussianSuperGLUE + оценка производительности, довольно трудновоспроизводимый бенчмарк.
MOROCCO O
: O
RussianSuperGLUE B-TERM
+ O
оценка O
производительности O
, O
довольно O
трудновоспроизводимый O
бенчмарк O
. O

# text =  RuSentEval: бенчмарк BERT-подобных энкодеров предложений на лингвистических задачах.
RuSentEval B-TERM
: O
бенчмарк O
BERT B-TERM
- I-TERM
подобных I-TERM
энкодеров I-TERM
предложений I-TERM
на O
лингвистических O
задачах O
. O

# text =  SentEvalRu и deepPavlovEval: два хороших, но давно не обновлявшихся прикладных бенчмарка.
SentEvalRu B-TERM
и O
deepPavlovEval B-TERM
: O
два O
хороших O
, O
но O
давно O
не O
обновлявшихся O
прикладных O
бенчмарка O
. O

# text =  С тех пор появилось много новых русскоязычных моделей, включая rubert-tiny2, поэтому и бенчмарк пришло время обновить.
С O
тех O
пор O
появилось O
много O
новых O
русскоязычных O
моделей O
, O
включая O
rubert B-TERM
- I-TERM
tiny2 I-TERM
, O
поэтому O
и O
бенчмарк O
пришло O
время O
обновить O
. O

# text =  В основу бенчмарка легли BERT-подобные модели: sbert_large_nlu_ru, sbert_large_mt_nlu_ru, и ruRoberta-large от Сбера; rubert-base-cased-sentence, rubert-base-cased-conversational, distilrubert-tiny-cased-conversational, и distilrubert-base-cased-conversational от DeepPavlov; мои   rubert-tiny и rubert-tiny2; мультиязычные LaBSE (плюс урезанная версия LaBSE-en-ru) и старый добрый bert-base-multilingual-cased.
В O
основу O
бенчмарка O
легли O
BERT B-TERM
- I-TERM
подобные I-TERM
модели I-TERM
: O
sbert_large_nlu_ru B-TERM
, O
sbert_large_mt_nlu_ru B-TERM
, O
и O
ruRoberta B-TERM
- I-TERM
large I-TERM
от O
Сбера B-TERM
; O
rubert B-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
- I-TERM
sentence I-TERM
, O
rubert B-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
- I-TERM
conversational I-TERM
, O
distilrubert B-TERM
- I-TERM
tiny I-TERM
- I-TERM
cased I-TERM
- I-TERM
conversational I-TERM
, O
и O
distilrubert B-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
- I-TERM
conversational I-TERM
от O
DeepPavlov B-TERM
; O
мои O
   O
rubert B-TERM
- I-TERM
tiny I-TERM
и O
rubert B-TERM
- I-TERM
tiny2 I-TERM
; O
мультиязычные O
LaBSE B-TERM
( O
плюс O
урезанная O
версия O
LaBSE B-TERM
- I-TERM
en I-TERM
- I-TERM
ru I-TERM
) O
и O
старый O
добрый O
bert B-TERM
- I-TERM
base I-TERM
- I-TERM
multilingual I-TERM
- I-TERM
cased I-TERM
. O

# text =  Кроме этого, я добавил в бенчмарк разные T5 модели, т.к. они тоже должны хорошо понимать тексты: мои rut5-small, rut5-base, rut5-base-multitask, и rut5-base-paraphraser, и Сберовские ruT5-base и ruT5-large.
Кроме O
этого O
, O
я O
добавил O
в O
бенчмарк O
разные O
T5 B-TERM
модели I-TERM
, O
т.к. O
они O
тоже O
должны O
хорошо O
понимать O
тексты O
: O
мои O
rut5-small B-TERM
, O
rut5-base B-TERM
, O
rut5-base B-TERM
- I-TERM
multitask I-TERM
, O
и O
rut5-base B-TERM
- I-TERM
paraphraser I-TERM
, O
и O
Сберовские B-TERM
ruT5-base B-TERM
и O
ruT5-large B-TERM
. O

# text =  Помимо BERTов и T5, я включил в бенчмарк большие мультиязычные модели Laser от FAIR и USE-multilingual-large от Google.
Помимо O
BERTов O
и O
T5 O
, O
я O
включил O
в O
бенчмарк O
большие O
мультиязычные O
модели O
Laser B-TERM
от O
FAIR B-TERM
и O
USE B-TERM
- I-TERM
multilingual I-TERM
- I-TERM
large I-TERM
от O
Google B-TERM
. O

# text =  В качестве быстрого бейзлайна, я добавил FastText, а именно, geowac_tokens_none_fasttextskipgram_300_5_2020  с RusVectores, а также его сжатую версию.
В O
качестве O
быстрого O
бейзлайна O
, O
я O
добавил O
FastText B-TERM
, O
а O
именно O
, O
geowac_tokens_none_fasttextskipgram_300_5_2020 B-TERM
с O
RusVectores B-TERM
, O
а O
также O
его O
сжатую O
версию O
. O

# text =  Наконец, я добавил парочку "моделей", которые вообще не выучивают никаких параметров, а просто используют HashingVectorizer для превращения текста в вектор признаков.
Наконец O
, O
я O
добавил O
парочку O
" O
моделей O
" O
, O
которые O
вообще O
не O
выучивают O
никаких O
параметров O
, O
а O
просто O
используют O
HashingVectorizer B-TERM
для O
превращения O
текста O
в O
вектор O
признаков O
. O

# text =  Это доработанная версия rubert-tiny: я расширил словарь модели c 30К до 80К токенов, увеличил максимальную длину текста с 512 до 2048 токенов, и дообучил модель на комбинации задач masked language modelling, natural language inference, и аппроксимации эмбеддингов LaBSE.
Это O
доработанная O
версия O
rubert B-TERM
- I-TERM
tiny I-TERM
: O
я O
расширил O
словарь O
модели O
c O
30К O
до O
80К O
токенов O
, O
увеличил O
максимальную O
длину O
текста O
с O
512 O
до O
2048 O
токенов O
, O
и O
дообучил O
модель O
на O
комбинации O
задач O
masked B-TERM
language I-TERM
modelling I-TERM
, O
natural B-TERM
language I-TERM
inference I-TERM
, O
и O
аппроксимации B-TERM
эмбеддингов I-TERM
LaBSE I-TERM
. O

# text =  В новой версии бенчмарка я оставил всё те же 10 задач, что и в прежней, но слегка изменил формат некоторых из них:Semantic text similarity (STS) на основе переведённого датасета STS-B; Paraphrase identification (PI) на основе датасета paraphraser.ru;Natural language inference (NLI) на датасете XNLI; Sentiment analysis (SA) на данных SentiRuEval2016.
В O
новой O
версии O
бенчмарка O
я O
оставил O
всё O
те O
же O
10 O
задач O
, O
что O
и O
в O
прежней O
, O
но O
слегка O
изменил O
формат O
некоторых O
из O
них O
: O
Semantic B-TERM
text I-TERM
similarity I-TERM
( O
STS B-TERM
) O
на O
основе O
переведённого O
датасета O
STS B-TERM
- I-TERM
B I-TERM
; O
Paraphrase B-TERM
identification I-TERM
( O
PI B-TERM
) O
на O
основе O
датасета O
paraphraser.ru B-TERM
; O
Natural B-TERM
language I-TERM
inference I-TERM
( O
NLI B-TERM
) O
на O
датасете O
XNLI B-TERM
; O
Sentiment B-TERM
analysis I-TERM
( O
SA B-TERM
) O
на O
данных O
SentiRuEval2016 B-TERM
. O

# text =  В прошлой версии бенчмарка я собрал кривые тестовые выборки, поэтому этот датасет я переделал; Toxicity identification (TI) на датасете токсичных комментариев из OKMLCup; Inappropriateness identification (II) на датасете Сколтеха; Intent classification (IC) и её кросс-язычная версия ICX на датасете NLU-evaluation-data, который я автоматически перевёл на русский.
В O
прошлой O
версии O
бенчмарка O
я O
собрал O
кривые O
тестовые O
выборки O
, O
поэтому O
этот O
датасет O
я O
переделал O
; O
Toxicity B-TERM
identification I-TERM
( O
TI B-TERM
) O
на O
датасете B-TERM
токсичных I-TERM
комментариев I-TERM
из O
OKMLCup B-TERM
; O
Inappropriateness B-TERM
identification I-TERM
( O
II B-TERM
) O
на O
датасете B-TERM
Сколтеха I-TERM
; O
Intent B-TERM
classification I-TERM
( O
IC B-TERM
) O
и O
её O
кросс O
- O
язычная O
версия O
ICX B-TERM
на O
датасете O
NLU B-TERM
- I-TERM
evaluation I-TERM
- I-TERM
data I-TERM
, O
который O
я O
автоматически O
перевёл O
на O
русский B-TERM
. O

# text =  В IC классификатор обучается на русских данных, а в ICX – на английских, а тестируется в обоих случаях на русских.
В O
IC B-TERM
классификатор I-TERM
обучается O
на O
русских B-TERM
данных O
, O
а O
в O
ICX B-TERM
– O
на O
английских B-TERM
, O
а O
тестируется O
в O
обоих O
случаях O
на O
русских O
. O

# text =  Распознавание именованных сущностей () на датасетах factRuEval-2016E1) и RuDReC (NE2).
Распознавание O
именованных O
сущностей O
( O
) O
на O
датасетах O
factRuEval-2016E1 B-TERM
) O
и O
RuDReC B-TERM
( O
NE2 B-TERM
) O
. O

# text =  Эти две задачи требуют получать эмбеддинги отдельных токенов, а не целых предложений; поэтому модели USE и Laser, не выдающие эмбеддинги токенов "из коробки", в оценке этих задач не участвовали.
Эти O
две O
задачи O
требуют O
получать O
эмбеддинги O
отдельных O
токенов O
, O
а O
не O
целых O
предложений O
; O
поэтому O
модели O
USE B-TERM
и O
Laser B-TERM
, O
не O
выдающие O
эмбеддинги O
токенов O
" O
из O
коробки O
" O
, O
в O
оценке O
этих O
задач O
не O
участвовали O
. O

# text =  В задачах STS, PI и NLI оценивается степень связи двух текстов.
В O
задачах O
STS B-TERM
, O
PI B-TERM
и O
NLI B-TERM
оценивается O
степень O
связи O
двух O
текстов O
. O

# text =  Хороший энкодер предложений должен отражать эту степень в их косинусной близости, поэтому для STS и PI мы измеряем качество как Спирмановскую корреляцию косинусной близости и человеческих оценок сходства.
Хороший O
энкодер O
предложений O
должен O
отражать O
эту O
степень O
в O
их O
косинусной B-TERM
близости B-TERM
, O
поэтому O
для O
STS B-TERM
и O
PI B-TERM
мы O
измеряем O
качество O
как O
Спирмановскую B-TERM
корреляцию I-TERM
косинусной O
близости O
и O
человеческих O
оценок O
сходства O
. O

# text =  Для NLI я обучил трёхклассовую (entail/contradict/neutral) логистическую регрессию поверх косинусной близости, и измеряю её точность (accuracy).
Для O
NLI B-TERM
я O
обучил O
трёхклассовую O
( O
entail O
/ O
contradict O
/ O
neutral O
) O
логистическую B-TERM
регрессию I-TERM
поверх O
косинусной B-TERM
близости I-TERM
, O
и O
измеряю O
её O
точность B-TERM
( O
accuracy B-TERM
) O
. O

# text =  Для задач бинарной классификации TI и II я измеряю ROC AUC, а в задачах многоклассовой классификации SA, IC и ICX – точность (accuracy).
Для O
задач O
бинарной O
классификации O
TI B-TERM
и O
II B-TERM
я O
измеряю O
ROC B-TERM
AUC I-TERM
, O
а O
в O
задачах O
многоклассовой O
классификации O
SA B-TERM
, O
IC B-TERM
и O
ICX B-TERM
– O
точность B-TERM
( O
accuracy B-TERM
) O
. O

# text =  Для всех задач классификации я обучаю логистическую регрессию либо KNN поверх эмбеддингов предложений, и выбираю лучшую модель из двух.
Для O
всех O
задач O
классификации B-TERM
я O
обучаю O
логистическую B-TERM
регрессию I-TERM
либо O
KNN B-TERM
поверх O
эмбеддингов O
предложений O
, O
и O
выбираю O
лучшую O
модель O
из O
двух O
. O

# text =  Для задач NER я классифицировал токены логистической регрессией поверх их эмбеддингов, и измерял macro F1 по всем классам токенов, кроме О. 
Для O
задач O
NER B-TERM
я O
классифицировал O
токены O
логистической B-TERM
регрессией I-TERM
поверх O
их O
эмбеддингов O
, O
и O
измерял O
macro B-TERM
F1 I-TERM
по O
всем O
классам O
токенов O
, O
кроме O
О O
.

# text = Поскольку разные модели токенизируют тексты по-разному, я токенизировал все тексты razdel'ом, и вычислял эмбеддинг слова как средний эмбеддинг его токенов.
Поскольку O
разные O
модели O
токенизируют O
тексты O
по O
- O
разному O
, O
я O
токенизировал O
все O
тексты O
razdel'ом B-TERM
, O
и O
вычислял O
эмбеддинг O
слова O
как O
средний O
эмбеддинг O
его O
токенов O
. O

# text =  Единого победителя нет, но MUSE, sbert_large_mt_nlu_ru и rubert-base-cased-sentence взяли по многу призовых мест.
Единого O
победителя O
нет O
, O
но O
MUSE B-TERM
, O
sbert_large_mt_nlu_ru B-TERM
и O
rubert B-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
- I-TERM
sentence I-TERM
взяли O
по O
многу O
призовых O
мест O
. O

# text =  Удивительно, но модели T5 очень хорошо показали себя на задачах NER.
Удивительно O
, O
но O
модели O
T5 B-TERM
очень O
хорошо O
показали O
себя O
на O
задачах O
NER B-TERM
. O


# text =  Самыми качественными энкодерами предложений оказались мультиязычные MUSE, LaBSE и Laser.
Самыми O
качественными O
энкодерами O
предложений O
оказались O
мультиязычные O
MUSE B-TERM
, O
LaBSE B-TERM
и O
Laser B-TERM
. O


# text =  Но выбирать стоит из Парето-оптимальных моделей: таких, что ни одна другая модель не превосходит их по всем критериям.
Но O
выбирать O
стоит O
из O
Парето B-TERM
- I-TERM
оптимальных I-TERM
моделей I-TERM
: O
таких O
, O
что O
ни O
одна O
другая O
модель O
не O
превосходит O
их O
по O
всем O
критериям O
. O

# text =  Из 25 моделей только 12 Парето-оптимальны:MUSE, rubert-tiny2, FT_geowac, Hashing_1000_char и Hashing_1000 обладают самым лучшим качеством для своей скорости на CPU; MUSE, LaBSE, rubert-tiny2, и distilbert-tiny обладают наилучшим качеством для своей скорости на GPU;MUSE, LaBSE, rubert-tiny2, rubert-tiny, FT_geowac_21mb, и Hashing_1000_char обладают наилучшим качеством для своего размера.
Из O
25 O
моделей O
только O
12 O
Парето O
- O
оптимальны O
: O
MUSE B-TERM
, O
rubert B-TERM
- I-TERM
tiny2 I-TERM
, O
FT_geowac B-TERM
, O
Hashing_1000_char B-TERM
и O
Hashing_1000 B-TERM
обладают O
самым O
лучшим O
качеством O
для O
своей O
скорости O
на O
CPU O
; O
MUSE B-TERM
, O
LaBSE B-TERM
, O
rubert B-TERM
- I-TERM
tiny2 I-TERM
, O
и O
distilbert B-TERM
- I-TERM
tiny I-TERM
обладают O
наилучшим O
качеством O
для O
своей O
скорости O
на O
GPU O
; O
MUSE B-TERM
, O
LaBSE B-TERM
, O
rubert B-TERM
- I-TERM
tiny2 I-TERM
, O
rubert B-TERM
- I-TERM
tiny I-TERM
, O
FT_geowac_21 B-TERM
mb O
, O
и O
Hashing_1000_char B-TERM
обладают O
наилучшим O
качеством O
для O
своего O
размера O
. O

# text =  Актуальный лидерборд смотрите в репозитории: https://github.com/avidale/encodechka
Актуальный O
лидерборд O
смотрите O
в O
репозитории O
: O
https://github.com/avidale/encodechka B-TERM
. O
# text =  Поддержка NlpCraft IDL добавлена в систему начиная с версии 0.7.5.
Поддержка O
NlpCraft B-TERM
IDL I-TERM
добавлена O
в O
систему O
начиная O
с O
версии O
0.7.5 O
. O

# text =  Новая версия декларативного языка определения интентов, получившая название NlpCraft IDL (NlpCraft Intents Definition Language), значительно упростила процесс работы с интентами в диалоговых и поисковых системах, построенных на базе проекта Apache NlpCraft и вместе с тем расширила возможности системы.
Новая O
версия O
декларативного O
языка O
определения O
интентов O
, O
получившая O
название O
NlpCraft B-TERM
IDL I-TERM
( O
NlpCraft B-TERM
Intents I-TERM
Definition I-TERM
Language I-TERM
) O
, O
значительно O
упростила O
процесс O
работы O
с O
интентами O
в O
диалоговых O
и O
поисковых O
системах O
, O
построенных O
на O
базе O
проекта O
Apache B-TERM
NlpCraft I-TERM
и O
вместе O
с O
тем O
расширила O
возможности O
системы O
. O

# text =  NlpCraft IDL - это декларативный язык, позволяющий создавать определения интентов для их последующего использования в моделях Apache NlpCraft.
NlpCraft B-TERM
IDL I-TERM
- O
это O
декларативный O
язык O
, O
позволяющий O
создавать O
определения O
интентов O
для O
их O
последующего O
использования O
в O
моделях O
Apache B-TERM
NlpCraft I-TERM
. O

# text =  Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
Чаще O
всего O
на O
практике O
в O
NLP B-TERM
приходится O
сталкиваться O
с O
задачей O
построения B-TERM
эмбеддингов I-TERM
. O

# text =  Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
Для O
ее O
решения O
обычно O
используют O
один O
из O
следующих O
инструментов O
: O
Готовые O
векторы B-TERM
/ O
эмбеддинги B-TERM
слов O
[ O
6 O
] O
; O
Внутренние O
состояния O
CNN B-TERM
, O
натренированных O
на O
таких O
задачах O
как O
, O
как O
определение B-TERM
фальшивых I-TERM
предложений I-TERM
/ O
языковое B-TERM
моделирование I-TERM
/ O
классификация B-TERM
[ O
7 O
] O
; O
Комбинация O
выше O
перечисленных O
методов O
; O
Кроме O
того O
, O
уже O
много O
раз O
было O
показано O
[ O
9 O
] O
, O
что O
в O
качестве O
хорошего O
бейслайна O
для O
эмбеддингов O
предложений O
можно O
взять O
и O
просто O
усредненные O
( O
с O
парой O
незначительных O
деталей O
, O
которые O
сейчас O
опустим O
) O
векторы O
слов O
. O

# text =  Если для дальнейшей обработки не важен порядок слов, то текст упаковывают в Мешок слов (Bag-of-words).
Если O
для O
дальнейшей O
обработки O
не O
важен O
порядок O
слов O
, O
то O
текст O
упаковывают O
в O
Мешок B-TERM
слов I-TERM
( O
Bag B-TERM
- I-TERM
of I-TERM
- I-TERM
words I-TERM
) O

# text =  В обучающей выборке мы имеем письма с отметками спам/не спам, и скармливаем их в нейросеть: в полносвязную сеть и CNN подаем Bag-of-words, а в RNN уже можно учесть порядок слов, отправив ей Word Vector.
В O
обучающей O
выборке O
мы O
имеем O
письма O
с O
отметками O
спам O
/ O
не O
спам O
, O
и O
скармливаем O
их O
в O
нейросеть O
: O
в O
полносвязную O
сеть O
и O
CNN B-TERM
подаем O
Bag B-TERM
- I-TERM
of I-TERM
- I-TERM
words I-TERM
, O
а O
в O
RNN B-TERM
уже O
можно O
учесть O
порядок O
слов O
, O
отправив O
ей O
Word B-TERM
Vector I-TERM
. O

# text =  Сначала Яндекс распознаёт речь в текст с указанием таймингов и спикеров (за это отвечает голосовая биометрия).
Сначала O
Яндекс B-TERM
распознаёт B-TERM
речь I-TERM
в O
текст O
с O
указанием O
таймингов O
и O
спикеров O
( O
за O
это O
отвечает O
голосовая B-TERM
биометрия I-TERM
) O
. O

# text =  Добавили поддержку немецкого, французского и испанского языков.
Добавили O
поддержку O
немецкого B-TERM
, O
французского B-TERM
и O
испанского B-TERM
языков O
. O

# text =  Поэтому всё время существования как Яндекс Браузера, так и Яндекс Переводчика мы стараемся не просто переводить, но и помогать учить язык.
Поэтому O
всё O
время O
существования O
как O
Яндекс B-TERM
Браузера I-TERM
, O
так O
и O
Яндекс B-TERM
Переводчика I-TERM
мы O
стараемся O
не O
просто O
переводить O
, O
но O
и O
помогать O
учить O
язык O
. O

# text =  С 27 по 30 мая в Российском государственном гуманитарном университете (РГГУ) пройдет международная научная конференция по компьютерной лингвистике «Диалог».
С O
27 O
по O
30 O
мая O
в O
Российском B-TERM
государственном I-TERM
гуманитарном I-TERM
университете I-TERM
( O
РГГУ B-TERM
) O
пройдет O
международная O
научная O
конференция O
по O
компьютерной B-TERM
лингвистике I-TERM
« O
Диалог O
» O
. O

# text =  Подробно о том, что такое «Диалог» и почему ABBYY организует эту конференцию, мы писали здесь .
Подробно O
о O
том O
, O
что O
такое O
« O
Диалог O
» O
и O
почему O
ABBYY B-TERM
организует O
эту O
конференцию O
, O
мы O
писали O
здесь O
. O

# text =  Главной задачей проведенных ранее тестирований был автоматический анализ тональности в целом небольших текстов – отзывов пользователей (о фильмах, книгах, цифровых фотокамерах) или мнений, выраженных в форме прямой или косвенной речи (новости).
Главной O
задачей O
проведенных O
ранее O
тестирований O
был O
автоматический B-TERM
анализ I-TERM
тональности I-TERM
в O
целом O
небольших O
текстов O
– O
отзывов O
пользователей O
( O
о O
фильмах O
, O
книгах O
, O
цифровых O
фотокамерах O
) O
или O
мнений O
, O
выраженных O
в O
форме O
прямой O
или O
косвенной O
речи O
( O
новости O
) O
. O

# text =  Основной целью нового цикла тестирований является автоматическая оценка тональности по отношению к заданному объекту и его конкретным свойствам.
Основной O
целью O
нового O
цикла O
тестирований O
является O
автоматическая B-TERM
оценка I-TERM
тональности I-TERM
по O
отношению O
к O
заданному O
объекту O
и O
его O
конкретным O
свойствам O
. O

# text =  Фишинговые электронные письма - это сообщения, которые кажутся очень похожими на настоящие, например, рассылку от вашего любимого интернет-магазина, но при этом они заманивают людей нажимать на прикрепленные вредоносные ссылки или документы.
Фишинговые B-TERM
электронные I-TERM
письма I-TERM
- O
это O
сообщения O
, O
которые O
кажутся O
очень O
похожими O
на O
настоящие O
, O
например O
, O
рассылку O
от O
вашего O
любимого O
интернет O
- O
магазина O
, O
но O
при O
этом O
они O
заманивают O
людей O
нажимать O
на O
прикрепленные O
вредоносные O
ссылки O
или O
документы O
. O

# text =  Поэтому в статье предлагается способ обнаружения фишинговых сообщений, называемый Federated Phish Bowl (далее FPB), использующий федеративное обучение и рекуррентную нейронную сеть с долгой краткосрочной памятью (LSTM).
Поэтому O
в O
статье O
предлагается O
способ O
обнаружения B-TERM
фишинговых I-TERM
сообщений I-TERM
, O
называемый O
Federated B-TERM
Phish I-TERM
Bowl I-TERM
( O
далее O
FPB B-TERM
) O
, O
использующий O
федеративное O
обучение O
и O
рекуррентную B-TERM
нейронную I-TERM
сеть I-TERM
с I-TERM
долгой I-TERM
краткосрочной I-TERM
памятью I-TERM
( O
LSTM B-TERM
) O
. O

# text =  Для работы с текстовыми последовательностями были придуманы рекуррентные нейронные сети (RNN, их улучшение - LSTM, которая бывает двунаправленной, когда последовательность обрабатывается в двух направлениях).
Для O
работы O
с O
текстовыми O
последовательностями O
были O
придуманы O
рекуррентные O
нейронные O
сети O
( O
RNN B-TERM
, O
их O
улучшение O
- O
LSTM B-TERM
, O
которая O
бывает O
двунаправленной O
, O
когда O
последовательность O
обрабатывается O
в O
двух O
направлениях O
) O
. O

# text =  Например, сеть можно сделать двунаправленной (Bidirectional LSTM).
Например O
, O
сеть O
можно O
сделать O
двунаправленной O
( O
Bidirectional B-TERM
LSTM I-TERM
) O
. O

# text =  FPB предлагает использовать подход, показанный на следующем изображении: 
FPB B-TERM
предлагает O
использовать O
подход O
, O
показанный O
на O
следующем O
изображении O
: O

# text =  Федеративное обучение - это метод машинного обучения, который обучает алгоритм на нескольких децентрализованных устройствах или серверах, содержащих локальные образцы данных, без обмена ими.
Федеративное B-TERM
обучение I-TERM
- O
это O
метод O
машинного O
обучения O
, O
который O
обучает O
алгоритм O
на O
нескольких O
децентрализованных O
устройствах O
или O
серверах O
, O
содержащих O
локальные O
образцы O
данных O
, O
без O
обмена O
ими O
. O

# text =  Для обучения модели FPB с использованием федеративного обучения (FL) сервер параметров (PS) инициализирует глобальную модель (DL) на основе вышеупомянутых двунаправленных нейронных сетей LSTM и отправляет глобальную модель с глобальной матрицей преобразования слов в векторы всем клиентам на первом этапе обучения (см.
Для O
обучения O
модели O
FPB B-TERM
с O
использованием O
федеративного B-TERM
обучения I-TERM
( O
FL B-TERM
) O
сервер B-TERM
параметров I-TERM
( O
PS B-TERM
) O
инициализирует O
глобальную B-TERM
модель I-TERM
( O
DL B-TERM
) O
на O
основе O
вышеупомянутых O
двунаправленных O
нейронных O
сетей O
LSTM B-TERM
и O
отправляет O
глобальную O
модель O
с O
глобальной O
матрицей O
преобразования O
слов O
в O
векторы O
всем O
клиентам O
на O
первом O
этапе O
обучения O
. O

# text =  В рамках курса вы узнаете: Как латентные переменные применяются в задачах анализа текстов и как строить глубинные генеративные модели с латентными дискретными переменными.
В O
рамках O
курса O
вы O
узнаете O
: O
Как O
латентные B-TERM
переменные I-TERM
применяются O
в O
задачах O
анализа B-TERM
текстов I-TERM
и O
как O
строить O
глубинные B-TERM
генеративные I-TERM
модели I-TERM
с O
латентными O
дискретными O
переменными O
. O

# text =  Что такое semantic parsing: как строить формальные представления смысла текста, извлекая при этом неявные значения.
Что O
такое O
semantic B-TERM
parsing I-TERM
: O
как O
строить O
формальные O
представления O
смысла O
текста O
, O
извлекая O
при O
этом O
неявные O
значения O
. O

# text =  Британские ученые обучили ИИ трансформировать устную речь в видео с виртуальным сурдопереводчиком.
Британские O
ученые O
обучили O
ИИ O
трансформировать O
устную B-TERM
речь I-TERM
в O
видео O
с O
виртуальным O
сурдопереводчиком B-TERM
. O

# text =  В Университете Суррея разработчики создали алгоритм сурдоперевода нового поколения.
В O
Университете B-TERM
Суррея I-TERM
разработчики O
создали O
алгоритм B-TERM
сурдоперевода I-TERM
нового O
поколения O
. O

# text =  После этого последовательность поз подается сверточной нейросети U-Net.
После O
этого O
последовательность O
поз O
подается O
сверточной O
нейросети O
U B-TERM
- I-TERM
Net I-TERM
. O

# text =  Один из самых известных продуктов — анимированный виртуальный переводчик от IBM.
Один O
из O
самых O
известных O
продуктов O
— O
анимированный O
виртуальный O
переводчик O
от O
IBM B-TERM
. O

# text =  Программа, придуманная учеными из Новосибирского академгородка, распознает речь, анализирует смысл и переводит на жестовый язык.
Программа O
, O
придуманная O
учеными O
из O
Новосибирского B-TERM
академгородка I-TERM
, O
распознает B-TERM
речь I-TERM
, O
анализирует B-TERM
смысл I-TERM
и O
переводит O
на O
жестовый B-TERM
язык I-TERM
. O

# text =  В то время считали, что разработка станет такой же популярной, как Google Translator.
В O
то O
время O
считали O
, O
что O
разработка O
станет O
такой O
же O
популярной O
, O
как O
Google B-TERM
Translator I-TERM
. O


# text =  Российские ученые из Института проблем управления им. В.А. Трапезникова РАН (ИПУ РАН) несколько лет назад начали разработку подобного ИИ.
Российские O
ученые O
из O
Института B-TERM
проблем I-TERM
управления I-TERM
им. I-TERM 
В.А. I-TERM
Трапезникова I-TERM
РАН I-TERM
( O
ИПУ B-TERM
РАН I-TERM
) O
несколько O
лет O
назад O
начали O
разработку O
подобного O
ИИ O
. O

# text =  Она несколько лет развивает сайт «Сурдосервер».
Она O
несколько O
лет O
развивает O
сайт O
« O
Сурдосервер B-TERM
» O
. O

# text =  N-грамм это просто последовательности букв из слова.
N B-TERM
- I-TERM
грамм I-TERMV
это O
просто O
последовательности O
букв O
из O
слова O
. O

# text =  Создаются лексические и синтаксические признаки токенов текста.
Создаются O
лексические B-TERM
и O
синтаксические B-TERM
признаки I-TERM
токенов B-TERM
текста B-TERM
. O

# text =  В качестве классификатора намерений применяем Transformer.
В 
качестве O
классификатора B-TERM
намерений O
применяем O
Transformer B-TERM
. O
# text = Как отличить хороший ремонт от плохого, или как мы в SRG сделали из Томита-парсера многопоточную Java-библиотеку. 
Как O
отличить O
хороший O
ремонт O
от O
плохого O
, O
или O
как O
мы O
в O
SRG B-TERM
сделали O
из O
Томита B-TERM
- I-TERM
парсера I-TERM
многопоточную O
Java B-TERM
- I-TERM
библиотеку I-TERM
. O

# text = В этой статье речь пойдет о том, как мы интегрировали разработанный Яндексом Томита-парсер в нашу систему, превратили его в динамическую библиотеку, подружили с Java, сделали многопоточной и решили с её помощью задачу классификации текста для оценки недвижимости.
В O
этой O
статье O
речь O
пойдет O
о O
том O
, O
как O
мы O
интегрировали O
разработанный O
Яндексом B-TERM
Томита B-TERM
- I-TERM
парсер I-TERM
в O
нашу O
систему O
, O
превратили O
его O
в O
динамическую O
библиотеку O
, O
подружили O
с O
Java B-TERM
, O
сделали O
многопоточной O
и O
решили O
с O
её O
помощью O
задачу O
классификации B-TERM
текста O
для O
оценки B-TERM
недвижимости I-TERM
. O

# text =  Итак, у нас есть текст объявления, который необходимо классифицировать в одну из категорий согласно состоянию ремонта в квартире (без отделки, чистовой, средний, хороший, отличный, эксклюзивный).
Итак O
, O
у O
нас O
есть O
текст B-TERM
объявления I-TERM
, O
который O
необходимо O
классифицировать O
в O
одну O
из O
категорий O
согласно O
состоянию O
ремонта O
в O
квартире O
( O
без O
отделки O
, O
чистовой O
, O
средний O
, O
хороший O
, O
отличный O
, O
эксклюзивный O
) O
. O

# text =  Таким образом, по мере решения сформировалась вторая большая и интересная задача — научиться извлекать всю достаточную и необходимую информацию о ремонте из объявления, а именно обеспечить быстрый синтаксический и морфологический анализ текста, который сможет работать параллельно под нагрузкой в режиме библиотеки.
Таким O
образом O
, O
по O
мере O
решения O
сформировалась O
вторая O
большая O
и O
интересная O
задача O
— O
научиться O
извлекать B-TERM
всю I-TERM
достаточную I-TERM
и I-TERM
необходимую I-TERM
информацию I-TERM
о O
ремонте O
из O
объявления O
, O
а O
именно O
обеспечить O
быстрый O
синтаксический O
и O
морфологический B-TERM
анализ I-TERM
текста I-TERM
, O
который O
сможет O
работать O
параллельно O
под O
нагрузкой O
в O
режиме O
библиотеки O
. O

# text =  Из доступных средств для извлечения фактов из текста на основе контекстно-свободных грамматик, способных работать с русским языком, наше внимание привлекли Томита-парсер и библиотека Yagry на питоне.
Из O
доступных O
средств O
для O
извлечения B-TERM
фактов I-TERM
из O
текста B-TERM
на O
основе O
контекстно B-TERM
- I-TERM
свободных I-TERM
грамматик I-TERM
, O
способных O
работать O
с O
русским O
языком O
, O
наше O
внимание O
привлекли O
Томита B-TERM
- I-TERM
парсер I-TERM
и O
библиотека O
Yagry B-TERM
на O
питоне O
. O

# text =  Многопоточный вариант Томиты — TomitaPooledParser использует для парсинга пул объектов TomitaParser, одинаковым образом сконфигурированных.
Многопоточный O
вариант O
Томиты B-TERM
— O
TomitaPooledParser B-TERM
использует O
для O
парсинга B-TERM
пул O
объектов O
TomitaParser B-TERM
, O
одинаковым O
образом O
сконфигурированных O
. O

# text =  Приведу только показатели качества классификации, которые были нами получены на тестах: Accuracy = 95% F1 score = 93%
Приведу O
только O
показатели O
качества O
классификации B-TERM
, O
которые O
были O
нами O
получены O
на O
тестах O
: O
Accuracy B-TERM
= O
95 B-TERM
% I-TERM

F1 B-TERM
score I-TERM
= O
93 B-TERM
% I-TERM

# text = JavaScript-библиотека для обработки текстов на русском языке
JavaScript B-TERM
- I-TERM
библиотека I-TERM
для O
обработки B-TERM
текстов I-TERM
на O
русском O
языке O

# text =  Бессвязность текстов в нынешней версии «Генератора» вызвана тем, что на самом деле никакого анализа он производить не умеет.
Бессвязность O
текстов O
в O
нынешней O
версии O
« O
Генератора B-TERM
» O
вызвана O
тем O
, O
что O
на O
самом O
деле O
никакого O
анализа O
он O
производить O
не O
умеет O
. O

# text =  На данный момент библиотека умеет две вещи: токенизацию и анализ морфологии.
На O
данный O
момент O
библиотека O
умеет O
две O
вещи O
: O
токенизацию B-TERM
и O
анализ B-TERM
морфологии I-TERM
. O

# text =  Полный список граммем можно найти на странице проекта OpenCorpora.
Полный O
список O
граммем O
можно O
найти O
на O
странице O
проекта O
OpenCorpora B-TERM
. O

# text =  Кроме того, для анализа используется словарь OpenCorpora, упакованный в специальном формате, но об этом ниже.
Кроме O
того O
, O
для O
анализа B-TERM
используется O
словарь O
OpenCorpora B-TERM
, O
упакованный O
в O
специальном O
формате O
, O
но O
об O
этом O
ниже O
. O

# text =  Вообще создатели проекта OpenCorpora большие молодцы и я вам рекомендую не только ознакомиться с ним, но и принять участие в коллаборативной разметке корпуса — это также поможет и другим опенсорсным проектам.
Вообще O
создатели O
проекта O
OpenCorpora B-TERM
большие O
молодцы O
и O
я O
вам O
рекомендую O
не O
только O
ознакомиться O
с O
ним O
, O
но O
и O
принять O
участие O
в O
коллаборативной O
разметке B-TERM
корпуса I-TERM
— O
это O
также O
поможет O
и O
другим O
опенсорсным O
проектам O
. O

# text =  По сути эта часть библиотеки — порт замечательного морфологического анализатора pymorphy2 за авторством kmike (на Хабре была пара статей об этой библиотеке).
По O
сути O
эта O
часть O
библиотеки O
— O
порт O
замечательного O
морфологического B-TERM
анализатора I-TERM
pymorphy2 B-TERM
за O
авторством O
kmike B-TERM
( O
на O
Хабре B-TERM
была O
пара O
статей O
об O
этой O
библиотеке O
) O
. O

# text =  Анализируем тональность текстов с помощью Fast.ai
Анализируем O
тональность O
текстов O
с O
помощью O
Fast.ai B-TERM

# text = В статье пойдет речь о классификации тональности текстовых сообщений на русском языке (а по сути любой классификации текстов, используя те же технологии).
В O
статье O
пойдет O
речь O
о O
классификации B-TERM
тональности I-TERM
текстовых I-TERM
сообщений I-TERM
на O
русском B-TERM
языке O
( O
а O
по O
сути O
любой O
классификации O
текстов O
, O
используя O
те O
же O
технологии O
) O
. O

# text =  За основу возьмем данную статью, в которой была рассмотрена классификация тональности на архитектуре CNN с использованием Word2vec модели.
За O
основу O
возьмем O
данную O
статью O
, O
в O
которой O
была O
рассмотрена O
классификация B-TERM
тональности I-TERM
на O
архитектуре O
CNN B-TERM
с O
использованием O
Word2vec B-TERM
модели O
. O

# text =  В нашем примере будем решать ту же самую задачу разделения твитов на позитивные и негативные на том же самом датасете с использованием модели ULMFit.
В O
нашем O
примере O
будем O
решать O
ту O
же O
самую O
задачу O
разделения B-TERM
твитов I-TERM
на O
позитивные B-TERM
и O
негативные B-TERM
на O
том O
же O
самом O
датасете O
с O
использованием O
модели O
ULMFit B-TERM
. O

# text =  Результат из статьи (average F1-score = 0.78142) примем в качестве baseline.
Результат O
из O
статьи O
( O
average O
F1-score B-TERM
= O
0.78142 B-TERM
) O
примем O
в O
качестве O
baseline O
. O

# text =  Модель ULMFIT была представлена разработчиками fast.ai (Jeremy Howard, Sebastian Ruder) в 2018 году.
Модель O
ULMFIT B-TERM
была O
представлена O
разработчиками O
fast.ai B-TERM
( O
Jeremy B-TERM
Howard I-TERM
, O
Sebastian B-TERM
Ruder I-TERM
) O
в O
2018 B-TERM
году O
. O

# text =  Суть подхода состоит в использовании transfer learning в задачах NLP, когда вы используете предобученные модели, сокращая время на обучение своих моделей и снижая требования к размерам размеченной тестовой выборки.
Суть O
подхода O
состоит O
в O
использовании O
transfer B-TERM
learning I-TERM
в O
задачах O
NLP B-TERM
, O
когда O
вы O
используете O
предобученные O
модели O
, O
сокращая O
время O
на O
обучение O
своих O
моделей O
и O
снижая O
требования O
к O
размерам O
размеченной O
тестовой O
выборки O
. O

# text =  Для задачи моделирования языка ULMFit использует архитектуру AWD-LSTM, которая предполагает активное использование dropout везде, где только можно и имеет смысл.
Для O
задачи O
моделирования B-TERM
языка O
ULMFit B-TERM
использует O
архитектуру O
AWD B-TERM
- I-TERM
LSTM I-TERM
, O
которая O
предполагает O
активное O
использование O
dropout O
везде O
, O
где O
только O
можно O
и O
имеет O
смысл O
. O

# text =  Результат, показанный на тестовой выборке average F1-score = 0,80064.
Результат O
, O
показанный O
на O
тестовой O
выборке O
average O
F1-score B-TERM
= O
0,80064 B-TERM
. O

# text =  Добавьте возможности IBM Watson платформы в ваши приложения, разработанные на платформе IBM Cloud, или в сторонние приложения!
Добавьте O
возможности O
IBM B-TERM
Watson I-TERM
платформы O
в O
ваши O
приложения O
, O
разработанные O
на O
платформе O
IBM B-TERM
Cloud I-TERM
, O
или O
в O
сторонние O
приложения O
! O

# text =  IBM Automation Platform для цифрового бизнеса — это интегрированная платформа с пятью возможностями автоматизации, которая помогает бизнесу быстро и масштабно управлять практически всеми типами проектов автоматизации — от повторяющихся и административных до работы на уровне экспертов.
IBM B-TERM
Automation I-TERM
Platform I-TERM
для O
цифрового B-TERM
бизнеса I-TERM
— O
это O
интегрированная O
платформа O
с O
пятью O
возможностями O
автоматизации O
, O
которая O
помогает O
бизнесу O
быстро O
и O
масштабно O
управлять O
практически O
всеми O
типами O
проектов O
автоматизации O
— O
от O
повторяющихся O
и O
административных O
до O
работы O
на O
уровне O
экспертов O
. O
# text =  HuggingArtists | Генерируем текст песен с трансформером за 5 минут 
HuggingArtists B-TERM
| O
Генерируем B-TERM
текст B-TERM
песен I-TERM
с O
трансформером O
за O
5 O
минут O

# text =  В HuggingArtists, мы можем создавать тексты песен на основе конкретного исполнителя.
В O
HuggingArtists B-TERM
, O
мы O
можем O
создавать B-TERM
тексты I-TERM
песен O
на O
основе O
конкретного O
исполнителя O
. O

# text =  Это было сделано путем fine-tune (точной настройки) предварительно обученного трансформера HuggingFace  на собранных данных Genius.
Это O
было O
сделано O
путем O
fine B-TERM
- I-TERM
tune I-TERM
( O
точной B-TERM
настройки I-TERM
) O
предварительно O
обученного O
трансформера O
HuggingFace B-TERM
  O
на O
собранных O
данных O
Genius B-TERM
. O

# text =  Кроме того, мы используем интеграцию Weights & Biases для автоматического учета производительности и прогнозов модели.
Кроме O
того O
, O
мы O
используем O
интеграцию O
Weights B-TERM
& I-TERM
Biases I-TERM
для O
автоматического O
учета O
производительности O
и O
прогнозов O
модели O
. O

# text = Анализ тональности текста с использованием фреймворка Lightautoml 
Анализ B-TERM
тональности I-TERM
текста I-TERM
с O
использованием O
фреймворка O
Lightautoml B-TERM

# text = Сентиментный анализ (анализ тональности) – это область компьютерной лингвистики, занимающаяся изучением эмоций в текстовых документах, в основе которой лежит машинное обучение.
Сентиментный B-TERM
анализ I-TERM
( O
анализ B-TERM
тональности I-TERM
) O
– O
это O
область O
компьютерной O
лингвистики O
, O
занимающаяся O
изучением O
эмоций O
в O
текстовых O
документах O
, O
в O
основе O
которой O
лежит O
машинное O
обучение O
. O

# text = В этой статье я покажу, как мы использовали для этих целей внутреннюю разработку компании – фреймворк LightAutoML, в котором имеется всё для решения поставленной задачи – предобученные готовые векторные представления слов FastText и готовые текстовые пресеты, в которых необходимо только указать гиперпараметры.
В O
этой O
статье O
я O
покажу O
, O
как O
мы O
использовали O
для O
этих O
целей O
внутреннюю O
разработку O
компании O
– O
фреймворк O
LightAutoML B-TERM
, O
в O
котором O
имеется O
всё O
для O
решения O
поставленной O
задачи O
– O
предобученные B-TERM
готовые I-TERM
векторные I-TERM
представления I-TERM
слов I-TERM
FastText B-TERM
и O
готовые O
текстовые O
пресеты O
, O
в O
которых O
необходимо O
только O
указать O
гиперпараметры O
. O

# text =  При обучении модели значение метрики F1-score достигло 0.894, соответственно можно сделать вывод о том, что модель хорошо справляется с задачей определения нейтральных и негативных обращений.
При O
обучении O
модели O
значение O
метрики O
F1-score B-TERM
достигло O
0.894 B-TERM
, O
соответственно O
можно O
сделать O
вывод O
о O
том O
, O
что O
модель O
хорошо O
справляется O
с O
задачей O
определения B-TERM
нейтральных I-TERM
и I-TERM
негативных I-TERM
обращений I-TERM
. O

# text =  Также одним из способов оценить работу модели в целом можно по кривой ROC-AUC, которая описывает площадь под кривой (Area Under Curve – Receiver Operating Characteristic).
Также O
одним O
из O
способов O
оценить O
работу O
модели O
в O
целом O
можно O
по O
кривой O
ROC B-TERM
- I-TERM
AUC I-TERM
, O
которая O
описывает O
площадь O
под O
кривой O
( O
Area B-TERM
Under I-TERM
Curve I-TERM
– I-TERM
Receiver I-TERM
Operating I-TERM
Characteristic I-TERM
) O
. O

# text =  В качестве подтверждения вышесказанного можно привести работу встроенного в LAMA модуля – LIME, который раскрывает работу модели окрашивая слова в тот или иной цвет, в зависимости от их эмоционального окраса.
В O
качестве O
подтверждения O
вышесказанного O
можно O
привести O
работу O
встроенного O
в O
LAMA B-TERM
модуля O
– O
LIME B-TERM
, O
который O
раскрывает O
работу O
модели O
окрашивая O
слова O
в O
тот O
или O
иной O
цвет O
, O
в O
зависимости O
от O
их O
эмоционального O
окраса O
. O

# text =  Также фреймворк может решать задачи регрессионного анализа, целью которого является определение зависимости между переменными и оценкой функции регрессии.
Также O
фреймворк O
может O
решать O
задачи O
регрессионного B-TERM
анализа I-TERM
, O
целью O
которого O
является O
определение B-TERM
зависимости I-TERM
между I-TERM
переменными I-TERM
и O
оценкой B-TERM
функции I-TERM
регрессии I-TERM
. O

# text =  .Работа с текстомВ LightAutoML имеется большое количество вариантов разработки той или иной модели, работающей с текстом.
.Работа O
с O
текстомВ O
LightAutoML B-TERM
имеется O
большое O
количество O
вариантов O
разработки O
той O
или O
иной O
модели O
, O
работающей O
с O
текстом O
. O

# text =  Библиотека предоставляет не только получение стандартных признаков на основе TF-IDF, но и на основе эмбеддингов:1) На основе встроенного FastText, который можно тренировать на том или ином корпусе2) Предобученных моделей Gensim3) Любой другой объект, который имеет вид словаря, где на вход подается слово, а на выходе его эмбеддинги
Библиотека O
предоставляет O
не O
только O
получение O
стандартных O
признаков O
на O
основе O
TF B-TERM
- I-TERM
IDF I-TERM
, O
но O
и O
на O
основе O
эмбеддингов O
: O
1 O
) O
На O
основе O
встроенного O
FastText B-TERM
, O
который O
можно O
тренировать O
на O
том O
или O
ином O
корпусе O
2 O
) O
Предобученных O
моделей O
Gensim3 B-TERM
) O
Любой O
другой O
объект O
, O
который O
имеет O
вид O
словаря O
, O
где O
на O
вход O
подается O
слово O
, O
а O
на O
выходе O
его O
эмбеддинги O

# text = Среди используемых стратегий извлечения представлений текстов из эмбеддингов слов, можно выделить:1) Weighted Average Transformer (WAT) – взвешивается каждое слово с некоторым весом
Среди O
используемых O
стратегий O
извлечения O
представлений O
текстов O
из O
эмбеддингов O
слов O
, O
можно O
выделить:1 O
) O
Weighted B-TERM
Average I-TERM
Transformer I-TERM
( O
WAT B-TERM
) O
– O
взвешивается O
каждое O
слово O
с O
некоторым O
весом O
			
# text = Bag of Random Embedding Projections (BOREP) – строится линейная модель со случайными весами  
Bag B-TERM
of I-TERM
Random I-TERM
Embedding I-TERM
Projections I-TERM
( O
BOREP O
) O
– O
строится O
линейная O
модель O
со O
случайными O
весами O

# text = Bert Pooling – получение эмбеддинга с последнего выхода модели Transformer  
Bert B-TERM
Pooling I-TERM
– O
получение O
эмбеддинга O
с O
последнего O
выхода O
модели O
Transformer B-TERM

# text = За препроцессинг текста отвечает класс токенайзера, по умолчанию применяется только для TF-IDF.
За O
препроцессинг O
текста O
отвечает O
класс O
токенайзера O
, O
по O
умолчанию O
применяется O
только O
для O
TF B-TERM
- I-TERM
IDF I-TERM
. O

# text = Подводя итоги стоит сказать, что LightAutoML благодаря встроенному инструментарию способен показывать достаточно хорошие результаты в задачах бинарной или мультиклассовой классификации и регрессии.
Подводя O
итоги O
стоит O
сказать O
, O
что O
LightAutoML O
благодаря O
встроенному O
инструментарию O
способен O
показывать O
достаточно O
хорошие O
результаты O
в O
задачах O
бинарной O
или O
мультиклассовой B-TERM
классификации I-TERM
и O
регрессии O
. O

# text = Конкретно в нашем случае нам удалось создать модель сентиментного анализа, которая с 89% точностью определяет эмоциональный окрас обращения и слова, которые оказывают на это наибольшее влияние.
Конкретно O
в O
нашем O
случае O
нам O
удалось O
создать O
модель O
сентиментного O
анализа O
, O
которая O
с O
89 B-TERM
% I-TERM
точностью B-TERM
определяет O
эмоциональный O
окрас O
обращения O
и O
слова O
, O
которые O
оказывают O
на O
это O
наибольшее O
влияние O
. O

# text =  Яндекс открывает датасеты Беспилотных автомобилей, Погоды и Переводчика, чтобы помочь решить проблему сдвига данных в ML       
Яндекс B-TERM
открывает O
датасеты O
Беспилотных O
автомобилей O
, O
Погоды B-TERM
и O
Переводчика B-TERM
, O
чтобы O
помочь O
решить O
проблему O
сдвига B-TERM
данных I-TERM
в O
ML B-TERM

# text =  Для современных моделей, которые используются в машинном переводе, такой язык представляет серьезную проблему, так как большинство переводчиков обучаются на чуть более формальном языке: классической литературе, юридических документах или статьях Википедии.
Для O
современных O
моделей O
, O
которые O
используются O
в O
машинном O
переводе O
, O
такой O
язык O
представляет O
серьезную O
проблему O
, O
так O
как O
большинство O
переводчиков O
обучаются O
на O
чуть O
более O
формальном O
языке O
: O
классической B-TERM
литературе I-TERM
, O
юридических B-TERM
документах I-TERM
или O
статьях B-TERM
Википедии B-TERM
. O

# text =  В треке перевода мы использовали для обучения англо-русский корпус WMT’20, который в основном состоит из государственных и новостных текстов.
В O
треке O
перевода O
мы O
использовали O
для O
обучения O
англо O
- O
русский O
корпус O
WMT’20 B-TERM
, O
который O
в O
основном O
состоит O
из O
государственных O
и O
новостных B-TERM
текстов I-TERM
. O

# text =  Данные без сдвига взяты из англо-русского корпуса Newstest’19, а также из корпуса новостных текстов, собранных службой Global Voices и переведенных Яндексом.
Данные O
без O
сдвига O
взяты O
из O
англо B-TERM
- O
русского B-TERM
корпуса O
Newstest’19 B-TERM
, O
а O
также O
из O
корпуса O
новостных O
текстов O
, O
собранных O
службой O
Global B-TERM
Voices I-TERM
и O
переведенных O
Яндексом B-TERM
. O

# text =  Данные со сдвигом для отладки взяты из подготовленного для WMT Robustness Challenge корпуса Reddit и также переведены Яндексом.
Данные O
со O
сдвигом O
для O
отладки O
взяты O
из O
подготовленного O
для O
WMT O
Robustness O
Challenge O
корпуса O
Reddit B-TERM
и O
также O
переведены O
Яндексом B-TERM
. O

# text =  Для проверки модели на данных со сдвигом мы также собрали, перевели и разметили дополнительные данные с Reddit.
Для O
проверки O
модели O
на O
данных O
со O
сдвигом O
мы O
также O
собрали O
, O
перевели O
и O
разметили O
дополнительные O
данные O
с O
Reddit B-TERM
. O
# text =  Парсить комментарии мы будем с помощью официального API ВКонтакте для Python
Парсить O
комментарии O
мы O
будем O
с O
помощью O
официального O
API B-TERM
ВКонтакте I-TERM
для O
Python B-TERM

# text =  Необходимо убрать из комментария направление, чтобы при поиске расстояния Левенштейна меньше ошибаться.
Необходимо O
убрать O
из O
комментария O
направление O
, O
чтобы O
при O
поиске O
расстояния B-TERM
Левенштейна I-TERM
меньше O
ошибаться O
. O

# text =  Небольшая справка: расстояние Левенштейна — минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую.
Небольшая O
справка O
: O
расстояние B-TERM
Левенштейна I-TERM
— O
минимальное O
количество O
операций O
вставки O
одного O
символа O
, O
удаления O
одного O
символа O
и O
замены O
одного O
символа O
на O
другой O
, O
необходимых O
для O
превращения O
одной O
строки O
в O
другую O
. O

# text =  Его мы будем находить с помощью библиотеки fuzzywuzzy.
Его O
мы O
будем O
находить O
с O
помощью O
библиотеки O
fuzzywuzzy B-TERM
. O

# text =  Для ускорения работы авторы библиотеки советуют также установить библиотеку python-Levenshtein.
Для O
ускорения O
работы O
авторы O
библиотеки O
советуют O
также O
установить O
библиотеку O
python B-TERM
- I-TERM
Levenshtein I-TERM
. O

# text =  Его мне любезно предоставил разработчик приложения GoTrans, Александр Козлов.
Его O
мне O
любезно O
предоставил O
разработчик O
приложения O
GoTrans B-TERM
, O
Александр B-TERM
Козлов I-TERM
. O
# text =  Самый сложный кроссворд, составленный компьютером
Самый O
сложный O
кроссворд B-TERM
, O
составленный O
компьютером O

# text =  Пример Deep Blue показывает, что программы ИИ могут участвовать в викторинах и обыгрывать людей.
Пример O
Deep B-TERM
Blue I-TERM
показывает O
, O
что O
программы O
ИИ O
могут O
участвовать O
в O
викторинах B-TERM
и O
обыгрывать O
людей O
. O

# text =  Американский разработчик Мэтью Гинсберг (Matthew Ginsberg) создал программу под названием Dr Fill, которая справляется с кроссвордами гораздо лучше, чем абсолютное большинство людей, пишет New Scientist.
Американский O
разработчик O
Мэтью B-TERM
Гинсберг I-TERM
( O
Matthew B-TERM
Ginsberg I-TERM
) O
создал O
программу O
под O
названием O
Dr B-TERM
Fill I-TERM
, O
которая O
справляется O
с O
кроссвордами B-TERM
гораздо O
лучше O
, O
чем O
абсолютное O
большинство O
людей O
, O
пишет O
New B-TERM
Scientist I-TERM
. O
# text =  Анализ тональности текстов с помощью сверточных нейронных сетей 
Анализ B-TERM
тональности I-TERM
текстов I-TERM
с O
помощью O
сверточных B-TERM
нейронных I-TERM
сетей I-TERM

# text =  Есть много способов решать такую задачу, и один из них — свёрточные нейронные сети (Convolutional Neural Networks).
Есть O
много O
способов O
решать O
такую O
задачу O
, O
и O
один O
из O
них O
— O
свёрточные B-TERM
нейронные I-TERM
сети I-TERM
( O
Convolutional B-TERM
Neural I-TERM
Networks I-TERM
) O
. O

# text =  CNN изначально были разработаны для обработки изображений, однако они успешно справляются с решением задач в сфере автоматической обработки текстов.
CNN B-TERM
изначально O
были O
разработаны O
для O
обработки O
изображений O
, O
однако O
они O
успешно O
справляются O
с O
решением O
задач O
в O
сфере O
автоматической B-TERM
обработки I-TERM
текстов I-TERM
. O

# text =  Я познакомлю вас с бинарным анализом тональности русскоязычных текстов с помощью свёрточной нейронной сети, для которой векторные представления слов были сформированы на основе обученной Word2Vec модели.
Я O
познакомлю O
вас O
с O
бинарным O
анализом B-TERM
тональности I-TERM
русскоязычных B-TERM
текстов B-TERM
с O
помощью O
свёрточной B-TERM
нейронной I-TERM
сети I-TERM
, O
для O
которой O
векторные B-TERM
представления I-TERM
слов I-TERM
были O
сформированы O
на O
основе O
обученной O
Word2Vec B-TERM
модели O
. O

# text =  Для обучения я выбрал корпус коротких текстов Юлии Рубцовой, сформированный на основе русскоязычных сообщений из Twitter [4].
Для O
обучения O
я O
выбрал O
корпус B-TERM
коротких I-TERM
текстов I-TERM
Юлии B-TERM
Рубцовой I-TERM
, O
сформированный O
на O
основе O
русскоязычных B-TERM
сообщений B-TERM
из O
Twitter B-TERM
[ O
4 O
] O
. O

# text =  Визуализация кластеров похожих слов с использование t-SNE.
Визуализация O
кластеров B-TERM
похожих O
слов O
с O
использование O
t B-TERM
- I-TERM
SNE I-TERM
. O

# text =  На следующем этапе каждый текст был отображен в массив идентификаторов токенов.
На O
следующем O
этапе O
каждый O
текст B-TERM
был O
отображен O
в O
массив O
идентификаторов O
токенов B-TERM
. O
# text =  Вот пусть комментаторы поправят, но кроме модуля LanguageTool для Open Office (о нём мы ещё поговорим) даже в голову ничего не приходит.
Вот O
пусть O
комментаторы O
поправят O
, O
но O
кроме O
модуля O
LanguageTool B-TERM
для O
Open B-TERM
Office I-TERM
( O
о O
нём O
мы O
ещё O
поговорим O
) O
даже O
в O
голову O
ничего O
не O
приходит O
. O

# text =  Было бы здорово составить базу с инструкциями не для людей, а для роботов, подумали инженеры из Института искусственного интеллекта при Бременском университете (Германия), авторы проекта RoboHow.
Было O
бы O
здорово O
составить O
базу O
с O
инструкциями B-TERM
не O
для O
людей O
, O
а O
для O
роботов O
, O
подумали O
инженеры O
из O
Института B-TERM
искусственного I-TERM
интеллекта I-TERM
при O
Бременском B-TERM
университете I-TERM
( O
Германия O
) O
, O
авторы O
проекта O
RoboHow B-TERM
. O

# text =  С такой базой wiki-инструкций роботы смогут передавать информацию друг другу.
С O
такой O
базой O
wiki B-TERM
- I-TERM
инструкций I-TERM
роботы O
смогут O
передавать O
информацию O
друг O
другу O
. O

# text =  Созданный в Бременском университете робот PR2 (на фото вверху) учится понимать и выполнять «человеческие» инструкции из базы WikiHow.
Созданный O
в O
Бременском B-TERM
университете I-TERM
робот O
PR2 O
( O
на O
фото O
вверху O
) O
учится O
понимать O
и O
выполнять O
« O
человеческие O
» O
инструкции B-TERM
из O
базы O
WikiHow B-TERM
. O

# text =  Успешно выполнив задачу, то есть усвоив урок, робот делится приобретёнными знаниями со своими собратьями через онлайновую базу Open Ease.
Успешно O
выполнив O
задачу O
, O
то O
есть O
усвоив O
урок O
, O
робот O
делится O
приобретёнными O
знаниями O
со O
своими O
собратьями O
через O
онлайновую O
базу O
Open B-TERM
Ease I-TERM
. O

# text =  Здесь инструкции записаны в машиночитаемом виде, на языке, похожем на язык Семантической сети.
Здесь O
инструкции B-TERM
записаны O
в O
машиночитаемом O
виде O
, O
на O
языке O
, O
похожем O
на O
язык O
Семантической B-TERM
сети I-TERM
. O

# text =  Это очень сложная задача, которая сочетает в себе тесную интеграцию распознавания речи, интерпретации команд на естественном языке, машинного зрения и планирования сложных действий через алгоритмы осуществления отдельных манипуляций.
Это O
очень O
сложная O
задача O
, O
которая O
сочетает O
в O
себе O
тесную O
интеграцию O
распознавания B-TERM
речи I-TERM
, O
интерпретации B-TERM
команд I-TERM
на I-TERM
естественном I-TERM
языке I-TERM
, O
машинного O
зрения O
и O
планирования O
сложных O
действий O
через O
алгоритмы O
осуществления O
отдельных O
манипуляций O
. O

# text =  «М.видео-Эльдорадо» внедряет нейросеть для ответов на вопросы покупателей 
« O
М.видео B-TERM
- I-TERM
Эльдорадо I-TERM
» O
внедряет O
нейросеть O
для O
ответов B-TERM
на I-TERM
вопросы I-TERM
покупателей O

# text =  Президент Ассоциации больших данных Анна Серебряникова отметила, что ИИ в ретейле может применяться для прогнозирования открытия новых торговых точек, а также для персонализации предложений для клиентов и создания чат-ботов для службы поддержки.
Президент O
Ассоциации B-TERM
больших I-TERM
данных I-TERM
Анна B-TERM
Серебряникова I-TERM
отметила O
, O
что O
ИИ O
в O
ретейле O
может O
применяться O
для O
прогнозирования O
открытия O
новых O
торговых O
точек O
, O
а O
также O
для O
персонализации B-TERM
предложений I-TERM
для O
клиентов O
и O
создания B-TERM
чат I-TERM
- I-TERM
ботов I-TERM
для O
службы O
поддержки O
. O

# text =  В Facebook AI продемонстрировали прямой машинный перевод с одного языка на другой
В O
Facebook B-TERM
AI I-TERM
продемонстрировали O
прямой O
машинный O
перевод O
с O
одного O
языка O
на O
другой O

# text = Facebook AI представила новую систему машинного перевода M2M-100 с 15 млрд параметров.
Facebook B-TERM
AI I-TERM
представила O
новую O
систему B-TERM
машинного I-TERM
перевода I-TERM
M2M-100 B-TERM
с O
15 O
млрд O
параметров O
. O

# text =  Она способна переводить с одного языка на другой напрямую, не используя английский в качестве промежуточного.
Она O
способна O
переводить B-TERM
с I-TERM
одного I-TERM
языка I-TERM
на I-TERM
другой I-TERM
напрямую O
, O
не O
используя O
английский O
в O
качестве O
промежуточного O
. O

# text =  Она способна осуществлять переводы между парами из ста языков.
Она O
способна O
осуществлять O
переводы B-TERM
между O
парами O
из O
ста O
языков O
. O

# text =  Модель обучали на наборе данных из более чем 7,5 млрд предложений как из базы Facebook, так и из других источников.
Модель O
обучали O
на O
наборе O
данных O
из O
более O
чем O
7,5 O
млрд O
предложений O
как O
из O
базы O
Facebook B-TERM
, O
так O
и O
из O
других O
источников O
. O

# text =  При разработке использовали инструмент CommonCrawl, который поддерживает открытый репозиторий данных веб-сканирования, и систему классификации текстов FastText, которую в Facebook представили несколько лет назад.
При O
разработке O
использовали O
инструмент O
CommonCrawl B-TERM
, O
который O
поддерживает O
открытый O
репозиторий O
данных O
веб O
- O
сканирования O
, O
и O
систему O
классификации O
текстов O
FastText B-TERM
, O
которую O
в O
Facebook B-TERM
представили O
несколько O
лет O
назад O
. O

# text =  Согласно метрикам BLEU, M2M-100 на 10 баллов опережает предшественника, где английский язык был промежуточным.
Согласно O
метрикам O
BLEU B-TERM
, O
M2M-100 B-TERM
на O
10 O
баллов O
опережает O
предшественника O
, O
где O
английский O
язык O
был O
промежуточным O
. O

# text =  Facebook AI отмечает, что эта модель может быть полезной не только при машинном переводе, но и при изучении языков.
Facebook B-TERM
AI I-TERM
отмечает O
, O
что O
эта O
модель O
может O
быть O
полезной O
не O
только O
при O
машинном B-TERM
переводе I-TERM
, O
но O
и O
при O
изучении O
языков O
. O
# text =  Я тестировала Google Translate на одних и тех же текстах в марте и декабре 2011, январе 2016 и декабре 2017 года.
Я O
тестировала O
Google B-TERM
Translate I-TERM
на O
одних O
и O
тех O
же O
текстах O
в O
марте O
и O
декабре O
2011 O
, O
январе O
2016 O
и O
декабре O
2017 O
года O
. O

# text =  Брала одни и те же отрывки на английском, русском, немецком, французском, украинском и польском языках и переводила каждый на остальные пять языков из выборки.
Брала O
одни O
и O
те O
же O
отрывки O
на O
английском B-TERM
, O
русском B-TERM
, O
немецком B-TERM
, O
французском B-TERM
, O
украинском B-TERM
и O
польском B-TERM
языках O
и O
переводила O
каждый O
на O
остальные O
пять O
языков O
из O
выборки O
. O

# text =  Результаты cross-verification в целом совпали с тенденциями в первоначальной выборке.
Результаты O
cross B-TERM
- I-TERM
verification I-TERM
в O
целом O
совпали O
с O
тенденциями O
в O
первоначальной O
выборке O
. O

# text =  С марта 2017 года нейросеть стали использовать для перевода на русский.
С O
марта O
2017 B-TERM
года O
нейросеть O
стали O
использовать O
для O
перевода B-TERM
на O
русский B-TERM
. O

# text =  Сервис не переводит дословно, результат стал более свободным: адекватная перефразировка, перегруппировка слов, перестановка слов из начала в конец предложения, если того требуют правила языка (в немецком это реализовано великолепно).
Сервис O
не O
переводит O
дословно O
, O
результат O
стал O
более O
свободным O
: O
адекватная O
перефразировка B-TERM
, O
перегруппировка B-TERM
слов I-TERM
, O
перестановка B-TERM
слов I-TERM
из O
начала O
в O
конец O
предложения O
, O
если O
того O
требуют O
правила O
языка O
( O
в O
немецком B-TERM
это O
реализовано O
великолепно O
) O
. O

# text =  В отличие от предыдущего уровня (phrase-based translation– однократное нахождение соответствий отдельных слов и фраз), нейронный переводчик в какой-то степени трансформирует предложения, анализирует их как единое целое и устанавливает соответствия «из конца в конец» в несколько стадий(end-to-end mapping – сквозное преобразование, полного цикла, непрерывная трансформация многообразия данных со входа на выход).
В O
отличие O
от O
предыдущего O
уровня O
( O
phrase B-TERM
- I-TERM
based I-TERM
translation I-TERM
– O
однократное B-TERM
нахождение I-TERM
соответствий I-TERM
отдельных I-TERM
слов I-TERM
и O
фраз O
) O
, O
нейронный O
переводчик O
в O
какой O
- O
то O
степени O
трансформирует O
предложения O
, O
анализирует O
их O
как O
единое O
целое O
и O
устанавливает O
соответствия O
« O
из O
конца O
в O
конец O
» O
в O
несколько O
стадий O
( O
end B-TERM
- I-TERM
to I-TERM
- I-TERM
end I-TERM
mapping I-TERM
– O
сквозное B-TERM
преобразование I-TERM
, O
полного O
цикла O
, O
непрерывная O
трансформация B-TERM
многообразия I-TERM
данных I-TERM
со O
входа O
на O
выход O
) O
. O

# text =  Сейчас в Яндексе мой основной проект это Алиса, голосовой помощник, который Яндекс запустил в октябре прошлого года, и моя группа отвечает за то, что можно условно назвать мозгами Алисы.
Сейчас O
в O
Яндексе B-TERM
мой O
основной O
проект O
это O
Алиса B-TERM
, O
голосовой B-TERM
помощник I-TERM
, O
который O
Яндекс B-TERM
запустил O
в O
октябре O
прошлого O
года O
, O
и O
моя O
группа O
отвечает O
за O
то O
, O
что O
можно O
условно O
назвать O
мозгами O
Алисы B-TERM
. O

# text =  Мы интерпретируем то, что сказал пользователь на естественном языке и превращаем это в некоторое структурированное представление.
Мы O
интерпретируем O
то O
, O
что O
сказал O
пользователь O
на O
естественном B-TERM
языке I-TERM
и O
превращаем O
это O
в O
некоторое O
структурированное B-TERM
представление B-TERM
. O

# text =  Есть Siri, единственный голосовой помощник, который тоже понимает русский язык, но он работает только на iOS и MacOS, это как бы не самая популярная платформа в России, и к Siri как к продукту тоже есть определенные вопросы.
Есть O
Siri B-TERM
, O
единственный O
голосовой B-TERM
помощник I-TERM
, O
который O
тоже O
понимает O
русский B-TERM
язык O
, O
но O
он O
работает O
только O
на O
iOS B-TERM
и O
MacOS B-TERM
, O
это O
как O
бы O
не O
самая O
популярная O
платформа O
в O
России O
, O
и O
к O
Siri B-TERM
как O
к O
продукту O
тоже O
есть O
определенные O
вопросы O
. O

# text =  На самом деле у нас уже есть модель которая оценивает градацию этой оскорбительности, и если бы возникла продуктовая необходимость, мы уже могли бы сделать такой ползунок который делает ответы более или менее дерзкими.
На O
самом O
деле O
у O
нас O
уже O
есть O
модель O
которая O
оценивает B-TERM
градацию I-TERM
этой O
оскорбительности I-TERM
, O
и O
если O
бы O
возникла O
продуктовая O
необходимость O
, O
мы O
уже O
могли O
бы O
сделать O
такой O
ползунок O
который O
делает O
ответы O
более O
или O
менее O
дерзкими O
. O
# text =  Это генеративная нейронная сеть, способная решать множество задач по обработке естествнного языка (NLP).
Это O
генеративная B-TERM
нейронная I-TERM
сеть I-TERM
, O
способная O
решать O
множество O
задач O
по O
обработке B-TERM
естествнного I-TERM
языка I-TERM
( O
NLP B-TERM
) O
. O

# text =  Это такие задачи как суммаризация (сделать из большого текста его резюме), понимание текста (NLU), вопросно-ответные системы, генерация (например, стихов, — на Хабре была хорошая статья) и другие.
Это O
такие O
задачи O
как O
суммаризация B-TERM
( O
сделать O
из O
большого O
текста O
его O
резюме O
) O
, O
понимание B-TERM
текста I-TERM
( O
NLU B-TERM
) O
, O
вопросно B-TERM
- I-TERM
ответные I-TERM
системы I-TERM
, O
генерация B-TERM
( O
например O
, O
стихов O
, O
— O
на O
Хабре B-TERM
была O
хорошая O
статья O
) O
и O
другие O
. O
# text =  В Яндекс.Браузер внедрили машинный перевод видеороликов 
В O
Яндекс B-TERM
. I-TERM
Браузер I-TERM
внедрили O
машинный O
перевод O
видеороликов O

# text =  Алгоритм отслеживает темп речи говорящего, за счет чего переводчик делает паузы, замедляет или ускоряет речь, чтобы закадровый голос совпадал с картинкой.Перевод доступен в Яндекс.Браузере для Windows и macOS.
Алгоритм O
отслеживает O
темп B-TERM
речи I-TERM
говорящего O
, O
за O
счет O
чего O
переводчик O
делает O
паузы B-TERM
, O
замедляет O
или O
ускоряет O
речь B-TERM
, O
чтобы O
закадровый O
голос B-TERM
совпадал O
с O
картинкой O
. O
Перевод O
доступен O
в O
Яндекс B-TERM
. O
Браузере O
для O
Windows B-TERM
и O
macOS B-TERM
. O

# text =  Тогда к статистической модели, которая была в «Переводчике» с момента запуска, добавили технологию перевода с помощью нейросети.
Тогда O
к O
статистической O
модели O
, O
которая O
была O
в O
« O
Переводчике B-TERM
» O
с O
момента O
запуска O
, O
добавили O
технологию O
перевода B-TERM
с O
помощью O
нейросети O
. O

# text =  Компания объясняла, что ее технология не разбивает текст на отдельные слова, а рассматривает его целиком, чтобы лучше передать смысл.В июне Яндекс открыл доступ к нейросети «Балабоба» для всех пользователей.
Компания O
объясняла O
, O
что O
ее O
технология O
не O
разбивает O
текст B-TERM
на O
отдельные O
слова B-TERM
, O
а O
рассматривает O
его O
целиком O
, O
чтобы O
лучше O
передать O
смысл O
. O
В O
июне O
Яндекс B-TERM
открыл O
доступ O
к O
нейросети O
« O
Балабоба B-TERM
» O
для O
всех O
пользователей O
. O

# text =  Она работает на языковой модели из семейства YaLM (Yet another Language Model).
Она O
работает O
на O
языковой O
модели O
из O
семейства O
YaLM B-TERM
( O
Yet B-TERM
another I-TERM
Language I-TERM
Model I-TERM
) O
. O

# text =  Эта модель помогает нейросети запоминать правила языка, выбирать подходящие слова и связывать их по смыслу.
Эта O
модель O
помогает O
нейросети O
запоминать B-TERM
правила I-TERM
языка I-TERM
, O
выбирать B-TERM
подходящие I-TERM
слова I-TERM
и O
связывать O
их O
по O
смыслу O
. O

# text =  У «Балабобы» нет своего мнения, она выдает случайные продолжения и может закончить историю, придумать подпись или написать небольшой рассказ.
У O
« O
Балабобы O
» O
нет O
своего O
мнения O
, O
она O
выдает O
случайные O
продолжения O
и O
может O
закончить O
историю O
, O
придумать O
подпись O
или O
написать O
небольшой O
рассказ O
. O
# text =  AntiToxicBot — бот, распознающий токсичных пользователей в телеграм чатах.
AntiToxicBot O
— O
бот O
, O
распознающий B-TERM
токсичных I-TERM
пользователей I-TERM
в O
телеграм O
чатах O
. O

# text =  Почему же выбрано CNN+GRU, а не просто GRU или CNN?
Почему O
же O
выбрано O
CNN+GRU B-TERM
, O
а O
не O
просто O
GRU B-TERM
или O
CNN B-TERM
? O

# text =  Нейросеть состоит из 3-х основных частей(CNN, GRU, Linear).
Нейросеть O
состоит O
из O
3-х O
основных O
частей O
( O
CNN B-TERM
, O
GRU B-TERM
, O
Linear B-TERM
) O
. O

# text =  Как и в классификации картинок, свёрточный слой выделяет “признаки”, но в нашем случае векторизированный текст.
Как O
и O
в O
классификации O
картинок O
, O
свёрточный O
слой O
выделяет O
“ O
признаки O
” O
, O
но O
в O
нашем O
случае O
векторизированный B-TERM
текст I-TERM
. O

# text =  То-есть данная часть сети учится выделять признаки токсичных и позитивных сообщений.
То O
- O
есть O
данная O
часть O
сети O
учится O
выделять B-TERM
признаки I-TERM
токсичных I-TERM
и I-TERM
позитивных I-TERM
сообщений I-TERM
. O

# text =   GRU - Recurrent Neural Network
GRU B-TERM
- O
Recurrent B-TERM
Neural I-TERM
Network

# text =  Чтобы обрабатывать последовательности произвольной длины, используют рекуррентные слои.
Чтобы O
обрабатывать B-TERM
последовательности I-TERM
произвольной I-TERM
длины I-TERM
, O
используют O
рекуррентные O
слои O
. O

# text =  В архитектуре используется рекуррентный слой GRU.
В O
архитектуре O
используется O
рекуррентный O
слой O
GRU B-TERM
. O

# text = Данный слой учится делать заключительное решение по определению тональности текста на основе предыдущих слоёв.
Данный O
слой O
учится O
делать O
заключительное O
решение O
по O
определению B-TERM
тональности I-TERM
текста I-TERM
на O
основе O
предыдущих O
слоёв O
. O

# text =  Датасет был взят с сайта kaggle.
Датасет O
был O
взят O
с O
сайта O
kaggle B-TERM
. O

# text =  Около 14000 комментариев с разметкой токсичное сообщение или нет.
Около O
14000 O
комментариев O
с O
разметкой B-TERM
токсичное B-TERM
сообщение I-TERM
или O
нет O
. O

# text =  Для решения данной проблемы была использована библиотека Yandex Speller, которая исправляет орфографические ошибки.
Для O
решения O
данной O
проблемы O
была O
использована O
библиотека O
Yandex B-TERM
Speller I-TERM
, O
которая O
исправляет B-TERM
орфографические I-TERM
ошибки I-TERM
. O

# text =  Можно было обучить собственный Word2Vec на основе данного набора данных, но лучше взять уже обученный.
Можно O
было O
обучить O
собственный O
Word2Vec B-TERM
на O
основе O
данного O
набора O
данных O
, O
но O
лучше O
взять O
уже O
обученный O
. O

# text =  Например: Navec.
Например O
: O
Navec B-TERM
. O

# text =  Модель обучали на русской литературе (~150gb), что говорит о качественной векторизации текста.
Модель O
обучали O
на O
русской B-TERM
литературе I-TERM
( O
~150 O
gb O
) O
, O
что O
говорит O
о O
качественной O
векторизации O
текста O
. O

# text = Для классификации используется обыкновенная функция потерь – кросс энтропия.
Для O
классификации B-TERM
используется O
обыкновенная O
функция B-TERM
потерь I-TERM
– I-TERM
кросс I-TERM
энтропия I-TERM
. O

# text = При обучении сети надо обращать внимание на основные параметры такие, как loss, precision и accuracy.
При O
обучении O
сети O
надо O
обращать O
внимание O
на O
основные O
параметры O
такие O
, O
как O
loss B-TERM
, O
precision B-TERM
и O
accuracy B-TERM
. O

# text =  В ~80% случаев нейросеть классифицирует тональность текста правильно.
В O
~80 B-TERM
% O
случаев O
нейросеть O
классифицирует O
тональность O
текста O
правильно O
. O

# text =  Теперь нейронная сеть указала конкретные сцены, написанные не Шекспиром, и определила, кто на самом деле их написал.
Теперь O
нейронная B-TERM
сеть I-TERM
указала O
конкретные O
сцены O
, O
написанные O
не O
Шекспиром O
, O
и O
определила O
, O
кто O
на O
самом O
деле O
их O
написал O
. O

# text =  Плехач обучил алгоритм распознавать стиль Шекспира на пьесах «Кориолан», «Цимбелин», «Зимняя сказка» и «Буря».
Плехач B-TERM
обучил O
алгоритм O
распознавать B-TERM
стиль I-TERM
Шекспира O
на O
пьесах B-TERM
« O
Кориолан O
» O
, O
« O
Цимбелин O
» O
, O
« O
Зимняя O
сказка O
» O
и O
« O
Буря O
» O
. O

# text =  В результате искусственный интеллект согласился с анализом Спеддинга.
В O
результате O
искусственный O
интеллект O
согласился O
с O
анализом B-TERM
Спеддинга I-TERM
. O

# text =  В прошлом году учёные из Университета Торонто, Мельбурнского Университета и подразделения IBM в Австралии научили искусственный интеллект генерировать сонеты в шекспировском стиле.
В O
прошлом O
году O
учёные O
из O
Университета B-TERM
Торонто I-TERM
, O
Мельбурнского B-TERM
Университета I-TERM
и O
подразделения O
IBM B-TERM
в O
Австралии O
научили O
искусственный O
интеллект O
генерировать O
сонеты O
в O
шекспировском O
стиле O
. O

# text =  Алгоритм под названием Deepspeare обучали на 2,7 тыс. сонетов Шекспира, после чего он научился писать собственные, придерживаясь похожего стиля.
Алгоритм O
под O
названием O
Deepspeare B-TERM
обучали O
на O
2,7 O
тыс. O 
сонетов B-TERM
Шекспира O
, O
после O
чего O
он O
научился O
писать O
собственные O
, O
придерживаясь O
похожего O
стиля O
. O

# text =  Как научить свою нейросеть генерировать стихи
Как O
научить O
свою O
нейросеть O
генерировать B-TERM
стихи I-TERM

# text =  Языковые модели определяют вероятность появления последовательности слов  в данном языке: .
Языковые O
модели O
определяют B-TERM
вероятность I-TERM
появления I-TERM
последовательности I-TERM
слов I-TERM
в O
данном O
языке B-TERM
: O
. O

# text =  Кажется, самым простым способом построить такую модель является использование N-граммной статистики.
Кажется O
, O
самым O
простым O
способом O
построить O
такую O
модель O
является O
использование O
N B-TERM
- I-TERM
граммной I-TERM
статистики I-TERM
. O

# text =  Для решения такой проблемы используют обычно сглаживание Kneser–Ney или Katz’s backing-off.
Для O
решения O
такой O
проблемы O
используют O
обычно O
сглаживание B-TERM
Kneser I-TERM
– I-TERM
Ney I-TERM
или O
Katz B-TERM
’s I-TERM
backing I-TERM
- I-TERM
off I-TERM
. O

# text =  За более подробной информацией про методы сглаживания N-грамм стоит обратиться к известной книге Кристофера Маннинга “Foundations of Statistical Natural Language Processing”.
За O
более O
подробной O
информацией O
про O
методы O
сглаживания B-TERM
N I-TERM
- I-TERM
грамм I-TERM
стоит O
обратиться O
к O
известной O
книге O
Кристофера B-TERM
Маннинга I-TERM
“ O
Foundations B-TERM
of I-TERM
Statistical I-TERM
Natural I-TERM
Language I-TERM
Processing I-TERM
” O
. O

# text =  Хочу заметить, что 5-граммы слов я назвал не просто так: именно их (со сглаживанием, конечно) Google демонстрирует в статье “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling” — и показывает результаты, весьма сопоставимые с результатами у рекуррентных нейронных сетей — о которых, собственно, и пойдет далее речь.
Хочу O
заметить O
, O
что O
5-граммы O
слов O
я O
назвал O
не O
просто O
так O
: O
именно O
их O
( O
со O
сглаживанием O
, O
конечно O
) O
Google B-TERM
демонстрирует O
в O
статье O
“ B-TERM
One I-TERM
Billion I-TERM
Word I-TERM
Benchmark I-TERM
for I-TERM
Measuring I-TERM
Progress I-TERM
in I-TERM
Statistical I-TERM
Language I-TERM
Modeling I-TERM
” I-TERM
— O
и O
показывает O
результаты O
, O
весьма O
сопоставимые O
с O
результатами O
у O
рекуррентных B-TERM
нейронных I-TERM
сетей I-TERM
— O
о O
которых O
, O
собственно O
, O
и O
пойдет O
далее O
речь O
. O

# text =  Преимущество рекуррентных нейронных сетей — в возможности использовать неограниченно длинный контекст.
Преимущество O
рекуррентных B-TERM
нейронных I-TERM
сетей I-TERM
— O
в O
возможности O
использовать O
неограниченно O
длинный O
контекст O
. O

# text =  На практике классические RNN страдают от затухания градиента — по сути, отсутствия возможности помнить контекст дальше, чем на несколько слов.
На O
практике O
классические O
RNN B-TERM
страдают O
от O
затухания O
градиента O
— O
по O
сути O
, O
отсутствия O
возможности O
помнить O
контекст O
дальше O
, O
чем O
на O
несколько O
слов O
. O

# text =  Самыми популярными являются LSTM и GRU.
Самыми O
популярными O
являются O
LSTM B-TERM
и O
GRU B-TERM
. O

# text =  В дальнейшем, говоря о рекуррентном слое, я всегда буду подразумевать LSTM.
В O
дальнейшем O
, O
говоря O
о O
рекуррентном O
слое O
, O
я O
всегда O
буду O
подразумевать O
LSTM B-TERM
. O

# text =  Вспомним теперь, что для нашей задачи языковая модель нужна для выбора наиболее подходящего следующего слова по уже сгенерированной последовательности.
Вспомним O
теперь O
, O
что O
для O
нашей O
задачи O
языковая O
модель O
нужна O
для O
выбора B-TERM
наиболее I-TERM
подходящего I-TERM
следующего I-TERM
слова I-TERM
по O
уже O
сгенерированной B-TERM
последовательности I-TERM
. O

# text =  Метрические правила определяют последовательность ударных и безударных слогов в строке.
Метрические B-TERM
правила I-TERM
определяют O
последовательность O
ударных O
и O
безударных O
слогов B-TERM
в O
строке O
. O

# text =  Для решения этой проблемы мы делаем лучевой поиск (beam search), выбирая на каждом шаге вместо одного сразу N путей с наивысшими вероятностями.
Для O
решения O
этой O
проблемы O
мы O
делаем O
лучевой B-TERM
поиск I-TERM
( O
beam B-TERM
search I-TERM
) O
, O
выбирая O
на O
каждом O
шаге O
вместо O
одного O
сразу O
N O
путей O
с O
наивысшими O
вероятностями O
. O
# text =  Автоматическое определение эмоций в текстовых беседах с использованием нейронных сетей
Автоматическое B-TERM
определение I-TERM
эмоций I-TERM
в O
текстовых B-TERM
беседах I-TERM
с O
использованием O
нейронных B-TERM
сетей I-TERM

#text = Одна из основных задач диалоговых систем состоит не только в предоставлении нужной пользователю информации, но и в генерации как можно более человеческих ответов.
Одна O
из O
основных O
задач O
диалоговых O
систем O
состоит O
не O
только O
в O
предоставлении B-TERM
нужной I-TERM
пользователю I-TERM
информации I-TERM
, O
но O
и O
в O
генерации B-TERM
как O
можно O
более O
человеческих O
ответов I-TERM
. O

# text =  В этой статье мы рассмотрим архитектуру рекуррентной нейросети для определения эмоций в текстовых беседах, которая принимала участие в SemEval-2019 Task 3 “EmoContext”, ежегодном соревновании по компьютерной лингвистике.
В O
этой O
статье O
мы O
рассмотрим O
архитектуру O
рекуррентной B-TERM
нейросети I-TERM
для O
определения B-TERM
эмоций I-TERM
в O
текстовых B-TERM
беседах I-TERM
, O
которая O
принимала O
участие O
в O
SemEval-2019 O
Task O
3 O
“ O
EmoContext O
” O
, O
ежегодном O
соревновании O
по O
компьютерной B-TERM
лингвистике I-TERM
. O

# text =  Задача состояла в классификации эмоций (“happy”, “sad”, “angry” и “others”) в беседе из трех реплик, в которой участвовали чат-бот и человек.
Задача O
состояла O
в O
классификации B-TERM
эмоций I-TERM
( O
“ O
happy B-TERM
” O
, O
“ O
sad B-TERM
” O
, O
“ O
angry B-TERM
” O
и O
“ O
others B-TERM
” O
) O
в O
беседе O
из O
трех O
реплик O
, O
в O
которой O
участвовали O
чат O
- O
бот O
и O
человек O
. O

# text =  В четвёртой части мы опишем архитектуру LSTM, которую мы использовали в соревновании.
В O
четвёртой O
части O
мы O
опишем O
архитектуру O
LSTM B-TERM
, O
которую O
мы O
использовали O
в O
соревновании O
. O

# text =  Код написан на языке Python с использованием библиотеки Keras.
Код O
написан O
на O
языке O
Python B-TERM
с O
использованием O
библиотеки O
Keras B-TERM
. O

# text =  Подробное описание представлено здесь: (Chatterjee et al., 2019).
Подробное O
описание O
представлено O
здесь O
: O
( O
Chatterjee B-TERM
et I-TERM
al I-TERM
. I-TERM
, I-TERM
2019 I-TERM
) O
. O

# text =  Примеры из датасета EmoContext (Chatterjee et al., 2019)
Примеры O
из O
датасета O
EmoContext B-TERM
( O
Chatterjee B-TERM
et I-TERM
al I-TERM
. I-TERM
, I-TERM
2019 I-TERM
) O

# text =  Данные предоставлены Microsoft, скачать их можно в официальной группе в LinkedIn.
Данные O
предоставлены O
Microsoft B-TERM
, O
скачать O
их O
можно O
в O
официальной O
группе O
в O
LinkedIn B-TERM
. O

# text =  В дополнение к этим данным мы собрали 900 тыс. англоязычных сообщений из Twitter, чтобы создать Distant-датасет (300 тыс. твитов на каждую эмоцию).
В O
дополнение O
к O
этим O
данным O
мы O
собрали O
900 O
тыс. O
англоязычных O
сообщений O
из O
Twitter B-TERM
, O
чтобы O
создать O
Distant B-TERM
- O
датасет O
( O
300 O
тыс. O
твитов O
на O
каждую O
эмоцию O
) O
. O

# text =  При его создании мы придерживались стратегии Go et al. (2009), в рамках которой просто ассоциировали сообщения с наличием относящихся к эмоциям слов, таких как #angry, #annoyed, #happy, #sad, #surprised и так далее.
При O
его O
создании O
мы O
придерживались O
стратегии O
Go B-TERM
et I-TERM
al I-TERM
. I-TERM
( I-TERM
2009 I-TERM
) I-TERM
, O
в O
рамках O
которой O
просто O
ассоциировали B-TERM
сообщения I-TERM
с I-TERM
наличием I-TERM
относящихся I-TERM
к I-TERM
эмоциям I-TERM
слов I-TERM
, O
таких O
как O
# O
angry B-TERM
, O
# O
annoyed B-TERM
, O
# O
happy B-TERM
, O
# O
sad B-TERM
, O
# O
surprised B-TERM
и O
так O
далее O
. O

# text =  Список терминов основан на терминах из SemEval-2018 AIT DISC (Duppada et al., 2018).
Список O
терминов O
основан O
на O
терминах O
из O
SemEval-2018 B-TERM
AIT I-TERM
DISC i-TERM
( O
Duppada B-TERM
et I-TERM
al I-TERM
. I-TERM
, I-TERM
2018 I-TERM
) O
. O

# text =  Главной метрикой качества в соревновании EmoContext является усредненная F1-мера для трёх классов эмоций, то есть для классов «happy», «sad» и «angry».
Главной O
метрикой O
качества O
в O
соревновании O
EmoContext O
является O
усредненная O
F1-мера B-TERM
для O
трёх O
классов O
эмоций O
, O
то O
есть O
для O
классов O
« O
happy B-TERM
» O
, O
« O
sad B-TERM
» O
и O
« O
angry B-TERM
» O
. O

# text =  Перед обучением мы предварительно обработали тексты с помощью инструмента Ekphrasis (Baziotis et al., 2017).
Перед O
обучением O
мы O
предварительно O
обработали O
тексты B-TERM
с O
помощью O
инструмента O
Ekphrasis B-TERM
( O
Baziotis B-TERM
et I-TERM
al I-TERM
. I-TERM
, I-TERM
2017 I-TERM
) O
. O

# text =  Он помогает исправить орфографию, нормализовать слова, сегментировать, а также определить, какие токены следует отбросить, нормализовать или аннотировать с помощью специальных тегов.
Он O
помогает O
исправить B-TERM
орфографию I-TERM
, O
нормализовать B-TERM
слова I-TERM
, O
сегментировать B-TERM
, O
а O
также O
определить O
, O
какие O
токены O
следует O
отбросить O
, O
нормализовать O
или O
аннотировать O
с O
помощью O
специальных O
тегов B-TERM
. O

# text =  Кроме того, Emphasis содержит токенизатор, который может идентифицировать большинство эмодзи, эмотиконов и сложных выражений, а также даты, время, валюты и акронимы.
Кроме O
того O
, O
Emphasis B-TERM
содержит O
токенизатор B-TERM
, O
который O
может O
идентифицировать B-TERM
большинство I-TERM
эмодзи I-TERM
, O
эмотиконов I-TERM
и O
сложных I-TERM
выражений I-TERM
, O
а O
также O
даты I-TERM
, O
время I-TERM
, O
валюты I-TERM
и O
акронимы I-TERM
. O
