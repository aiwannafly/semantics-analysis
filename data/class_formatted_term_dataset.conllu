# sent_id = 1
# text =   1 January 2010 at 07:59 Заметки об NLP (часть 2) Artificial Intelligence Natural Language Processing.
1	1 O
2	January O
3	2010 O
4	at O
5	07:59 O
6	Заметки O
7	об O
8	NLP B-ShortName_Science
9	( O
10	часть O
11	2 O
12	) O
13	Artificial B-Science
14	Intelligence I-Science
15	Natural B-Science
16	Language I-Science
17	Processing I-Science
18	. O

# sent_id = 2
# text =   Хотя в первой части я и говорил, что не собираюсь останавливаться на морфологии, видимо, совсем без неё не получится
1	Хотя O
2	в O
3	первой O
4	части O
5	я O
6	и O
7	говорил O
8	, O
9	что O
10	не O
11	собираюсь O
12	останавливаться O
13	на O
14	морфологии B-Science
15	, O
16	видимо O
17	, O
18	совсем O
19	без O
20	неё O
21	не O
22	получится O
23	. O

# sent_id = 3
# text =   Всё-таки обработка предложений сильно завязана на предшествующий морфологический анализ.
1	Всё O
2	- O
3	таки O
4	обработка O
5	предложений B-Subject
6	сильно O
7	завязана O
8	на O
9	предшествующий O
10	морфологический B-Method
11	анализ I-Method
12	. O

# sent_id = 4
# text =   Наш с вами родной русский язык очень хорош (для нас) и труден (для иностранцев) богатой фонетикой и разнообразием грамматических средств.
1	Наш O
2	с O
3	вами O
4	родной O
5	русский O
6	язык B-Object
7	очень O
8	хорош O
9	( O
10	для O
11	нас O
12	) O
13	и O
14	труден O
15	( O
16	для O
17	иностранцев O
18	) O
19	богатой O
20	фонетикой B-Science
21	и O
22	разнообразием O
23	грамматических B-Object
24	средств I-Object
25	. O

# sent_id = 5
# text =   Во-первых, в них не так много незнакомых нам фонем.
1	Во O
2	- O
3	первых O
4	, O
5	в O
6	них O
7	не O
8	так O
9	много O
10	незнакомых O
11	нам O
12	фонем B-Subject
13	. O

# sent_id = 6
# text =   Во-вторых, обилие грамматических явлений редко сталкивает нас с чем-либо непонятным.
1	Во O
2	- O
3	вторых O
4	, O
5	обилие O
6	грамматических B-Object
7	явлений I-Object
8	редко O
9	сталкивает O
10	нас O
11	с O
12	чем O
13	- O
14	либо O
15	непонятным O
16	. O

# sent_id = 7
# text =   А для американца, например, само понятие рода или падежа совершенно неочевидно.
1	А O
2	для O
3	американца O
4	, O
5	например O
6	, O
7	само O
8	понятие O
9	рода B-Subject
10	или O
11	падежа B-Subject
12	совершенно O
13	неочевидно O
14	. O

# sent_id = 8
# text =   Теперь о морфологии.
1	Теперь O
2	о O
3	морфологии B-Science
4	. O

# sent_id = 9
# text =   Автоматические морфологические анализаторы работают хорошо.
1	Автоматические B-Subject
2	морфологические I-Subject
3	анализаторы I-Subject
4	работают O
5	хорошо O
6	. O

# sent_id = 10
# text =   Если кому интересно посмотреть, как работает автоматический анализатор — можно поэкспериментировать на сайте С.А. Старостина.
1	Если O
2	кому O
3	интересно O
4	посмотреть O
5	, O
6	как O
7	работает O
8	автоматический B-Subject
9	анализатор B-Subject
10	— O
11	можно O
12	поэкспериментировать O
13	на O
14	сайте O
15	С.А. B-Person
16	Старостина I-Person
17	. O

# sent_id = 11
# text =   Смею предположить, что едва ли не все морфологические анализаторы русского так или иначе опираются на Грамматический словарь Зализняка.
1	Смею O
2	предположить O
3	, O
4	что O
5	едва O
6	ли O
7	не O
8	все O
9	морфологические B-Subject
10	анализаторы I-Subject
11	русского O
12	так O
13	или O
14	иначе O
15	опираются O
16	на O
17	Грамматический B-InfoResource
18	словарь I-InfoResource
19	Зализняка I-InfoResource
20	. O

# sent_id = 12
# text =   Сам я пользуюсь разработками Алексея Сокирко, «обёрнутыми» в удобный интерфейс на сайте Lemmatizer.
1	Сам O
2	я O
3	пользуюсь O
4	разработками O
5	Алексея B-Person
6	Сокирко I-Person
7	, O
8	« O
9	обёрнутыми O
10	» O
11	в O
12	удобный O
13	интерфейс O
14	на O
15	сайте O
16	Lemmatizer B-InfoResource
17	. O

# sent_id = 13
# text =   Судите сами: упомянутый русский морфологический анализатор Алексея Сокирко оперирует базой данных в 18,5 мегабайт.
1	Судите O
2	сами O
3	: O
4	упомянутый O
5	русский B-Subject
6	морфологический B-Subject
7	анализатор I-Subject
8	Алексея I-Subject
9	Сокирко I-Subject
10	оперирует O
11	базой B-InfoResource
12	данных I-InfoResource
13	в O
14	18,5 O
15	мегабайт O
16	. O

# sent_id = 14
# text =   На Грамоте предлагают относить их к «предикативам», но общепринятого подхода нет.
1	На O
2	Грамоте B-InfoResource
3	предлагают O
4	относить O
5	их O
6	к O
7	« O
8	предикативам O
9	» O
10	, O
11	но O
12	общепринятого O
13	подхода O
14	нет O
15	. O

# sent_id = 15
# text =   Например, ещё одна «фича» анализатора Сокирко: он называет глаголы в личной форме («бегаю») глаголами, а в начальной форме («бегать») — инфинитивами.
1	Например O
2	, O
3	ещё O
4	одна O
5	« O
6	фича O
7	» O
8	анализатора B-InfoResource
9	Сокирко I-InfoResource
10	: O
11	он O
12	называет O
13	глаголы O
14	в O
15	личной O
16	форме O
17	( O
18	« O
19	бегаю O
20	» O
21	) O
22	глаголами O
23	, O
24	а O
25	в O
26	начальной O
27	форме O
28	( O
29	« O
30	бегать O
31	» O
32	) O
33	— O
34	инфинитивами O
35	. O

# sent_id = 16
# text =   Tags: NLP, обработка текстовб, компьютерная лингвистика.
1	Tags O
2	: O
3	NLP B-ShortName_Science
4	, O
5	обработка B-Task
6	текстов B-Task
7	, O
8	компьютерная B-Science
9	лингвистика I-Science

# sent_id = 17
# text =   Туториал по фреймворку для программирования датасетов MTS AI corporate blog.
1	Туториал O
2	по O
3	фреймворку O
4	для O
5	программирования O
6	датасетов O
7	MTS B-Organization
8	AI I-Organization
9	corporate B-InfoResource
10	blog I-InfoResource

# sent_id = 18
# text =   Я Игорь Буянов, старший разработчик группы разметки данных MTS AI.
1	Я O
2	Игорь B-Person
3	Буянов I-Person
4	, O
5	старший O
6	разработчик O
7	группы O
8	разметки B-Method
9	данных O
10	MTS B-Organization
11	AI I-Organization
12	. O

# sent_id = 19
# text =   Недавно рассказывал о том, как делать иерархически датасет из Википедии.
1	Недавно O
2	рассказывал O
3	о O
4	том O
5	, O
6	как O
7	делать O
8	иерархически O
9	датасет O
10	из O
11	Википедии B-InfoResource
12	. O

# sent_id = 20
# text =   В этом посте хочу рассказать вам о Сноркеле - фреймворке для программирования данных (data programming).
1	В O
2	этом O
3	посте O
4	хочу O
5	рассказать O
6	вам O
7	о O
8	Сноркеле B-Application
9	- O
10	фреймворке B-Environment
11	для I-Environment
12	программирования I-Environment
13	данных I-Environment
14	( O
15	data B-Task
16	programming I-Task
17	) O
18	. O

# sent_id = 21
# text =   Проект стартовал в Стэнфорде как инструмент для помощи в разметке датасетов для задачи information extraction, а сейчас разработчики делают платформу для пользования внешними заказчиками. 
1	Проект O
2	стартовал O
3	в O
4	Стэнфорде B-Organization
5	как O
6	инструмент B-Object
7	для O
8	помощи O
9	в O
10	разметке B-Method
11	датасетов O
12	для O
13	задачи O
14	information B-Task
15	extraction I-Task
16	, O
17	а O
18	сейчас O
19	разработчики O
20	делают O
21	платформу O
22	для O
23	пользования O
24	внешними O
25	заказчиками O
26	. O

# sent_id = 22
# text =   В разметочные функции (labeling functions) закодированы все возможные правила, по которым можно поставить какую-либо метку каждому примеру из набора данных.
1	В O
2	разметочные B-Method
3	функции I-Method
4	( O
5	labeling B-Method
6	functions I-Method
7	) O
8	закодированы O
9	все O
10	возможные O
11	правила O
12	, O
13	по O
14	которым O
15	можно O
16	поставить O
17	какую O
18	- O
19	либо O
20	метку O
21	каждому O
22	примеру O
23	из O
24	набора O
25	данных O
26	. O

# sent_id = 23
# text =   В качестве основы для таких функций используются:внешние базы данных, такие как WordNet или WikiBase.
1	В O
2	качестве O
3	основы B-Subject
4	для O
5	таких O
6	функций O
7	используются O
8	: O
9	внешние O
10	базы B-InfoResource
11	данных I-InfoResource
12	, O
13	такие O
14	как O
15	WordNet B-Result
16	или O
17	WikiBase B-InfoResource
18	. O

# sent_id = 24
# text =   Генеративная модель, являющаяся сердцем Сноркеля, попытается учесть недостатки отдельных функций.
1	Генеративная B-Object
2	модель I-Object
3	, O
4	являющаяся O
5	сердцем O
6	Сноркеля B-InfoResource
7	, O
8	попытается O
9	учесть O
10	недостатки O
11	отдельных O
12	функций O
13	. O

# sent_id = 25
# text =   Для наглядности оставляю здесь иллюстрацию с последовательностью работы со Снокрелем для задачи information extraction из оригинальной статьи.
1	Для O
2	наглядности O
3	оставляю O
4	здесь O
5	иллюстрацию O
6	с O
7	последовательностью O
8	работы O
9	со O
10	Снокрелем B-InfoResource
11	для O
12	задачи O
13	information B-Science
14	extraction I-Science
15	из O
16	оригинальной O
17	статьи O
18	. O

# sent_id = 26
# text =   Авторы оригинальной статьи представляют ее как факторный граф, или графическую вероятностную модель.
1	Авторы O
2	оригинальной O
3	статьи O
4	представляют O
5	ее O
6	как O
7	факторный B-Object
8	граф I-Object
9	, O
10	или O
11	графическую B-Model
12	вероятностную I-Model
13	модель I-Model
14	. O

# sent_id = 27
# text =   Тогда модель определяется так, чтобы обучить эту модель без доступа к истинным меткам, это нужно обучаться с помощью логарифмического негативного маргинализированного правдоподобия, зная матрицу Оптимизацию авторы проводили с помощью SGD с семплированием Гиббса.
1	Тогда O
2	модель B-Object
3	определяется O
4	, O
5	чтобы O
6	обучить O
7	эту O
8	модель O
9	без O
10	доступа O
11	к O
12	истинным O
13	меткам O
14	, O
15	это O
16	нужно O
17	обучаться O
18	с O
19	помощью O
20	логарифмического B-Method
21	негативного I-Method
22	маргинализированного I-Method
23	правдоподобия I-Method
24	, O
25	зная O
26	матрицу B-Object
27	Оптимизацию I-Object
28	авторы O
29	проводили O
30	с O
31	помощью O
32	SGD B-ShortName_Method
33	с O
34	семплированием B-Method
35	Гиббса I-Method
36	. O

# sent_id = 28
# text =   Загрузим заранее обученную модель fastText, чей выбор объясняется наличием огромного количества опечаток в текстах.
1	Загрузим O
2	заранее O
3	обученную O
4	модель B-Object
5	fastText B-Model
6	, O
7	чей O
8	выбор O
9	объясняется O
10	наличием O
11	огромного O
12	количества O
13	опечаток O
14	в O
15	текстах B-Object
16	. O

# sent_id = 29
# text =   Таким образом мы получили опорный вектор для класса "диарея".
1	Таким O
2	образом O
3	мы O
4	получили O
5	опорный B-Method
6	вектор I-Method
7	для O
8	класса B-Object
9	" O
10	диарея O
11	" O
12	. O

# sent_id = 30
# text =   Зайдя на несколько из них я увидел что большая половина типа Wix используют технологию Искусственного Интеллекта, чтобы создать шаблон разметки страницы и далее её уже заполнить.
1	Зайдя O
2	на O
3	несколько O
4	из O
5	них O
6	я O
7	увидел O
8	что O
9	большая O
10	половина O
11	типа O
12	Wix B-Organization
13	используют O
14	технологию O
15	Искусственного B-Science
16	Интеллекта I-Science
17	, O
18	чтобы O
19	создать O
20	шаблон B-Object
21	разметки B-Method
22	страницы O
23	и O
24	далее O
25	её O
26	уже O
27	заполнить O
28	. O

# sent_id = 31
# text =   В финале я могу его редактировать путем Drag & Drop.
1	В O
2	финале O
3	я O
4	могу O
5	его O
6	редактировать O
7	путем O
8	Drag B-Method
9	& I-Method
10	Drop I-Method
11	. O

# sent_id = 32
# text =   Чатботы и искусственный интеллект для понимания естественного языка (NLU – Natural Language Understanding) тема достаточно горячая, про нее не раз говорилось на Хабре.
1	Чатботы O
2	и O
3	искусственный B-Science
4	интеллект I-Science
5	для O
6	понимания B-Task
7	естественного I-Task
8	языка I-Task
9	( O
10	NLU B-Abbrev_Task
11	– O
12	Natural B-Task
13	Language I-Task
14	Understanding I-Task
15	) O
16	тема O
17	достаточно O
18	горячая O
19	, O
20	про O
21	нее O
22	не O
23	раз O
24	говорилось O
25	на O
26	Хабре O
27	. O

# sent_id = 33
# text =   Хотя AI — это достаточно широкая область, включающая в себя машинное зрение, предиктивный анализ, машинный перевод и другие области – понимание естественного языка (NLU) и его генерация (NLG) является значительной и быстрорастущей его частью.
1	Хотя O
2	AI B-ShortName_Science
3	— O
4	это O
5	достаточно O
6	широкая O
7	область O
8	, O
9	включающая O
10	в O
11	себя O
12	машинное B-Science
13	зрение I-Science
14	, O
15	предиктивный B-Method
16	анализ I-Method
17	, O
18	машинный B-Task
19	перевод I-Task
20	и O
21	другие O
22	области O
23	– O
24	понимание B-Task
25	естественного I-Task
26	языка i-Task
27	( O
28	NLU B-ShortName
29	) O
30	и O
31	его O
32	генерация B-Task
33	( O
34	NLG B-Abbrev_Task
35	) O
36	является O
37	значительной O
38	и O
39	быстрорастущей O
40	его O
41	частью O
42	. O

# sent_id = 34
# text =   Опуская историю, начавшуюся еще в 50-е годы с Алана Тьюринга и программы Элиза в 60-е годы, а также научные исследования в области лингвистики и машинного обучения 90-х годов, значимым событием более новой истории стало появление языка разметки AIML (Artificial Intelligence Markup Language), разработанной в 2001-м году Ричардом Уэлсом (Richard Wallace) и созданным на его основе чатботом A.L.I.C.E.
1	Опуская O
2	историю O
3	, O
4	начавшуюся O
5	еще O
6	в O
7	50-е O
8	годы O
9	с O
10	Алана B-Person
11	Тьюринга I-Person
12	и O
13	программы O
14	Элиза B-Application
15	в O
16	60-е O
17	годы O
18	, O
19	а O
20	также O
21	научные O
22	исследования O
23	в O
24	области O
25	лингвистики O
26	и O
27	машинного B-Science
28	обучения I-Science
29	90-х O
30	годов O
31	, O
32	значимым O
33	событием O
34	более O
35	новой O
36	истории O
37	стало O
38	появление O
39	языка O
40	разметки O
41	AIML B-ShortName_Method
42	( O
43	Artificial B-Method
44	Intelligence I-Method
45	Markup I-Method
46	Language I-Method
47	) O
48	, O
49	разработанной O
50	в O
51	2001-м B-Date_Method
52	году O
53	Ричардом B-Person
54	Уэлсом I-Person
55	( O
56	Richard I-Person
57	Wallace I-Person
58	) O
59	и O
60	созданным O
61	на O
62	его O
63	основе O
64	чатботом O
65	A.L.I.C.E. B-Abberv_Application

# sent_id = 35
# text =   В течение последующих десяти лет подходы к написанию чатботов во многом представляли из себя переработки или улучшения этой методологии, получившей название «rule-based подход» или «подход на основе формальных правил».
1	В O
2	течение O
3	последующих O
4	десяти O
5	лет O
6	подходы O
7	к O
8	написанию O
9	чатботов O
10	во O
11	многом O
12	представляли O
13	из O
14	себя O
15	переработки O
16	или O
17	улучшения O
18	этой O
19	методологии O
20	, O
21	получившей O
22	название O
23	« O
24	rule B-Method
25	- I-Method
26	based I-Method
27	подход O
28	» O
29	или O
30	« O
31	подход O
32	на O
33	основе O
34	формальных B-Object
35	правил I-Object
36	» O
37	. O

# sent_id = 36
# text =   Именно эти технологии, вместе с заметным продвижением в области технологий синтеза и распознавания речи, а также распространением мессенджеров и вебчатов – обусловили стремительный рост количества внедрений NLU-технологий в 2015-2018-м годах.
1	Именно O
2	эти O
3	технологии O
4	, O
5	вместе O
6	с O
7	заметным O
8	продвижением O
9	в O
10	области O
11	технологий O
12	синтеза B-Task
13	и I-Task
14	распознавания B-Task
15	речи I-Task
16	, O
17	а O
18	также O
19	распространением O
20	мессенджеров O
21	и O
22	вебчатов O
23	– O
24	обусловили O
25	стремительный O
26	рост O
27	количества O
28	внедрений O
29	NLU B-Method
30	- I-Method
31	технологий I-Method
32	в O
33	2015 B-Date_Method
34	- I-Date_Method
35	2018-м I-Date_Method
36	годах O
37	. O

# sent_id = 37
# text =   Голосовые ассистенты (IVA): Alexa от Amazon, Google Assistant от Google, Siri от Apple, Cortana от Microsoft, Алиса от Яндекса – они определяют интенты (намерения) пользователей и исполняют команды.
1	Голосовые O
2	ассистенты O
3	( O
4	IVA O
5	) O
6	: O
7	Alexa B-Application
8	от O
9	Amazon B-Organization
10	, O
11	Google B-Application
12	Assistant I-Application
13	от O
14	Google B-Organization
15	, O
16	Siri B-Application
17	от O
18	Apple B-Organization
19	, O
20	Cortana B-Application
21	от O
22	Microsoft B-Organization
23	, O
24	Алиса B-Application
25	от O
26	Яндекса B-Organization
27	– O
28	они O
29	определяют O
30	интенты B-Object
31	( O
32	намерения B-Object
33	) O
34	пользователей O
35	и O
36	исполняют O
37	команды O
38	. O

# sent_id = 38
# text =   В качестве каналов могут выступать умные устройства, ассистенты, встроенные в устройства или мобильные телефоны, привычный звонок на номер телефона, мессенджеры или вебчаты, подобные популярным в России Livetex, Jivosite или Webim.
1	В O
2	качестве O
3	каналов O
4	могут O
5	выступать O
6	умные O
7	устройства O
8	, O
9	ассистенты O
10	, O
11	встроенные O
12	в O
13	устройства O
14	или O
15	мобильные O
16	телефоны O
17	, O
18	привычный O
19	звонок O
20	на O
21	номер O
22	телефона O
23	, O
24	мессенджеры O
25	или O
26	вебчаты O
27	, O
28	подобные O
29	популярным O
30	в O
31	России O
32	Livetex B-Application
33	, O
34	Jivosite B-Application
35	или O
36	Webim B-Application
37	. O

# sent_id = 39
# text =   За эту конвертацию отвечают платформы ASR (распознавание речи), TTS (синтез речи), системы интеграции с телефонией.
1	За O
2	эту O
3	конвертацию O
4	отвечают O
5	платформы O
6	ASR B-Abbrev_Task
7	( O
8	распознавание B-Task
9	речи I-Task
10	) O
11	, O
12	TTS B-ShortName_Method
13	( O
14	синтез B-Method
15	речи I-Method
16	) O
17	, O
18	системы O
19	интеграции O
20	с O
21	телефонией O
22	. O

# sent_id = 40
# text =   Наличие развитого rule-based синтаксиса может ускорить разработку чатботов в разы.
1	Наличие O
2	развитого O
3	rule B-Method
4	- I-Method
5	based I-Method
6	синтаксиса O
7	может O
8	ускорить O
9	разработку O
10	чатботов O
11	в O
12	разы O
13	. O

# sent_id = 41
# text =   Анализ эмоций, богатая и глубокая аналитика, специальные фильтры (например, на использование ненормативной лексики), языковая поддержка, хранение контекста, как и собственно, точность работы используемых нейросетевых алгоритмов, а также производительность, масштабируемость и стабильность – все это также важные, хотя и не всегда очевидные со стороны, особенности диалоговых платформ.
1	Анализ B-Task
2	эмоций I-Task
3	, O
4	богатая O
5	и O
6	глубокая B-Task
7	аналитика I-Task
8	, O
9	специальные O
10	фильтры O
11	( O
12	например O
13	, O
14	на O
15	использование O
16	ненормативной O
17	лексики O
18	) O
19	, O
20	языковая O
21	поддержка O
22	, O
23	хранение O
24	контекста O
25	, O
26	как O
27	и O
28	собственно O
29	, O
30	точность O
31	работы O
32	используемых O
33	нейросетевых O
34	алгоритмов O
35	, O
36	а O
37	также O
38	производительность O
39	, O
40	масштабируемость O
41	и O
42	стабильность O
43	– O
44	все O
45	это O
46	также O
47	важные O
48	, O
49	хотя O
50	и O
51	не O
52	всегда O
53	очевидные O
54	со O
55	стороны O
56	, O
57	особенности O
58	диалоговых B-Application
59	платформ I-Application
60	. O

# sent_id = 42
# text =   Алгоритм понимания естественного языка (Natural Language Understanding, NLU) Microsoft DeBERTa превзошел человеческие возможности в одном из самых сложных тестов для подобных алгоритмов SuperGLUE.
1	Алгоритм O
2	понимания O
3	естественного O
4	языка B-Object
5	( O
6	Natural B-Task
7	Language I-Task
8	Understanding I-Task
9	, O
10	NLU B-Abbrev_Task
11	) O
12	Microsoft B-Organization
13	DeBERTa B-Application
14	превзошел O
15	человеческие O
16	возможности O
17	в O
18	одном O
19	из O
20	самых O
21	сложных O
22	тестов O
23	для O
24	подобных O
25	алгоритмов O
26	SuperGLUE B-Method
27	. O

# sent_id = 43
# text =   На данный момент модель занимает первое место в рейтинге с показателем в 90,3, в то время как среднее значение человеческих возможностей составляет 89,8 баллов.
1	На O
2	данный O
3	момент O
4	модель O
5	занимает O
6	первое O
7	место O
8	в O
9	рейтинге O
10	с O
11	показателем O
12	в O
13	90,3 B-Value
14	, O
15	в O
16	то O
17	время O
18	как O
19	среднее O
20	значение O
21	человеческих O
22	возможностей O
23	составляет O
24	89,8 B-Value
25	баллов O
26	. O

# sent_id = 44
# text =  Тест SuperGLUE включает в себя ряд задач, которые разработаны для оценки способности ИИ-моделей распознавать и понимать естественный язык, например, дать правильный ответ на вопрос на базе прочитанного абзаца, определить, правильно ли используется многозначное слово в определенном контексте и т.д.
1	Тест O
2	SuperGLUE B-Method
3	включает O
4	в O
5	себя O
6	ряд O
7	задач O
8	, O
9	которые O
10	разработаны O
11	для O
12	оценки O
13	способности O
14	ИИ O
15	- O
16	моделей O
17	распознавать O
18	и O
19	понимать O
20	естественный O
21	язык O
22	, O
23	например O
24	, O
25	дать B-Task
26	правильный I-Task
27	ответ I-Task
28	на I-Task
29	вопрос I-Task
30	на I-Task
31	базе I-Task
32	прочитанного I-Task
33	абзаца I-Task
34	, O
35	определить O
36	, O
37	правильно O
38	ли O
39	используется O
40	многозначное B-Subject
41	слово I-Subject
42	в O
43	определенном O
44	контексте O
45	и O
46	т.д. O

# sent_id = 45
# text =   Тест был разработан группой исследователей в 2019 году.
1	Тест O
2	был O
3	разработан O
4	группой O
5	исследователей O
6	в O
7	2019 B-Date
8	году O
9	. O

# sent_id = 46
# text =  Для того чтобы добиться текущего результата в 90,3 балла, DeBERTa получила масштабное обновление архитектуры: теперь она состоит из 48 слоев и имеет 1,5 млрд параметров.
1	Для O
2	того O
3	чтобы O
4	добиться O
5	текущего O
6	результата O
7	в O
8	90,3 B-Value
9	балла O
10	, O
11	DeBERTa B-Application
12	получила O
13	масштабное O
14	обновление O
15	архитектуры O
16	: O
17	теперь O
18	она O
19	состоит O
20	из O
21	48 B-Value
22	слоев O
23	и O
24	имеет O
25	1,5 O
26	млрд O
27	параметров O
28	. O

# sent_id = 47
# text =   Кроме того, DeBERTa будет интегрирована в следующую версию Тьюринговой модели Microsoft Turing (Turing NLRv4).
1	Кроме O
2	того O
3	, O
4	DeBERTa B-Application
5	будет O
6	интегрирована O
7	в O
8	следующую O
9	версию O
10	Тьюринговой B-Model
11	модели I-Model
12	Microsoft I-Model
13	Turing I-Model
14	( O
15	Turing B-Model
16	NLRv4 I-Model
17	) O
18	. O

# sent_id = 48
# text =   Тьюринговые модели используются в таких продуктах Microsoft, как Bing, Office, Dynamics и Azure Cognitive Services, чтобы совершенствовать, к примеру, взаимодействие с чат-ботами, предоставление рекомендаций и ответов на вопросы, поиск, автоматизацию поддержки клиентов, создание контента и решение многих других задач на пользу сотен миллионов пользователей.
1	Тьюринговые B-Model
2	модели I-Model
3	используются O
4	в O
5	таких O
6	продуктах O
7	Microsoft B-Organization
8	, O
9	как O
10	Bing B-Application
11	, O
12	Office B-Application
13	, O
14	Dynamics B-Application
15	и O
16	Azure B-Application
17	Cognitive I-Application
18	Services I-Application
19	, O
20	чтобы O
21	совершенствовать O
22	, O
23	к O
24	примеру O
25	, O
26	взаимодействие O
27	с O
28	чат B-Application
29	- I-Application
30	ботами I-Application
31	, O
32	предоставление O
33	рекомендаций O
34	и O
35	ответов O
36	на O
37	вопросы O
38	, O
39	поиск O
40	, O
41	автоматизацию 
42	поддержки O
43	клиентов O
44	, O
45	создание B-Task
46	контента I-Task
47	и O
48	решение O
49	многих O
50	других O
51	задач O
52	на O
53	пользу O
54	сотен O
55	миллионов O
56	пользователей O
57	. O

# sent_id = 49
# text =   В отличии от машин, люди хорошо умеют использовать знания, ранее полученные при выполнении различных задач, для решения новых – это называется композиционным обобщением (англ. compositional generalization).
1	В O
2	отличии O
3	от O
4	машин O
5	, O
6	люди O
7	хорошо O
8	умеют O
9	использовать O
10	знания O
11	, O
12	ранее O
13	полученные O
14	при O
15	выполнении O
16	различных O
17	задач O
18	, O
19	для O
20	решения O
21	новых O
22	– O
23	это O
24	называется O
25	композиционным B-Method
26	обобщением I-Method
27	( O
28	англ O
29	. O
30	compositional B-Method
31	generalization I-Method
32	) O
33	. O

# sent_id = 50
# text =   Типичным методом обучения без учителя является кластеризация, благодаря которому обучающая выборка разбивается на устойчивые группы или кластеры.
1	Типичным O
2	методом O
3	обучения O
4	без O
5	учителя O
6	является O
7	кластеризация B-Method
8	, O
9	благодаря O
10	которому O
11	обучающая O
12	выборка B-Object
13	разбивается O
14	на O
15	устойчивые O
16	группы O
17	или O
18	кластеры B-Result
19	. O

# sent_id = 51
# text =   Другой подход обучения без учителя для текстов называется тематическим моделированием (topic modeling), позволяющим выявить в неразмеченных текстах основные тематики.
1	Другой O
2	подход O
3	обучения O
4	без O
5	учителя O
6	для O
7	текстов O
8	называется O
9	тематическим B-Method
10	моделированием I-Method
11	( O
12	topic B-Method
13	modeling I-Method
14	) O
15	, O
16	позволяющим O
17	выявить O
18	в O
19	неразмеченных O
20	текстах O
21	основные O
22	тематики O
23	. O

# sent_id = 52
# text =   Если отказываемся от методов unsupervised learning, то логично обратиться к методам обучения с учителем (supervised learning) и в частности к классификации.
1	Если O
2	отказываемся O
3	от O
4	методов O
5	unsupervised B-Method
6	learning I-Method
7	, O
8	то O
9	логично O
10	обратиться O
11	к O
12	методам B-Method
13	обучения I-Method
14	с I-Method
15	учителем I-Method
16	( O
17	supervised B-Method
18	learning I-Method
19	) O
20	и O
21	в O
22	частности O
23	к O
24	классификации B-Task
25	. O

# sent_id = 53
# text =   Результатом работы языковой модели являются эмбеддинги — это отображение из пространства слов в пространство векторов конкретной фиксированной длины, причем векторы, соответствующие близким по смыслу словам, будут расположены в новом пространстве рядом, а далекие по смыслу — далеко.
1	Результатом O
2	работы O
3	языковой O
4	модели O
5	являются O
6	эмбеддинги B-Object
7	— O
8	это O
9	отображение O
10	из O
11	пространства O
12	слов B-Subject
13	в O
14	пространство O
15	векторов O
16	конкретной O
17	фиксированной O
18	длины O
19	, O
20	причем O
21	векторы O
22	, O
23	соответствующие O
24	близким O
25	по O
26	смыслу B-Subject
27	словам I-Subject
28	, O
29	будут O
30	расположены O
31	в O
32	новом O
33	пространстве O
34	рядом O
35	, O
36	а O
37	далекие O
38	по O
39	смыслу O
40	— O
41	далеко O
42	. O

# sent_id = 54
# text =   При использовании TF-IDF (например, вот) подхода с фильтром по частотам и логистической регрессии уже можно получить прекрасные результаты: изначально в краулер отправлялись очень разные тексты, и модель прекрасно справляется.
1	При O
2	использовании O
3	TF B-Metric
4	- I-Metric
5	IDF I-Metric
6	( O
7	например O
8	, O
9	вот O
10	) O
11	подхода O
12	с O
13	фильтром O
14	по O
15	частотам O
16	и O
17	логистической B-Metric
18	регрессии I-Metric
19	уже O
20	можно O
21	получить O
22	прекрасные O
23	результаты O
24	: O
25	изначально O
26	в O
27	краулер O
28	отправлялись O
29	очень O
30	разные O
31	тексты O
32	, O
33	и O
34	модель O
35	прекрасно O
36	справляется O
37	. O

# sent_id = 55
# text =   Для каждой из популяций рассчитаем word2vec расстояние до центра положительной обучающей выборки.
1	Для O
2	каждой O
3	из O
4	популяций O
5	рассчитаем O
6	word2vec B-Result
7	расстояние O
8	до O
9	центра O
10	положительной O
11	обучающей O
12	выборки B-Object
13	. O

# sent_id = 56
# text =   Распределения можно разделить, и для оценки расстояния между распределениями в первую очередь логично обратиться к Дивергенции Кульбака-Лейблера (ДКЛ).
1	Распределения O
2	можно O
3	разделить O
4	, O
5	и O
6	для O
7	оценки O
8	расстояния O
9	между O
10	распределениями O
11	в O
12	первую O
13	очередь O
14	логично O
15	обратиться O
16	к O
17	Дивергенции B-Metric
18	Кульбака I-Metric
19	- I-Metric
20	Лейблера I-Metric
21	( O
22	ДКЛ B-ShortName_Metric
23	) O
24	. O

# sent_id = 57
# text = Основатель компании Imagination Engines, Stephen L. Thaler продвигает свою нейронную сеть по имени DABUS (Device for the Autonomous Boot-strapping of Unified Sentience), указывая ее в качестве автора изобретения в заявках на патенты на разные изобретения, сгенерированные этой сетью.
1	Основатель O
2	компании O
3	Imagination B-Organization
4	Engines I-Organization
5	, O
6	Stephen B-Person
7	L. I-Person
8	Thaler I-Person
9	продвигает O
10	свою O
11	нейронную O
12	сеть O
13	по O
14	имени O
15	DABUS B-ShortName_Model
16	( O
17	Device B-Model
18	for I-Model
19	the I-Model
20	Autonomous I-Model
21	Boot I-Model
22	- I-Model
23	strapping I-Model
24	of I-Model
25	Unified I-Model
26	Sentience I-Model
27	) O
28	, O
29	указывая O
30	ее O
31	в O
32	качестве O
33	автора O
34	изобретения O
35	в O
36	заявках O
37	на O
38	патенты O
39	на O
40	разные O
41	изобретения O
42	, O
43	сгенерированные O
44	этой O
45	сетью O
46	. O

# sent_id = 58
# text =   Специалисты Data Science часто применяют различные методы получения датасетов.
1	Специалисты O
2	Data B-Science
3	Science I-Science
4	часто O
5	применяют O
6	различные O
7	методы O
8	получения O
9	датасетов O
10	. O

# sent_id = 59
# text =   Цель этой статьи — представить краткий обзор трех разных методов извлечения данных с использованием языка Python.
1	Цель O
2	этой O
3	статьи O
4	— O
5	представить O
6	краткий O
7	обзор O
8	трех O
9	разных O
10	методов O
11	извлечения O
12	данных O
13	с O
14	использованием O
15	языка B-Object
16	Python B-Environment
17	. O

# sent_id = 60
# text =   Я расскажу, как делать это с помощью Jupyter Notebook.
1	Я O
2	расскажу O
3	, O
4	как O
5	делать O
6	это O
7	с O
8	помощью O
9	Jupyter B-Environment
10	Notebook I-Environment
11	. O

# sent_id = 61
# text =   Библиотека SQLAlchemy позволит связать ваш код в ноутбуке с наиболее распространенными типами баз данных.
1	Библиотека O
2	SQLAlchemy B-Environment
3	позволит O
4	связать O
5	ваш O
6	код O
7	в O
8	ноутбуке O
9	с O
10	наиболее O
11	распространенными O
12	типами O
13	баз B-InfoResource
14	данных I-InfoResource
15	. O

# sent_id = 62
# text =   Мы собираемся применить Beautiful Soup и библиотеку urllib, чтобы соскрапить названия отелей и цены на них с веб-сайта TripAdvisor.
1	Мы O
2	собираемся O
3	применить O
4	Beautiful B-Environment
5	Soup I-Environment
6	и O
7	библиотеку O
8	urllib B-Environment
9	, O
10	чтобы O
11	соскрапить O
12	названия O
13	отелей O
14	и O
15	цены O
16	на O
17	них O
18	с O
19	веб O
20	- O
21	сайта O
22	TripAdvisor B-InfoResource
23	. O

# sent_id = 63
# text =   Я приведу простой пример извлечения данных о погоде с общедоступного API Dark Sky.
1	Я O
2	приведу O
3	простой O
4	пример O
5	извлечения O
6	данных O
7	о O
8	погоде O
9	с O
10	общедоступного O
11	API B-Environment
12	Dark I-Environment
13	Sky I-Environment
14	. O

# sent_id = 64
# text =   Для доступа к данным из Dark Sky я воспользуюсь библиотекой requests.
1	Для O
2	доступа O
3	к O
4	данным O
5	из O
6	Dark B-Environment
7	Sky I-Environment
8	я O
9	воспользуюсь O
10	библиотекой O
11	requests B-Environment
12	. O

# sent_id = 65
# text =   Речь шла о морфологической разметке (part of speech tagging) современных текстов на русском языке.
1	Речь B-Subject
2	шла O
3	о O
4	морфологической B-Labeling
5	разметке B-Method
6	( O
7	part B-Method
8	of I-Method
9	speech I-Method
10	tagging I-Method
11	) O
12	современных O
13	текстов B-Object
14	на O
15	русском B-Lang
16	языке I-Lang
17	. O

# sent_id = 66
# text =   Как обычно, результат разметки будет опубликован на условиях лицензии Creative Commons.
1	Как O
2	обычно O
3	, O
4	результат O
5	разметки B-Method
6	будет O
7	опубликован O
8	на O
9	условиях O
10	лицензии O
11	Creative B-Environment
12	Commons I-Environment
13	. O

# sent_id = 67
# text =   Извлечение именованных сущностей из текста — одна из востребованных функций текстовой аналитики.
1	Извлечение B-Task
2	именованных I-Task
3	сущностей I-Task
4	из O
5	текста O
6	— O
7	одна O
8	из O
9	востребованных O
10	функций O
11	текстовой O
12	аналитики B-Activity
13	. O

# sent_id = 68
# text =   Известный учёный Алан Тьюринг в 1950 году усомнился в том, что машина не может мыслить, и для проверки предложил свой знаменитый тест.
1	Известный O
2	учёный O
3	Алан B-Person
4	Тьюринг I-Person
5	в O
6	1950 B-Date
7	году O
8	усомнился O
9	в O
10	том O
11	, O
12	что O
13	машина O
14	не O
15	может O
16	мыслить O
17	, O
18	и O
19	для O
20	проверки O
21	предложил O
22	свой O
23	знаменитый O
24	тест B-Object
25	. O

# sent_id = 69
# text =  В 1954 году прошёл Джорджтаунский эксперимент.
1	В O
2	1954 B-Date_Experiment
3	году O
4	прошёл O
5	Джорджтаунский B-Experiment
6	эксперимент I-Experiment
7	. O

# sent_id = 70
# text =   В его рамках демонстрировалась система, которая автоматически перевела 60 предложений с русского языка на французский.
1	В O
2	его O
3	рамках O
4	демонстрировалась O
5	система B-Object
6	, O
7	которая O
8	автоматически O
9	перевела O
10	60 O
11	предложений B-Subject
12	с O
13	русского B-Lang
14	языка B-Object
15	на O
16	французский B-Lang
17	. O

# sent_id = 71
# text =   В 1960-е годы появились первые чат-боты, очень примитивные: в основном они перефразировали то, что говорил им собеседник-человек.
1	В O
2	1960-е B-Date_Result
3	годы O
4	появились O
5	первые O
6	чат B-Result
7	- I-Result
8	боты I-Result
9	, O
10	очень O
11	примитивные O
12	: O
13	в O
14	основном O
15	они O
16	перефразировали O
17	то O
18	, O
19	что O
20	говорил O
21	им O
22	собеседник O
23	- O
24	человек O
25	. O

# sent_id = 72
# text =   Даже знаменитый чат-бот Женя Густман, который, как считается, прошёл одну из версий теста Тьюринга, сделал это не благодаря хитрым алгоритмам.
1	Даже O
2	знаменитый O
3	чат B-Result
4	- I-Result
5	бот I-Result
6	Женя I-Result
7	Густман I-Result
8	, O
9	который O
10	, O
11	как O
12	считается O
13	, O
14	прошёл O
15	одну O
16	из O
17	версий O
18	теста B-Result
19	Тьюринга I-Result
20	, O
21	сделал O
22	это O
23	не O
24	благодаря O
25	хитрым O
26	алгоритмам O
27	. O

# sent_id = 73
# text =   Учёные пытались всё формализовать, построить формальную модель, онтологию, понятия, связи, общие правила синтаксического разбора и универсальную грамматику.
1	Учёные O
2	пытались O
3	всё O
4	формализовать O
5	, O
6	построить O
7	формальную B-Model
8	модель B-Model
9	, O
10	онтологию B-Result
11	, O
12	понятия B-Object
13	, O
14	связи B-Object
15	, O
16	общие O
17	правила O
18	синтаксического B-Method
19	разбора I-Method
20	и O
21	универсальную B-Result
22	грамматику I-Result
23	. O

# sent_id = 74
# text =   Тогда возникла теория грамматик Хомского.
1	Тогда O
2	возникла O
3	теория B-Method
4	грамматик I-Method
5	Хомского I-Method
6	. O

# sent_id = 75
# text =   Поэтому в 1980-е годы внимание переключилось на систему другого класса: на алгоритмы машинного обучения и так называемую корпусную лингвистику.
1	Поэтому O
2	в O
3	1980-е B-Date
4	годы O
5	внимание O
6	переключилось O
7	на O
8	систему O
9	другого O
10	класса B-Object
11	: O
12	на O
13	алгоритмы B-Method
14	машинного B-Method
15	обучения I-Method
16	и O
17	так O
18	называемую O
19	корпусную B-Science
20	лингвистику I-Science
21	. O

# sent_id = 76
# text =   В 1990-е годы эта область получила очень мощный толчок благодаря развитию Всемирной паутины с большим количеством слабоструктурированного текста, по которому нужно было искать, его требовалось каталогизировать.
1	В O
2	1990-е B-Date
3	годы O
4	эта O
5	область O
6	получила O
7	очень O
8	мощный O
9	толчок O
10	благодаря O
11	развитию O
12	Всемирной B-Application
13	паутины I-Application
14	с O
15	большим O
16	количеством O
17	слабоструктурированного B-Object
18	текста I-Object
19	, O
20	по O
21	которому O
22	нужно O
23	было O
24	искать O
25	, O
26	его O
27	требовалось O
28	каталогизировать O
29	. O

# sent_id = 77
# text =   В 2000-е анализ естественных языков начал применяться уже не только для поиска в Интернете, но и для решения разнообразных задач.
1	В O
2	2000-е O
3	анализ B-Method
4	естественных I-Method
5	языков B-Method
6	начал O
7	применяться O
8	уже O
9	не O
10	только O
11	для O
12	поиска O
13	в O
14	Интернете B-Application
15	, O
16	но O
17	и O
18	для O
19	решения O
20	разнообразных O
21	задач O
22	. O

# sent_id = 78
# text =   Возникли модели, основанные на краудсорсинге: мы не только пытаемся что-то понять с помощью машины, а подключаем людей, которые за небольшую плату определяют, на каком языке написан текст.
1	Возникли O
2	модели B-Object
3	, O
4	основанные O
5	на O
6	краудсорсинге B-Method
7	: O
8	мы O
9	не O
10	только O
11	пытаемся O
12	что O
13	- O
14	то O
15	понять O
16	с O
17	помощью O
18	машины O
19	, O
20	а O
21	подключаем O
22	людей O
23	, O
24	которые O
25	за O
26	небольшую O
27	плату O
28	определяют O
29	, O
30	на O
31	каком O
32	языке O
33	написан O
34	текст O
35	. O

# sent_id = 79
# text =   В некотором смысле начали возрождаться идеи использования формальных онтологий, но теперь онтологии крутятся вокруг краудсорсинговых баз знаний, в частности баз на основе Linked Open Data.
1	В O
2	некотором O
3	смысле O
4	начали O
5	возрождаться O
6	идеи O
7	использования O
8	формальных B-Object
9	онтологий I-Object
10	, O
11	но O
12	теперь O
13	онтологии B-Object
14	крутятся O
15	вокруг O
16	краудсорсинговых O
17	баз O
18	знаний O
19	, O
20	в O
21	частности O
22	баз O
23	на O
24	основе O
25	Linked B-InfoResource
26	Open I-InfoResource
27	Data I-InfoResource
28	. O

# sent_id = 80
# text =   Это целый набор баз знаний, его центр — машиночитаемый вариант «Википедии» DBpedia, который тоже наполняется по краудсорсинговой модели.
1	Это O
2	целый O
3	набор O
4	баз O
5	знаний O
6	, O
7	его O
8	центр O
9	— O
10	машиночитаемый O
11	вариант O
12	« O
13	Википедии B-InfoResource
14	» O
15	DBpedia B-InfoResource
16	, O
17	который O
18	тоже O
19	наполняется O
20	по O
21	краудсорсинговой B-Model
22	модели I-Model
23	. O

# sent_id = 81
# text =   В частности, семантический анализ (о чём документ?), генерация автоматической аннотации и автоматического summary, перевод и создание документов.
1	В O
2	частности O
3	, O
4	семантический B-Method
5	анализ I-Method
6	( O
7	о O
8	чём O
9	документ O
10	? O
11	) O
12	, O
13	генерация B-Task
14	автоматической I-Task
15	аннотации I-Task
16	и O
17	автоматического B-Object
18	summary I-Object
19	, O
20	перевод B-Task
21	и O
22	создание B-Task
23	документов I-Task
24	. O

# sent_id = 82
# text =   Все наверняка слышали об известном генераторе научных статей SCIgen, который создал статью «Корчеватель: Алгоритм типичной унификации точек доступа и избыточности».
1	Все O
2	наверняка O
3	слышали O
4	об O
5	известном O
6	генераторе O
7	научных O
8	статей O
9	SCIgen B-Technology
10	, O
11	который O
12	создал O
13	статью O
14	« O
15	Корчеватель O
16	: O
17	Алгоритм O
18	типичной O
19	унификации O
20	точек O
21	доступа O
22	и O
23	избыточности O
24	» O
25	. O

# sent_id = 83
# text =   Но в случае с лентой такие рекомендации работают плохо: здесь постоянно возникает ситуация холодного старта.
1	Но O
2	в O
3	случае O
4	с O
5	лентой O
6	такие O
7	рекомендации O
8	работают O
9	плохо O
10	: O
11	здесь O
12	постоянно O
13	возникает O
14	ситуация B-Object
15	холодного I-Object
16	старта I-Object
17	. O

# sent_id = 84
# text =   Поэтому применим классический воркэраунд для задачи холодного старта и построим систему контентных рекомендаций: попробуем научить машину понимать, о чём написан пост.
1	Поэтому O
2	применим O
3	классический O
4	воркэраунд B-Method
5	для O
6	задачи O
7	холодного B-Task
8	старта I-Task
9	и O
10	построим O
11	систему O
12	контентных O
13	рекомендаций O
14	: O
15	попробуем O
16	научить O
17	машину O
18	понимать O
19	, O
20	о O
21	чём O
22	написан O
23	пост O
24	. O

# sent_id = 85
# text =   Соответственно, требуется метод семантического анализа.
1	Соответственно O
2	, O
3	требуется O
4	метод B-Method
5	семантического I-Method
6	анализа I-Method
7	. O

# sent_id = 86
# text =   Тут поможет анализ эмоциональной окраски.
1	Тут O
2	поможет O
3	анализ B-Method
4	эмоциональной I-Method
5	окраски I-Method
6	. O

# sent_id = 87
# text =   В частности, это Apache Tika, японская библиотека language-detection и одна из последних разработок — питоновский пакет Ldig, который как раз работает на инфинитиграммах.
1	В O
2	частности O
3	, O
4	это O
5	Apache B-Library
6	Tika I-Library
7	, O
8	японская O
9	библиотека O
10	language B-Library
11	- I-Library
12	detection I-Library
13	и O
14	одна O
15	из O
16	последних O
17	разработок O
18	— O
19	питоновский O
20	пакет O
21	Ldig B-Library
22	, O
23	который O
24	как O
25	раз O
26	работает O
27	на O
28	инфинитиграммах O
29	. O

# sent_id = 88
# text =   Но если текст короткий, из одного предложения или нескольких слов, то классический подход, основанный на триграммах, очень часто ошибается.
1	Но O
2	если O
3	текст B-Object
4	короткий O
5	, O
6	из O
7	одного O
8	предложения B-Subject
9	или O
10	нескольких O
11	слов B-Subject
12	, O
13	то O
14	классический O
15	подход O
16	, O
17	основанный O
18	на O
19	триграммах B-Subject
20	, O
21	очень O
22	часто O
23	ошибается O
24	. O

# sent_id = 89
# text =   Исправить ситуацию могут инфинитиграммы, но это новая область, далеко не для всех языков уже есть обученные и готовые классификаторы.
1	Исправить O
2	ситуацию O
3	могут O
4	инфинитиграммы B-Object
5	, O
6	но O
7	это O
8	новая O
9	область O
10	, O
11	далеко O
12	не O
13	для O
14	всех O
15	языков O
16	уже O
17	есть O
18	обученные O
19	и O
20	готовые O
21	классификаторы O
22	. O

# sent_id = 90
# text =   Первый основан на так называемом фонетическом матчинге.
1	Первый O
2	основан O
3	на O
4	так O
5	называемом O
6	фонетическом B-Method
7	матчинге I-Method
8	. O

# sent_id = 91
# text =   Альтернативный подход — так называемое редакционное расстояние, с помощью которого мы ищем в словаре максимально похожие слова-аналоги.
1	Альтернативный O
2	подход O
3	— O
4	так O
5	называемое O
6	редакционное B-Metric
7	расстояние I-Metric
8	, O
9	с O
10	помощью O
11	которого O
12	мы O
13	ищем O
14	в O
15	словаре O
16	максимально O
17	похожие O
18	слова O
19	- O
20	аналоги O
21	. O

# sent_id = 92
# text =   Первая концепция — стемминг, мы пытаемся найти основу слова.
1	Первая O
2	концепция O
3	— O
4	стемминг B-Method
5	, O
6	мы O
7	пытаемся O
8	найти O
9	основу B-Subject
10	слова I-Subject
11	. O

# sent_id = 93
# text =   Здесь используется подход affix stripping.
1	Здесь O
2	используется O
3	подход O
4	affix B-Method
5	stripping I-Method
6	. O

# sent_id = 94
# text =   Есть известная реализация, так называемый стеммер Портера, или проект Snowball.
1	Есть O
2	известная O
3	реализация O
4	, O
5	так O
6	называемый O
7	стеммер B-Object
8	Портера I-TERM
9	, O
10	или O
11	проект O
12	Snowball B-Project
13	. O

# sent_id = 95
# text =   Самый распространённый, наверное, инструмент — реализация в пакете Apache Lucene.
1	Самый O
2	распространённый O
3	, O
4	наверное O
5	, O
6	инструмент O
7	— O
8	реализация O
9	в O
10	пакете O
11	Apache B-Library
12	Lucene I-Library
13	. O

# sent_id = 96
# text =   Вторая концепция, альтернатива стемминга — лемматизация.
1	Вторая O
2	концепция O
3	, O
4	альтернатива O
5	стемминга B-Method
6	— O
7	лемматизация B-Method
8	. O

# sent_id = 97
# text =   Она пытается привести слово не к основе или корню, а к базовой, словарной форме — т. е. лемме.
1	Она O
2	пытается O
3	привести O
4	слово B-Subject
5	не O
6	к O
7	основе B-Subject
8	или O
9	корню B-Object
10	, O
11	а O
12	к O
13	базовой O
14	, O
15	словарной B-Subject
16	форме I-Subject
17	— O
18	т O
19	. O
20	  O
21	е O
22	. O
23	лемме B-Subject
24	. O

# sent_id = 98
# text =   Существует множество реализаций, и тема очень хорошо проработана именно для user generated текстов, пользовательски зашумлённых текстов.
1	Существует O
2	множество O
3	реализаций O
4	, O
5	и O
6	тема B-Object
7	очень O
8	хорошо O
9	проработана O
10	именно O
11	для O
12	user B-Object
13	generated I-Object
14	текстов I-Object
15	, O
16	пользовательски O
17	зашумлённых O
18	текстов B-Object
19	. O

# sent_id = 99
# text =   Теперь отобразим это в векторном пространстве, потому что почти все математические модели работают в векторных пространствах больших размерностей.
1	Теперь O
2	отобразим O
3	это O
4	в O
5	векторном B-Object
6	пространстве I-Object
7	, O
8	потому O
9	что O
10	почти O
11	все O
12	математические B-Model
13	модели B-Model
14	работают O
15	в O
16	векторных B-Object
17	пространствах I-Object
18	больших O
19	размерностей O
20	. O

# sent_id = 100
# text =   Базовый подход, который используют многие модели, — метод "мешка слов".
1	Базовый O
2	подход O
3	, O
4	который O
5	используют O
6	многие O
7	модели O
8	, O
9	— O
10	метод B-Method
11	" I-Method
12	мешка I-Method
13	слов I-Method
14	" I-Method
15	. O

# sent_id = 101
# text =   Доминирует так называемый TF-IDF.
1	Доминирует O
2	так O
3	называемый O
4	TF B-Metric
5	- I-Metric
6	IDF I-Metric
7	. O

# sent_id = 102
# text =   Частоту слова (term frequency, TF) определяют по-разному.
1	Частоту B-Metric
2	слова B-Metric
3	( O
4	term B-Metric
5	frequency I-Metric
6	, O
7	TF B-Metric
8	) O
9	определяют O
10	по O
11	- O
12	разному O
13	. O

# sent_id = 103
# text =   Определив TF в документе, мы перемножаем её с обратной частотой документа (inverse document frequency, IDF).
1	Определив O
2	TF B-Metric
3	в O
4	документе O
5	, O
6	мы O
7	перемножаем O
8	её O
9	с O
10	обратной B-Metric
11	частотой I-Metric
12	документа I-Metric
13	( O
14	inverse B-Metric
15	document I-Metric
16	frequency I-Metric
17	, O
18	IDF B-Metric
19	) O
20	. O

# sent_id = 104
# text =   IDF обычно вычисляют как логарифм от числа документов в корпусе, разделённый на количество документов, где это слово представлено.
1	IDF B-Metric
2	обычно O
3	вычисляют O
4	как O
5	логарифм B-Object
6	от O
7	числа O
8	документов O
9	в O
10	корпусе B-Object
11	, O
12	разделённый O
13	на O
14	количество O
15	документов O
16	, O
17	где O
18	это O
19	слово B-Subject
20	представлено O
21	. O

# sent_id = 105
# text =   Например, при анализе эмоциональной окраски очень важно, к чему относилось, условно говоря, слово «хороший» или «нет».
1	Например O
2	, O
3	при O
4	анализе B-Method
5	эмоциональной I-Method
6	окраски I-Method
7	очень O
8	важно O
9	, O
10	к O
11	чему O
12	относилось O
13	, O
14	условно O
15	говоря O
16	, O
17	слово B-Subject
18	« O
19	хороший O
20	» O
21	или O
22	« O
23	нет O
24	» O
25	. O

# sent_id = 106
# text =   Тогда наряду с мешком слов поможет мешок N-грамм: мы добавляем в словарь не только слова, но и словосочетания.
1	Тогда O
2	наряду O
3	с O
4	мешком B-Method
5	слов I-Method
6	поможет O
7	мешок B-Method
8	N I-Method
9	- I-Method
10	грамм I-Method
11	: O
12	мы O
13	добавляем O
14	в O
15	словарь O
16	не O
17	только O
18	слова B-Subject
19	, O
20	но O
21	и O
22	словосочетания B-Subject
23	. O

# sent_id = 107
# text =   Мы не будем вносить все словосочетания, потому что это приведёт к комбинаторному взрыву, но часто используемые статистически значимые пары или пары, соответствующие именованным сущностям, можно добавить, и это повысит качество работы итоговой модели.
1	Мы O
2	не O
3	будем O
4	вносить O
5	все O
6	словосочетания B-Subject
7	, O
8	потому O
9	что O
10	это O
11	приведёт O
12	к O
13	комбинаторному O
14	взрыву O
15	, O
16	но O
17	часто O
18	используемые O
19	статистически O
20	значимые O
21	пары O
22	или O
23	пары O
24	, O
25	соответствующие O
26	именованным B-Subject
27	сущностям I-Subject
28	, O
29	можно O
30	добавить O
31	, O
32	и O
33	это O
34	повысит O
35	качество O
36	работы O
37	итоговой O
38	модели B-Object
39	. O

# sent_id = 108
# text =   Отчасти эти ситуации позволяют обработать методы построения "векторных представлений слов", например, знаменитый word2vec или более модные skip-gramm.
1	Отчасти O
2	эти O
3	ситуации O
4	позволяют O
5	обработать O
6	методы O
7	построения O
8	" O
9	векторных B-Subject
10	представлений I-Subject
11	слов I-Subject
12	" O
13	, O
14	например O
15	, O
16	знаменитый O
17	word2vec B-Model
18	или O
19	более O
20	модные O
21	skip B-Model
22	- I-Model
23	gramm I-Model
24	. O

# sent_id = 109
# text =   Стандартные хеш-функции равномерно размазывают данные по пространству хешей.
1	Стандартные O
2	хеш B-Method
3	- I-Method
4	функции I-Method
5	равномерно O
6	размазывают O
7	данные O
8	по O
9	пространству O
10	хешей O
11	. O

# sent_id = 110
# text =   Локально-чувствительный хеш похожие объекты поместит в пространстве объектов близко.
1	Локально B-Method
2	- I-Method
3	чувствительный I-Method
4	хеш I-Method
5	похожие O
6	объекты O
7	поместит O
8	в O
9	пространстве O
10	объектов O
11	близко O
12	. O

# sent_id = 111
# text =   Мы выбираем случайный базис из случайных векторов.
1	Мы O
2	выбираем O
3	случайный O
4	базис B-Object
5	из O
6	случайных B-Object
7	векторов I-Object
8	. O

# sent_id = 112
# text =   Задача семантического анализа достаточно старая.
1	Задача O
2	семантического B-Task
3	анализа B-Task
4	достаточно O
5	старая O
6	. O

# sent_id = 113
# text =   Современный подход — анализ семантики без учителя, поэтому его называют анализом скрытой (латентной) семантики.
1	Современный O
2	подход O
3	— O
4	анализ B-Method
5	семантики I-Method
6	без I-Method
7	учителя I-Method
8	, O
9	поэтому O
10	его O
11	называют O
12	анализом B-Method
13	скрытой I-Method
14	( I-Method
15	латентной I-Method
16	) I-Method
17	семантики I-Method
18	. O

# sent_id = 114
# text =   Исторически первый подход к латентно-семантическому анализу — это латентно-семантическое индексирование.
1	Исторически O
2	первый O
3	подход O
4	к O
5	латентно B-Method
6	- I-Method
7	семантическому I-Method
8	анализу B-Method
9	— O
10	это O
11	латентно B-Method
12	- I-Method
13	семантическое I-Method
14	индексирование I-Method
15	. O

# sent_id = 115
# text =   Мы уже использовали для решения задач коллаборативных рекомендаций хорошо зарекомендовавшие себя техники факторизации матриц.
1	Мы O
2	уже O
3	использовали O
4	для O
5	решения O
6	задач B-Task
7	коллаборативных I-Task
8	рекомендаций I-Task
9	хорошо O
10	зарекомендовавшие O
11	себя O
12	техники O
13	факторизации O
14	матриц O
15	. O

# sent_id = 116
# text =   В чём суть факторизации?
1	В O
2	чём O
3	суть O
4	факторизации B-Method
5	? O

# sent_id = 117
# text =   Одной из альтернатив стал так называемый вероятностный латентно-семантический индекс.
1	Одной O
2	из O
3	альтернатив O
4	стал O
5	так O
6	называемый O
7	вероятностный B-Metric
8	латентно I-Metric
9	- I-Metric
10	семантический I-Metric
11	индекс B-Metric
12	. O

# sent_id = 118
# text =   Важно понять, что техника вероятностного латентно-семантического индекса — это техника факторизации матрицы.
1	Важно O
2	понять O
3	, O
4	что O
5	техника B-Method
6	вероятностного I-Method
7	латентно I-Method
8	- I-Method
9	семантического I-Method
10	индекса B-Method
11	— O
12	это O
13	техника I-Method
14	факторизации I-Method
15	матрицы I-Method
16	. O

# sent_id = 119
# text =   По сравнению с классической факторизацией на основе сингулярного разложения у вероятностной генерирующей модели есть важное преимущество.
1	По O
2	сравнению O
3	с O
4	классической B-Method
5	факторизацией I-Method
6	на O
7	основе O
8	сингулярного B-Object
9	разложения I-Object
10	у O
11	вероятностной O
12	генерирующей O
13	модели O
14	есть O
15	важное O
16	преимущество O
17	. O

# sent_id = 120
# text =   Для этого используется перплексия.
1	Для O
2	этого O
3	используется O
4	перплексия B-Method
5	. O

# sent_id = 121
# text =   Есть так называемый EM-алгоритм.
1	Есть O
2	так O
3	называемый O
4	EM B-Method
5	- I-Method
6	алгоритм I-Method
7	. O

# sent_id = 122
# text =  Как поясняет сам Томас Димсон, This Word Does Not Exist является вариацией нейросети GPT-2.
1	Как O
2	поясняет O
3	сам O
4	Томас B-Person
5	Димсон I-Person
6	, O
7	This B-Model
8	Word I-Model
9	Does I-Model
10	Not I-Model
11	Exist I-Model
12	является O
13	вариацией O
14	нейросети O
15	GPT-2 B-Model
16	. O

# sent_id = 123
# text =   Существует также твиттер-бот проекта.
1	Существует O
2	также O
3	твиттер B-Technology
4	- I-Technology
5	бот I-Technology
6	проекта O
7	. O

# sent_id = 124
# text =   Чтобы натренировать свою нейросеть на основе загруженных файлов, Димсон рекомендует воспользоваться контентом Apple Dictionary или Urban Dictionary.
1	Чтобы O
2	натренировать O
3	свою O
4	нейросеть O
5	на O
6	основе O
7	загруженных O
8	файлов O
9	, O
10	Димсон B-Person
11	рекомендует O
12	воспользоваться O
13	контентом O
14	Apple B-InfoResource
15	Dictionary I-InfoResource
16	или O
17	Urban B-InfoResource
18	Dictionary I-InfoResource
19	. O

# sent_id = 125
# text =  Правда, пользователи YCombinator уже заметили, что This Word Does Not Exist иногда предлагает уже существующие слова — например, refactoring.
1	Правда O
2	, O
3	пользователи O
4	YCombinator B-Application
5	уже O
6	заметили O
7	, O
8	что O
9	This B-Model
10	Word I-Model
11	Does I-Model
12	Not I-Model
13	Exist I-Model
14	иногда O
15	предлагает O
16	уже O
17	существующие O
18	слова O
19	— O
20	например O
21	, O
22	refactoring O
23	. O

# sent_id = 126
# text =   После выхода учебника я читал курс на его основе в УрФУ, ШАДе, ИТМО и СПбГУ и убедился, что наличие перевода очень помогает.
1	После O
2	выхода O
3	учебника O
4	я O
5	читал O
6	курс O
7	на O
8	его O
9	основе O
10	в O
11	УрФУ B-Abbrev_Organization
12	, O
13	ШАДе B-Abbrev_Organization
14	, O
15	ИТМО B-Abbrev_Organization
16	и O
17	СПбГУ B-Abbrev_Organization
18	и O
19	убедился O
20	, O
21	что O
22	наличие O
23	перевода O
24	очень O
25	помогает O
26	. O

# sent_id = 127
# text =   В случае NLP потребность в «локализованных» учебных материалах еще заметнее, чем в информационном поиске.
1	В O
2	случае O
3	NLP B-ShortName_Science
4	потребность O
5	в O
6	« O
7	локализованных O
8	» O
9	учебных O
10	материалах O
11	еще O
12	заметнее O
13	, O
14	чем O
15	в O
16	информационном B-Task
17	поиске I-Task
18	. O

# sent_id = 128
# text =   Слушателям предлагается самостоятельно реализовать методы морфологического анализа, определения тональности текста, автоматического реферирования документов, извлечения именованных сущностей и машинного перевода.
1	Слушателям O
2	предлагается O
3	самостоятельно O
4	реализовать O
5	методы B-Method
6	морфологического I-Method
7	анализа I-Method
8	, O
9	определения B-Task
10	тональности I-Task
11	текста I-Task
12	, O
13	автоматического B-Task
14	реферирования I-Task
15	документов I-Task
16	, O
17	извлечения B-Task
18	именованных I-Task
19	сущностей I-Task
20	и O
21	машинного B-Task
22	перевода I-Task
23	. O

# sent_id = 129
# text =   Желательно, чтобы слушатели обладали базовыми знаниями линейной алгебры, теории вероятностей, математической статистики и машинного обучения, а также навыками программирования (необходимы для решения практических заданий).
1	Желательно O
2	, O
3	чтобы O
4	слушатели O
5	обладали O
6	базовыми O
7	знаниями O
8	линейной B-Science
9	алгебры I-Science
10	, O
11	теории B-Science
12	вероятностей I-Science
13	, O
14	математической B-Science
15	статистики I-Science
16	и O
17	машинного B-Method
18	обучения I-Method
19	, O
20	а O
21	также O
22	навыками O
23	программирования B-Science
24	( O
25	необходимы O
26	для O
27	решения O
28	практических O
29	заданий O
30	) O
31	. O

# sent_id = 130
# text =   Он опубликовал программу (репозиторий на гитхабе), которая делает именно это: генерирует политические речи, удивительно похожие на настоящие.
1	Он O
2	опубликовал O
3	программу O
4	( O
5	репозиторий O
6	на O
7	гитхабе O
8	) O
9	, O
10	которая O
11	делает O
12	именно O
13	это O
14	: O
15	генерирует B-Task
16	политические I-Task
17	речи B-Task
18	, O
19	удивительно O
20	похожие O
21	на O
22	настоящие O
23	. O

# sent_id = 131
# text =  Ученые Новосибирского государственного технического университета НЭТИ завершают разработку системы распознавания русского жестового языка.
1	Ученые O
2	Новосибирского B-Organization
3	государственного I-Organization
4	технического I-Organization
5	университета I-Organization
6	НЭТИ B-ShortName_Organization
7	завершают O
8	разработку O
9	системы O
10	распознавания O
11	русского B-Lang
12	жестового B-Object
13	языка I-Object
14	. O

# sent_id = 132
# text =   Точность распознавания составляет 92%.
1	Точность B-Metric
2	распознавания O
3	составляет O
4	92 B-Value
5	% I-Value
6	. O

# sent_id = 133
# text =   «Мы также вели работу над выделением эпентезы (межжестовое движение).
1	« O
2	Мы O
3	также O
4	вели O
5	работу O
6	над O
7	выделением B-Task
8	эпентезы I-Task
9	( O
10	межжестовое B-Object
11	движение I-Object
12	) O
13	. O

# sent_id = 134
# text =   Сейчас точность выделения жестов в видеопотоке составляет 85—90%.
1	Сейчас O
2	точность B-Metric
3	выделения B-Task
4	жестов I-Task
5	в O
6	видеопотоке O
7	составляет O
8	85—90 B-Value
9	% I-Value
10	. O

# sent_id = 135
# text =   С моделью от OpenAI связано сразу несколько новостей — хорошая и не очень.
1	С O
2	моделью B-Object
3	от O
4	OpenAI B-Organization
5	связано O
6	сразу O
7	несколько O
8	новостей O
9	— O
10	хорошая O
11	и O
12	не O
13	очень O
14	. O

# sent_id = 136
# text =   Сделка OpenAI и Microsoft. Начать придется с менее приятной — компания Майкрософт завладела эксклюзивными правами на GPT-3.
1	Сделка O
2	OpenAI B-Organization
3	и O
4	Microsoft B-Organization
5	. O
6	Начать O
7	придется O
8	с O
9	менее O
10	приятной O
11	— O
12	компания O
13	Майкрософт B-Organization
14	завладела O
15	эксклюзивными O
16	правами O
17	на O
18	GPT-3 B-Model
19	. O

# sent_id = 137
# text =   Сделка предсказуемо вызвала негодование — Элон Маск, основатель OpenAI, а ныне бывший член совета директоров компании, заявил, что Майкрософт по сути захватили OpenAI.
1	Сделка O
2	предсказуемо O
3	вызвала O
4	негодование O
5	— O
6	Элон B-Person
7	Маск I-Person
8	, O
9	основатель O
10	OpenAI B-Organization
11	, O
12	а O
13	ныне O
14	бывший O
15	член O
16	совета O
17	директоров O
18	компании O
19	, O
20	заявил O
21	, O
22	что O
23	Майкрософт B-Organization
24	по O
25	сути O
26	захватили O
27	OpenAI B-Organization
28	. O

# sent_id = 138
# text =   Дело в том, что OpenAI изначально создавалась как некоммерческая организация с высокой миссией — не позволить искусственному интеллекту оказаться в руках отдельного государства или корпорации.
1	Дело O
2	в O
3	том O
4	, O
5	что O
6	OpenAI B-Organization
7	изначально O
8	создавалась O
9	как O
10	некоммерческая O
11	организация O
12	с O
13	высокой O
14	миссией O
15	— O
16	не O
17	позволить O
18	искусственному B-Object
19	интеллекту I-Object
20	оказаться O
21	в O
22	руках O
23	отдельного O
24	государства O
25	или O
26	корпорации O
27	. O

# sent_id = 139
# text =   ruGPT3 от Сбера. Теперь к более приятной новости — исследователи из Сбера выложили в открытый доступ модель, которая повторяет архитектуру GPT-3 и основана на коде GPT-2 и, самое главное, обучена на русскоязычном корпусе.
1	ruGPT3 B-Model
2	от O
3	Сбера B-Organization
4	. O
5	Теперь O
6	к O
7	более O
8	приятной O
9	новости O
10	— O
11	исследователи O
12	из O
13	Сбера B-Organization
14	выложили O
15	в O
16	открытый O
17	доступ O
18	модель B-Object
19	, O
20	которая O
21	повторяет O
22	архитектуру O
23	GPT-3 B-Model
24	и O
25	основана O
26	на O
27	коде O
28	GPT-2 B-Model
29	и O
30	, O
31	самое O
32	главное O
33	, O
34	обучена O
35	на O
36	русскоязычном O
37	корпусе B-Object
38	. O

# sent_id = 140
# text =   Если коммерческие организации можно оправдать тем, что код часто вплетен в инфраструктуру проектов, то что говорить про исследовательские институты и некоммерческие компании вроде DeepMind и OpenAI?
1	Если O
2	коммерческие O
3	организации O
4	можно O
5	оправдать O
6	тем O
7	, O
8	что O
9	код O
10	часто O
11	вплетен O
12	в O
13	инфраструктуру O
14	проектов O
15	, O
16	то O
17	что O
18	говорить O
19	про O
20	исследовательские O
21	институты O
22	и O
23	некоммерческие O
24	компании O
25	вроде O
26	DeepMind B-Organization
27	и O
28	OpenAI B-Organization
29	? O

# sent_id = 141
# text =   Платформа для видеозвонков Maxine объединяет в себе целый зоопарк ML-алгоритмов.
1	Платформа O
2	для O
3	видеозвонков O
4	Maxine B-Application
5	объединяет O
6	в O
7	себе O
8	целый O
9	зоопарк O
10	ML O
11	- O
12	алгоритмов O
13	. O

# sent_id = 142
# text =   Google Meet поделились кейсом создания своего алгоритма для качественного удаления фона на основе фреймворка от Mediapipe (который умеет отслеживание движение глаз, головы и рук).
1	Google B-Organization
2	Meet I-Organization
3	поделились O
4	кейсом O
5	создания O
6	своего O
7	алгоритма O
8	для O
9	качественного O
10	удаления O
11	фона B-Subject
12	на O
13	основе B-Subject
14	фреймворка O
15	от O
16	Mediapipe B-Organization
17	( O
18	который O
19	умеет O
20	отслеживание O
21	движение O
22	глаз O
23	, O
24	головы O
25	и O
26	рук O
27	) O
28	. O

# sent_id = 143
# text =   Google также запустил новую функцию для сервиса YouTube Stories на iOS, который позволяет улучшать качество речи.
1	Google B-Organization
2	также O
3	запустил O
4	новую O
5	функцию O
6	для O
7	сервиса O
8	YouTube B-Technology
9	Stories I-Technology
10	на O
11	iOS B-Environment
12	, O
13	который O
14	позволяет O
15	улучшать O
16	качество O
17	речи B-Subject
18	. O

# sent_id = 144
# text =   Прорывы #DeepPavlov в 2019 году: обзор и итоги года Московский физико-технический институт (МФТИ).
1	Прорывы O
2	# O
3	DeepPavlov B-Model
4	в O
5	2019 B-Date
6	году O
7	: O
8	обзор O
9	и O
10	итоги O
11	года O
12	Московский B-Organization
13	физико I-Organization
14	- I-Organization
15	технический I-Organization
16	институт I-Organization
17	( O
18	МФТИ B-ShortName_Organization
19	) O
20	. O

# sent_id = 145
# text =   Библиотеке #DeepPavlov, на минуточку, уже два года, и мы рады, что наше сообщество с каждым днем растет.
1	Библиотеке O
2	# O
3	DeepPavlov B-Library
4	, O
5	на O
6	минуточку O
7	, O
8	уже O
9	два O
10	года O
11	, O
12	и O
13	мы O
14	рады O
15	, O
16	что O
17	наше O
18	сообщество O
19	с O
20	каждым O
21	днем O
22	растет O
23	. O

# sent_id = 146
# text =   Увеличилось количество коммерческих решений за счет state-of-art технологий, реализованных в DeepPavlov, в разных отраслях от ритейла до промышленности.
1	Увеличилось O
2	количество O
3	коммерческих O
4	решений O
5	за O
6	счет O
7	state B-Method
8	- I-Method
9	of I-Method
10	- I-Method
11	art I-Method
12	технологий O
13	, O
14	реализованных O
15	в O
16	DeepPavlov B-Model
17	, O
18	в O
19	разных O
20	отраслях O
21	от O
22	ритейла O
23	до O
24	промышленности O
25	. O

# sent_id = 147
# text =   Вышел первый релиз DeepPavlov Agent.
1	Вышел O
2	первый O
3	релиз O
4	DeepPavlov B-Application
5	Agent B-Application
6	. O

# sent_id = 148
# text =   DeepPavlov решает проблемы такие как: классификация текста, исправление опечаток, распознавание именованных сущностей, ответы на вопросы по базе знаний и многие другие.
1	DeepPavlov B-Model
2	решает O
3	проблемы O
4	такие O
5	как O
6	: O
7	классификация B-Task
8	текста B-Task
9	, O
10	исправление B-Task
11	опечаток I-Task
12	, O
13	распознавание B-Task
14	именованных I-Task
15	сущностей I-Task
16	, O
17	ответы B-Task
18	на I-Task
19	вопросы I-Task
20	по O
21	базе O
22	знаний O
23	и O
24	многие O
25	другие O
26	. O

# sent_id = 149
# text =   Библиотека поддерживает платформы Linux и Windows.
1	Библиотека O
2	поддерживает O
3	платформы O
4	Linux B-Environment
5	и O
6	Windows B-Environment
7	. O

# sent_id = 150
# text =   В настоящее время современные результаты во многих задачах были достигнуты благодаря применению моделей на основе BERT.
1	В O
2	настоящее O
3	время O
4	современные O
5	результаты O
6	во O
7	многих O
8	задачах O
9	были O
10	достигнуты O
11	благодаря O
12	применению O
13	моделей O
14	на O
15	основе O
16	BERT B-ShortName_Model
17	. O

# sent_id = 151
# text =   Команда DeepPavlov интегрировала BERT в три последующие задачи: классификация текста, распознавание именованных сущностей и ответы на вопросы.
1	Команда O
2	DeepPavlov B-Organization
3	интегрировала O
4	BERT B-ShortName_Model
5	в O
6	три O
7	последующие O
8	задачи O
9	: O
10	классификация B-Task
11	текста B-Task
12	, O
13	распознавание B-Task
14	именованных I-Task
15	сущностей I-Task
16	и O
17	ответы B-Task
18	на I-Task
19	вопросы I-Task
20	. O

# sent_id = 152
# text =   Модель классификации текста на основе BERT DeepPavlov служит, например, для решения проблемы обнаружения оскорблений.
1	Модель O
2	классификации O
3	текста O
4	на O
5	основе O
6	BERT B-ShortName_Model
7	DeepPavlov B-Model
8	служит O
9	, O
10	например O
11	, O
12	для O
13	решения O
14	проблемы O
15	обнаружения B-Task
16	оскорблений I-Task
17	. O

# sent_id = 153
# text =   В дополнение к моделям классификации текста DeepPavlov содержит модель на основе BERT для распознавания именованных сущностей (NER).
1	В O
2	дополнение O
3	к O
4	моделям O
5	классификации B-Task
6	текста I-Task
7	DeepPavlov B-Model
8	содержит O
9	модель O
10	на O
11	основе O
12	BERT B-ShortName_Model
13	для O
14	распознавания B-Task
15	именованных I-Task
16	сущностей I-Task
17	( O
18	NER B-Abbrev_Task
19	) O
20	. O

# sent_id = 154
# text =   Например, модель может извлечь важную информацию из резюме, чтобы облегчить работу специалистов по кадрам.
1	Например O
2	, O
3	модель B-Object
4	может O
5	извлечь B-Task
6	важную I-Task
7	информацию I-Task
8	из O
9	резюме O
10	, O
11	чтобы O
12	облегчить O
13	работу O
14	специалистов O
15	по O
16	кадрам O
17	. O

# sent_id = 155
# text =   Кроме того, NER может использоваться для идентификации соответствующих объектов в запросах клиентов, таких как спецификации продуктов, названия компаний или данные о филиалах компании.
1	Кроме O
2	того O
3	, O
4	NER B-Abbrev_Task
5	может O
6	использоваться O
7	для O
8	идентификации B-Task
9	соответствующих I-Task
10	объектов I-Task
11	в O
12	запросах O
13	клиентов O
14	, O
15	таких O
16	как O
17	спецификации O
18	продуктов O
19	, O
20	названия O
21	компаний O
22	или O
23	данные O
24	о O
25	филиалах O
26	компании O
27	. O

# sent_id = 156
# text =   Команда DeepPavlov обучила модель NER на англоязычном корпусе OntoNotes, который имеет 19 типов разметки, включая PER (человек), LOC (местоположение), ORG (организация) и многие другие.
1	Команда O
2	DeepPavlov B-Organization
3	обучила O
4	модель B-Model
5	NER I-Model
6	на O
7	англоязычном O
8	корпусе B-Corpus
9	OntoNotes I-Corpus
10	, O
11	который O
12	имеет O
13	19 O
14	типов O
15	разметки B-Method
16	, O
17	включая O
18	PER Labeling
19	( O
20	человек O
21	) O
22	, O
23	LOC Labeling
24	( O
25	местоположение O
26	) O
27	, O
28	ORG Labeling
29	( O
30	организация O
31	) O
32	и O
33	многие O
34	другие O
35	. O

# sent_id = 157
# text =   Одним из основных переломных моментов в этой области стал выпуск Стэнфордского набора данных для ответов на вопросы (SQuAD).
1	Одним O
2	из O
3	основных O
4	переломных O
5	моментов O
6	в O
7	этой O
8	области O
9	стал O
10	выпуск O
11	Стэнфордского B-InfoResource
12	набора I-InfoResource
13	данных I-InfoResource
14	для I-InfoResource
15	ответов I-InfoResource
16	на I-InfoResource
17	вопросы I-InfoResource
18	( O
19	SQuAD B-Abbrev_InfoResource
20	) O
21	. O

# sent_id = 158
# text =   Набор данных SQuAD привел к появлению бесчисленных подходов к решению задачи вопросно-ответных систем.
1	Набор O
2	данных O
3	SQuAD B-Abbrev_InfoResource
4	привел O
5	к O
6	появлению O
7	бесчисленных O
8	подходов O
9	к O
10	решению O
11	задачи O
12	вопросно B-Task
13	- I-Task
14	ответных I-Task
15	систем I-Task
16	. O

# sent_id = 159
# text =   Одной из наиболее успешных является модель DeepPavlov BERT.
1	Одной O
2	из O
3	наиболее O
4	успешных O
5	является O
6	модель O
7	DeepPavlov B-Model
8	BERT I-Model
9	. O

# sent_id = 160
# text =   Чтобы использовать модель QA на основе BERT с DeepPavlov, необходимо следующее.
1	Чтобы O
2	использовать O
3	модель O
4	QA B-Model
5	на O
6	основе O
7	BERT B-ShortName_Model
8	с O
9	DeepPavlov B-TERM
10	, O
11	необходимо O
12	следующее O
13	. O

# sent_id = 161
# text =   DeepPavlov Agent — платформа для создания многозадачных чат-ботов.
1	DeepPavlov B-Technology
2	Agent I-Technology
3	— O
4	платформа O
5	для O
6	создания O
7	многозадачных O
8	чат O
9	- O
10	ботов O
11	. O

# sent_id = 162
# text =   При разработке разговорных агентов в основном применяется модульная архитектура для целенаправленного диалога, при котором разворачивается сценарий.
1	При O
2	разработке O
3	разговорных B-Technology
4	агентов I-Technology
5	в O
6	основном O
7	применяется O
8	модульная B-Object
9	архитектура I-Object
10	для O
11	целенаправленного B-Task
12	диалога B-Task
13	, O
14	при O
15	котором O
16	разворачивается O
17	сценарий O
18	. O

# sent_id = 163
# text =   Для решения этой задачи в октябре 2019 года вышел первый релиз DeepPavlov Agent 1.0 — платформы для создания многозадачных чат-ботов.
1	Для O
2	решения O
3	этой O
4	задачи O
5	в O
6	октябре O
7	2019 B-Date_Application
8	года I-Date_Application
9	вышел O
10	первый O
11	релиз O
12	DeepPavlov B-Technology
13	Agent I-Technology
14	1.0 I-Technology
15	— O
16	платформы O
17	для O
18	создания O
19	многозадачных O
20	чат O
21	- O
22	ботов O
23	. O

# sent_id = 164
# text =   Агент помогает разработчикам производственных чатботов организовать несколько NLP моделей в одном конвейере.
1	Агент O
2	помогает O
3	разработчикам O
4	производственных O
5	чатботов O
6	организовать B-Task
7	несколько I-Task
8	NLP I-Task
9	моделей I-Task
10	в O
11	одном O
12	конвейере O
13	. O

# sent_id = 165
# text =   Чтобы упростить работу с предобученными NLP моделями из DeepPavlov, в сентябрь 2019 года был запущен SaaS сервис.
1	Чтобы O
2	упростить O
3	работу O
4	с O
5	предобученными O
6	NLP O
7	моделями O
8	из O
9	DeepPavlov B-Model
10	, O
11	в O
12	сентябрь O
13	2019 B-Date_Application
14	года I-Date_Application
15	был O
16	запущен O
17	SaaS B-Technology
18	сервис I-Technology
19	. O

# sent_id = 166
# text =   DeepPavlov Cloud позволяет анализировать текст, а также хранить документы в облачном хранилище.
1	DeepPavlov B-Technology
2	Cloud I-Technology
3	позволяет O
4	анализировать B-Task
5	текст I-Task
6	, O
7	а O
8	также O
9	хранить B-Task
10	документы I-Task
11	в O
12	облачном O
13	хранилище O
14	. O

# sent_id = 167
# text =   Оценка состояния диалога (DST — Dialogue State Traking) является основным компонентом в таких диалоговых системах.
1	Оценка B-Task
2	состояния I-Task
3	диалога I-Task
4	( O
5	DST B-Task
6	— O
7	Dialogue B-Task
8	State I-Task
9	Traking I-Task
10	) O
11	является O
12	основным O
13	компонентом O
14	в O
15	таких O
16	диалоговых O
17	системах O
18	. O

# sent_id = 168
# text =   DST отвечает за перевод высказываний на человеческом языке в семантическое представление языка, в частности, за извлечение намерений (intets) и пар слот-значение (slot, value), соответствующих цели пользователя.
1	DST B-Task
2	отвечает O
3	за O
4	перевод B-Task
5	высказываний I-Task
6	на O
7	человеческом O
8	языке O
9	в O
10	семантическое O
11	представление O
12	языка O
13	, O
14	в O
15	частности O
16	, O
17	за O
18	извлечение O
19	намерений O
20	( O
21	intets O
22	) O
23	и O
24	пар O
25	слот O
26	- O
27	значение O
28	( O
29	slot O
30	, O
31	value O
32	) O
33	, O
34	соответствующих O
35	цели O
36	пользователя O
37	. O

# sent_id = 169
# text =   В ходе участия команды в DSTC8 была разработана модель GOLOMB (GOaL-Oriented Multi-task BERT-based dialogue state tracker) — целеориентированная мультизадачная модель на базе BERT для отслеживания состояния диалога.
1	В O
2	ходе O
3	участия O
4	команды O
5	в O
6	DSTC8 O
7	была O
8	разработана O
9	модель O
10	GOLOMB B-ShortName_Model
11	( O
12	GOaL B-Model
13	- I-Model
14	Oriented I-Model
15	Multi I-Model
16	- I-Model
17	task I-Model
18	BERT I-Model
19	- I-Model
20	based I-Model
21	dialogue I-Model
22	state I-Model
23	tracker I-Model
24	) O
25	— O
26	целеориентированная O
27	мультизадачная O
28	модель O
29	на O
30	базе O
31	BERT B-ShortName_Model
32	для O
33	отслеживания B-Task
34	состояния I-Task
35	диалога I-Task
36	. O

# sent_id = 170
# text =   Для предсказания состояния диалога модель решает несколько классификационных задач и задачу поиска подстроки.
1	Для O
2	предсказания O
3	состояния O
4	диалога O
5	модель O
6	решает O
7	несколько O
8	классификационных B-Task
9	задач I-Task
10	и O
11	задачу B-Task
12	поиска I-Task
13	подстроки I-Task
14	. O

# sent_id = 171
# text =   В скором времени данная модель появится библиотеке DeepPavlov.
1	В O
2	скором O
3	времени O
4	данная O
5	модель O
6	появится O
7	библиотеке O
8	DeepPavlov B-Model
9	. O

# sent_id = 172
# text =   Как было сказано ранее, DeepPavlov поставляется с несколькими предобученными компонентами, работающими на TensorFlow и Keras.
1	Как O
2	было O
3	сказано O
4	ранее O
5	, O
6	DeepPavlov B-Model
7	поставляется O
8	с O
9	несколькими O
10	предобученными O
11	компонентами O
12	, O
13	работающими O
14	на O
15	TensorFlow B-Application
16	и O
17	Keras B-Application
18	. O

# sent_id = 173
# text =   На основании триггера на определенные ключевые слова она сможет определять, к примеру, признаки обмана, мошенничества.
1	На O
2	основании O
3	триггера O
4	на O
5	определенные O
6	ключевые B-Subject
7	слова I-Subject
8	она O
9	сможет O
10	определять O
11	, O
12	к O
13	примеру O
14	, O
15	признаки O
16	обмана O
17	, O
18	мошенничества O
19	. O

# sent_id = 174
# text =   То есть, сформировав некоторый корпус слов-триггеров, вполне возможно классифицировать сайты по их текстовому содержанию.
1	То O
2	есть O
3	, O
4	сформировав O
5	некоторый O
6	корпус B-InfoResource
7	слов I-InfoResource
8	- I-InfoResource
9	триггеров I-InfoResource
10	, O
11	вполне O
12	возможно O
13	классифицировать O
14	сайты O
15	по O
16	их O
17	текстовому O
18	содержанию O
19	. O

# sent_id = 175
# text =   Задача распознавания текста относится к сфере обработки естественного языка или NLP (natural language processing).
1	Задача B-Task
2	распознавания I-Task
3	текста I-Task
4	относится O
5	к O
6	сфере O
7	обработки B-Science
8	естественного I-Science
9	языка I-Science
10	или O
11	NLP B-ShortName_Science
12	( O
13	natural B-Science
14	language I-Science
15	processing I-Science
16	) O
17	. O

# sent_id = 176
# text =   NLP — направление искусственного интеллекта, нацеленное на обработку и анализ данных на естественном языке и обучение машин взаимодействию с людьми [1].
1	NLP B-Science
2	— O
3	направление O
4	искусственного B-Science
5	интеллекта I-Science
6	, O
7	нацеленное O
8	на O
9	обработку O
10	и O
11	анализ B-Method
12	данных I-Method
13	на O
14	естественном O
15	языке O
16	и O
17	обучение O
18	машин O
19	взаимодействию O
20	с O
21	людьми O
22	[ O
23	1 O
24	] O
25	. O

# sent_id = 177
# text =   Такой подход называется методом вложения слов (word embedding).
1	Такой O
2	подход O
3	называется O
4	методом B-Method
5	вложения I-Method
6	слов I-Method
7	( O
8	word B-Subject
9	embedding I-Subject
10	) O
11	. O

# sent_id = 178
# text =   Используя данные, состоящие из таких векторов, мы можем применять различные методы Machine Learning.
1	Используя O
2	данные O
3	, O
4	состоящие O
5	из O
6	таких O
7	векторов O
8	, O
9	мы O
10	можем O
11	применять O
12	различные O
13	методы O
14	Machine B-Science
15	Learning I-Science
16	. O

# sent_id = 179
# text =   И поскольку искусственные нейронные сети лучшим образом справляются с векторно-матричными вычислениями, то выбор в их пользу становиться очевидным.
1	И O
2	поскольку O
3	искусственные B-Method
4	нейронные I-Method
5	сети I-Method
6	лучшим O
7	образом O
8	справляются O
9	с O
10	векторно B-Method
11	- I-Method
12	матричными I-Method
13	вычислениями I-Method
14	, O
15	то O
16	выбор O
17	в O
18	их O
19	пользу O
20	становиться O
21	очевидным O
22	. O

# sent_id = 180
# text =   Искусственная нейронная сеть — это математическая модель, а также ее программное или аппаратное воплощение, построенные по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.
1	Искусственная B-Method
2	нейронная I-Method
3	сеть I-Method
4	— O
5	это O
6	математическая B-Model
7	модель I-Model
8	, O
9	а O
10	также O
11	ее O
12	программное O
13	или O
14	аппаратное O
15	воплощение O
16	, O
17	построенные O
18	по O
19	принципу O
20	организации O
21	и O
22	функционирования O
23	биологических O
24	нейронных O
25	сетей O
26	— O
27	сетей O
28	нервных O
29	клеток O
30	живого O
31	организма O
32	. O

# sent_id = 181
# text =   Современные модели представляют собой так называемые глубокие модели.
1	Современные O
2	модели O
3	представляют O
4	собой O
5	так O
6	называемые O
7	глубокие B-Model
8	модели I-Model
9	. O

# sent_id = 182
# text =   И в ее решении наилучшие метрики точности были достигнуты рекуррентными нейронными сетями, LSTM (сети с долгой краткосрочной памятью).
1	И O
2	в O
3	ее O
4	решении O
5	наилучшие O
6	метрики O
7	точности B-Metric
8	были O
9	достигнуты O
10	рекуррентными B-Model
11	нейронными I-Model
12	сетями I-Model
13	, O
14	LSTM B-ShortName_Model
15	( O
16	сети B-Model
17	с I-Model
18	долгой I-Model
19	краткосрочной I-Model
20	памятью I-Model
21	) O
22	. O

# sent_id = 183
# text =   Позже свое превосходство в этой нише обрели NLP-модели-трансформеры.
1	Позже O
2	свое O
3	превосходство O
4	в O
5	этой O
6	нише O
7	обрели O
8	NLP B-Model
9	- I-Model
10	модели B-Model
11	- I-Model
12	трансформеры I-Model
13	. O

# sent_id = 184
# text =  Описание упомянутых рекуррентных нейросетей (RNN), LSTM и GRU выходит за рамки темы статьи.
1	Описание O
2	упомянутых O
3	рекуррентных B-Model
4	нейросетей I-Model
5	( O
6	RNN B-ShortName_Model
7	) O
8	, O
9	LSTM B-ShortName_Model
10	и O
11	GRU B-ShortName_Model
12	выходит O
13	за O
14	рамки O
15	темы O
16	статьи O
17	. O

# sent_id = 185
# text =   Однако RNN способны фиксировать зависимости только в одном направлении языка.
1	Однако O
2	RNN B-ShortName_Model
3	способны O
4	фиксировать B-Task
5	зависимости I-Task
6	только O
7	в O
8	одном O
9	направлении O
10	языка O
11	. O

# sent_id = 186
# text =   Кроме этого, RNN не очень хороши в захвате долгосрочных зависимостей.
1	Кроме O
2	этого O
3	, O
4	RNN B-ShortName_Model
5	не O
6	очень O
7	хороши O
8	в O
9	захвате B-Task
10	долгосрочных I-Task
11	зависимостей I-Task
12	. O

# sent_id = 187
# text =  LSTM избегают проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.
1	LSTM B-ShortName_Model
2	избегают O
3	проблемы O
4	долговременной O
5	зависимости O
6	, O
7	запоминая O
8	значения O
9	как O
10	на O
11	короткие O
12	, O
13	так O
14	и O
15	на O
16	длинные O
17	промежутки O
18	времени O
19	. O

# sent_id = 188
# text =   Это объясняется тем, что LSTM не использует функцию активации внутри своих рекуррентных компонентов.
1	Это O
2	объясняется O
3	тем O
4	, O
5	что O
6	LSTM B-ShortName_Model
7	не O
8	использует O
9	функцию B-Method
10	активации I-Method
11	внутри O
12	своих O
13	рекуррентных O
14	компонентов O
15	. O

# sent_id = 189
# text =   LSTM часто используются в машинном переводе и в задачах генерирования текстов на естественном языке.
1	LSTM B-ShortName_Model
2	часто O
3	используются O
4	в O
5	машинном B-Science
6	переводе I-Science
7	и O
8	в O
9	задачах O
10	генерирования B-Task
11	текстов I-Task
12	на O
13	естественном O
14	языке O
15	. O

# sent_id = 190
# text =   Прежде чем использовать такой мощный и в то же время сложный инструмент, наша команда протестировала и более простые NLP-методы машинного обучения, в том числе «наивный байесовский классификатор», алгоритмы, использующие bag-of-words («мешок слов» — метод представления слов) и tf-idf (метрика определения частоты вхождения слов), а также простейшие модели нейронных сетей, состоящие из небольшого количества скрытых слоев.
1	Прежде O
2	чем O
3	использовать O
4	такой O
5	мощный O
6	и O
7	в O
8	то O
9	же O
10	время O
11	сложный O
12	инструмент O
13	, O
14	наша O
15	команда O
16	протестировала O
17	и O
18	более O
19	простые O
20	NLP O
21	- O
22	методы O
23	машинного O
24	обучения O
25	, O
26	в O
27	том O
28	числе O
29	« O
30	наивный B-Method
31	байесовский I-Method
32	классификатор I-Method
33	» O
34	, O
35	алгоритмы O
36	, O
37	использующие O
38	bag B-Method
39	- I-Method
40	of I-Method
41	- I-Method
42	words I-Method
43	( O
44	« O
45	мешок B-Method
46	слов I-Method
47	» O
48	— O
49	метод B-Method
50	представления I-Method
51	слов I-Method
52	) O
53	и O
54	tf B-Metric
55	- I-Metric
56	idf B-Metric
57	( O
58	метрика B-TERM
59	определения B-Subject
60	частоты I-TERM
61	вхождения I-TERM
62	слов O
63	) O
64	, O
65	а O
66	также O
67	простейшие O
68	модели O
69	нейронных O
70	сетей O
71	, O
72	состоящие O
73	из O
74	небольшого O
75	количества O
76	скрытых O
77	слоев O
78	. O

# sent_id = 191
# text =   BERT, или Bidirectional Encoder Representations from Transformers, — нейросетевая модель-трансформер от Google, на которой сегодня строится большинство инструментов автоматической обработки языка.
1	BERT B-ShortName_Model
2	, O
3	или O
4	Bidirectional B-TEModelRM
5	Encoder I-Model
6	Representations I-Model
7	from I-Model
8	Transformers I-Model
9	, O
10	— O
11	нейросетевая O
12	модель O
13	- O
14	трансформер O
15	от O
16	Google B-Organization
17	, O
18	на O
19	которой O
20	сегодня O
21	строится O
22	большинство O
23	инструментов O
24	автоматической O
25	обработки O
26	языка O
27	. O

# sent_id = 192
# text =  Релиз BERT в 2018 году стал некоторой переломной точкой в развитии NLP-моделей.
1	Релиз O
2	BERT B-ShortName_Model
3	в O
4	2018 B-Date
5	году I-Date
6	стал O
7	некоторой O
8	переломной O
9	точкой O
10	в O
11	развитии O
12	NLP B-Model
13	- I-Model
14	моделей I-Model
15	. O

# sent_id = 193
# text =   Его появлению предшествовал ряд недавних разработок в области обработки естественного языка (BERT, ELMO и Ко в картинках — как в NLP пришло трансферное обучение): Semi-supervised Sequence learning (Andrew Dai и Quoc Le), ELMo (Matthew Peters и исследователи из AI2 и UW CSE), ULMFiT (Jeremy Howard и Sebastian Ruder), OpenAI Transformer (исследователи OpenAI Radford, Narasimhan, Salimans, и Sutskever) и Трансформер (Vaswani et al).
1	Его O
2	появлению O
3	предшествовал O
4	ряд O
5	недавних O
6	разработок O
7	в O
8	области O
9	обработки O
10	естественного O
11	языка O
12	( O
13	BERT B-ShortName_Model
14	, O
15	ELMO B-ShortName_Model
16	и O
17	Ко B-ShortName_Model
18	в O
19	картинках O
20	— O
21	как O
22	в O
23	NLP B-ShortName_Science
24	пришло O
25	трансферное B-Method
26	обучение I-Method
27	): O
28	Semi B-Method
29	- I-Method
30	supervised I-Method
31	Sequence I-Method
32	learning I-Method
33	( O
34	Andrew B-Person
35	Dai I-Person
36	и O
37	Quoc B-Person
38	Le I-Person
39	) O
40	, O
41	ELMo B-Model
42	( O
43	Matthew B-Person
44	Peters I-Person
45	и O
46	исследователи O
47	из O
48	AI2 B-Organization
49	и O
50	UW B-Organization
51	CSE B-Organization
52	) O
53	, O
54	ULMFiT B-Model
55	( O
56	Jeremy B-Person
57	Howard I-Person
58	и O
59	Sebastian B-Person
60	Ruder I-Person
61	) O
62	, O
63	OpenAI B-TERM
64	Transformer I-TERM
65	( O
66	исследователи O
67	OpenAI B-Organization
68	Radford I-Organization
69	, O
70	Narasimhan B-Person
71	, O
72	Salimans B-Person
73	, O
74	и O
75	Sutskever B-Person
76	) O
77	и O
78	Трансформер B-Model
79	( O
80	Vaswani B-Person
81	et O
82	al O
83	)O 
84	. O

# sent_id = 194
# text =  Трансформеры в машинном обучении — это семейство архитектур нейронных сетей, общая идея которых основана на так называемом «самовнимании» (self-attention).
1	Трансформеры B-Model
2	в O
3	машинном B-Science
4	обучении I-Science
5	— O
6	это O
7	семейство O
8	архитектур O
9	нейронных O
10	сетей O
11	, O
12	общая O
13	идея O
14	которых O
15	основана O
16	на O
17	так O
18	называемом O
19	« O
20	самовнимании B-Method
21	» O
22	( O
23	self B-Method
24	- I-Method
25	attention I-Method
26	) O
27	. O

# sent_id = 195
# text =   Однако алгоритм Self-attention не сразу поймет смысл предложения.
1	Однако O
2	алгоритм O
3	Self B-Method
4	- I-Method
5	attention I-Method
6	не O
7	сразу O
8	поймет O
9	смысл O
10	предложения O
11	. O

# sent_id = 196
# text =   Потом результаты сетей объединяется.По своей сути BERT — это обученный стек энкодеров Трансформера.
1	Потом O
2	результаты O
3	сетей O
4	объединяется O
5	. O
6	По O
7	своей O
8	сути O
9	BERT B-ShortName_Model
10	— O
11	это O
12	обученный O
13	стек O
14	энкодеров B-Method
15	Трансформера I-Method
16	. O

# sent_id = 197
# text =   Разработкой и обучением модели BERT занималась целая группа исследователей Google AI Language на многомиллионном наборе слов на разных языках (более 100).
1	Разработкой O
2	и O
3	обучением O
4	модели O
5	BERT B-ShortName_Model
6	занималась O
7	целая O
8	группа O
9	исследователей O
10	Google B-Organization
11	AI I-Organization
12	Language I-Organization
13	на O
14	многомиллионном O
15	наборе O
16	слов O
17	на O
18	разных O
19	языках O
20	( O
21	более O
22	100 O
23	) O
24	. O

# sent_id = 198
# text =   И мы дообучили BERT распознавать текстовое содержимое сайтов по 63 категориям (медицина, здоровье, видео, интернет-магазины, юмористические сайты, эротика, оружие, секты, криминал и пр.).
1	И O
2	мы O
3	дообучили O
4	BERT B-ShortName_Model
5	распознавать O
6	текстовое O
7	содержимое O
8	сайтов O
9	по O
10	63 O
11	категориям O
12	( O
13	медицина B-Science
14	, O
15	здоровье O
16	, O
17	видео O
18	, O
19	интернет O
20	- O
21	магазины O
22	, O
23	юмористические O
24	сайты O
25	, O
26	эротика O
27	, O
28	оружие O
29	, O
30	секты O
31	, O
32	криминал O
33	) O
34	. O

# sent_id = 199
# text =   Smart-Cat на первом этапе проводит их предобработку.
1	Smart B-App_system
2	- I-App_system
3	Cat I-App_system
4	на O
5	первом O
6	этапе O
7	проводит O
8	их O
9	предобработку O
10	. O

# sent_id = 200
# text =   Для удобства работы с категоризатором Smart-Cat мы создали специальный Telegram-бот.
1	Для O
2	удобства O
3	работы O
4	с O
5	категоризатором O
6	Smart B-App_system
7	- I-App_system
8	Cat I-App_system
9	мы O
10	создали O
11	специальный O
12	Telegram B-Technology
13	- I-Technology
14	бот I-Technology
15	. O

# sent_id = 201
# text =   После своей работы BERT-bot отправит CSV-таблицу.
1	После O
2	своей O
3	работы O
4	BERT B-Technology
5	- I-Technology
6	bot I-Technology
7	отправит O
8	CSV O
9	- O
10	таблицу O
11	. O

# sent_id = 202
# text =   Но, как я уже сказал ранее, такие модели могут применяться не только в задачах классификации текста.
1	Но O
2	, O
3	как O
4	я O
5	уже O
6	сказал O
7	ранее O
8	, O
9	такие O
10	модели O
11	могут O
12	применяться O
13	не O
14	только O
15	в O
16	задачах O
17	классификации B-Task
18	текста I-Task
19	. O

# sent_id = 203
# text =   В современном мире всё большую популярность приобретает методика под названием customer development для тестирования идей и гипотез о будущем продукте.
1	В O
2	современном O
3	мире O
4	всё O
5	большую O
6	популярность O
7	приобретает O
8	методика O
9	под O
10	названием O
11	customer B-Method
12	development I-Method
13	для O
14	тестирования O
15	идей O
16	и O
17	гипотез O
18	о O
19	будущем O
20	продукте O
21	. O

# sent_id = 204
# text =   В данном решении была использована готовая нейросеть от сервиса RusVectores, обученная на корпусе НКРЯ с использованием алгоритма word2vec CBOW с длиной вектора 300.
1	В O
2	данном O
3	решении O
4	была O
5	использована O
6	готовая O
7	нейросеть O
8	от O
9	сервиса O
10	RusVectores B-Technology
11	, O
12	обученная O
13	на O
14	корпусе O
15	НКРЯ B-ShortName
16	с O
17	использованием O
18	алгоритма O
19	word2vec B-Model
20	CBOW B-Method
21	с O
22	длиной O
23	вектора O
24	300.

# sent_id = 205
# text =  НКРЯ – это совокупность русскоязычных текстов, Национальный Корпус Русского Языка в полном объёме.
1	НКРЯ B-Abbrev_InfoResource
2	– O
3	это O
4	совокупность O
5	русскоязычных O
6	текстов O
7	, O
8	Национальный B-Result
9	Корпус I-Result
10	Русского I-Result
11	Языка I-Result
12	в O
13	полном O
14	объёме O
15	. O

# sent_id = 206
# text =  Word2vec CBOW — алгоритм, благодаря которому слово на естественном языке представляется в виде числового вектора.
1	word2vec B-Model
2	CBOW B-Method
3	— O
4	алгоритм O
5	, O
6	благодаря O
7	которому O
8	слово O
9	на O
10	естественном O
11	языке O
12	представляется O
13	в O
14	виде O
15	числового O
16	вектора O
17	. O

# sent_id = 207
# text =   CBOW – это аббревиатура Continuous Bag of Words.
1	CBOW B-ShortName_Method
2	– O
3	это O
4	аббревиатура O
5	Continuous B-Method
6	Bag I-Method
7	of I-Method
8	Words I-Method
9	. O

# sent_id = 208
# text =  Она обозначает алгоритм, который есть в word2vec.
1	Она O
2	обозначает O
3	алгоритм O
4	, O
5	который O
6	есть O
7	в O
8	word2vec B-Model
9	. O

# sent_id = 209
# text =   Данный алгоритм называют моделью «мешка слов», он предсказывает слово по контексту.
1	Данный O
2	алгоритм O
3	называют O
4	моделью O
5	« O
6	мешка B-Method
7	слов I-Method
8	» O
9	, O
10	он O
11	предсказывает B-Task
12	слово I-Task
13	по I-Task
14	контексту I-Task
15	. O

# sent_id = 210
# text =   Ещё один алгоритм в word2vec - Skip-gram предсказывает контекст по слову.
1	Ещё O
2	один O
3	алгоритм O
4	в O
5	word2vec B-Model
6	- O
7	Skip B-Method
8	- I-Method
9	gram I-Method
10	предсказывает O
11	контекст O
12	по O
13	слову O
14	. O

# sent_id = 211
# text =  С помощью данных алгоритмов генерируют близкие по смыслу слова при запросе в поисковой системе, сравнивают документы по смыслу, определяют смысловую близость слов и предложений.
1	С O
2	помощью O
3	данных O
4	алгоритмов O
5	генерируют O
6	близкие O
7	по O
8	смыслу O
9	слова O
10	при O
11	запросе O
12	в O
13	поисковой O
14	системе O
15	, O
16	сравнивают B-Task
17	документы I-Task
18	по I-Task
19	смыслу I-Task
20	, O
21	определяют B-Task
22	смысловую I-Task
23	близость I-Task
24	слов I-Task
25	и O
26	предложений O
27	. O

# sent_id = 212
# text =  Более подробно о word2vec можно почитать в статье "Немного про word2vec: полезная теория".
1	Более O
2	подробно O
3	о O
4	word2vec B-Application
5	можно O
6	почитать O
7	в O
8	статье O
9	" O
10	Немного O
11	про O
12	word2vec O
13	: O
14	полезная O
15	теория" O
16	. O

# sent_id = 213
# text =  О векторном представлении слов (эмбеддинге) хорошо и с примерами описано в статье "Что такое эмбеддинги и как они помогают машинам понимать тексты".
1	О O
2	векторном B-Object
3	представлении I-Object
4	слов I-Object
5	( O
6	эмбеддинге B-Object
7	) O
8	хорошо O
9	и O
10	с O
11	примерами O
12	описано O
13	в O
14	статье O
15	" O
16	Что O
17	такое O
18	эмбеддинги O
19	и O
20	как O
21	они O
22	помогают O
23	машинам O
24	понимать O
25	тексты" O
26	. O

# sent_id = 214
# text =   Т.к. у меня таких мощностей нет, я воспользовался доступным онлайн сервисом RusVectores.
1	Т.к O
2	. O
3	у O
4	меня O
5	таких O
6	мощностей O
7	нет O
8	, O
9	я O
10	воспользовался O
11	доступным O
12	онлайн O
13	сервисом O
14	RusVectores B-Model
15	. O

# sent_id = 215
# text =   Эти модели всегда ищут синонимы — даже для устоявшихся словосочетаний.
1	Эти O
2	модели O
3	всегда O
4	ищут B-Task
5	синонимы I-Task
6	— O
7	даже O
8	для O
9	устоявшихся O
10	словосочетаний B-Subject
11	. O

# sent_id = 216
# text =   Перед переходом к самим метрикам необходимо ввести важную концепцию для описания этих метрик в терминах ошибок классификации — confusion matrix (матрица ошибок).
1	Перед O
2	переходом O
3	к O
4	самим O
5	метрикам O
6	необходимо O
7	ввести O
8	важную O
9	концепцию O
10	для O
11	описания O
12	этих O
13	метрик O
14	в O
15	терминах B-Subject
16	ошибок O
17	классификации O
18	— O
19	confusion B-TERM
20	matrix I-TERM
21	( O
22	матрица B-TERM
23	ошибок I-TERM
24	) O
25	. O

# sent_id = 217
# text =   Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма.
1	Интуитивно O
2	понятной O
3	, O
4	очевидной O
5	и O
6	почти O
7	неиспользуемой O
8	метрикой O
9	является O
10	accuracy B-TERM
11	— O
12	доля O
13	правильных O
14	ответов O
15	алгоритма O
16	. O

# sent_id = 218
# text =   Для оценки качества работы алгоритма на каждом из классов по отдельности введем метрики precision (точность) и recall (полнота).
1	Для O
2	оценки O
3	качества O
4	работы O
5	алгоритма O
6	на O
7	каждом O
8	из O
9	классов B-Object
10	по O
11	отдельности O
12	введем O
13	метрики O
14	precision B-Metric
15	( O
16	точность B-Metric
17	) O
18	и O
19	recall B-Metric
20	( O
21	полнота B-Metric
22	) O
23	. O

# sent_id = 219
# text =   Precision можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными, а recall показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.
1	Precision B-Metric
2	можно O
3	интерпретировать O
4	как O
5	долю O
6	объектов O
7	, O
8	названных O
9	классификатором O
10	положительными O
11	и O
12	при O
13	этом O
14	действительно O
15	являющимися O
16	положительными O
17	, O
18	а O
19	recall B-Metric
20	показывает O
21	, O
22	какую O
23	долю O
24	объектов O
25	положительного O
26	класса O
27	из O
28	всех O
29	объектов O
30	положительного O
31	класса O
32	нашел O
33	алгоритм O
34	. O

# sent_id = 220
# text =   F-мера достигает максимума при полноте и точности, равными единице, и близка к нулю, если один из аргументов близок к нулю.
1	F B-Metric
2	- I-Metric
3	мера I-Metric
4	достигает O
5	максимума O
6	при O
7	полноте B-Metric
8	и O
9	точности B-Metric
10	, O
11	равными O
12	единице O
13	, O
14	и O
15	близка O
16	к O
17	нулю O
18	, O
19	если O
20	один O
21	из O
22	аргументов B-Object
23	близок O
24	к O
25	нулю O
26	. O

# sent_id = 221
# text =   Одним из способов оценить модель в целом, не привязываясь к конкретному порогу, является AUC-ROC (или ROC AUC) — площадь (Area Under Curve) под кривой ошибок (Receiver Operating Characteristic curve ).
1	Одним O
2	из O
3	способов O
4	оценить O
5	модель B-Object
6	в O
7	целом O
8	, O
9	не O
10	привязываясь O
11	к O
12	конкретному O
13	порогу O
14	, O
15	является O
16	AUC B-TERM
17	- I-TERM
18	ROC I-TERM
19	( O
20	или O
21	ROC B-TERM
22	AUC I-TERM
23	) O
24	— O
25	площадь B-TERM
26	( O
27	Area B-TERM
28	Under I-TERM
29	Curve I-TERM
30	) O
31	под O
32	кривой O
33	ошибок O
34	( O
35	Receiver B-TERM
36	Operating I-TERM
37	Characteristic I-TERM
38	curve I-TERM
39	) O
40	. O

# sent_id = 222
# text =   Интуитивно можно представить минимизацию logloss как задачу максимизации accuracy путем штрафа за неверные предсказания.
1	Интуитивно O
2	можно O
3	представить O
4	минимизацию O
5	logloss B-TERM
6	как O
7	задачу B-TERM
8	максимизации I-TERM
9	accuracy I-TERM
10	путем O
11	штрафа O
12	за O
13	неверные O
14	предсказания O
15	. O

# sent_id = 223
# text =   Вчера OpenAI выпустили Whisper.
1	Вчера O
2	OpenAI B-Organization
3	выпустили O
4	Whisper B-Application
5	. O

# sent_id = 224
# text =   По сути авторы попытались: Исключить транскрипты других систем ASR из датасета; Привести пунктуации к некому стандарту.
1	По O
2	сути O
3	авторы O
4	попытались O
5	: O
6	Исключить O
7	транскрипты O
8	других O
9	систем O
10	ASR B-Abberv_Application
11	из O
12	датасета; O
13	Привести O
14	пунктуации O
15	к O
16	некому O
17	стандарту O
18	. O

# sent_id = 225
# text =   Серьезной нормализации или денормализации текста не делалось.
1	Серьезной O
2	нормализации B-Method
3	или O
4	денормализации B-Method
5	текста O
6	не O
7	делалось O
8	. O

# sent_id = 226
# text =   Под капотом же seq2seq модель, глядишь сама всё и так выучит.
1	Под O
2	капотом O
3	же O
4	seq2seq B-Model
5	модель O
6	, O
7	глядишь O
8	сама O
9	всё O
10	и O
11	так O
12	выучит O
13	. O

# sent_id = 227
# text =   Ведь исходя из названий FAIR, OpenAI и прочие же FOSS - альтруисты, борющиеся за наше будущее, они же выложили код для тренировки (а повторить смогут лишь GAFA компании) и все датасеты, не так ли?
1	Ведь O
2	исходя O
3	из O
4	названий O
5	FAIR B-Organization
6	, O
7	OpenAI B-Organization
8	и O
9	прочие O
10	же O
11	FOSS B-Organization
12	- O
13	альтруисты O
14	, O
15	борющиеся O
16	за O
17	наше O
18	будущее O
19	, O
20	они O
21	же O
22	выложили O
23	код O
24	для O
25	тренировки O
26	( O
27	а O
28	повторить O
29	смогут O
30	лишь O
31	GAFA B-Organization
32	компании O
33	) O
34	и O
35	все O
36	датасеты O
37	, O
38	не O
39	так O
40	ли O
41	? O

# sent_id = 228
# text =   На практике OpenAI уже давно не Open, а недавняя история с DALLE-2 / Midjourney / Stable Diffusion скорее иллюстрируют тренд.
1	На O
2	практике O
3	OpenAI B-Organization
4	уже O
5	давно O
6	не O
7	Open O
8	, O
9	а O
10	недавняя O
11	история O
12	с O
13	DALLE-2 B-Model
14	/ O
15	Midjourney B-Model
16	/ O
17	Stable B-Model
18	Diffusion I-Model
19	скорее O
20	иллюстрируют O
21	тренд O
22	. O

# sent_id = 229
# text =   Наверное стоит только сказать, что это sequence-to-sequence encoder-decoder трансформерная модель, без особого снижения длины инпута с довольном стандартным окном в 25 миллисекунд и шагом в 10 миллисекунд, работающая на аудио в 16 килогерц.
1	Наверное O
2	стоит O
3	только O
4	сказать O
5	, O
6	что O
7	это O
8	sequence B-Model
9	- I-Model
10	to I-Model
11	- I-Model
12	sequence I-Model
13	encoder I-Model
14	- I-Model
15	decoder I-Model
16	трансформерная O
17	модель O
18	, O
19	без O
20	особого O
21	снижения O
22	длины O
23	инпута O
24	с O
25	довольном O
26	стандартным O
27	окном O
28	в O
29	25 O
30	миллисекунд O
31	и O
32	шагом O
33	в O
34	10 O
35	миллисекунд O
36	, O
37	работающая O
38	на O
39	аудио O
40	в O
41	16 O
42	килогерц O
43	. O

# sent_id = 230
# text =   Решать конечно вам для вашего конкретного приложения, но если сравнивать только саму модель распознавания, а не весь обвес в виде сервиса (понятно, что тут VAD и детектор языка запихали тоже в модель), например с древними бенчмарками из silero-models, то самые маленькие модели на CPU в расчете на 1 ядро (1 ядро = 2 потока) отличаются по скорости … на два порядка.
1	Решать O
2	конечно O
3	вам O
4	для O
5	вашего O
6	конкретного O
7	приложения O
8	, O
9	но O
10	если O
11	сравнивать O
12	только O
13	саму O
14	модель O
15	распознавания O
16	, O
17	а O
18	не O
19	весь O
20	обвес O
21	в O
22	виде O
23	сервиса O
24	( O
25	понятно O
26	, O
27	что O
28	тут O
29	VAD B-ShortName_Method
30	и O
31	детектор O
32	языка O
33	запихали O
34	тоже O
35	в O
36	модель O
37	) O
38	, O
39	например O
40	с O
41	древними O
42	бенчмарками O
43	из O
44	silero B-Method
45	- I-Method
46	models I-Method
47	, O
48	то O
49	самые O
50	маленькие O
51	модели O
52	на O
53	CPU O
54	в O
55	расчете O
56	на O
57	1 O
58	ядро O
59	( O
60	1 O
61	ядро O
62	= O
63	2 O
64	потока O
65	) O
66	отличаются O
67	по O
68	скорости O
69	… O
70	на O
71	два O
72	порядка O
73	. O

# sent_id = 231
# text =   Для наших моделей из прошлого релиза, многие из этих датасетов тоже как бы "zero-shot" (то есть у нас нет соответствующего большого тренировочного датасета).
1	Для O
2	наших O
3	моделей O
4	из O
5	прошлого O
6	релиза O
7	, O
8	многие O
9	из O
10	этих O
11	датасетов O
12	тоже O
13	как O
14	бы O
15	" O
16	zero B-Method
17	- I-Method
18	shot I-Method
19	" O
20	( O
21	то O
22	есть O
23	у O
24	нас O
25	нет O
26	соответствующего O
27	большого O
28	тренировочного O
29	датасета O
30	) O
31	. O

# sent_id = 232
# text =   В течение четырех лет вышло несколько версий модели, способных транскрибировать лекции, телефонные разговоры, телевизионные программы, радиошоу и другие прямые трансляции с «человеческой точностью».
1	В O
2	течение O
3	четырех O
4	лет O
5	вышло O
6	несколько O
7	версий O
8	модели O
9	, O
10	способных O
11	транскрибировать B-Task
12	лекции I-Task
13	, O
14	телефонные O
15	разговоры O
16	, O
17	телевизионные O
18	программы O
19	, O
20	радиошоу O
21	и O
22	другие O
23	прямые O
24	трансляции O
25	с O
26	« O
27	человеческой O
28	точностью O
29	» O
30	. O

# sent_id = 233
# text =   Модель DeepSpeech представляет собой сквозную обучаемую архитектуру на уровне символов, которая может транскрибировать аудио на различных языках.
1	Модель O
2	DeepSpeech B-Model
3	представляет O
4	собой O
5	сквозную O
6	обучаемую O
7	архитектуру O
8	на O
9	уровне O
10	символов O
11	, O
12	которая O
13	может O
14	транскрибировать B-Task
15	аудио I-Task
16	на O
17	различных O
18	языках O
19	. O

# sent_id = 234
# text =   Вдохновленная этими усилиями по сбору данных, исследовательская группа Mozilla в сотрудничестве с группой открытых инноваций запустила проект Common Voice, цель которого заключалась в сборе и проверке речевых данных.
1	Вдохновленная O
2	этими O
3	усилиями O
4	по O
5	сбору O
6	данных O
7	, O
8	исследовательская O
9	группа O
10	Mozilla B-Organization
11	в O
12	сотрудничестве O
13	с O
14	группой O
15	открытых O
16	инноваций O
17	запустила O
18	проект O
19	Common B-Project
20	Voice I-Project
21	, O
22	цель B-Object
23	которого O
24	заключалась O
25	в O
26	сборе O
27	и O
28	проверке B-Task
29	речевых I-Task
30	данных I-Task
31	. O

# sent_id = 235
# text =   Common Voice включает не только речевые записи, но и из добровольно предоставленные метаданные, такие как возраст, пол и акцент говорящего.
1	Common B-Project
2	Voice I-Project
3	включает O
4	не O
5	только O
6	речевые B-Object
7	записи I-Object
8	, O
9	но O
10	и O
11	из O
12	добровольно O
13	предоставленные O
14	метаданные B-Object
15	, O
16	такие O
17	как O
18	возраст O
19	, O
20	пол O
21	и O
22	акцент B-Subject
23	говорящего O
24	. O

# sent_id = 236
# text =   Сегодня Common Voice является одним из крупнейших в мире мультиязычных корпусов, являющихся общественным достоянием, с более чем 9 тысячами часов голосовых данных на 60 различных языках, включая такие редкие языки, как валлийский и киньяруанда.
1	Сегодня O
2	Common B-Project
3	Voice I-Project
4	является O
5	одним O
6	из O
7	крупнейших O
8	в O
9	мире O
10	мультиязычных B-Object
11	корпусов I-Object
12	, O
13	являющихся O
14	общественным O
15	достоянием O
16	, O
17	с O
18	более O
19	чем O
20	9 O
21	тысячами O
22	часов O
23	голосовых O
24	данных O
25	на O
26	60 O
27	различных O
28	языках O
29	, O
30	включая O
31	такие O
32	редкие O
33	языки O
34	, O
35	как O
36	валлийский B-Lang
37	и O
38	киньяруанда B-Lang
39	. O

# sent_id = 237
# text =  В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3.
1	В O
2	этой O
3	статье O
4	мы O
5	научим O
6	вас O
7	генерировать B-Task
8	текст I-Task
9	с O
10	помощью O
11	предварительно O
12	обученного O
13	GPT-2 B-Model
14	— O
15	более O
16	легкого O
17	предшественника O
18	GPT-3 B-Model
19	. O

# sent_id = 238
# text =   Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface.
1	Мы O
2	будем O
3	использовать O
4	именитую O
5	библиотеку O
6	Transformers B-Library
7	, O
8	разработанную O
9	Huggingface B-Organization
10	. O

# sent_id = 239
# text =   Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.
1	Модель B-Object
2	по O
3	умолчанию O
4	для O
5	конвейера O
6	генерации B-Task
7	текста I-Task
8	— O
9	GPT-2 B-Model
10	, O
11	самая O
12	популярная O
13	модель O
14	декодирующего O
15	трансформера B-Model
16	для O
17	генерации B-Task
18	языка I-Task
19	. O

# sent_id = 240
# text =   Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.
1	Эта O
2	модель O
3	GPT2 B-Model
4	от O
5	CKIPLab B-Organization
6	предварительно O
7	обучена O
8	на O
9	китайском B-Lang
10	корпусе B-Object
11	, O
12	поэтому O
13	мы O
14	можем O
15	использовать O
16	их O
17	модель O
18	без O
19	необходимости O
20	заниматься O
21	настройкой O
22	самостоятельно O
23	. O

# sent_id = 241
# text =   Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком.
1	Однажды O
2	нам O
3	понадобилось O
4	выбрать O
5	синтаксический B-Application
6	парсер I-Application
7	для O
8	работы O
9	с O
10	русским O
11	языком O
12	. O

# sent_id = 242
# text =   Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение.
1	Для O
2	этого O
3	мы O
4	углубились O
5	в O
6	дебри O
7	морфологии B-Science
8	и O
9	токенизации B-Method
10	, O
11	протестировали O
12	разные O
13	варианты O
14	и O
15	оценили O
16	их O
17	применение O
18	. O

# sent_id = 243
# text =   В первой строке предложение разобрано в рамках грамматики зависимостей.
1	В O
2	первой O
3	строке O
4	предложение O
5	разобрано O
6	в O
7	рамках O
8	грамматики B-Method
9	зависимостей I-Method
10	. O

# sent_id = 244
# text =   Во второй строке разбор идет в соответствии с грамматикой непосредственно составляющих.
1	Во O
2	второй O
3	строке O
4	разбор O
5	идет O
6	в O
7	соответствии O
8	с O
9	грамматикой B-Method
10	непосредственно I-Method
11	составляющих I-Method
12	. O

# sent_id = 245
# text =   Поэтому в автоматическом парсинге русского языка принято работать исходя из грамматики зависимостей.
1	Поэтому O
2	в O
3	автоматическом B-Task
4	парсинге I-Task
5	русского B-Lang
6	языка O
7	принято O
8	работать O
9	исходя O
10	из O
11	грамматики B-Method
12	зависимостей I-Method
13	. O

# sent_id = 246
# text =   Чтобы облегчить себе выбор парсера, мы обратили свой взгляд на проект Universal Dependencies и недавно прошедшее в его рамках соревнование CoNLL Shared Task.
1	Чтобы O
2	облегчить O
3	себе O
4	выбор O
5	парсера O
6	, O
7	мы O
8	обратили O
9	свой O
10	взгляд O
11	на O
12	проект O
13	Universal B-Project
14	Dependencies I-Project
15	и O
16	недавно O
17	прошедшее O
18	в O
19	его O
20	рамках O
21	соревнование O
22	CoNLL O
23	Shared O
24	Task O
25	. O

# sent_id = 247
# text =   Universal Dependencies — это проект по унификации разметки синтаксических корпусов (трибанков) в рамках грамматики зависимостей.
1	Universal B-Project
2	Dependencies I-Project
3	— O
4	это O
5	проект O
6	по O
7	унификации B-Task
8	разметки I-Task
9	синтаксических B-Object
10	корпусов I-Object
11	( O
12	трибанков B-Object
13	) O
14	в O
15	рамках O
16	грамматики B-Method
17	зависимостей I-Method
18	. O

# sent_id = 248
# text =   Мы можем оценивать, правильно ли нашли вершину слова — метрика UAS (Unlabeled attachment score).
1	Мы O
2	можем O
3	оценивать O
4	, O
5	правильно O
6	ли O
7	нашли O
8	вершину O
9	слова O
10	— O
11	метрика O
12	UAS B-ShortName
13	( O
14	Unlabeled B-Metric
15	attachment I-Metric
16	score I-Metric
17	) O
18	. O

# sent_id = 249
# text =   Или оценивать, правильно ли найдена как вершина, так и тип зависимости — метрика LAS (Labeled attachment score).
1	Или O
2	оценивать O
3	, O
4	правильно O
5	ли O
6	найдена O
7	как O
8	вершина O
9	, O
10	так O
11	и O
12	тип O
13	зависимости O
14	— O
15	метрика O
16	LAS B-ShortName_Metric
17	( O
18	Labeled B-Metric
19	attachment I-Metric
20	score I-Metric
21	) O
22	. O

# sent_id = 250
# text =   Казалось бы, здесь напрашивается оценка точности (accuracy) — считаем, сколько раз мы попали из общего количества случаев.
1	Казалось O
2	бы O
3	, O
4	здесь O
5	напрашивается O
6	оценка O
7	точности B-Metric
8	( O
9	accuracy B-Metric
10	) O
11	— O
12	считаем O
13	, O
14	сколько O
15	раз O
16	мы O
17	попали O
18	из O
19	общего O
20	количества O
21	случаев O
22	. O

# sent_id = 251
# text =   Разработчики, решающие задачи автоматического парсинга, часто берут на вход сырой текст, который в соответствии с пирамидой анализа проходит этапы токенизации и морфологического анализа.
1	Разработчики O
2	, O
3	решающие O
4	задачи O
5	автоматического B-Task
6	парсинга I-Task
7	, O
8	часто O
9	берут O
10	на O
11	вход O
12	сырой O
13	текст O
14	, O
15	который O
16	в O
17	соответствии O
18	с O
19	пирамидой O
20	анализа O
21	проходит O
22	этапы O
23	токенизации B-Method
24	и O
25	морфологического B-Method
26	анализа I-Method
27	. O

# sent_id = 252
# text =   Поэтому формулой оценки в данном случае является ф-мера, где точность (precision) — доля точных попаданий относительно общего числа предсказаний, а полнота — доля точных попаданий относительно числа связей в размеченных данных.
1	Поэтому O
2	формулой O
3	оценки O
4	в O
5	данном O
6	случае O
7	является O
8	ф B-Metric
9	- I-Metric
10	мера I-Metric
11	, O
12	где O
13	точность B-Metric
14	( O
15	precision B-Metric
16	) O
17	— O
18	доля O
19	точных O
20	попаданий O
21	относительно O
22	общего O
23	числа O
24	предсказаний O
25	, O
26	а O
27	полнота B-Metric
28	— O
29	доля O
30	точных O
31	попаданий O
32	относительно O
33	числа O
34	связей O
35	в O
36	размеченных O
37	данных O
38	. O

# sent_id = 253
# text =   Очевидно, что все эксперименты проводятся на SynTagRus (разработка ИППИ РАН), в котором более миллиона токенов.
1	Очевидно O
2	, O
3	что O
4	все O
5	эксперименты O
6	проводятся O
7	на O
8	SynTagRus B-Dataset
9	( O
10	разработка O
11	ИППИ B-Organization
12	РАН I-Organization
13	) O
14	, O
15	в O
16	котором O
17	более O
18	миллиона O
19	токенов B-Subject
20	. O

# sent_id = 254
# text =   По итогам соревнования прошлого года модели, которые обучались на одном и том же SynTagRus, достигли следующих показателей LAS:
1	По O
2	итогам O
3	соревнования O
4	прошлого O
5	года O
6	модели O
7	, O
8	которые O
9	обучались O
10	на O
11	одном O
12	и O
13	том O
14	же O
15	SynTagRus B-Dataset
16	, O
17	достигли O
18	следующих O
19	показателей O
20	LAS B-ShortName_Metric
21	: O

# sent_id = 255
# text =   Забегая вперед, заметим, что новая версия UDPipe (Future) оказалась еще выше в этом году.
1	Забегая O
2	вперед O
3	, O
4	заметим O
5	, O
6	что O
7	новая O
8	версия O
9	UDPipe B-Application
10	( O
11	Future B-Organization
12	) O
13	оказалась O
14	еще O
15	выше O
16	в O
17	этом O
18	году O
19	. O

# sent_id = 256
# text =   В список не вошел Syntaxnet — парсер Google.
1	В O
2	список O
3	не O
4	вошел O
5	Syntaxnet B-Application
6	— O
7	парсер O
8	Google B-Organization
9	. O

# sent_id = 257
# text =   Ответ прост: Syntaxnet начинался лишь с этапа морфологического анализа.
1	Ответ O
2	прост O
3	: O
4	Syntaxnet B-Application
5	начинался O
6	лишь O
7	с O
8	этапа O
9	морфологического B-Method
10	анализа I-Method
11	. O

# sent_id = 258
# text =   В качестве начальных данных у нас есть табличка выше с лидирующим Syntaxnet и с UDPipe 2.0 где-то на 7 месте.
1	В O
2	качестве O
3	начальных O
4	данных O
5	у O
6	нас O
7	есть O
8	табличка O
9	выше O
10	с O
11	лидирующим O
12	Syntaxnet B-Application
13	и O
14	с O
15	UDPipe B-Application
16	2.0 I-Application
17	где O
18	- O
19	то O
20	на O
21	7 O
22	месте O
23	. O

# sent_id = 259
# text =   Синтаксис, разумеется, далеко не единственный модуль «под капотом» real-time системы, поэтому тратить на него больше десятка миллисекунд не стоит.
1	Синтаксис B-Science
2	, O
3	разумеется O
4	, O
5	далеко O
6	не O
7	единственный O
8	модуль O
9	« O
10	под O
11	капотом O
12	» O
13	real B-Application
14	- I-Application
15	time I-Application
16	системы O
17	, O
18	поэтому O
19	тратить O
20	на O
21	него O
22	больше O
23	десятка O
24	миллисекунд O
25	не O
26	стоит O
27	. O

# sent_id = 260
# text =   Для русского языка у нас есть достаточно хорошие морфологические анализаторы, которые могут встроиться в нашу пирамиду.
1	Для O
2	русского B-Lang
3	языка O
4	у O
5	нас O
6	есть O
7	достаточно O
8	хорошие O
9	морфологические B-Subject
10	анализаторы I-Subject
11	, O
12	которые O
13	могут O
14	встроиться O
15	в O
16	нашу O
17	пирамиду O
18	. O

# sent_id = 261
# text =   Затем начинает работу теггер — штука, которая предсказывает морфологические свойства токена: в каком падеже слово стоит, в каком числе.
1	Затем O
2	начинает O
3	работу O
4	теггер B-Subject
5	— O
6	штука O
7	, O
8	которая O
9	предсказывает O
10	морфологические B-Subject
11	свойства I-Subject
12	токена I-Subject
13	: O
14	в O
15	каком O
16	падеже O
17	слово O
18	стоит O
19	, O
20	в O
21	каком O
22	числе O
23	. O

# sent_id = 262
# text =   В UDPipe есть еще лемматизатор, который подбирает для слов начальную форму.
1	В O
2	UDPipe B-Application
3	есть O
4	еще O
5	лемматизатор B-Application
6	, O
7	который O
8	подбирает O
9	для O
10	слов O
11	начальную B-Subject
12	форму I-Subject
13	. O

# sent_id = 263
# text =   UDPipe — это transition-based архитектура: она работает быстро, за линейное время проходя по всем токенам один раз.
1	UDPipe B-App_system
2	— O
3	это O
4	transition B-Model
5	- I-Model
6	based I-Model
7	архитектура O
8	: O
9	она O
10	работает O
11	быстро O
12	, O
13	за O
14	линейное O
15	время O
16	проходя O
17	по O
18	всем O
19	токенам B-Subject
20	один O
21	раз O
22	. O

# sent_id = 264
# text =   RightArc — то же самое, но зависимость строится в другую сторону, и отбрасывается верхушка.
1	RightArc B-Model
2	— O
3	то O
4	же O
5	самое O
6	, O
7	но O
8	зависимость O
9	строится O
10	в O
11	другую O
12	сторону O
13	, O
14	и O
15	отбрасывается O
16	верхушка O
17	. O

# sent_id = 265
# text =   У классических transition-based parser возможны три операции, перечисленные выше: стрелка в одну сторону, стрелка в другую сторону и шифт.
1	У O
2	классических O
3	transition B-Application
4	- I-Application
5	based I-Application
6	parser I-Application
7	возможны O
8	три O
9	операции O
10	, O
11	перечисленные O
12	выше O
13	: O
14	стрелка O
15	в O
16	одну O
17	сторону O
18	, O
19	стрелка O
20	в O
21	другую O
22	сторону O
23	и O
24	шифт O
25	. O

# sent_id = 266
# text =   Анализатор Mystem (разработка яндекса) в определении частей речи достигает лучших результатов, чем UDPipe.
1	Анализатор O
2	Mystem B-Application
3	( O
4	разработка O
5	яндекса B-Organization
6	) O
7	в O
8	определении O
9	частей O
10	речи O
11	достигает O
12	лучших O
13	результатов O
14	, O
15	чем O
16	UDPipe B-App_system
17	. O

# sent_id = 267
# text =   Многие знают, что Mystem не полностью понимает морфологическую омонимию.
1	Многие O
2	знают O
3	, O
4	что O
5	Mystem B-Application
6	не O
7	полностью O
8	понимает O
9	морфологическую B-Object
10	омонимию I-Object
11	. O

# sent_id = 268
# text =   При помощи анализатора RNNMorph.
1	При O
2	помощи O
3	анализатора O
4	RNNMorph B-Application
5	. O

# sent_id = 269
# text =   Про него мало кто слышал, но в прошлом году он выиграл соревнование среди морфологических анализаторов, проводившееся в рамках конференции «Диалог».
1	Про O
2	него O
3	мало O
4	кто O
5	слышал O
6	, O
7	но O
8	в O
9	прошлом O
10	году O
11	он O
12	выиграл O
13	соревнование O
14	среди O
15	морфологических B-Subject
16	анализаторов I-Subject
17	, O
18	проводившееся O
19	в O
20	рамках O
21	конференции O
22	« O
23	Диалог O
24	» O
25	. O

# sent_id = 270
# text =   Хотя если сравнивать их чисто на уровне качества морфологической разметки (данные с MorphoRuEval-2017), то проигрыш получается значительный — порядка 15%, если считать accuracy по словам.
1	Хотя O
2	если O
3	сравнивать O
4	их O
5	чисто O
6	на O
7	уровне O
8	качества O
9	морфологической B-Labeling
10	разметки B-Method
11	( O
12	данные O
13	с O
14	MorphoRuEval-2017 O
15	) O
16	, O
17	то O
18	проигрыш O
19	получается O
20	значительный O
21	— O
22	порядка O
23	15 O
24	% O
25	, O
26	если O
27	считать O
28	accuracy B-Metric
29	по O
30	словам O
31	. O

# sent_id = 271
# text =   Дальше буду сравнивать нас с Syntaxnet и остальными алгоритмами.
1	Дальше O
2	буду O
3	сравнивать O
4	нас O
5	с O
6	Syntaxnet B-App_system
7	и O
8	остальными O
9	алгоритмами O
10	. O

# sent_id = 272
# text =   Интересно, что мы почти дотянулись по метрике LAS до версии Syntaxnet.
1	Интересно O
2	, O
3	что O
4	мы O
5	почти O
6	дотянулись O
7	по O
8	метрике O
9	LAS B-ShortName_Metric
10	до O
11	версии O
12	Syntaxnet B-App_system
13	. O

# sent_id = 273
# text =   В архитектуре стенфордского парсера и Syntaxnet заложена другая концепия: сначала они генерируют полный ориентированный граф, и дальше работа алгоритма состоит в том, чтобы оставить тот скелет (минимальное остовное дерево), который будет наиболее вероятным.
1	В O
2	архитектуре O
3	стенфордского O
4	парсера O
5	и O
6	Syntaxnet B-App_system
7	заложена O
8	другая O
9	концепия O
10	: O
11	сначала O
12	они O
13	генерируют O
14	полный O
15	ориентированный B-Object
16	граф I-Object
17	, O
18	и O
19	дальше O
20	работа O
21	алгоритма O
22	состоит O
23	в O
24	том O
25	, O
26	чтобы O
27	оставить O
28	тот O
29	скелет O
30	( O
31	минимальное B-Object
32	остовное I-Object
33	дерево I-Object
34	) O
35	, O
36	который O
37	будет O
38	наиболее O
39	вероятным O
40	. O

# sent_id = 274
# text =   Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
1	Чаще O
2	всего O
3	на O
4	практике O
5	в O
6	NLP B-ShortName_Science
7	приходится O
8	сталкиваться O
9	с O
10	задачей O
11	построения B-Task
12	эмбеддингов I-Task
13	. O

# sent_id = 275
# text =   Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
1	Для O
2	ее O
3	решения O
4	обычно O
5	используют O
6	один O
7	из O
8	следующих O
9	инструментов O
10	: O
11	Готовые O
12	векторы B-Object
13	/ O
14	эмбеддинги B-Subject
15	слов I-Subject
16	[ O
17	6 O
18	] O
19	; O
20	Внутренние O
21	состояния O
22	CNN B-ShortName_Method
23	, O
24	натренированных O
25	на O
26	таких O
27	задачах O
28	как O
29	, O
30	как O
31	определение B-Task
32	фальшивых I-Task
33	предложений I-Task
34	/ O
35	языковое B-Task
36	моделирование I-Task
37	/ O
38	классификация B-Task
39	[ O
40	7 O
41	] O
42	; O
43	Комбинация O
44	выше O
45	перечисленных O
46	методов O
47	; O
48	Кроме O
49	того O
50	, O
51	уже O
52	много O
53	раз O
54	было O
55	показано O
56	[ O
57	9 O
58	] O
59	, O
60	что O
61	в O
62	качестве O
63	хорошего O
64	бейслайна O
65	для O
66	эмбеддингов O
67	предложений O
68	можно O
69	взять O
70	и O
71	просто O
72	усредненные O
73	( O
74	с O
75	парой O
76	незначительных O
77	деталей O
78	, O
79	которые O
80	сейчас O
81	опустим O
82	) O
83	векторы O
84	слов O
85	. O

# sent_id = 276
# text =   Выраженная в тексте эмоциональная оценка называется тональностью или сентиментом (от англ. sentiment — чувство; мнение, настроение) текста.
1	Выраженная O
2	в O
3	тексте O
4	эмоциональная B-Object
5	оценка I-Object
6	называется O
7	тональностью B-Object
8	или O
9	сентиментом B-Object
10	( O
11	от O
12	англ O
13	sentiment O
14	— O
15	чувство O
16	; O
17	мнение O
18	, O
19	настроение O
20	) O
21	текста B-Object
22	. O

# sent_id = 277
# text =   Исторически сложилось так, что традиционный подход к сентимент анализу представляет собой задачу классификации текста (части текста) на две-три категории (негативный, позитивный, нейтральный или просто: негативный или позитивный) [Pang & Lee; Turney ].
1	Исторически O
2	сложилось O
3	так O
4	, O
5	что O
6	традиционный O
7	подход O
8	к O
9	сентимент B-Method
10	анализу I-Method
11	представляет O
12	собой O
13	задачу O
14	классификации B-Task
15	текста O
16	( O
17	части O
18	текста O
19	) O
20	на O
21	две O
22	- O
23	три O
24	категории O
25	( O
26	негативный O
27	, O
28	позитивный O
29	, O
30	нейтральный O
31	или O
32	просто O
33	: O
34	негативный O
35	или O
36	позитивный O
37	) O
38	[ O
39	Pang B-Person
40	& O
41	Lee B-Person
42	; O
43	Turney B-Person
44	] O
45	. O

# sent_id = 278
# text =   Такой вид сентимент анализа называется объектной тональностью (object-based).
1	Такой O
2	вид O
3	сентимент B-Method
4	анализа I-Method
5	называется O
6	объектной B-Object
7	тональностью I-Object
8	( O
9	object B-Object
10	- I-Object
11	based I-Object
12	) O
13	. O

# sent_id = 279
# text =   Таким образом, тональность высказывания определяется тремя компонентами: субъектом тональности (кто высказал оценку), объектом тональности (о ком или о чём высказана оценка) и собственно тональной оценкой (как оценили).
1	Таким O
2	образом O
3	, O
4	тональность B-Subject
5	высказывания I-Subject
6	определяется O
7	тремя O
8	компонентами O
9	: O
10	субъектом B-Object
11	тональности I-Object
12	( O
13	кто O
14	высказал O
15	оценку O
16	) O
17	, O
18	объектом B-Object
19	тональности I-Object
20	( O
21	о O
22	ком O
23	или O
24	о O
25	чём O
26	высказана O
27	оценка O
28	) O
29	и O
30	собственно O
31	тональной B-Object
32	оценкой I-Object
33	( O
34	как O
35	оценили O
36	) O
37	. O

# sent_id = 280
# text =   Еще одним направлением сентимент анализа является выявление негативности / позитивности атрибутов объекта тональности (feature-based / aspect-based sentiment analysis).
1	Еще O
2	одним O
3	направлением O
4	сентимент B-Method
5	анализа I-Method
6	является O
7	выявление B-Task
8	негативности I-Task
9	/ I-Task
10	позитивности I-Task
11	атрибутов I-Task
12	объекта I-Task
13	тональности I-Task
14	( O
15	feature B-Method
16	- I-Method
17	based I-Method
18	/ I-Method
19	aspect I-Method
20	- I-Method
21	based I-Method
22	sentiment B-Method
23	analysis I-Method
24	) O
25	. O

# sent_id = 281
# text =   При статистическом подходе для решения задачи общей классификации текстов на классы тональности широко используют метод опорных векторов (SVM), Байесовы модели, различного рода регрессии [Chetviorkin & Loukachevitch — описание соревнования ROMIP-2011 по сентимент анализу данных, практически все участники использовали SVM или Байес].
1	При O
2	статистическом O
3	подходе O
4	для O
5	решения O
6	задачи O
7	общей O
8	классификации B-Task
9	текстов O
10	на O
11	классы O
12	тональности O
13	широко O
14	используют O
15	метод B-Method
16	опорных I-Method
17	векторов I-Method
18	( O
19	SVM B-ShortName_Method
20	) O
21	, O
22	Байесовы B-Model
23	модели I-Model
24	, O
25	различного O
26	рода O
27	регрессии B-Model
28	[ O
29	Chetviorkin B-Person
30	& I-TERM
31	Loukachevitch I-Person
32	— O
33	описание O
34	соревнования O
35	ROMIP-2011 O
36	по O
37	сентимент B-Method
38	анализу I-Method
39	данных O
40	, O
41	практически O
42	все O
43	участники O
44	использовали O
45	SVM B-ShortName_Method
46	или O
47	Байес B-Method
48	] O
49	. O

# sent_id = 282
# text =   Если же целью является определение тональности у определенного, заранее заданного объекта (нескольких объектов), то применяют более сложные статистические алгоритмы, такие как CRF [Антонова и Соловьев], алгоритмы семантической близости (например, латентно-семантический анализ – LSA, латентное размещение Дирихле — LDA) и др., а также методы, основанные на правилах [Пазельская и Соловьев].
1	Если O
2	же O
3	целью O
4	является O
5	определение B-Task
6	тональности I-Task
7	у O
8	определенного O
9	, O
10	заранее O
11	заданного O
12	объекта O
13	( O
14	нескольких O
15	объектов O
16	) O
17	, O
18	то O
19	применяют O
20	более O
21	сложные O
22	статистические B-Method
23	алгоритмы I-Method
24	, O
25	такие O
26	как O
27	CRF B-ShortName_Method
28	[ O
29	Антонова B-Person
30	и O
31	Соловьев B-Person
32	] O
33	, O
34	алгоритмы B-Method
35	семантической I-Method
36	близости I-Method
37	( O
38	например O
39	, O
40	латентно B-TERM
41	- I-TERM
42	семантический I-TERM
43	анализ B-Method
44	– O
45	LSA B-ShortName_Method
46	, O
47	латентное B-Method
48	размещение I-Method
49	Дирихле I-Method
50	— O
51	LDA B-ShortName_Method
52	) O
53	, O
54	а O
55	также O
56	методы O
57	, O
58	основанные O
59	на O
60	правилах O
61	[ O
62	Пазельская B-Person
63	и O
64	Соловьев B-Person
65	] O
66	. O

# sent_id = 283
# text =   Удалось найти лишь это упоминание про систему Deepgram.
1	Удалось O
2	найти O
3	лишь O
4	это O
5	упоминание O
6	про O
7	систему O
8	Deepgram B-App_system
9	. O

# sent_id = 284
# text =   Также очень похожая функциональность есть у Microsoft в Streams, но нигде не нашел упоминания про поддержку русского языка, судя по всему, ее тоже нет.
1	Также O
2	очень O
3	похожая O
4	функциональность O
5	есть O
6	у O
7	Microsoft B-Organization
8	в O
9	Streams B-App_system
10	, O
11	но O
12	нигде O
13	не O
14	нашел O
15	упоминания O
16	про O
17	поддержку O
18	русского O
19	языка O
20	, O
21	судя O
22	по O
23	всему O
24	, O
25	ее O
26	тоже O
27	нет O
28	. O

# sent_id = 285
# text =   Нейросети, которые могут преобразовывать речь в текст называются (сюрприз-сюрприз), speech-to-text.
1	Нейросети O
2	, O
3	которые O
4	могут O
5	преобразовывать B-Task
6	речь I-Task
7	в I-Task
8	текст I-Task
9	называются O
10	( O
11	сюрприз O
12	- O
13	сюрприз O
14	) O
15	, O
16	speech B-Task
17	- I-Task
18	to I-Task
19	- I-Task
20	text I-Task
21	. O

# sent_id = 286
# text =   Если получится найти публичный сервис speech-to-text, то его можно использовать, чтобы «оцифровать» речь во всех вебинарах, а сделать потом нечеткий поиск по тексту – более простая задача.
1	Если O
2	получится O
3	найти O
4	публичный O
5	сервис O
6	speech B-Task
7	- I-Task
8	to I-Task
9	- I-Task
10	text I-Task
11	, O
12	то O
13	его O
14	можно O
15	использовать O
16	, O
17	чтобы O
18	« O
19	оцифровать O
20	» O
21	речь O
22	во O
23	всех O
24	вебинарах O
25	, O
26	а O
27	сделать O
28	потом O
29	нечеткий O
30	поиск O
31	по O
32	тексту O
33	– O
34	более O
35	простая O
36	задача O
37	. O

# sent_id = 287
# text =   Поиск сервисов, способных делать speech-to-text показал, что таких систем масса, в том числе и разработанных в России, есть среди них также глобальные облачные провайдеры вроде Google, Amazon, MS Azure.
1	Поиск O
2	сервисов O
3	, O
4	способных O
5	делать O
6	speech B-Task
7	- I-Task
8	to I-Task
9	- I-Task
10	text I-Task
11	показал O
12	, O
13	что O
14	таких O
15	систем O
16	масса O
17	, O
18	в O
19	том O
20	числе O
21	и O
22	разработанных O
23	в O
24	России O
25	, O
26	есть O
27	среди O
28	них O
29	также O
30	глобальные O
31	облачные O
32	провайдеры O
33	вроде O
34	Google B-Organization
35	, O
36	Amazon B-Organization
37	, O
38	MS B-Organization
39	Azure I-Organization
40	. O

# sent_id = 288
# text =   Custom Vocabularies – позволяет создать «словарь» из тех, слов, которые должна «выучить» нейросеть перед тем, как приступить к распознаванию.
1	Custom B-App_system
2	Vocabularies I-App_system
3	– O
4	позволяет O
5	создать O
6	« O
7	словарь O
8	» O
9	из O
10	тех O
11	, O
12	слов O
13	, O
14	которые O
15	должна O
16	« O
17	выучить O
18	» O
19	нейросеть O
20	перед O
21	тем O
22	, O
23	как O
24	приступить O
25	к O
26	распознаванию B-Task
27	. O

# sent_id = 289
# text =   Можно попробовать прикрутить к итоговому набору текстов алгоритм BERT (Bi-directional Encoder Representation from Transformer), описание есть тут.
1	Можно O
2	попробовать O
3	прикрутить O
4	к O
5	итоговому O
6	набору O
7	текстов O
8	алгоритм O
9	BERT B-ShortName_Model
10	( O
11	Bi B-Model
12	- I-Model
13	directional I-Model
14	Encoder I-Model
15	Representation I-Model
16	from I-Model
17	Transformer I-Model
18	) O
19	, O
20	описание O
21	есть O
22	тут O
23	. O

# sent_id = 290
# text =  В этой статье мы расскажем о методе Propensity Score Adjustment, который применим для коррекции смещений и улучшения данных, полученных на онлайн-панелях.
1	В O
2	этой O
3	статье O
4	мы O
5	расскажем O
6	о O
7	методе B-Method
8	Propensity I-Method
9	Score I-Method
10	Adjustment I-Method
11	, O
12	который O
13	применим O
14	для O
15	коррекции O
16	смещений O
17	и O
18	улучшения O
19	данных O
20	, O
21	полученных O
22	на O
23	онлайн O
24	- O
25	панелях O
26	. O

# sent_id = 291
# text =  Итоговые коэффициенты, корректирующие смещение онлайн-выборки, можно рассчитать по методу Хорвица-Томпсона.
1	Итоговые O
2	коэффициенты O
3	, O
4	корректирующие O
5	смещение O
6	онлайн O
7	- O
8	выборки O
9	, O
10	можно O
11	рассчитать O
12	по O
13	методу B-Method
14	Хорвица I-Method
15	- I-Method
16	Томпсона I-Method
17	. O

# sent_id = 292
# text =  Взвешивание (Weighting) - метод предназначен для коррекции известных перекосов выборок по социально-демографическим атрибутам.
1	Взвешивание B-Method
2	( O
3	Weighting B-Method
4	) O
5	метод O
6	предназначен O
7	для O
8	коррекции O
9	известных O
10	перекосов O
11	выборок O
12	по O
13	социально O
14	- O
15	демографическим O
16	атрибутам O
17	. O

# sent_id = 293
# text =  Авторы оригинального исследования Pew Research рекомендуют использовать для корректировки онлайн-опросов модели случайных лесов (random forest).
1	Авторы O
2	оригинального O
3	исследования O
4	Pew B-Research
5	Research I-Research
6	рекомендуют O
7	использовать O
8	для O
9	корректировки O
10	онлайн O
11	- O
12	опросов O
13	модели O
14	случайных B-Method
15	лесов I-Method
16	( O
17	random B-Method
18	forest I-Method
19	) O
20	. O

# sent_id = 294
# text =  Сейчас стандарт коррекции онлайн-выборок находится на стадии обсуждения и разработки и метод Propensity Score Adjustment, который мы рассмотрели, может стать общепринятым способом коррекции онлайн-панелей.
1	Сейчас O
2	стандарт O
3	коррекции O
4	онлайн O
5	- O
6	выборок O
7	находится O
8	на O
9	стадии O
10	обсуждения O
11	и O
12	разработки O
13	и O
14	метод B-Method
15	Propensity I-Method
16	Score I-Method
17	Adjustment I-Method
18	, O
19	который O
20	мы O
21	рассмотрели O
22	, O
23	может O
24	стать O
25	общепринятым O
26	способом O
27	коррекции O
28	онлайн O
29	- O
30	панелей O
31	. O

# sent_id = 295
# text =   Некоторое время назад к нам обратился заказчик с не совсем обычной задачей — воспроизвести сервис IBM Watson Personality Insights, который анализировал текст, написанный человеком и определял по нему ряд личностных характеристик.
1	Некоторое O
2	время O
3	назад O
4	к O
5	нам O
6	обратился O
7	заказчик O
8	с O
9	не O
10	совсем O
11	обычной O
12	задачей O
13	— O
14	воспроизвести O
15	сервис O
16	IBM B-Technology
17	Watson I-Technology
18	Personality I-Technology
19	Insights I-Technology
20	, O
21	который O
22	анализировал B-Task
23	текст I-Task
24	, O
25	написанный O
26	человеком O
27	и O
28	определял O
29	по O
30	нему O
31	ряд O
32	личностных B-Object
33	характеристик I-Object
34	. O

# sent_id = 296
# text =   Основная идея данного сервиса состояла в том, что он получает на вход текст написанный определенным человеком и определяет по этому тексту четыре группы характеристик личности.
1	Основная O
2	идея O
3	данного O
4	сервиса O
5	состояла O
6	в O
7	том O
8	, O
9	что O
10	он O
11	получает O
12	на O
13	вход O
14	текст O
15	написанный O
16	определенным O
17	человеком O
18	и O
19	определяет B-Task
20	по O
21	этому O
22	тексту O
23	четыре O
24	группы I-Task
25	характеристик I-Task
26	личности I-Task
27	. O

# sent_id = 297
# text =   Например, Personality Insights использовался в психотерапии для оценки состояния пациентов [5], в искусстве (оценка личности персонажей пьес Шекспира) [6], определении спама [7] а также в научных исследованиях.
1	Например O
2	, O
3	Personality B-Technology
4	Insights I-Technology
5	использовался O
6	в O
7	психотерапии B-Science
8	для O
9	оценки B-Task
10	состояния I-Task
11	пациентов I-Task
12	[ O
13	5 O
14	] O
15	, O
16	в O
17	искусстве O
18	( O
19	оценка B-Task
20	личности I-Task
21	персонажей I-Task
22	пьес I-Task
23	Шекспира I-Task
24	) O
25	[ O
26	6 O
27	] O
28	, O
29	определении B-Task
30	спама I-Task
31	[ O
32	7 O
33	] O
34	а O
35	также O
36	в O
37	научных O
38	исследованиях O
39	. O

# sent_id = 298
# text =   На сайте Personality Insights качество моделей Watson оценивалось с помощью двух показателей — средней абсолютной ошибки (MAE) и коэффициента корреляции.
1	На O
2	сайте O
3	Personality B-Technology
4	Insights I-Technology
5	качество O
6	моделейO
7	Watson B-Technology
8	оценивалось O
9	с O
10	помощью O
11	двух O
12	показателей O
13	— O
14	средней B-Metric
15	абсолютной I-Metric
16	ошибки I-Metric
17	( O
18	MAE B-ShortName_Metric
19	) O
20	и O
21	коэффициента B-Metric
22	корреляции I-Metric
23	. O

# sent_id = 299
# text =   В литературе для предсказания характеристик Big 5 использовались различные методы линейная регрессия с использованием признаков полученных латентным семантическим анализом [11], ридж-регрессия по большому набору собранных вручную признаков [12], SVM с признаками TF/IDF [13], word2vec и doc2vec [14].
1	В O
2	литературе O
3	для O
4	предсказания O
5	характеристик O
6	Big O
7	5 O
8	использовались O
9	различные O
10	методы O
11	линейная B-Method
12	регрессия I-Method
13	с O
14	использованием O
15	признаков O
16	полученных O
17	латентным O
18	семантическим O
19	анализом O
20	[ O
21	11 O
22	] O
23	, O
24	ридж B-Method
25	- I-Method
26	регрессия I-Method
27	по O
28	большому O
29	набору O
30	собранных O
31	вручную O
32	признаков O
33	[ O
34	12 O
35	] O
36	, O
37	SVM B-ShortName_Method
38	с O
39	признаками O
40	TF B-Metric
41	/ O
42	IDF B-Metric
43	[ O
44	13 O
45	] O
46	, O
47	word2vec B-Model
48	и O
49	doc2vec B-Model
50	[ O
51	14 O
52	] O
53	. O

# sent_id = 300
# text =   В более современных работах присутствуют сверточные нейронные сети [15, 16], а также предобученные модели BERT [17]
1	В O
2	более O
3	современных O
4	работах O
5	присутствуют O
6	сверточные B-Method
7	нейронные B-Method
8	сети I-Method
9	[ O
10	15 O
11	, O
12	16 O
13	] O
14	, O
15	а O
16	также O
17	предобученные O
18	модели O
19	BERT B-Model
20	[ O
21	17 O
22	] O

# sent_id = 301
# text =  Модель, которую построил заказчик использовала вектора слов word2vec и рекуррентную нейронную сеть на базе GRU (gated recurrent unit) (Рис 1а).
1	Модель O
2	, O
3	которую O
4	построил O
5	заказчик O
6	использовала O
7	вектора O
8	слов O
9	word2vec B-Model
10	и O
11	рекуррентную O
12	нейронную O
13	сеть O
14	на O
15	базе O
16	GRU B-ShortName_Method
17	( O
18	gated B-Method
19	recurrent I-Method
20	unit I-Method
21	) O
22	( O
23	Рис O
24	1а O
25	) O
26	. O

# sent_id = 302
# text =   Обучалась модель с функцией ошибки MSE (среднеквадратичное отклонение).
1	Обучалась O
2	модель B-Object
3	с O
4	функцией O
5	ошибки O
6	MSE B-ShortName_Method
7	( O
8	среднеквадратичное B-Method
9	отклонение I-Method
10	) O
11	. O

# sent_id = 303
# text =   Сигмоидная функция активации обычно не очень хорошо подходит для задачи регрессии.
1	Сигмоидная B-Method
2	функция I-Method
3	активации I-Method
4	обычно O
5	не O
6	очень O
7	хорошо O
8	подходит O
9	для O
10	задачи O
11	регрессии B-Task
12	. O

# sent_id = 304
# text =   В литературе для регрессии рекомендуют использовать линейную активацию или RelU.
1	В O
2	литературе O
3	для O
4	регрессии B-Task
5	рекомендуют O
6	использовать O
7	линейную B-Method
8	активацию I-Method
9	или O
10	RelU B-Method
11	. O

# sent_id = 305
# text =   Вычислив MAE отдельно для характеристики личности и отдельно для потребительских предпочтений получили значения 0.11 и 0.148 соответственно, т. е. потребительские предпочтения сильно портят общую картину.
1	Вычислив O
2	MAE B-Metric
3	отдельно O
4	для O
5	характеристики O
6	личности O
7	и O
8	отдельно O
9	для O
10	потребительских O
11	предпочтений O
12	получили O
13	значения O
14	0.11 B-Value
15	и O
16	0.148 B-Value
17	соответственно O
18	, O
19	т O
20	. O
21	  O
22	е O
23	. O
24	потребительские O
25	предпочтения O
26	сильно O
27	портят O
28	общую O
29	картину O
30	. O

# sent_id = 306
# text =   Замена BERT на более современную модель XLM RoBERTa large позволило улучшить результаты (эта модель более ресурсозатратная и медленная, но заказчик сказал, что скорость работы не критична).
1	Замена O
2	BERT O
3	на O
4	более O
5	современную O
6	модель O
7	XLM B-Model
8	RoBERTa I-Model
9	large I-Model
10	позволило O
11	улучшить O
12	результаты O
13	( O
14	эта O
15	модель O
16	более O
17	ресурсозатратная O
18	и O
19	медленная O
20	, O
21	но O
22	заказчик O
23	сказал O
24	, O
25	что O
26	скорость O
27	работы O
28	не O
29	критична O
30	) O
31	. O

# sent_id = 307
# text =   Итоговый MAE составил 0.073 для характеристик личности и 0.098 для потребительских предпочтений.
1	Итоговый O
2	MAE B-Metric
3	составил O
4	0.073 B-Value
5	для O
6	характеристик O
7	личности O
8	и O
9	0.098 B-Value
10	для O
11	потребительских O
12	предпочтений O
13	. O

# sent_id = 308
# text =   Получились немного разные цифры, но средний коэффициент корреляции по всем параметрам составил 0.68, что говорит о том, что характеристики, выдаваемые с разных переводов одного текста должны быть весьма похожи.
1	Получились O
2	немного O
3	разные O
4	цифры O
5	, O
6	но O
7	средний O
8	коэффициент B-Metric
9	корреляции I-Metric
10	по O
11	всем O
12	параметрам O
13	составил O
14	0.68 B-Value
15	, O
16	что O
17	говорит O
18	о O
19	том O
20	, O
21	что O
22	характеристики O
23	, O
24	выдаваемые O
25	с O
26	разных O
27	переводов O
28	одного O
29	текста O
30	должны O
31	быть O
32	весьма O
33	похожи O
34	. O

# sent_id = 309
# text =   Надо сказать, что признаки, формируемые верхними слоями подобных моделей не всегда являются самыми лучшими, точнее даже сказать, как правило, не являются — в классических задачах, таких как поиск именованных сущностей или ответы на вопросы по тексту, признаки верхних уровней работают хуже, чем признаки промежуточных [18].
1	Надо O
2	сказать O
3	, O
4	что O
5	признаки O
6	, O
7	формируемые O
8	верхними O
9	слоями O
10	подобных O
11	моделей O
12	не O
13	всегда O
14	являются O
15	самыми O
16	лучшими O
17	, O
18	точнее O
19	даже O
20	сказать O
21	, O
22	как O
23	правило O
24	, O
25	не O
26	являются O
27	— O
28	в O
29	классических O
30	задачах O
31	, O
32	таких O
33	как O
34	поиск B-Task
35	именованных I-Task
36	сущностей I-Task
37	или O
38	ответы O
39	на O
40	вопросы O
41	по O
42	тексту O
43	, O
44	признаки O
45	верхних O
46	уровней O
47	работают O
48	хуже O
49	, O
50	чем O
51	признаки O
52	промежуточных O
53	[ O
54	18 O
55	] O
56	. O

# sent_id = 310
# text =   Veridical Data Science — программная статья о методологии верификации моделей.
1	Veridical B-Publication
2	Data I-Publication
3	Science I-Publication
4	— O
5	программная O
6	статья O
7	о O
8	методологии O
9	верификации B-Task
10	моделей I-Task
11	. O

# sent_id = 311
# text =  Чтобы обеспечить надежную проверку и разработать механизмы проверки и пополнения знаний, нужны специалисты смежных областей, одновременно обладающие компетенциями в ML и в предметной области (медицине, лингвистике, нейробиологии, образовании и т.д.).
1	Чтобы O
2	обеспечить O
3	надежную O
4	проверку O
5	и O
6	разработать O
7	механизмы O
8	проверки O
9	и O
10	пополнения O
11	знаний O
12	, O
13	нужны O
14	специалисты O
15	смежных O
16	областей O
17	, O
18	одновременно O
19	обладающие O
20	компетенциями O
21	в O
22	ML O
23	и O
24	в O
25	предметной O
26	области O
27	( O
28	медицине B-Science
29	, O
30	лингвистике B-Science
31	, O
32	нейробиологии B-Science
33	, O
34	образовании B-Science
35	и O
36	т.д. O
37	) O
38	. O

# sent_id = 312
# text =   В частности, развивается causal inference и commonsense reasoning.
1	В O
2	частности O
3	, O
4	развивается O
5	causal B-Method
6	inference I-Method
7	и O
8	commonsense B-Method
9	reasoning I-Method
10	. O

# sent_id = 313
# text =   Часть докладов посвящена мета-обучению (о том, как учиться учиться) и соединению DL-технологий с логикой 1 и 2 порядка — термин Artificial General Intelligence (AGI) становится обычным термином в выступлениях спикеров.
1	Часть O
2	докладов O
3	посвящена O
4	мета O
5	- O
6	обучению O
7	( O
8	о O
9	том O
10	, O
11	как O
12	учиться O
13	учиться O
14	) O
15	и O
16	соединению O
17	DL B-Technology
18	- I-Technology
19	технологий I-Technology
20	с O
21	логикой O
22	1 O
23	и O
24	2 O
25	порядка O
26	— O
27	термин O
28	Artificial B-Application
29	General I-Application
30	Intelligence I-Application
31	( O
32	AGI B-Abberv_Application
33	) O
34	становится O
35	обычным O
36	термином o
37	в O
38	выступлениях O
39	спикеров O
40	. O

# sent_id = 314
# text =   Google запускает Coral ai – аналог raspberry pi, мини-компьютер для внедрения нейросетей в экспериментальные установки.
1	Google B-Organization
2	запускает O
3	Coral B-Technology
4	ai I-Technology
5	– O
6	аналог O
7	raspberry B-Technology
8	pi I-Technology
9	, O
10	мини O
11	- O
12	компьютер O
13	для O
14	внедрения O
15	нейросетей O
16	в O
17	экспериментальные O
18	установки O
19	. O

# sent_id = 315
# text =   Federated learning – направление ML, в котором отдельные модели учатся независимо друг от друга, а затем объединяются в единую модель (без централизации исходных данных), с поправками на редкие события, аномалии, персонализацию и т.д.
1	Federated B-Method
2	learning I-Method
3	– O
4	направление O
5	ML B-ShortName_Method
6	, O
7	в O
8	котором O
9	отдельные O
10	модели O
11	учатся O
12	независимо O
13	друг O
14	от O
15	друга O
16	, O
17	а O
18	затем O
19	объединяются O
20	в O
21	единую O
22	модель O
23	( O
24	без O
25	централизации O
26	исходных O
27	данных O
28	) O
29	, O
30	с O
31	поправками O
32	на O
33	редкие O
34	события O
35	, O
36	аномалии O
37	, O
38	персонализацию O
39	и O
40	т.д. O

# sent_id = 316
# text =   Генеративные модели на основании federated learning – будущее перспективное направление по мнению Google, которое находится “в ранних стадиях экспоненциального роста”.
1	Генеративные B-Model
2	модели I-Model
3	на O
4	основании O
5	federated B-Method
6	learning I-Method
7	– O
8	будущее O
9	перспективное O
10	направление O
11	по O
12	мнению O
13	Google B-Organization
14	, O
15	которое O
16	находится O
17	“ O
18	в O
19	ранних O
20	стадиях O
21	экспоненциального O
22	роста O
23	” O
24	. O

# sent_id = 317
# text =   МТИ Technology Review протестировали два инструмента — MyInterview и Curious Thing.
1	МТИ B-Organization
2	Technology I-Organization
3	Review I-Organization
4	протестировали O
5	два O
6	инструмента O
7	— O
8	MyInterview B-Technology
9	и O
10	Curious B-Technology
11	Thing I-Technology
12	. O

# sent_id = 318
# text =   Потом появилась нейросеть GPT-2, которая была как минимум в 10 раз мощнее и была способна обрабатывать 1,5 миллиарда параметров (переменных, определяющих возможности машинного обучения).
1	Потом O
2	появилась O
3	нейросеть O
4	GPT-2 B-Model
5	, O
6	которая O
7	была O
8	как O
9	минимум O
10	в O
11	10 O
12	раз O
13	мощнее O
14	и O
15	была O
16	способна O
17	обрабатывать O
18	1,5 O
19	миллиарда O
20	параметров O
21	( O
22	переменных O
23	, O
24	определяющих O
25	возможности O
26	машинного B-Science
27	обучения I-Science
28	) O
29	. O

# sent_id = 319
# text =   Используя новый алгоритм упаковки, в Graphcore ускорили обработку естественного языка более чем в 2 раза при обучении BERT-Large.
1	Используя O
2	новый O
3	алгоритм B-Method
4	упаковки I-Method
5	, O
6	в O
7	Graphcore B-Organization
8	ускорили O
9	обработку O
10	естественного O
11	языка O
12	более O
13	чем O
14	в O
15	2 O
16	раза O
17	при O
18	обучении O
19	BERT B-Model
20	- I-Model
21	Large I-Model
22	. O

# sent_id = 320
# text =   В Graphcore предполагают, что алгоритм упаковки также может применяться в геномике, в моделях фолдинга белков и других моделях с перекошенным распределением длины, оказывая гораздо более широкое влияние на различные отрасли и приложения.
1	В O
2	Graphcore B-Organization
3	предполагают O
4	, O
5	что O
6	алгоритм B-Method
7	упаковки I-Method
8	также O
9	может O
10	применяться O
11	в O
12	геномике B-Science
13	, O
14	в O
15	моделях B-Model
16	фолдинга I-Model
17	белков O
18	и O
19	других O
20	моделях O
21	с O
22	перекошенным O
23	распределением O
24	длины O
25	, O
26	оказывая O
27	гораздо O
28	более O
29	широкое O
30	влияние O
31	на O
32	различные O
33	отрасли O
34	и O
35	приложения O
36	. O

# sent_id = 321
# text =   В новой работе Graphcore представили высокоэффективный алгоритм гистограммной упаковки с неотрицательными наименьшими квадратами (или NNLSHP), а также алгоритм BERT, применяемый к упакованным последовательностям.
1	В O
2	новой O
3	работе O
4	Graphcore B-Organization
5	представили O
6	высокоэффективный O
7	алгоритм B-Method
8	гистограммной I-Method
9	упаковки I-Method
10	с O
11	неотрицательными O
12	наименьшими O
13	квадратами O
14	( O
15	или O
16	, O
17	а O
18	также O
19	алгоритм O
20	BERT B-Model
21	, O
22	применяемый O
23	к O
24	упакованным O
25	последовательностям O
26	. O

# sent_id = 322
# text =   Сбер создал и опубликовал в открытом доступе программную библиотеку PyTorch-LifeStream, содержащую несколько алгоритмов построения эмбеддингов событийных данных.
1	Сбер B-Organization
2	создал O
3	и O
4	опубликовал O
5	в O
6	открытом O
7	доступе O
8	программную O
9	библиотеку O
10	PyTorch B-Application
11	- I-Application
12	LifeStream I-Application
13	, O
14	содержащую O
15	несколько O
16	алгоритмов O
17	построения O
18	эмбеддингов O
19	событийных O
20	данных O
21	. O

# sent_id = 323
# text =   Эмбеддинг (Embedding) – преобразования сложноструктурированных данных,  например слов, текстов, атрибутов событий, событий и их последовательностей, в машинно-читаемый набор чисел – числовой вектор.Событийные данные – разные последовательности.
1	Эмбеддинг B-Object
2	( O
3	Embedding B-Object
4	) O
5	– O
6	преобразования O
7	сложноструктурированных O
8	данных O
9	, O
10	например O
11	слов O
12	, O
13	текстов O
14	, O
15	атрибутов O
16	событий O
17	, O
18	событий O
19	и O
20	их O
21	последовательностей O
22	, O
23	в O
24	машинно O
25	- O
26	читаемый O
27	набор O
28	чисел O
29	– O
30	числовой O
31	вектор O
32	. O
33	Событийные O
34	данные O
35	– O
36	разные O
37	последовательности O
38	. O

# sent_id = 324
# text =   Самой популярной из трёх задач соревнования стала главная – Matching.
1	Самой O
2	популярной O
3	из O
4	трёх O
5	задач O
6	соревнования O
7	стала O
8	главная O
9	– O
10	Matching B-Task
11	. O

# sent_id = 325
# text =   Стоит отметить, что и для них всё было непросто – конкурсная задача матчинга позволила удачно применить разработанный в Лаборатории ИИ метод генерации эмбеддингов транзакционных данных одновременно для двух разных доменов событийных данных (транзакции и кликстрим – атрибуты посещения веб-страниц). 
1	Стоит O
2	отметить O
3	, O
4	что O
5	и O
6	для O
7	них O
8	всё O
9	было O
10	непросто O
11	– O
12	конкурсная O
13	задача O
14	матчинга O
15	позволила O
16	удачно O
17	применить O
18	разработанный O
19	в O
20	Лаборатории B-Organization
21	ИИ I-Organization
22	метод B-Method
23	генерации I-Method
24	эмбеддингов I-Method
25	транзакционных I-Method
26	данных I-Method
27	одновременно O
28	для O
29	двух O
30	разных O
31	доменов O
32	событийных O
33	данных O
34	( O
35	транзакции B-Object
36	и O
37	кликстрим B-Object
38	– O
39	атрибуты O
40	посещения O
41	веб O
42	- O
43	страниц O
44	) O
45	. O

# sent_id = 326
# text =   Победители создали лучшее решение благодаря применению собственной библиотеки PyTorch-LifeStream, которая позволила ускорить разработку решения, так как содержит много готовых инструментов для работы с событийными данными, и дала возможность стать фаворитом престижного конкурса. 
1	Победители O
2	создали O
3	лучшее O
4	решение O
5	благодаря O
6	применению O
7	собственной O
8	библиотеки O
9	PyTorch B-Application
10	- I-Application
11	LifeStream I-Application
12	, O
13	которая O
14	позволила O
15	ускорить O
16	разработку O
17	решения O
18	, O
19	так O
20	как O
21	содержит O
22	много O
23	готовых O
24	инструментов O
25	для O
26	работы O
27	с O
28	событийными O
29	данными O
30	, O
31	и O
32	дала O
33	возможность O
34	стать O
35	фаворитом O
36	престижного O
37	конкурса O

# sent_id = 327
# text =   Фичи для транзакций и кликов объединялись и подавались в алгоритм бустинга.
1	Фичи O
2	для O
3	транзакций O
4	и O
5	кликов O
6	объединялись O
7	и O
8	подавались O
9	в O
10	алгоритм O
11	бустинга B-Method
12	. O

# sent_id = 328
# text =   Алгоритм обучался как задача бинарной классификации.
1	Алгоритм O
2	обучался O
3	как O
4	задача B-Task
5	бинарной I-Task
6	классификации I-Task
7	. O

# sent_id = 329
# text =   Команда решила использовать схему обучения, похожую на сиамскую сеть.
1	Команда O
2	решила O
3	использовать O
4	схему O
5	обучения O
6	, O
7	похожую O
8	на O
9	сиамскую B-Method
10	сеть I-Method
11	. O

# sent_id = 330
# text =   Метки использованы для выборки положительных и отрицательных образцов для функции потерь Softmax Loss.
1	Метки O
2	использованы O
3	для O
4	выборки O
5	положительных O
6	и O
7	отрицательных O
8	образцов O
9	для O
10	функции O
11	потерь O
12	Softmax B-Method
13	Loss I-Method
14	. O

# sent_id = 331
# text =  SequenceEncoder – рекурентно-нейронная сеть (RNN), совместно используемая для транзакций и кликов.
1	SequenceEncoder B-Method
2	– O
3	рекурентно B-Network
4	- I-Network
5	нейронная B-Network
6	сеть I-Network
7	( O
8	RNN B-ShortName_Method
9	) O
10	, O
11	совместно O
12	используемая O
13	для O
14	транзакций O
15	и O
16	кликов O
17	. O

# sent_id = 332
# text =   В итоге это дало самый большой прирост качества: для ансамбля из пяти моделей метрика качества R1 выросла с 0.2819 до 0.2949.
1	В O
2	итоге O
3	это O
4	дало O
5	самый O
6	большой O
7	прирост O
8	качества O
9	: O
10	для O
11	ансамбля O
12	из O
13	пяти O
14	моделей O
15	метрика O
16	качества O
17	R1 B-Metric
18	выросла O
19	с O
20	0.2819 B-Value
21	до O
22	0.2949 B-Value
23	. O

# sent_id = 333
# text =   Решать задачу будем с использованием нейронных сетей, но оптимизируемых генетическим алгоритмом (ГА) – такой процесс называют нейроэволюцией.
1	Решать O
2	задачу O
3	будем O
4	с O
5	использованием O
6	нейронных O
7	сетей O
8	, O
9	но O
10	оптимизируемых O
11	генетическим B-Method
12	алгоритмом I-Method
13	( O
14	ГА B-ShortName
15	) O
16	– O
17	такой O
18	процесс O
19	называют O
20	нейроэволюцией O
21	. O

# sent_id = 334
# text =   Мы воспользовались методом NEAT (NeuroEvolution of Augmenting Topologies), изобретенным Кеннетом Стенли и Ристо Мииккулайненом в начале века [1]: во-первых, он хорошо зарекомендовал себя в важных для народного хозяйства проблемах, во-вторых, к началу работы над проектом у нас уже был свой фреймворк, реализующий NEAT.
1	Мы O
2	воспользовались O
3	методом O
4	NEAT B-ShortName
5	( O
6	NeuroEvolution B-Method
7	of I-Method
8	Augmenting I-Method
9	Topologies I-Method
10	) O
11	, O
12	изобретенным O
13	Кеннетом B-Person
14	Стенли I-Person
15	и O
16	Ристо B-Person
17	Мииккулайненом I-Person
18	в O
19	начале O
20	века O
21	[ O
22	1 O
23	] O
24	: O
25	во O
26	- O
27	первых O
28	, O
29	он O
30	хорошо O
31	зарекомендовал O
32	себя O
33	в O
34	важных O
35	для O
36	народного O
37	хозяйства O
38	проблемах O
39	, O
40	во O
41	- O
42	вторых O
43	, O
44	к O
45	началу O
46	работы O
47	над O
48	проектом O
49	у O
50	нас O
51	уже O
52	был O
53	свой O
54	фреймворк O
55	, O
56	реализующий O
57	NEAT B-Method
58	. O

# sent_id = 335
# text =   Строго говоря, нам нужно получить параллельный корпус из двух текстов.
1	Строго O
2	говоря O
3	, O
4	нам O
5	нужно O
6	получить O
7	параллельный B-Object
8	корпус I-Object
9	из O
10	двух O
11	текстов O
12	. O

# sent_id = 336
# text =   Для выравнивания воспользуемся библиотекой lingtrain-aligner, над которой я работаю около года и которая родилась из кучи скриптов на python, часть из которых еще ждет своего часа.
1	Для O
2	выравнивания B-Method
3	воспользуемся O
4	библиотекой O
5	lingtrain B-Application
6	- I-Application
7	aligner I-Application
8	, O
9	над O
10	которой O
11	я O
12	работаю O
13	около O
14	года O
15	и O
16	которая O
17	родилась O
18	из O
19	кучи O
20	скриптов O
21	на O
22	python O
23	, O
24	часть O
25	из O
26	которых O
27	еще O
28	ждет O
29	своего O
30	часа O
31	. O

# sent_id = 337
# text =   Хорошим решением мне видится регрессия на координаты строк при выравнивании батча и сдвиг окна на конец потока при выравнивании следующего.
1	Хорошим O
2	решением O
3	мне O
4	видится O
5	регрессия B-Method
6	на O
7	координаты O
8	строк O
9	при O
10	выравнивании B-Method
11	батча O
12	и O
13	сдвиг O
14	окна O
15	на O
16	конец O
17	потока O
18	при O
19	выравнивании O
20	следующего O
21	. O

# sent_id = 338
# text =   Facebook представила систему распознавания речи wav2vec-U.
1	Facebook B-Organization
2	представила O
3	систему O
4	распознавания B-Task
5	речи I-Task
6	wav2vec B-TERM
7	- O
8	U O

# sent_id = 339
# text =   Система разбивает запись на речевые единицы, которые приблизительно соответствуют отдельным звукам.
1	Система O
2	разбивает O
3	запись O
4	на O
5	речевые B-Object
6	единицы I-Object
7	, O
8	которые O
9	приблизительно O
10	соответствуют O
11	отдельным O
12	звукам O
13	. O

# sent_id = 340
# text =   Чтобы научиться распознавать слова в аудиозаписи, Facebook обучила генеративную состязательную сеть (GAN).
1	Чтобы O
2	научиться O
3	распознавать O
4	слова O
5	в O
6	аудиозаписи O
7	, O
8	Facebook B-Organization
9	обучила O
10	генеративную B-Model
11	состязательную I-Model
12	сеть I-Model
13	( O
14	GAN B-ShortName
15	) O
16	. O

# sent_id = 341
# text =   Генератор берет каждый аудиосегмент и предсказывает фонему, соответствующую звуку на языке.
1	Генератор O
2	берет O
3	каждый O
4	аудиосегмент O
5	и O
6	предсказывает O
7	фонему B-Subject
8	, O
9	соответствующую O
10	звуку O
11	на O
12	языке B-Object
13	. O

# sent_id = 342
# text =   Новая модель распознавания речи Facebook AI — это последняя разработка за несколько лет работы над моделями распознавания речи.
1	Новая O
2	модель O
3	распознавания B-Task
4	речи I-Task
5	Facebook B-Organization
6	AI I-Organization
7	— O
8	это O
9	последняя O
10	разработка O
11	за O
12	несколько O
13	лет O
14	работы O
15	над O
16	моделями O
17	распознавания O
18	речи O
19	. O

# sent_id = 343
# text =   Ее предшественниками стали wav2letter, wav2vec, Librilight, wav2vec 2.0, XLSR и wav2vec 2.0.
1	ее O
2	предшественниками O
3	стали O
4	wav2letter B-Model
5	, O
6	wav2vec B-Model
7	, O
8	Librilight B-Model
9	, O
10	wav2vec B-Model
11	2.0 I-Model
12	, O
13	XLSR B-Model
14	и O
15	wav2vec B-Model
16	2.0 I-Model
17	. O

# sent_id = 344
# text =   В полку LLM прибыло: недавно специалисты из Французского национального центра научных исследований (French National Center for Scientific Research) объявили о релизе новой большой языковой модели под названием BLOOM (расшифровывается как BigScience Large Open-science Open-access Multilingual Language Model).
1	В O
2	полку O
3	LLM B-Model
4	прибыло O
5	: O
6	недавно O
7	специалисты O
8	из O
9	Французского B-Organization
10	национального I-Organization
11	центра I-Organization
12	научных I-Organization
13	исследований I-Organization
14	( O
15	French B-Organization
16	National I-Organization
17	Center I-Organization
18	for I-Organization
19	Scientific I-Organization
20	Research I-Organization
21	) O
22	объявили O
23	о O
24	релизе O
25	новой O
26	большой O
27	языковой O
28	модели O
29	под O
30	названием O
31	BLOOM B-ShortName
32	( O
33	расшифровывается O
34	как O
35	BigScience B-Model
36	Large I-Model
37	Open I-Model
38	- I-Model
39	science I-Model
40	Open I-Model
41	- I-Model
42	access I-Model
43	Multilingual I-Model
44	Language B-Model
45	Model I-Model
46	) O
47	. O

# sent_id = 345
# text =   Большие языковые модели или LLM (Large Language Models) — это алгоритмы глубокого обучения, которые обучаются на огромных объемах данных.
1	Большие O
2	языковые O
3	модели O
4	или O
5	LLM B-ShortName
6	( O
7	Large B-Model
8	Language I-Model
9	Models I-Model
10	) O
11	— O
12	это O
13	алгоритмы O
14	глубокого O
15	обучения O
16	, O
17	которые O
18	обучаются O
19	на O
20	огромных O
21	объемах O
22	данных O
23	. O

# sent_id = 346
# text =   Их можно использовать в качестве чат-ботов, для поиска информации, модерации онлайн-контента, анализа литературы или для создания совершенно новых фрагментов текста на основе подсказок (чем занимается, например, «Порфирьевич», который способен генерировать весьма забавные короткие рассказы).
1	Их O
2	можно O
3	использовать O
4	в O
5	качестве O
6	чат B-Object
7	- I-Object
8	ботов I-Object
9	, O
10	для O
11	поиска B-Task
12	информации I-Task
13	, O
14	модерации B-Task
15	онлайн I-Task
16	- I-Task
17	контента I-Task
18	, O
19	анализа B-Task
20	литературы I-Task
21	или O
22	для O
23	создания B-Task
24	совершенно I-Task
25	новых I-Task
26	фрагментов I-Task
27	текста I-Task
28	на O
29	основе B-Subject
30	подсказок O
31	( O
32	чем O
33	занимается O
34	, O
35	например O
36	, O
37	« O
38	Порфирьевич B-Application
39	» O
40	, O
41	который O
42	способен O
43	генерировать O
44	весьма O
45	забавные O
46	короткие O
47	рассказы O
48	) O
49	. O

# sent_id = 347
# text =   Новая LLM с открытым исходным кодом в отличие от таких известных LLM, как GPT-3 от OpenAI и LaMDA от Google, BLOOM является открытой языковой моделью, а исследователи охотно делятся подробностями о тех данных, на которых она обучалась, рассказывают о проблемах в ее разработке и о том, как они оценивали производительность BLOOM.
1	Новая O
2	LLM B-ShortName
3	с O
4	открытым O
5	исходным O
6	кодом O
7	в O
8	отличие O
9	от O
10	таких O
11	известных O
12	LLM B-ShortName
13	, O
14	как O
15	GPT-3 B-Model
16	от O
17	OpenAI B-Organization
18	и O
19	LaMDA B-Model
20	от O
21	Google B-Organization
22	, O
23	BLOOM B-Model
24	является O
25	открытой O
26	языковой O
27	моделью O
28	, O
29	а O
30	исследователи O
31	охотно O
32	делятся O
33	подробностями O
34	о O
35	тех O
36	данных O
37	, O
38	на O
39	которых O
40	она O
41	обучалась O
42	, O
43	рассказывают O
44	о O
45	проблемах O
46	в O
47	ее O
48	разработке O
49	и O
50	о O
51	том O
52	, O
53	как O
54	они O
55	оценивали O
56	производительность O
57	BLOOM B-Model
58	. O

# sent_id = 348
# text =   OpenAI и Google не делились своим кодом и не делали свои модели общедоступными.
1	OpenAI B-Organization
2	и O
3	Google B-Organization
4	не O
5	делились O
6	своим O
7	кодом O
8	и O
9	не O
10	делали O
11	свои O
12	модели O
13	общедоступными O
14	. O

# sent_id = 349
# text =   И уже сейчас над BLOOM работают более тысячи исследователей-добровольцев в рамках проекта под названием BigScience, который координирует стартап Hugging Face, существующий за счет финансовой поддержки французского правительства.
1	И O
2	уже O
3	сейчас O
4	над O
5	BLOOM B-Model
6	работают O
7	более O
8	тысячи O
9	исследователей O
10	- O
11	добровольцев O
12	в O
13	рамках O
14	проекта O
15	под O
16	названием O
17	BigScience B-Project
18	, O
19	который O
20	координирует O
21	стартап O
22	Hugging B-Organization
23	Face I-Organization
24	, O
25	существующий O
26	за O
27	счет O
28	финансовой O
29	поддержки O
30	французского O
31	правительства O
32	. O

# sent_id = 350
# text =   BLOOM может обрабатывать 46 языков, включая французский, испанский, арабский, вьетнамский, китайский, индонезийский, каталанский, целых 13 языков Индии (хинди, бенгали, маратхи и ряд других) и аж 20 африканских.
1	BLOOM B-Model
2	может O
3	обрабатывать O
4	46 O
5	языков O
6	, O
7	включая O
8	французский B-Lang
9	, O
10	испанский B-Lang
11	, O
12	арабский B-Lang
13	, O
14	вьетнамский B-Lang
15	, O
16	китайский B-Lang
17	, O
18	индонезийский B-Lang
19	, O
20	каталанский B-Lang
21	, O
22	целых O
23	13 O
24	языков O
25	Индии O
26	( O
27	хинди B-Lang
28	, O
29	бенгали B-Lang
30	, O
31	маратхи B-Lang
32	и O
33	ряд O
34	других O
35	) O
36	и O
37	аж O
38	20 O
39	африканских O
40	. O

# sent_id = 351
# text =   На русском BLOOM тоже пишет, но пока довольно вяло.
1	На O
2	русском B-Lang
3	тоже O
4	пишет O
5	, O
6	но O
7	пока O
8	довольно O
9	вяло O
10	. O

# sent_id = 352
# text =   Почти треть обучающих данных была введена в модель BLOOM на английском языке: следствие того, что именно английский является наиболее часто используемым языком в интернете.
1	Почти O
2	треть O
3	обучающих O
4	данных O
5	была O
6	введена O
7	в O
8	модель O
9	BLOOM B-Model
10	на O
11	английском B-Lang
12	языке O
13	: O
14	следствие O
15	того O
16	, O
17	что O
18	именно O
19	английский B-Lang
20	является O
21	наиболее O
22	часто O
23	используемым O
24	языком O
25	в O
26	интернете O
27	. O

# sent_id = 353
# text =   В июне текущего года инженер Google Блейк Лемуан (он на фото выше) ошарашил мировую общественность заявлением, что LLM LaMDA, над которой он работал вместе с другими программистами, может обладать некоторым подобием разума.
1	В O
2	июне O
3	текущего O
4	года O
5	инженер O
6	Google B-Organization
7	Блейк B-Person
8	Лемуан I-Person
9	( O
10	он O
11	на O
12	фото O
13	выше O
14	) O
15	ошарашил O
16	мировую O
17	общественность O
18	заявлением O
19	, O
20	что O
21	LLM B-ShortName
22	LaMDA I-ShortName
23	, O
24	над O
25	которой O
26	он O
27	работал O
28	вместе O
29	с O
30	другими O
31	программистами O
32	, O
33	может O
34	обладать O
35	некоторым O
36	подобием O
37	разума O
38	. O

# sent_id = 354
# text =   Американский ученый и исследователь ИИ Гэри Маркус еще до появления в сети откровений Лемуана опубликовал на портале Scientific American материал под названием «Общий ИИ не так неизбежен, как вы думаете».
1	Американский O
2	ученый O
3	и O
4	исследователь O
5	ИИ B-ShortName
6	Гэри B-Person
7	Маркус I-Person
8	еще O
9	до O
10	появления O
11	в O
12	сети O
13	откровений O
14	Лемуана B-Person
15	опубликовал O
16	на O
17	портале O
18	Scientific B-InfoResource
19	American I-InfoResource
20	материал O
21	под O
22	названием O
23	« B-Publication
24	Общий I-Publication
25	ИИ I-Publication
26	не I-Publication
27	так I-Publication
28	неизбежен I-Publication
29	, I-Publication
30	как I-Publication
31	вы I-Publication
32	думаете I-Publication
33	» I-Publication
34	. O

# sent_id = 355
# text =   В частности, DALL-E 2 от OpenAI провалил тест на различение изображений астронавтов, едущих на лошадях, перепутав их с лошадьми, оседлавшими астронавтов.
1	В O
2	частности O
3	, O
4	DALL B-Model
5	- I-Model
6	E I-Model
7	2 I-Model
8	от O
9	OpenAI B-Organization
10	провалил O
11	тест O
12	на O
13	различение O
14	изображений O
15	астронавтов O
16	, O
17	едущих O
18	на O
19	лошадях O
20	, O
21	перепутав O
22	их O
23	с O
24	лошадьми O
25	, O
26	оседлавшими O
27	астронавтов O
28	. O

# sent_id = 356
# text =   «Сбер» представил mGPT — версию нейросети GPT-3, способную генерировать тексты на 61 языке Open source *Machine learning *Artificial Intelligence IT-companies       
1	« O
2	Сбер B-Organization
3	» O
4	представил O
5	mGPT B-Model
6	— O
7	версию O
8	нейросети O
9	GPT-3 B-Model
10	, O
11	способную O
12	генерировать O
13	тексты O
14	на O
15	61 O
16	языке O

# sent_id = 357
# text =   21 апреля 2022 года команда разработчиков SberDevices представила многоязычную версию нейросети GPT-3 под названием mGPT.
1	21 B-Data
2	апреля I-Data
3	2022 I-Data
4	года O
5	команда O
6	разработчиков O
7	SberDevices B-Organization
8	представила O
9	многоязычную O
10	версию O
11	нейросети O
12	GPT-3 B-Model
13	под O
14	названием O
15	mGPT B-Model
16	. O

# sent_id = 358
# text =   «Сбер» рассказал, что модель mGPT может использоваться как просто для генерации текста, так и для решения различных задач в области обработки естественного языка на одном из поддерживаемых языков путем дообучения или в составе ансамблей моделей.
1	« O
2	Сбер B-Organization
3	» O
4	рассказал O
5	, O
6	что O
7	модель O
8	mGPT B-Model
9	может O
10	использоваться O
11	как O
12	просто O
13	для O
14	генерации B-Task
15	текста I-Task
16	, O
17	так O
18	и O
19	для O
20	решения O
21	различных O
22	задач O
23	в O
24	области O
25	обработки B-Science
26	естественного I-Science
27	языка I-Science
28	на O
29	одном O
30	из O
31	поддерживаемых O
32	языков O
33	путем O
34	дообучения O
35	или O
36	в O
37	составе O
38	ансамблей B-Method
39	моделей I-Method
40	. O

# sent_id = 359
# text =   Разработчики уточнили, что модель mGPT показывает выдающиеся результаты на многих задачах few-shot и zero-shot learning: в этой области машинного обучения не требуется отдельно доучивать модель, достаточно сформулировать задачу текстом и привести несколько примеров, после чего mGPT научится выполнять новую задачу.
1	Разработчики O
2	уточнили O
3	, O
4	что O
5	модель O
6	mGPT B-Model
7	показывает O
8	выдающиеся O
9	результаты O
10	на O
11	многих O
12	задачах O
13	few B-Task
14	- I-Task
15	shot I-Task
16	и O
17	zero B-Task
18	- I-Task
19	shot I-Task
20	learning I-Task
21	: O
22	в O
23	этой O
24	области O
25	машинного B-Science
26	обучения I-Science
27	не O
28	требуется O
29	отдельно O
30	доучивать O
31	модель O
32	, O
33	достаточно O
34	сформулировать O
35	задачу O
36	текстом O
37	и O
38	привести O
39	несколько O
40	примеров O
41	, O
42	после O
43	чего O
44	mGPT B-Model
45	научится O
46	выполнять O
47	новую O
48	задачу O
49	. O

# sent_id = 360
# text =   Это может использоваться для того, чтобы научить автоматизированную систему отвечать на вопросы, определять эмоциональную окраску текста, извлекать из текста имена, фамилии, названия компаний и тому подобное.
1	Это O
2	может O
3	использоваться O
4	для O
5	того O
6	, O
7	чтобы O
8	научить O
9	автоматизированную O
10	систему O
11	отвечать O
12	на O
13	вопросы O
14	, O
15	определять B-Task
16	эмоциональную I-Task
17	окраску I-Task
18	текста I-Task
19	, O
20	извлекать O
21	из O
22	текста O
23	имена O
24	, O
25	фамилии O
26	, O
27	названия O
28	компаний O
29	и O
30	тому O
31	подобное O
32	. O

# sent_id = 361
# text =   «Сбер» раскрыл, что модель mGPT может также использоваться как компонент различных речевых технологий — например, для улучшения качества распознавания речи, генерации сценариев диалоговых систем и других задачах.
1	« O
2	Сбер B-Organization
3	» O
4	раскрыл O
5	, O
6	что O
7	модель O
8	mGPT B-Model
9	может O
10	также O
11	использоваться O
12	как O
13	компонент O
14	различных O
15	речевых B-Science
16	технологий I-Science
17	— O
18	например O
19	, O
20	для O
21	улучшения B-Task
22	качества I-Task
23	распознавания I-Task
24	речи I-Task
25	, O
26	генерации B-Task
27	сценариев I-Task
28	диалоговых I-Task
29	систем I-Task
30	и O
31	других O
32	задачах O
33	. O

# sent_id = 362
# text =   Полный перечень языков, доступный в модели mGPT: азербайджанский, английский, арабский, армянский, африкаанс, баскский, башкирский, белорусский, бенгали, бирманский, болгарский, бурятский, венгерский, вьетнамский, голландский, греческий, грузинский, датский, иврит, индонезийский, испанский, итальянский, йоруба, казахский, калмыцкий, киргизский, китайский, корейский, латышский, литовский, малайский, малаялам, маратхи, молдавский, монгольский, немецкий, осетинский, персидский, польский, португальский, румынский, русский, суахили, таджикский, тайский, тамильский, татарский, телугу, тувинский, турецкий, туркменский, узбекский, украинский, урду, финский, французский, хинди, чувашский, шведский, якутский, японский.
1	Полный O
2	перечень O
3	языков O
4	, O
5	доступный O
6	в O
7	модели O
8	mGPT B-Model
9	: O
10	азербайджанский B-Lang
11	, O
12	английский B-Lang
13	, O
14	арабский B-Lang
15	, O
16	армянский B-Lang
17	, O
18	африкаанс B-Lang
19	, O
20	баскский B-Lang
21	, O
22	башкирский B-Lang
23	, O
24	белорусский B-Lang
25	, O
26	бенгали B-Lang
27	, O
28	бирманский B-Lang
29	, O
30	болгарский B-Lang
31	, O
32	бурятский B-Lang
33	, O
34	венгерский B-Lang
35	, O
36	вьетнамский B-Lang
37	, O
38	голландский B-Lang
39	, O
40	греческий B-Lang
41	, O
42	грузинский B-Lang
43	, O
44	датский B-Lang
45	, O
46	иврит B-Lang
47	, O
48	индонезийский B-Lang
49	, O
50	испанский B-Lang
51	, O
52	итальянский B-Lang
53	, O
54	йоруба B-Lang
55	, O
56	казахский B-Lang
57	, O
58	калмыцкий B-Lang
59	, O
60	киргизский B-Lang
61	, O
62	китайский B-Lang
63	, O
64	корейский B-Lang
65	, O
66	латышский B-Lang
67	, O
68	литовский B-Lang
69	, O
70	малайский B-Lang
71	, O
72	малаялам B-Lang
73	, O
74	маратхи B-Lang
75	, O
76	молдавский B-Lang
77	, O
78	монгольский B-Lang
79	, O
80	немецкий B-Lang
81	, O
82	осетинский B-Lang
83	, O
84	персидский B-Lang
85	, O
86	польский B-Lang
87	, O
88	португальский B-Lang
89	, O
90	румынский B-Lang
91	, O
92	русский B-Lang
93	, O
94	суахили B-Lang
95	, O
96	таджикский B-Lang
97	, O
98	тайский B-Lang
99	, O
100	тамильский B-Lang
101	, O
102	татарский B-Lang
103	, O
104	телугу B-Lang
105	, O
106	тувинский B-Lang
107	, O
108	турецкий B-Lang
109	, O
110	туркменский B-Lang
111	, O
112	узбекский B-Lang
113	, O
114	украинский B-Lang
115	, O
116	урду B-Lang
117	, O
118	финский B-Lang
119	, O
120	французский B-Lang
121	, O
122	хинди B-Lang
123	, O
124	чувашский B-Lang
125	, O
126	шведский B-Lang
127	, O
128	якутский B-Lang
129	, O
130	японский B-Lang
131	. O

# sent_id = 363
# text =   В 2020 году «Сбер» представил русскоязычную версию нейросети GPT-3, именно она используется в двух виртуальных ассистентах семейства «Салют» от «Сбера».
1	В O
2	2020 B-Date
3	году O
4	« O
5	Сбер B-Organization
6	» O
7	представил O
8	русскоязычную B-Lang
9	версию O
10	нейросети O
11	GPT-3 B-Model
12	, O
13	именно O
14	она O
15	используется O
16	в O
17	двух O
18	виртуальных O
19	ассистентах O
20	семейства O
21	« O
22	Салют B-Application
23	» O
24	от O
25	« O
26	Сбера B-Organization
27	» O
28	. O

# sent_id = 364
# text =   Русскоязычная версия GPT-3, разработанная «Сбером», доступна на платформе SmartMarket.
1	Русскоязычная O
2	версия O
3	GPT-3 B-Model
4	, O
5	разработанная O
6	« O
7	Сбером B-Organization
8	» O
9	, O
10	доступна O
11	на O
12	платформе O
13	SmartMarket B-Application
14	. O

# sent_id = 365
# text =   В ноябре 2021 года «Сбер» обучил нейросеть ruGPT-3 автоматически писать код и назвал эту функцию JARVIS.
1	В O
2	ноябре O
3	2021 B-Date
4	года O
5	« O
6	Сбер B-Organization
7	» O
8	обучил O
9	нейросеть O
10	ruGPT-3 B-Model
11	автоматически O
12	писать O
13	код O
14	и O
15	назвал O
16	эту O
17	функцию O
18	JARVIS B-Method
19	. O

# sent_id = 366
# text =   Не заблокированы: Sber AI — на GitHub; ruDALL-E — на GitHub; Russian GPT-3 models — GitHub.
1	Не O
2	заблокированы O
3	: O
4	Sber B-Model
5	AI I-Model
6	— O
7	на O
8	GitHub B-InfoResource
9	; O
10	ruDALL B-Model
11	- I-Model
12	E I-Model
13	— O
14	на O
15	GitHub B-InfoResource
16	; O
17	Russian B-Model
18	GPT-3 I-Model
19	models I-Model
20	— O
21	GitHub B-InfoResource
22	. O

# sent_id = 367
# text =   Заблокированы: большая часть ссылок на открытом портале Open Source от разработчиков «Сбера»; SberDevices; Sberbank AI Lab; Open source software developed by Sberbank-Technology.
1	Заблокированы O
2	: O
3	большая O
4	часть O
5	ссылок O
6	на O
7	открытом O
8	портале O
9	Open B-InfoResource
10	Source B-InfoResource
11	от O
12	разработчиков O
13	« O
14	Сбера B-Organization
15	» O
16	; O
17	SberDevices B-Application
18	; O
19	Sberbank B-Organization
20	AI I-Organization
21	Lab I-Organization
22	; O
23	Open B-Application
24	source I-Application
25	software I-Application
26	developed I-Application
27	by I-Application
28	Sberbank I-Application
29	- I-Application
30	Technology I-Application
31	. O

# sent_id = 368
# text =   В данной статье мы будем использовать модель трансформера для бинарной классификации текста.
1	В O
2	данной O
3	статье O
4	мы O
5	будем O
6	использовать O
7	модель B-Model
8	трансформера I-Model
9	для O
10	бинарной B-Task
11	классификации I-Task
12	текста I-Task
13	. O

# sent_id = 369
# text =   Самая простая и популярная связка – TF-IDF + линейная модель.
1	Самая O
2	простая O
3	и O
4	популярная O
5	связка O
6	– O
7	TF B-Metric
8	- I-Metric
9	IDF I-Metric
10	+ O
11	линейная B-Model
12	модель I-Model
13	. O

# sent_id = 370
# text =   В случае с BERT можно (даже нужно) опустить препроцессинг и сразу перейти к токенизации и обучению.
1	В O
2	случае O
3	с O
4	BERT B-Model
5	можно O
6	( O
7	даже O
8	нужно O
9	) O
10	опустить O
11	препроцессинг B-Method
12	и O
13	сразу O
14	перейти O
15	к O
16	токенизации B-Method
17	и O
18	обучению O
19	. O

# sent_id = 371
# text =   Необходимо обучить модель находить обращения с жалобой на сотрудника или другими словами – бинарная классификация.
1	Необходимо O
2	обучить O
3	модель O
4	находить O
5	обращения O
6	с O
7	жалобой O
8	на O
9	сотрудника O
10	или O
11	другими O
12	словами O
13	– O
14	бинарная B-Task
15	классификация I-Task
16	. O

# sent_id = 372
# text =  Для решения описанной задачи используется модель от DeepPavlov rubert-base-cased-sentence.
1	Для O
2	решения O
3	описанной O
4	задачи O
5	используется O
6	модель O
7	от O
8	DeepPavlov B-Model
9	rubert I-Model
10	- I-Model
11	base I-Model
12	- I-Model
13	cased I-Model
14	- I-Model
15	sentence I-Model
16	. O

# sent_id = 373
# text =   На выходе мы получаем метрику f1 = 0.91 Посмотрим, как модель классифицировала данные показанные в начале статьи.
1	На O
2	выходе O
3	мы O
4	получаем O
5	метрику O
6	f1 B-Metric
7	= O
8	0.91 B-Value
9	Посмотрим O
10	, O
11	как O
12	модель O
13	классифицировала O
14	данные O
15	показанные O
16	в O
17	начале O
18	статьи O
19	. O

# sent_id = 374
# text =   Обученные модели можно найти на сайтах HuggingFace и DeepPavlov.
1	Обученные O
2	модели O
3	можно O
4	найти O
5	на O
6	сайтах O
7	HuggingFace B-InfoResource
8	и O
9	DeepPavlov B-InfoResource
10	. O

# sent_id = 375
# text =   Соответственно, мы приходим к стандартной задаче Machine Learning (ML) – «многоклассовая классификация».
1	Соответственно O
2	, O
3	мы O
4	приходим O
5	к O
6	стандартной O
7	задаче O
8	Machine B-Science
9	Learning I-Science
10	( O
11	ML B-ShortName
12	) O
13	– O
14	« O
15	многоклассовая B-Task
16	классификация I-Task
17	» O
18	. O

# sent_id = 376
# text =   В результате данного анализа решается задача — сбор сводной аналитики по организации.
1	В O
2	результате O
3	данного O
4	анализа O
5	решается O
6	задача O
7	— O
8	сбор B-Task
9	сводной I-Task
10	аналитики I-Task
11	по I-Task
12	организации I-Task
13	. O

# sent_id = 377
# text =   В случае многоклассовой классификации число классов должно быть более 2 и может достигать даже многих тысяч.
1	В O
2	случае O
3	многоклассовой B-Task
4	классификации I-Task
5	число O
6	классов B-Object
7	должно O
8	быть O
9	более O
10	2 O
11	и O
12	может O
13	достигать O
14	даже O
15	многих O
16	тысяч O
17	. O

# sent_id = 378
# text =   Во-вторых, такого разброса тематик, связанных с техническими текстами, у нас еще не было: нейросети, переиспользование контента, автоматическое тестирование документации, встраивание текста в интерфейс, мастерство технических коммуникаций, построение процессов перевода и принципы написания документов с расчетом на их последующую локализацию.
1	Во O
2	- O
3	вторых O
4	, O
5	такого O
6	разброса O
7	тематик O
8	, O
9	связанных O
10	с O
11	техническими B-Object
12	текстами I-Object
13	, O
14	у O
15	нас O
16	еще O
17	не O
18	было O
19	: O
20	нейросети O
21	, O
22	переиспользование B-Task
23	контента I-Task
24	, O
25	автоматическое B-Task
26	тестирование I-Task
27	документации I-Task
28	, O
29	встраивание B-Task
30	текста I-Task
31	в I-Task
32	интерфейс I-Task
33	, O
34	мастерство B-Task
35	технических I-Task
36	коммуникаций I-Task
37	, O
38	построение B-Task
39	процессов I-Task
40	перевода I-Task
41	и O
42	принципы B-Task
43	написания I-Task
44	документов I-Task
45	с O
46	расчетом O
47	на O
48	их O
49	последующую O
50	локализацию O
51	. O

# sent_id = 379
# text =   Зачастую суммаризация предполагает работу с большими генеративными текстовыми моделями, куда надо «положить» все отзывы.
1	Зачастую O
2	суммаризация B-Task
3	предполагает O
4	работу O
5	с O
6	большими O
7	генеративными B-Object
8	текстовыми I-Object
9	моделями I-Object
10	, O
11	куда O
12	надо O
13	« O
14	положить O
15	» O
16	все O
17	отзывы O
18	. O

# sent_id = 380
# text =   То есть какие аспекты искать; выделять эти аспекты в отзывах; оценивать тональность высказываний.
1	То O
2	есть O
3	какие O
4	аспекты O
5	искать O
6	; O
7	выделять O
8	эти O
9	аспекты O
10	в O
11	отзывах O
12	; O
13	оценивать O
14	тональность B-Subject
15	высказываний I-Subject
16	. O

# sent_id = 381
# text =   Мы начали со внутреннего инструмента Яндекса — библиотеки регулярных выражений под названием Remorph.
1	Мы O
2	начали O
3	со O
4	внутреннего O
5	инструмента O
6	Яндекса B-Organization
7	— O
8	библиотеки B-Application
9	регулярных I-Application
10	выражений I-Application
11	под O
12	названием O
13	Remorph B-Application
14	. O

# sent_id = 382
# text =   Исследователи Массачусетского технологического университета разработали систему искусственного интеллекта, которая способна переписывать устаревшие предложения в статьях «Википедии».
1	Исследователи O
2	Массачусетского B-Organization
3	технологического I-Organization
4	университета I-Organization
5	разработали O
6	систему B-Application
7	искусственного I-Application
8	интеллекта I-Application
9	, O
10	которая O
11	способна O
12	переписывать O
13	устаревшие O
14	предложения O
15	в O
16	статьях O
17	« O
18	Википедии B-InfoResource
19	» O
20	. O

# sent_id = 383
# text =   Расширение статей, серьезные переписывания или другие рутинные изменения, такие как обновление номеров, дат, имен и местоположений в настоящее время добровольно выполняются пользователями из разных стран.
1	Расширение B-Task
2	статей I-Task
3	, O
4	серьезные O
5	переписывания O
6	или O
7	другие O
8	рутинные O
9	изменения O
10	, O
11	такие O
12	как O
13	обновление O
14	номеров O
15	, O
16	дат O
17	, O
18	имен O
19	и O
20	местоположений O
21	в O
22	настоящее O
23	время O
24	добровольно O
25	выполняются O
26	пользователями O
27	из O
28	разных O
29	стран O
30	. O

# sent_id = 384
# text =   Если она видит какие-либо противоречия между этими двумя высказываниями, то использует «маску нейтральности», чтобы определить те противоречивые слова, которые нужно удалить, и те, которые обязательно нужно сохранить.
1	Если O
2	она O
3	видит O
4	какие O
5	- O
6	либо O
7	противоречия O
8	между O
9	этими O
10	двумя O
11	высказываниями O
12	, O
13	то O
14	использует O
15	« O
16	маску B-Object
17	нейтральности I-Object
18	» O
19	, O
20	чтобы O
21	определить O
22	те O
23	противоречивые O
24	слова O
25	, O
26	которые O
27	нужно O
28	удалить O
29	, O
30	и O
31	те O
32	, O
33	которые O
34	обязательно O
35	нужно O
36	сохранить O
37	. O

# sent_id = 385
# text =   Отмечается, что систему также можно использовать для дополнения наборов данных, предназначенных для обучения детекторов фейкньюс, что потенциально снижает предвзятость и повышает точность информации.
1	Отмечается O
2	, O
3	что O
4	систему O
5	также O
6	можно O
7	использовать O
8	для O
9	дополнения B-Task
10	наборов I-Task
11	данных I-Task
12	, O
13	предназначенных O
14	для O
15	обучения B-Task
16	детекторов I-Task
17	фейкньюс I-Task
18	, O
19	что O
20	потенциально O
21	снижает O
22	предвзятость O
23	и O
24	повышает O
25	точность O
26	информации O
27	. O

# sent_id = 386
# text =  Наше выработанное решение – обучить нейронную сеть, которая способна по тексту обращения автоматически распознавать заранее ранжированные по классам проблемы, извлекать сущность (номер заказа и телефон клиента) и по определённым классам сделать автоматизацию решения.
1	Наше O
2	выработанное O
3	решение O
4	– O
5	обучить O
6	нейронную B-Method
7	сеть I-Method
8	, O
9	которая O
10	способна O
11	по O
12	тексту O
13	обращения O
14	автоматически B-Task
15	распознавать I-Task
16	заранее I-Task
17	ранжированные I-Task
18	по I-Task
19	классам I-Task
20	проблемы I-Task
21	, O
22	извлекать O
23	сущность O
24	( O
25	номер O
26	заказа O
27	и O
28	телефон O
29	клиента O
30	) O
31	и O
32	по O
33	определённым O
34	классам O
35	сделать O
36	автоматизацию O
37	решения O
38	. O

# sent_id = 387
# text =   На самом деле уже существуют продвинутые и проверенные методы ее обработки, использующие нейронные сети, с распознаванием смысла и контекста – BERT (Bidirectional Encoder Representations from Transformers).
1	На O
2	самом O
3	деле O
4	уже O
5	существуют O
6	продвинутые O
7	и O
8	проверенные O
9	методы O
10	ее O
11	обработки O
12	, O
13	использующие O
14	нейронные B-Method
15	сети I-Method
16	, O
17	с O
18	распознаванием O
19	смысла O
20	и O
21	контекста O
22	– O
23	BERT B-ShortName
24	( O
25	Bidirectional B-Method
26	Encoder I-Method
27	Representations I-Method
28	from I-Method
29	Transformers I-Method
30	) O
31	. O

# sent_id = 388
# text =   Перед тем как выбрать нейронные сети, мы протестировали несколько более стандартных архитектур, случайные леса и бустинг.
1	Архитектура O
2	модели B-Object
3	Перед O
4	тем O
5	как O
6	выбрать O
7	нейронные B-Method
8	сети I-Method
9	, O
10	мы O
11	протестировали O
12	несколько O
13	более O
14	стандартных O
15	архитектур O
16	, O
17	случайные B-Method
18	леса I-Method
19	и O
20	бустинг B-Method
21	. O

# sent_id = 389
# text =   Эта модель была обучена на огромном корпусе русскоязычного текста с двумя задачами – предсказать замаскированное слово в предложениях и предсказать, если одно из предложений следует по смыслу за вторым.
1	Эта O
2	модель O
3	была O
4	обучена O
5	на O
6	огромном O
7	корпусе O
8	русскоязычного O
9	текста O
10	с O
11	двумя O
12	задачами O
13	– O
14	предсказать B-Task
15	замаскированное I-Task
16	слово I-Task
17	в I-Task
18	предложениях I-Task
19	и O
20	предсказать O
21	, O
22	если O
23	одно O
24	из O
25	предложений O
26	следует O
27	по O
28	смыслу O
29	за O
30	вторым O
31	. O

# sent_id = 390
# text =   Наша задача – дообучить эту языковую модель для нашего приложения (одна модель для классификации и одна – для извлечения сущности).
1	Наша O
2	задача O
3	– O
4	дообучить O
5	эту O
6	языковую O
7	модель O
8	для O
9	нашего O
10	приложения O
11	( O
12	одна O
13	модель B-Object
14	для O
15	классификации B-Task
16	и O
17	одна O
18	– O
19	для O
20	извлечения B-Task
21	сущности I-Task
22	) O
23	. O

# sent_id = 391
# text =   результат первой модели – точность 77%
1	результат O
2	первой O
3	модели O
4	– O
5	точность B-Metric
6	77% B-Value

# sent_id = 392
# text =   Чтобы определить, какие ещё есть потенциальные классы, мы повели так называемое тематическое моделирование, используя несколько подходов: начиная от пробалистических моделей (латентное распределение Дирихле, ARTM) и всё те же нейронные сети (BERT).
1	Чтобы O
2	определить O
3	, O
4	какие O
5	ещё O
6	есть O
7	потенциальные O
8	классы O
9	, O
10	мы O
11	повели O
12	так O
13	называемое O
14	тематическое B-Method
15	моделирование I-Method
16	, O
17	используя O
18	несколько O
19	подходов O
20	: O
21	начиная O
22	от O
23	пробалистических B-Model
24	моделей B-Model
25	( O
26	латентное B-Method
27	распределение I-Method
28	Дирихле I-Method
29	, O
30	ARTM B-ShortName
31	) O
32	и O
33	всё O
34	те O
35	же O
36	нейронные B-Method
37	сети I-Method
38	( O
39	BERT B-Model
40	) O
41	. O

# sent_id = 393
# text =   Теперь нам нужно было использовать некоторые технические способы, чтобы сделать максимально высоким качество модели, которая на новых классах давала точность 72%.
1	Теперь O
2	нам O
3	нужно O
4	было O
5	использовать O
6	некоторые O
7	технические O
8	способы O
9	, O
10	чтобы O
11	сделать O
12	максимально O
13	высоким O
14	качество O
15	модели O
16	, O
17	которая O
18	на O
19	новых O
20	классах O
21	давала O
22	точность B-Metric
23	72 B-Value
24	% I-Value
25	. O

# sent_id = 394
# text =   Второе, мы стандартно провели экстенсивный тюнинг гиперпараметров и изменили нашу метрику с точности на F1, чтобы ставить больше акцента на точность по каждому классу, так как общая точность предвзято относится к доминирующим классам.
1	Второе O
2	, O
3	мы O
4	стандартно O
5	провели O
6	экстенсивный B-Method
7	тюнинг I-Method
8	гиперпараметров I-Method
9	и O
10	изменили O
11	нашу O
12	метрику O
13	с O
14	точности B-Metric
15	на O
16	F1 B-Metric
17	, O
18	чтобы O
19	ставить O
20	больше O
21	акцента O
22	на O
23	точность B-Metric
24	по O
25	каждому O
26	классу O
27	, O
28	так O
29	как O
30	общая O
31	точность B-Metric
32	предвзято O
33	относится O
34	к O
35	доминирующим O
36	классам O
37	. O

# sent_id = 395
# text =   Изменение оптимизирующей метрики на F1 позволило алгоритму обучения дольше обучаться, так как почти на каждом этапе происходило улучшение по F1, когда метрика была точность, мы достигали плато гораздо быстрее.
1	Изменение O
2	оптимизирующей O
3	метрики O
4	на O
5	F1 B-Metric
6	позволило O
7	алгоритму O
8	обучения O
9	дольше O
10	обучаться O
11	, O
12	так O
13	как O
14	почти O
15	на O
16	каждом O
17	этапе O
18	происходило O
19	улучшение O
20	по O
21	F1 B-Metric
22	, O
23	когда O
24	метрика O
25	была O
26	точность B-Metric
27	, O
28	мы O
29	достигали O
30	плато O
31	гораздо O
32	быстрее O
33	. O

# sent_id = 396
# text =   Изначально на этапе MVP (minimum viable product) мы применяли регулярные выражения для извлечения сущности.
1	Изначально O
2	на O
3	этапе O
4	MVP B-ShortName
5	( O
6	minimum B-Object
7	viable I-Object
8	product I-Object
9	) O
10	мы O
11	применяли O
12	регулярные B-Object
13	выражения I-Object
14	для O
15	извлечения B-Task
16	сущности I-Task
17	. O

# sent_id = 397
# text =   Протестировав поведение модели на продовских данных, мы обнаружили, что точность извлечения была около 50%.
1	Протестировав O
2	поведение O
3	модели O
4	на O
5	продовских O
6	данных O
7	, O
8	мы O
9	обнаружили O
10	, O
11	что O
12	точность B-Metric
13	извлечения O
14	была O
15	около O
16	50 B-Value
17	% I-Value
18	. O

# sent_id = 398
# text =   Мы поняли, что даже извлечение сущности зависит от контекста, и решили использовать BERT.
1	Мы O
2	поняли O
3	, O
4	что O
5	даже O
6	извлечение B-Task
7	сущности I-Task
8	зависит O
9	от O
10	контекста O
11	, O
12	и O
13	решили O
14	использовать O
15	BERT B-Model
16	. O

# sent_id = 399
# text =   На вход необходимо представить размеченные данные с маркировкой BIO (beginning, intermediate, O – пустота).
1	На O
2	вход O
3	необходимо O
4	представить O
5	размеченные O
6	данные O
7	с O
8	маркировкой B-Object
9	BIO B-ShortName
10	( O
11	beginning O
12	, O
13	intermediate O
14	, O
15	O O
16	– O
17	пустота O
18	) O
19	. O

# sent_id = 400
# text =   Мы производили разметку 800 обращений на DataTurcks: Точность подхода BERT – 94% на этапе обучения, она валидирована на тестовых данных.
1	Мы O
2	производили O
3	разметку B-Method
4	800 O
5	обращений O
6	на O
7	DataTurcks B-Dataset
8	: O
9	Точность B-Metric
10	подхода O
11	BERT O
12	– O
13	94 B-Value
14	% I-Value
15	на O
16	этапе O
17	обучения O
18	, O
19	она O
20	валидирована O
21	на O
22	тестовых O
23	данных O
24	. O

# sent_id = 401
# text =   Это постобработка увеличила точность до 98%.
1	Это O
2	постобработка O
3	увеличила O
4	точность B-Metric
5	до O
6	98% B-Value

# sent_id = 402
# text =   В иностранной литературе можно встретить термин Continuous Learning (CL), который объединяет различные методы использования новых данных для поддержания эффективности моделей.
1	В O
2	иностранной O
3	литературе O
4	можно O
5	встретить O
6	термин O
7	Continuous B-Method
8	Learning I-Method
9	( O
10	CL B-ShortName
11	) O
12	, O
13	который O
14	объединяет O
15	различные O
16	методы O
17	использования O
18	новых O
19	данных O
20	для O
21	поддержания O
22	эффективности O
23	моделей O
24	. O

# sent_id = 403
# text =   Методы CL были положены в основу пайплайна переобучения.
1	Методы O
2	CL B-ShortName
3	были O
4	положены O
5	в O
6	основу O
7	пайплайна O
8	переобучения O
9	. O

# sent_id = 404
# text =   Для решения этой проблемы требуется архитектура, которая позволяет GPT-3 анализировать содержание письма и оценивать, какая информация актуальна для ответа.
1	Для O
2	решения O
3	этой O
4	проблемы O
5	требуется O
6	архитектура O
7	, O
8	которая O
9	позволяет O
10	GPT-3 B-Model
11	анализировать B-Task
12	содержание I-Task
13	письма I-Task
14	и O
15	оценивать B-Task
16	, I-Task
17	какая I-Task
18	информация I-Task
19	актуальна I-Task
20	для O
21	ответа O
22	. O

# sent_id = 405
# text =   OpenAI представила модель машинного обучения GPT-3, обученную на 175 млрд параметров, в июне 2020 года.
1	OpenAI B-Organization
2	представила O
3	модель O
4	машинного O
5	обучения O
6	GPT-3 B-Model
7	, O
8	обученную O
9	на O
10	175 O
11	млрд O
12	параметров O
13	, O
14	в O
15	июне B-Date
16	2020 I-Date
17	года I-Date
18	. O

# sent_id = 406
# text =   В отличие от предшественников GPT-2 и GPT-1 ее исходный код или обучающий набор данных решили не открывать.
1	В O
2	отличие O
3	от O
4	предшественников O
5	GPT-2 B-Model
6	и O
7	GPT-1 B-Model
8	ее O
9	исходный O
10	код O
11	или O
12	обучающий O
13	набор O
14	данных O
15	решили O
16	не O
17	открывать O
18	. O

# sent_id = 407
# text =   Модель уже попытались применить в медицинской сфере для общения с пациентами, но результаты эксперимента оказались неутешительными.
1	Модель O
2	уже O
3	попытались O
4	применить O
5	в O
6	медицинской B-Science
7	сфере I-Science
8	для O
9	общения O
10	с O
11	пациентами O
12	, O
13	но O
14	результаты O
15	эксперимента O
16	оказались O
17	неутешительными O
18	. O

# sent_id = 408
# text =  Между тем создатели проекта GPT-Neo от EleutherAI решили воссоздать аналог GPT-3, но с открытым исходным кодом.
1	Между O
2	тем O
3	создатели O
4	проекта O
5	GPT B-Project
6	- B-Project
7	Neo B-Project
8	от O
9	EleutherAI B-Organization
10	решили O
11	воссоздать O
12	аналог O
13	GPT-3 B-Model
14	, O
15	но O
16	с O
17	открытым O
18	исходным O
19	кодом O
20	. O

# sent_id = 409
# text =   Однако только сейчас мы немного приблизились к сюжетам фантастических фильмов: можем попросить Алису убавить громкость, Google Assistant — заказать такси или Siri — завести будильник.
1	Однако O
2	только O
3	сейчас O
4	мы O
5	немного O
6	приблизились O
7	к O
8	сюжетам O
9	фантастических O
10	фильмов O
11	: O
12	можем O
13	попросить O
14	Алису B-Application
15	убавить B-Task
16	громкость I-Task
17	, O
18	Google B-Application
19	Assistant I-Application
20	— O
21	заказать B-Task
22	такси I-Task
23	или O
24	Siri B-Application
25	— O
26	завести B-Task
27	будильник I-Task
28	. O

# sent_id = 410
# text =   Технологии языкового процессинга востребованы в разработках, связанных с построением искусственного интеллекта: в поисковых системах, для извлечения фактов, оценки тональности текста, машинного перевода и диалога.
1	Технологии B-Method
2	языкового I-Method
3	процессинга I-Method
4	востребованы O
5	в O
6	разработках O
7	, O
8	связанных O
9	с O
10	построением O
11	искусственного O
12	интеллекта O
13	: O
14	в O
15	поисковых B-Application
16	системах I-Application
17	, O
18	для O
19	извлечения B-Task
20	фактов I-Task
21	, O
22	оценки B-Task
23	тональности I-Task
24	текста I-Task
25	, O
26	машинного B-Science
27	перевода I-Science
28	и O
29	диалога O
30	. O

# sent_id = 411
# text =   Первые разговоры об обработке естественного языка компьютером начались еще в 30-е годы XX-го века с философских рассуждений Айера — он предлагал отличать разумного человека от глупой машины с помощью эмпирического теста.
1	Первые O
2	разговоры O
3	об O
4	обработке B-Task
5	естественного I-Task
6	языка I-Task
7	компьютером O
8	начались O
9	еще O
10	в O
11	30-е B-Date
12	годы I-Date
13	XX I-Date
14	- I-Date
15	го I-Date
16	века I-Date
17	с O
18	философских O
19	рассуждений O
20	Айера B-Person
21	— O
22	он O
23	предлагал O
24	отличать O
25	разумного O
26	человека O
27	от O
28	глупой O
29	машины O
30	с O
31	помощью O
32	эмпирического O
33	теста O
34	. O

# sent_id = 412
# text =   В 1950 году Алан Тьюринг в философском журнале Mind предложил такой тест, где судья должен определить, с кем он ведет диалог: с человеком или компьютером.
1	В O
2	1950 B-Date
3	году O
4	Алан B-Person
5	Тьюринг I-Person
6	в O
7	философском O
8	журнале O
9	Mind B-Publication
10	предложил O
11	такой O
12	тест B-Object
13	, O
14	где O
15	судья O
16	должен O
17	определить O
18	, O
19	с O
20	кем O
21	он O
22	ведет O
23	диалог O
24	: O
25	с O
26	человеком O
27	или O
28	компьютером O
29	. O

# sent_id = 413
# text =   В 1954 году Джорджтаунский университет совместно с компанией IBM продемонстрировали программу машинного перевода с русского на английский, которая работала на базе словаря из 250 слов и набора из 6 грамматических правил.
1	В O
2	1954 B-Date
3	году O
4	Джорджтаунский B-Organization
5	университет I-Organization
6	совместно O
7	с O
8	компанией O
9	IBM B-TERM
10	продемонстрировали O
11	программу B-Application
12	машинного I-Application
13	перевода I-Application
14	с O
15	русского B-Lang
16	на O
17	английский B-Lang
18	, O
19	которая O
20	работала O
21	на O
22	базе O
23	словаря O
24	из O
25	250 O
26	слов O
27	и O
28	набора O
29	из O
30	6 O
31	грамматических O
32	правил O
33	. O

# sent_id = 414
# text =   Параллельно с попытками научить компьютер переводить текст, ученые и целые университеты думали над созданием робота, способного имитировать речевое поведение человека.
1	Параллельно O
2	с O
3	попытками O
4	научить O
5	компьютер O
6	переводить O
7	текст O
8	, O
9	ученые O
10	и O
11	целые O
12	университеты O
13	думали O
14	над O
15	созданием O
16	робота O
17	, O
18	способного O
19	имитировать B-Task
20	речевое I-Task
21	поведение I-Task
22	человека I-Task
23	. O

# sent_id = 415
# text =   Первой успешной реализацией чат-бота стал виртуальный собеседник ELIZA, написанный в 1966 году Джозефом Вейценбаумом.
1	Первой O
2	успешной O
3	реализацией O
4	чат O
5	- O
6	бота O
7	стал O
8	виртуальный B-Result
9	собеседник I-Result
10	ELIZA B-Result
11	, O
12	написанный O
13	в O
14	1966 B-Date
15	году I-Date
16	Джозефом B-Person
17	Вейценбаумом I-Person
18	. O

# sent_id = 416
# text =   Элиза пародировала поведение психотерапевта, выделяя значимые слова из фразы собеседника и задавая встречный вопрос.
1	Элиза B-Result
2	пародировала O
3	поведение O
4	психотерапевта O
5	, O
6	выделяя O
7	значимые O
8	слова O
9	из O
10	фразы O
11	собеседника O
12	и O
13	задавая O
14	встречный O
15	вопрос O
16	. O

# sent_id = 417
# text =   Можно считать, что это был первый чат-бот, построенный на правилах (rule-based bot), и он положил начало целому классу таких систем.
1	Можно O
2	считать O
3	, O
4	что O
5	это O
6	был O
7	первый O
8	чат O
9	- O
10	бот O
11	, O
12	построенный O
13	на O
14	правилах O
15	( O
16	rule B-Application
17	- I-Application
18	based I-Application
19	bot I-Application
20	) O
21	, O
22	и O
23	он O
24	положил O
25	начало O
26	целому O
27	классу O
28	таких O
29	систем O
30	. O

# sent_id = 418
# text =   Без Элизы не появились бы такие программы-собеседники, как Cleverbot, WeChat Xiaoice, Eugene Goostman — формально прошедший тест Тьюринга в 2014 году, — и даже Siri, Jarvis и Alexa.
1	Без O
2	Элизы B-Result
3	не O
4	появились O
5	бы O
6	такие O
7	программы O
8	- O
9	собеседники O
10	, O
11	как O
12	Cleverbot B-Result
13	, O
14	WeChat B-Result
15	Xiaoice I-Result
16	, O
17	Eugene B-Result
18	Goostman I-Result
19	— O
20	формально O
21	прошедший O
22	тест B-Method
23	Тьюринга I-Method
24	в O
25	2014 B-Date
26	году O
27	, O
28	— O
29	и O
30	даже O
31	Siri B-Application
32	, O
33	Jarvis B-Application
34	и O
35	Alexa B-Application
36	. O

# sent_id = 419
# text =   В 1968 году Терри Виноградом на языке LISP была разработана программа SHRDLU.
1	В O
2	1968 B-Date
3	году O
4	Терри B-Person
5	Виноградом I-Person
6	на O
7	языке O
8	LISP B-Lang
9	была O
10	разработана O
11	программа O
12	SHRDLU B-Result
13	. O

# sent_id = 420
# text =   Следующим шагом в развитии чат-ботов стала программа A.L.I.C.E., для которой Ричард Уоллес разработал специальный язык разметки — AIML (англ. Artificial Intelligence Markup Language).
1	Следующим O
2	шагом O
3	в O
4	развитии O
5	чат O
6	- O
7	ботов O
8	стала O
9	программа O
10	A.L.I.C.E. B-Application
11	, O
12	для O
13	которой O
14	Ричард B-Person
15	Уоллес I-Person
16	разработал O
17	специальный O
18	язык O
19	разметки B-Method
20	— O
21	AIML B-ShortName
22	( O
23	англ O
24	. O
25	Artificial B-Lang
26	Intelligence I-Lang
27	Markup I-Lang
28	Language I-Lang
29	) O
30	. O

# sent_id = 421
# text =   Разговоры о нейронных сетях и глубоком обучении ходили уже в 90-е годы, а первый нейрокомпьютер «Марк-1» появился вообще в 1958 году.
1	Разговоры O
2	о O
3	нейронных B-Method
4	сетях I-Method
5	и O
6	глубоком O
7	обучении O
8	ходили O
9	уже O
10	в O
11	90-е O
12	годы O
13	, O
14	а O
15	первый O
16	нейрокомпьютер B-Object
17	« O
18	Марк-1 B-Application
19	» O
20	появился O
21	вообще O
22	в O
23	1958 B-Date
24	году I-Date
25	. O

# sent_id = 422
# text =   1970 г. Машинный перевод на основе правил (англ. RBMT) был первой попыткой научить машину переводить.
1	1970 O
2	г. O
3	Машинный B-Science
4	перевод I-Science
5	на I-Science
6	основе I-Science
7	правил I-Science
8	( O
9	англ O
10	RBMT B-ShortName
11	. O
12	) O
13	был O
14	первой O
15	попыткой O
16	научить O
17	машину O
18	переводить O
19	. O

# sent_id = 423
# text =   1984 г. Машинный перевод на основе примеров (англ. EBMT) был способен переводить даже совсем не похожие друг на друга языки, где задавать какие-то правила было бесполезно.
1	1984 B-TERM
2	г. I-TERM
3	Машинный B-Science
4	перевод I-Science
5	на I-Science
6	основе I-Science
7	примеров I-Science
8	( O
9	англ O
10	. O
11	EBMT B-ShortName
12	) O
13	был O
14	способен O
15	переводить O
16	даже O
17	совсем O
18	не O
19	похожие O
20	друг O
21	на O
22	друга O
23	языки O
24	, O
25	где O
26	задавать O
27	какие O
28	- O
29	то O
30	правила O
31	было O
32	бесполезно O
33	. O

# sent_id = 424
# text =   1990 г. Статистический машинный перевод (англ. SMT) в эпоху развития интернета позволил использовать не только готовые языковые корпуса, но даже книги и вольно переведенные статьи.
1	1990 B-Date
2	г. O
3	Статистический B-Science
4	машинный I-Science
5	перевод I-Science
6	( O
7	англ I-TERM
8	SMT B-ShortName
9	) O
10	в O
11	эпоху O
12	развития O
13	интернета O
14	позволил O
15	использовать O
16	не O
17	только O
18	готовые O
19	языковые B-Object
20	корпуса I-Object
21	, O
22	но O
23	даже O
24	книги O
25	и O
26	вольно O
27	переведенные O
28	статьи O
29	. O

# sent_id = 425
# text =   Статистические методы и сейчас активно используются в языковом процессинге.
1	Статистические B-Method
2	методы I-Method
3	и O
4	сейчас O
5	активно O
6	используются O
7	в O
8	языковом B-Method
9	процессинге I-Method
10	. O

# sent_id = 426
# text =   По мере развития обработки естественного языка множество задач решалось классическими статистическими методами и множеством правил, однако проблему нечеткости и неоднозначности в языке это не решало.
1	По O
2	мере O
3	развития O
4	обработки B-Task
5	естественного I-Task
6	языка I-Task
7	множество O
8	задач O
9	решалось O
10	классическими O
11	статистическими B-Method
12	методами I-Method
13	и O
14	множеством O
15	правил O
16	, O
17	однако O
18	проблему B-Task
19	нечеткости I-Task
20	и I-Task
21	неоднозначности I-Task
22	в O
23	языке O
24	это O
25	не O
26	решало O
27	. O

# sent_id = 427
# text =   Так родился статистический метод анализа текста word2vec (англ. Word to vector).
1	Так O
2	родился O
3	статистический B-Method
4	метод I-Method
5	анализа I-Method
6	текста I-Method
7	word2vec B-Result
8	( O
9	англ O
10	. O
11	Word B-Result
12	to I-Result
13	vector I-Result
14	) O
15	. O

# sent_id = 428
# text =   Под эти критерии отлично подходит рекуррентная нейронная сеть (RNN), однако по мере увеличения расстояния между связанными частями текста необходимо увеличивать и размер RNN, из-за чего падает качество обработки информации.
1	Под O
2	эти O
3	критерии O
4	отлично O
5	подходит O
6	рекуррентная B-Network
7	нейронная I-Network
8	сеть I-Network
9	( O
10	RNN B-ShortName
11	) O
12	, O
13	однако O
14	по O
15	мере O
16	увеличения O
17	расстояния O
18	между O
19	связанными O
20	частями O
21	текста O
22	необходимо O
23	увеличивать O
24	и O
25	размер O
26	RNN B-ShortName
27	, O
28	из O
29	- O
30	за O
31	чего O
32	падает O
33	качество O
34	обработки O
35	информации O
36	. O

# sent_id = 429
# text =   Эту проблему решает сеть LSTM (англ. Long short-term memory).
1	Эту O
2	проблему O
3	решает O
4	сеть O
5	LSTM B-ShortName
6	( O
7	англ O
8	Long B-Method
9	short I-Method
10	- I-Method
11	term I-Method
12	memory I-Method
13	) O
14	. O

# sent_id = 430
# text =   Если говорить о языке Python, который часто используется для анализа данных, то это NLTK и Spacy.
1	Если O
2	говорить O
3	о O
4	языке O
5	Python B-Environment
6	, O
7	который O
8	часто O
9	используется O
10	для O
11	анализа B-Method
12	данных I-Method
13	, O
14	то O
15	это O
16	NLTK B-Application
17	и O
18	Spacy B-Application
19	. O

# sent_id = 431
# text =   Крупные компании также принимают участие в разработке библиотек для NLP, как например NLP Architect от Intel или PyTorch от исследователей из Facebook и Uber.
1	Крупные O
2	компании O
3	также O
4	принимают O
5	участие O
6	в O
7	разработке O
8	библиотек O
9	для O
10	NLP B-ShortName
11	, O
12	как O
13	например O
14	NLP B-Application
15	Architect I-Application
16	от O
17	Intel B-Organization
18	или O
19	PyTorch B-Application
20	от O
21	исследователей O
22	из O
23	Facebook B-Organization
24	и O
25	Uber B-Organization
26	. O

# sent_id = 432
# text =   Направление b2c не единственное, где можно применять чат-ботов.
1	Направление O
2	b2c B-ShortName
3	не O
4	единственное O
5	, O
6	где O
7	можно O
8	применять O
9	чат O
10	- O
11	ботов O
12	. O

# sent_id = 433
# text =   Участникам предлагалось определить потенциальные заболевания коров по реальным жалобам людей из открытых источников, а также научиться выделять из текстов симптомы заболеваний (NER - Named Entity Recognition).
1	Участникам O
2	предлагалось O
3	определить O
4	потенциальные O
5	заболевания O
6	коров O
7	по O
8	реальным O
9	жалобам O
10	людей O
11	из O
12	открытых O
13	источников O
14	, O
15	а O
16	также O
17	научиться O
18	выделять O
19	из O
20	текстов O
21	симптомы O
22	заболеваний O
23	( O
24	NER B-ShortName
25	- O
26	Named B-Science
27	Entity I-Science
28	Recognition I-Science
29	) O
30	. O

# sent_id = 434
# text =   Эта статья будет интересна не только тем, кто специализируется в NLP (Natural Language Processing), но и начинающим исследователям данных.
1	Эта O
2	статья O
3	будет O
4	интересна O
5	не O
6	только O
7	тем O
8	, O
9	кто O
10	специализируется O
11	в O
12	NLP B-ShortName
13	( O
14	Natural B-Science
15	Language I-Science
16	Processing I-Science
17	) O
18	, O
19	но O
20	и O
21	начинающим O
22	исследователям O
23	данных O
24	. O

# sent_id = 435
# text =   Спаны - это участки текста, которые содержат в себе определенный смысл.
1	Спаны B-Object
2	  O
3	- O
4	  O
5	это O
6	участки O
7	текста B-Object
8	, O
9	которые O
10	содержат O
11	в O
12	себе O
13	определенный O
14	смысл O
15	. O

# sent_id = 436
# text =   Программа для разметки YEDDA и процесс разметки.
1	Программа O
2	для O
3	разметки B-Method
4	YEDDA B-Application
5	и O
6	процесс O
7	разметки O
8	. O

# sent_id = 437
# text =   Так как задача является составной, то и метрика состояла из двух компонентов с весом 0.8 для задачи классификации и 0.2 для задачи NER.
1	Так O
2	как O
3	задача O
4	является O
5	составной O
6	, O
7	то O
8	и O
9	метрика O
10	состояла O
11	из O
12	двух O
13	компонентов O
14	с O
15	весом B-Metric
16	0.8 B-Value
17	для O
18	задачи O
19	классификации B-Task
20	и O
21	0.2 B-Value
22	для O
23	задачи O
24	NER B-Science
25	. O

# sent_id = 438
# text =   В задаче классификации использовался logloss, вычисляемый как среднее значение метрики sklearn.metrics.log_loss по классам болезней.
1	В O
2	задаче O
3	классификации B-Task
4	использовался O
5	logloss B-Method
6	, O
7	вычисляемый O
8	как O
9	среднее O
10	значение O
11	метрики O
12	sklearn.metrics.log_loss B-Metric
13	по O
14	классам O
15	болезней O
16	. O

# sent_id = 439
# text =  В задаче NER использовался span-based F1-score, рассчитываемый следующим образом: для каждого текста берутся предсказанные индексы начала и конца размеченных признаков болезни, по ним выделяются из текста токены (отдельные слова, разделенные пробелом) и сравниваются с истинной (экспертной) разметкой.
1	В O
2	задаче O
3	NER B-ShortName
4	использовался O
5	span B-Metric
6	- I-Metric
7	based I-Metric
8	F1-score I-Metric
9	, O
10	рассчитываемый O
11	следующим O
12	образом O
13	: O
14	для O
15	каждого O
16	текста B-Object
17	берутся O
18	предсказанные O
19	индексы B-Object
20	начала O
21	и O
22	конца O
23	размеченных O
24	признаков O
25	болезни O
26	, O
27	по O
28	ним O
29	выделяются O
30	из O
31	текста O
32	токены B-Subject
33	( O
34	отдельные O
35	слова B-Subject
36	, O
37	разделенные O
38	пробелом O
39	) O
40	и O
41	сравниваются O
42	с O
43	истинной O
44	( O
45	экспертной O
46	) O
47	разметкой B-Method
48	. O

# sent_id = 440
# text =   Код для подсчета метрики span-based F1-score.
1	Код O
2	для O
3	подсчета O
4	метрики O
5	span B-Metric
6	- I-Metric
7	based I-Metric
8	F1-score I-Metric

# sent_id = 441
# text =   Этим решением стало использование классификатора CatBoost, который прямо из коробки может обрабатывать текстовые фичи.
1	Этим O
2	решением O
3	стало O
4	использование O
5	классификатора O
6	CatBoost B-Method
7	, O
8	который O
9	прямо O
10	из O
11	коробки O
12	может O
13	обрабатывать O
14	текстовые O
15	фичи O
16	. O

# sent_id = 442
# text =   Решение для задачи распознавания симптомов мы давать не стали, чтобы участники Data Science чемпионата могли покреативить.
1	Решение O
2	для O
3	задачи B-Task
4	распознавания I-Task
5	симптомов I-Task
6	мы O
7	давать O
8	не O
9	стали O
10	, O
11	чтобы O
12	участники O
13	Data B-Science
14	Science I-Science
15	чемпионата O
16	могли O
17	покреативить O
18	. O

# sent_id = 443
# text =   Во-первых, конкретно для этого соревнования наиболее эффективный подход - это доразметка спанов тренировочных данных для задачи NER.
1	Во O
2	- O
3	первых O
4	, O
5	конкретно O
6	для O
7	этого O
8	соревнования O
9	наиболее O
10	эффективный O
11	подход O
12	- O
13	это O
14	доразметка B-Method
15	спанов I-Method
16	тренировочных I-Method
17	данных I-Method
18	для O
19	задачи O
20	NER B-Task
21	. O

# sent_id = 444
# text =   Во-вторых, участники использовали базовые подходы для NLP-задач: удаление стоп-слов и знаков пунктуации, приведение к нижнему регистру, стемминг и лемматизация.
1	Во O
2	- O
3	вторых O
4	, O
5	участники O
6	использовали O
7	базовые O
8	подходы O
9	для O
10	NLP B-ShortName
11	- O
12	задач O
13	: O
14	удаление O
15	стоп O
16	- O
17	слов O
18	и O
19	знаков O
20	пунктуации O
21	, O
22	приведение O
23	к O
24	нижнему O
25	регистру O
26	, O
27	стемминг B-Method
28	и O
29	лемматизация B-Method
30	. O

# sent_id = 445
# text =  Более же продвинутым подходом является аугментация данных.
1	Более O
2	же O
3	продвинутым O
4	подходом O
5	является O
6	аугментация B-Method
7	данных I-Method
8	. O

# sent_id = 446
# text =   Один из возможных способов аугментации текста - перифраз текста.
1	Один O
2	из O
3	возможных O
4	способов O
5	аугментации B-Task
6	текста I-Task
7	- O
8	перифраз B-Method
9	текста I-Method
10	. O

# sent_id = 447
# text =   Примером данного решения является использование парафрайзера на основе “rut5-base-paraphraser” из библиотеки huggingface.
1	Примером O
2	данного O
3	решения O
4	является O
5	использование O
6	парафрайзера B-Object
7	на O
8	основе O
9	“ O
10	rut5-base B-Model
11	- I-Model
12	paraphraser I-Model
13	” O
14	из O
15	библиотеки O
16	huggingface B-Application
17	. O

# sent_id = 448
# text =   Реализуется данный метод аналогично с предыдущим, как модель можно использовать “LaBSE-en-ru”.
1	Реализуется O
2	данный O
3	метод O
4	аналогично O
5	с O
6	предыдущим O
7	, O
8	как O
9	модель O
10	можно O
11	использовать O
12	“ O
13	LaBSE B-Model
14	- I-Model
15	en I-Model
16	- I-Model
17	ru I-Model
18	” O
19	. O

# sent_id = 449
# text =   Сначала решается задача выделения симптомов (NER), после чего в текстах убираются все слова, не являющиеся симптомами.
1	Сначала O
2	решается O
3	задача B-Task
4	выделения I-Task
5	симптомов I-Task
6	( O
7	NER B-ShortName
8	) O
9	, O
10	после O
11	чего O
12	в O
13	текстах B-Object
14	убираются O
15	все O
16	слова B-Subject
17	, O
18	не O
19	являющиеся O
20	симптомами O
21	. O

# sent_id = 450
# text =   Базовым вариантом эмбеддингов является TF-IDF, который зависит от частоты употребления слова в документе.
1	Базовым O
2	вариантом O
3	эмбеддингов B-TERM
4	является O
5	TF B-Metric
6	- I-Metric
7	IDF I-Metric
8	, O
9	который O
10	зависит O
11	от O
12	частоты B-Metric
13	употребления I-Metric
14	слова I-Metric
15	в O
16	документе O
17	. O

# sent_id = 451
# text =   И чтобы его улучшить, можно использовать эмбеддинги предобученных моделей, таких как Word2Vec, FastText и тд.
1	И O
2	чтобы O
3	его O
4	улучшить O
5	, O
6	можно O
7	использовать O
8	эмбеддинги B-Object
9	предобученных I-Object
10	моделей I-Object
11	, O
12	таких O
13	как O
14	Word2Vec B-Application
15	, O
16	FastText B-Application
17	и O
18	тд O
19	. O

# sent_id = 452
# text =   В частности, в одном из лучших решений использовался необычный FastText, предобученный на корпусе текстов RuDReC, который содержит отзывы потребителей на русском языке о фармацевтической продукции.
1	В O
2	частности O
3	, O
4	в O
5	одном O
6	из O
7	лучших O
8	решений O
9	использовался O
10	необычный O
11	FastText B-Model
12	, O
13	предобученный O
14	на O
15	корпусе O
16	текстов O
17	RuDReC B-Dataset
18	, O
19	который O
20	содержит O
21	отзывы O
22	потребителей O
23	на O
24	русском B-Lang
25	языке O
26	о O
27	фармацевтической O
28	продукции O
29	. O

# sent_id = 453
# text =   Напомним, что алгоритм работы с трансформерами можно представить следующим образом: сначала тексты преобразовываются токенизатором, далее обучается модель-трансформер.
1	Напомним O
2	, O
3	что O
4	алгоритм O
5	работы O
6	с O
7	трансформерами B-Object
8	можно O
9	представить O
10	следующим O
11	образом O
12	: O
13	сначала O
14	тексты O
15	преобразовываются O
16	токенизатором B-Method
17	, O
18	далее O
19	обучается O
20	модель B-Model
21	- I-Model
22	трансформер I-Model
23	. O

# sent_id = 454
# text =   Если же говорить о выборе моделей, то наилучшие результаты были получены следующими из них: RuBERT-base, RuBERT-large, LaBSE-en-ru.
1	Если O
2	же O
3	говорить O
4	о O
5	выборе O
6	моделей O
7	, O
8	то O
9	наилучшие O
10	результаты O
11	были O
12	получены O
13	следующими O
14	из O
15	них O
16	: O
17	RuBERT B-Model
18	- I-Model
19	base I-Model
20	, O
21	RuBERT B-Model
22	- I-Model
23	large I-Model
24	, O
25	LaBSE B-Model
26	- I-Model
27	en I-Model
28	- I-Model
29	ru I-Model
30	. O

# sent_id = 455
# text =   Предположим, что вы и так слышали о моделях семейства BERT (в предыдущей статье мы описывали, как применяем BERT в других задачах), а вот LaBSE - выбор совершенно неочевидный.
1	Предположим O
2	, O
3	что O
4	вы O
5	и O
6	так O
7	слышали O
8	о O
9	моделях O
10	семейства O
11	BERT B-Model
12	( O
13	в O
14	предыдущей O
15	статье O
16	мы O
17	описывали O
18	, O
19	как O
20	применяем O
21	BERT B-Model
22	в O
23	других O
24	задачах O
25	) O
26	, O
27	а O
28	вот O
29	LaBSE B-Model
30	- O
31	выбор O
32	совершенно O
33	неочевидный O
34	. O

# sent_id = 456
# text = Далее слова в тестовом наборе текстов также приводятся к векторам и сравниваются со словами из тренировочной разметки при помощи косинусной близости.
1	Далее O
2	слова O
3	в O
4	тестовом O
5	наборе O
6	текстов O
7	также O
8	приводятся O
9	к O
10	векторам O
11	и O
12	сравниваются O
13	со O
14	словами O
15	из O
16	тренировочной O
17	разметки O
18	при O
19	помощи O
20	косинусной B-Metric
21	близости I-Metric
22	. O

# sent_id = 457
# text =   Архитектура в свою очередь может содержать LSTM, BiLSTM, RNN или GRU слои.
1	Архитектура O
2	в O
3	свою O
4	очередь O
5	может O
6	содержать O
7	LSTM B-ShortName
8	, O
9	BiLSTM B-ShortName
10	, O
11	RNN B-ShortName
12	или O
13	GRU B-ShortName
14	слои O
15	. O

# sent_id = 458
# text =   Из интересных решений один из участников представил BiLSTM-сеть с CRF слоем.
1	Из O
2	интересных O
3	решений O
4	один O
5	из O
6	участников O
7	представил O
8	BiLSTM B-Model
9	- I-Model
10	сеть I-Model
11	с O
12	CRF B-Model
13	слоем O
14	. O

# sent_id = 459
# text =   Используются те же модели, поэтому расскажем о различии в подготовке данных для моделей.Для задачи NER тексты преобразовываются с помощью токенизатора и теггинга.
1	Используются O
2	те O
3	же O
4	модели O
5	, O
6	поэтому O
7	расскажем O
8	о O
9	различии O
10	в O
11	подготовке O
12	данных O
13	для O
14	моделей O
15	. O
16	Для O
17	задачи O
18	NER B-Task
19	тексты O
20	преобразовываются O
21	с O
22	помощью O
23	токенизатора B-Method
24	и O
25	теггинга B-Method
26	. O

# sent_id = 460
# text =   Сначала тексты при помощи токенизатора переводятся в вектора - это то, на чем обучается модель.
1	Сначала O
2	тексты O
3	при O
4	помощи O
5	токенизатора B-Method
6	переводятся O
7	в O
8	вектора B-Object
9	- O
10	это O
11	то O
12	, O
13	на O
14	чем O
15	обучается O
16	модель O
17	. O

# sent_id = 461
# text =   Далее создаются таргеты при помощи теггинга.
1	Далее O
2	создаются O
3	таргеты B-Object
4	при O
5	помощи O
6	теггинга B-Method
7	. O

# sent_id = 462
# text =   Самым распространенным алгоритмом теггинга является “Inside–outside–beginning”.
1	Самым O
2	распространенным O
3	алгоритмом O
4	теггинга O
5	является O
6	“ O
7	Inside B-Method
8	– I-Method
9	outside I-Method
10	– I-Method
11	beginning I-Method
12	” 
13	. O

# sent_id = 463
# text =   Тег указывает на то, что слово находится внутри спана.
1	Тег B-Object
2	указывает O
3	на O
4	то O
5	, O
6	что O
7	слово O
8	находится O
9	внутри O
10	спана B-Object
11	. O

# sent_id = 464
# text =   Среди решений были как кастомный код для обучения и инференса, так и код от huggingface, который можно использовать из коробки.
1	Среди O
2	решений O
3	были O
4	как O
5	кастомный O
6	код O
7	для O
8	обучения O
9	и O
10	инференса O
11	, O
12	так O
13	и O
14	код O
15	от O
16	huggingface B-Organization
17	, O
18	который O
19	можно O
20	использовать O
21	из O
22	коробки O
23	. O

# sent_id = 465
# text =   Безусловно, основной метрикой оценивания являлся лидерборд.
1	Безусловно O
2	, O
3	основной O
4	метрикой O
5	оценивания O
6	являлся O
7	лидерборд B-Metric
8	. O

# sent_id = 466
# text =   Для решения ситуации мы можем искусственно сгенерировать данные с помощью языка программирования.
1	Для O
2	решения O
3	ситуации O
4	мы O
5	можем O
6	искусственно O
7	сгенерировать B-Task
8	данные I-Task
9	с O
10	помощью O
11	языка O
12	программирования O
13	. O

# sent_id = 467
# text =   Пересмотрев множество примеров и статей, была найдена англоязычная статья, в которой рассмотрены три самых интересных, в плане функциональности и простоты использования, способа генерации синтетических данных с помощью пакетов Python.
1	Пересмотрев O
2	множество O
3	примеров O
4	и O
5	статей O
6	, O
7	была O
8	найдена O
9	англоязычная O
10	статья O
11	, O
12	в O
13	которой O
14	  O
15	рассмотрены O
16	три O
17	самых O
18	интересных O
19	, O
20	в O
21	плане O
22	функциональности O
23	и O
24	простоты O
25	использования O
26	, O
27	способа O
28	генерации B-Task
29	синтетических I-Task
30	данных I-Task
31	с O
32	помощью O
33	пакетов O
34	  O
35	Python B-Environment
36	. O

# sent_id = 468
# text =   Faker - это пакет Python, разработанный для упрощения генерации синтетических данных.
1	Faker B-Application
2	- O
3	это O
4	пакет O
5	Python B-Environment
6	, O
7	разработанный O
8	для O
9	упрощения O
10	генерации B-Task
11	синтетических I-Task
12	данных I-Task
13	. O

# sent_id = 469
# text =   SDV или Synthetic Data Vault - это пакет Python для генерации синтетических данных на основе предоставленного набора данных.
1	SDV B-ShortName
2	или O
3	Synthetic B-Application
4	Data I-Application
5	Vault I-Application
6	- O
7	это O
8	пакет O
9	Python B-Environment
10	для O
11	генерации B-Task
12	синтетических I-Task
13	данных I-Task
14	на O
15	основе O
16	предоставленного O
17	набора O
18	данных O
19	. O

# sent_id = 470
# text =   SDV генерирует данные, применяя математические методы и модели машинного обучения.
1	SDV B-ShortName
2	генерирует O
3	данные O
4	, O
5	применяя O
6	математические B-Method
7	методы I-Method
8	и O
9	модели B-Model
10	машинного I-Model
11	обучения I-Model
12	. O

# sent_id = 471
# text =   С помощью SVD можно обработать данные, даже если они содержат несколько типов данных и отсутствующие значения.
1	С O
2	помощью O
3	SVD B-ShortName
4	можно O
5	обработать B-Task
6	данные I-Task
7	, O
8	даже O
9	если O
10	они O
11	содержат O
12	несколько O
13	типов O
14	данных O
15	и O
16	отсутствующие O
17	значения O
18	. O

# sent_id = 472
# text =   Используем для этого одну из доступных моделей SVD Singular Table GaussianCopula.
1	Используем O
2	для O
3	этого O
4	одну O
5	из O
6	доступных O
7	моделей O
8	SVD B-ShortName
9	Singular B-Model
10	Table I-Model
11	GaussianCopula I-Model
12	. O

# sent_id = 473
# text =   Воспользуемся функцией evaluate из SDV.
1	Воспользуемся O
2	функцией O
3	evaluate B-Method
4	из O
5	SDV B-ShortName
6	. O

# sent_id = 474
# text =   Возьмем для примера статистические метрики (критерии Колмогорова–Смирнова и Хи-квадрат) и метрику обнаружения, основанную на классификаторе логистической регрессии.
1	Возьмем O
2	для O
3	примера O
4	статистические B-Metric
5	метрики I-Metric
6	( O
7	критерии B-Metric
8	Колмогорова I-Metric
9	– I-Metric
10	Смирнова I-Metric
11	и O
12	Хи B-Metric
13	- I-Metric
14	квадрат I-Metric
15	) O
16	и O
17	метрику B-Metric
18	обнаружения I-Metric
19	, O
20	основанную O
21	на O
22	классификаторе B-Method
23	логистической I-Method
24	регрессии I-Method
25	. O

# sent_id = 475
# text =   KSTest используется для сравнения столбцов с непрерывными данными, а CSTest с дискретными данными.
1	KSTest B-Metric
2	используется O
3	для O
4	сравнения O
5	столбцов O
6	с O
7	непрерывными O
8	данными O
9	, O
10	а O
11	CSTest B-Metric
12	с O
13	дискретными O
14	данными O
15	. O

# sent_id = 476
# text =   Метрика LogisticDetection при помощи машинного обучения позволяет оценить насколько сложно отличить синтетические данные от исходных.
1	Метрика O
2	LogisticDetection B-Metric
3	при O
4	помощи O
5	машинного O
6	обучения O
7	позволяет O
8	оценить O
9	насколько O
10	сложно O
11	отличить B-Task
12	синтетические I-Task
13	данные I-Task
14	от I-Task
15	исходных I-Task
16	. O

# sent_id = 477
# text =   Gretel или Gretel Synthetics – это пакет Python с открытым исходным кодом, основанный на рекуррентной нейронной сети для создания структурированных и не структурированных данных.
1	Gretel B-Application
2	или O
3	Gretel B-Application
4	Synthetics I-Application
5	– O
6	это O
7	пакет O
8	Python B-Environment
9	с O
10	открытым O
11	исходным O
12	кодом O
13	, O
14	основанный O
15	на O
16	рекуррентной B-Network
17	нейронной I-Network
18	сети I-Network
19	для O
20	создания O
21	структурированных O
22	и O
23	не O
24	структурированных O
25	данных O
26	. O

# sent_id = 478
# text =   Этот модуль работает непосредственно с датафреймами данных Pandas и позволяет автоматически разбивать датафрейм на более мелкие датафреймы (по кластерам столбцов), выполнять обучение модели и генерацию для каждого фрейма независимо.
1	Этот O
2	модуль O
3	работает O
4	  O
5	непосредственно O
6	с O
7	датафреймами O
8	данных O
9	Pandas B-Application
10	и O
11	позволяет O
12	автоматически O
13	разбивать O
14	датафрейм O
15	на O
16	более O
17	мелкие O
18	датафреймы O
19	( O
20	по O
21	кластерам O
22	столбцов O
23	) O
24	, O
25	выполнять O
26	обучение O
27	модели O
28	и O
29	генерацию O
30	для O
31	каждого O
32	фрейма O
33	независимо O
34	. O

# sent_id = 479
# text =   Теперь с помощью пакета Gretel cгенерируем синтетические данные для Stroke Prediction Dataset и проанализируем их относительно данных полученных с помощью пакета SVD из пункта 2.
1	Теперь O
2	с O
3	помощью O
4	пакета O
5	Gretel B-Application
6	cгенерируем O
7	синтетические O
8	данные O
9	для O
10	Stroke B-Dataset
11	Prediction I-Dataset
12	Dataset I-Dataset
13	и O
14	проанализируем O
15	их O
16	относительно O
17	данных O
18	полученных O
19	с O
20	помощью O
21	пакета O
22	SVD B-ShortName
23	из O
24	пункта O
25	2 O
26	. O

# sent_id = 480
# text =   Метрикой оценки качества является ROC-AUC.
1	Метрикой O
2	оценки O
3	качества O
4	является O
5	ROC B-Metric
6	- I-Metric
7	AUC I-Metric
8	. O

# sent_id = 481
# text =   Разработанный подход для решения задачи кредитного скоринга в дальнейшем легко переносим и на прочие банковские задачи: модели склонности, оттока и дохода.
1	Разработанный O
2	подход O
3	для O
4	решения O
5	задачи B-Task
6	кредитного I-Task
7	скоринга I-Task
8	в O
9	дальнейшем O
10	легко O
11	переносим O
12	и O
13	на O
14	прочие O
15	банковские O
16	задачи O
17	: O
18	модели O
19	склонности O
20	, O
21	оттока O
22	и O
23	дохода O
24	. O

# sent_id = 482
# text =   Токены, относящиеся к ФИО, мы выделяем с помощью клиентской базы и проверки с помощью библиотек для морфологического анализа.
1	Токены B-Subject
2	, O
3	относящиеся O
4	к O
5	ФИО O
6	, O
7	мы O
8	выделяем O
9	с O
10	помощью O
11	клиентской O
12	базы O
13	и O
14	проверки O
15	с O
16	помощью O
17	библиотек O
18	для O
19	морфологического B-Method
20	анализа I-Method
21	. O

# sent_id = 483
# text =  Лемматизация оставшихся токенов.
1	Лемматизация B-Method
2	оставшихся O
3	токенов B-Subject
4	. O

# sent_id = 484
# text =   Для этого корпуса мы обучили word2vec-модель, где для каждого токена выучили эмбеддинг размера 50.
1	Для O
2	этого O
3	корпуса O
4	мы O
5	обучили O
6	word2vec B-Model
7	- I-Model
8	модель B-Model
9	, O
10	где O
11	для O
12	каждого O
13	токена O
14	выучили O
15	эмбеддинг B-Object
16	размера O

# sent_id = 485
# text =   Благодаря богатому набору данных бустинг индивидуально имеет приличное качество.
1	Благодаря O
2	богатому O
3	набору O
4	данных O
5	бустинг B-Method
6	индивидуально O
7	имеет O
8	приличное O
9	качество O
10	. O

# sent_id = 486
# text =   Одной из первых практических задач было определение авторства политических текстов The Federalist Papers, написанных в США в 1780 годах.
1	Одной O
2	из O
3	первых O
4	практических O
5	задач O
6	было O
7	определение B-Task
8	авторства I-Task
9	политических O
10	текстов O
11	The O
12	Federalist O
13	Papers O
14	, O
15	написанных O
16	в O
17	США O
18	в O
19	1780 B-Date
20	годах I-Date
21	. O

# sent_id = 487
# text =   Я рассмотрю простейший способ анализа с помощью несложных расчетов и пакета Natural Language Toolkit, что в совокупности с matplotlib позволяет получить интересные результаты буквально в несколько строк кода.
1	Я O
2	рассмотрю O
3	простейший O
4	способ O
5	анализа O
6	с O
7	помощью O
8	несложных O
9	расчетов O
10	и O
11	пакета O
12	Natural B-Application
13	Language I-Application
14	Toolkit I-Application
15	, O
16	что O
17	в O
18	совокупности O
19	с O
20	matplotlib B-Application
21	позволяет O
22	получить O
23	интересные O
24	результаты O
25	буквально O
26	в O
27	несколько O
28	строк O
29	кода O
30	. O

# sent_id = 488
# text =   К этой группе относятся решения от крупнейших компаний: Amazon Machine Learning, Microsoft Azure Machine Learning и Microsoft Cognitive Services, Google Cloud Prediction API и Google Cloud Machine Learning, IBM Watson Cloud и AlchemyAPI, BigML и другие.
1	К O
2	этой O
3	группе O
4	относятся O
5	решения O
6	от O
7	крупнейших O
8	компаний O
9	: O
10	Amazon B-Application
11	Machine I-Application
12	Learning I-Application
13	, O
14	Microsoft B-Application
15	Azure I-Application
16	Machine I-Application
17	Learning I-Application
18	и O
19	Microsoft B-Application
20	Cognitive I-Application
21	Services I-Application
22	, O
23	Google B-Application
24	Cloud I-Application
25	Prediction I-Application
26	API I-Application
27	и O
28	Google B-Application
29	Cloud I-Application
30	Machine I-Application
31	Learning I-Application
32	, O
33	IBM B-Application
34	Watson I-Application
35	Cloud I-Application
36	и O
37	AlchemyAPI B-Application
38	, O
39	BigML B-Application
40	и O
41	другие O
42	. O

# sent_id = 489
# text =   Возможности этого сервиса в области анализа речи и естественного языка пока ограничиваются английским языком, однако многие другие сервисы поддерживают русский язык, например, полностью бесплатный wit.ai, приобретённый Facebook, и его российский конкурент api.ai (понимание текстовых и голосовых команд и вопросов на естественных языках, преобразование речи в текст), IBM AlchemyAPI (анализ тональности текста, выявление сущностей и ключевых слов), Google Natural Language API (классификация текстов, графы связей, извлечение информации из текстов, анализ тональности, намерений, извлечение инсайтов; поддерживает русский язык с помощью технологии машинного перевода Google Translate, использует глубокое обучение и word2vec).
1	Возможности O
2	этого O
3	сервиса O
4	в O
5	области O
6	анализа B-Task
7	речи I-Task
8	и O
9	естественного O
10	языка O
11	пока O
12	ограничиваются O
13	английским O
14	языком O
15	, O
16	однако O
17	многие O
18	другие O
19	сервисы O
20	поддерживают O
21	русский O
22	язык O
23	, O
24	например O
25	, O
26	полностью O
27	бесплатный O
28	wit.ai B-Application
29	, O
30	приобретённый O
31	Facebook B-Organization
32	, O
33	и O
34	его O
35	российский O
36	конкурент O
37	api.ai B-Application
38	( O
39	понимание B-Task
40	текстовых I-Task
41	и I-Task
42	голосовых I-Task
43	команд I-Task
44	и I-Task
45	вопросов I-Task
46	на O
47	естественных O
48	языках O
49	, O
50	преобразование B-Task
51	речи I-Task
52	в I-Task
53	текст I-Task
54	) O
55	, O
56	IBM B-Application
57	AlchemyAPI I-Application
58	( O
59	анализ B-Task
60	тональности I-Task
61	текста I-Task
62	, O
63	выявление B-Task
64	сущностей I-Task
65	и O
66	ключевых B-Subject
67	слов I-Subject
68	) O
69	, O
70	Google B-Application
71	Natural I-Application
72	Language I-Application
73	API I-TERM
74	( O
75	классификация B-Task
76	текстов O
77	, O
78	графы B-Object
79	связей I-Object
80	, O
81	извлечение B-Task
82	информации I-Task
83	из I-Task
84	текстов I-Task
85	, O
86	анализ B-Task
87	тональности I-Task
88	, O
89	намерений O
90	, O
91	извлечение O
92	инсайтов O
93	; O
94	поддерживает O
95	русский O
96	язык O
97	с O
98	помощью O
99	технологии O
100	машинного B-Science
101	перевода I-Science
102	Google B-Application
103	Translate I-Application
104	, O
105	использует O
106	глубокое O
107	обучение O
108	и O
109	word2vec B-Application
110	) O
111	. O

# sent_id = 490
# text =   Например, IBM Watson предлагает инструмент Personality Insights, позволяющий определять черты личности человека, его потребности и ценности, намерения и другие характеристики по его записям в Твиттере, социальных сетях или по другим текстовым источникам.
1	Например O
2	, O
3	IBM B-Application
4	Watson I-Application
5	предлагает O
6	инструмент O
7	Personality B-Application
8	Insights I-Application
9	, O
10	позволяющий O
11	определять O
12	черты O
13	личности O
14	человека O
15	, O
16	его O
17	потребности O
18	и O
19	ценности O
20	, O
21	намерения O
22	и O
23	другие O
24	характеристики O
25	по O
26	его O
27	записям O
28	в O
29	Твиттере B-Application
30	, O
31	социальных O
32	сетях O
33	или O
34	по O
35	другим O
36	текстовым O
37	источникам O
38	. O

# sent_id = 491
# text =   Например, Diffbot позволяет автоматически сканировать страницы сайтов, извлекать из них нужную информацию: тексты, изображения, видео, информацию о продуктах, комментарии и др., в очищенном в структурированном виде, а также позволяет классифицировать страницы.
1	Например O
2	, O
3	Diffbot B-Application
4	позволяет O
5	автоматически O
6	сканировать B-Task
7	страницы I-Task
8	сайтов I-Task
9	, O
10	извлекать O
11	из O
12	них O
13	нужную O
14	информацию O
15	: O
16	тексты O
17	, O
18	изображения O
19	, O
20	видео O
21	, O
22	информацию O
23	о O
24	продуктах O
25	, O
26	комментарии O
27	, O
28	в O
29	очищенном O
30	в O
31	структурированном O
32	виде O
33	, O
34	а O
35	также O
36	позволяет O
37	классифицировать B-TERM
38	страницы I-TERM
39	. O

# sent_id = 492
# text =   При этом используются широкий спектр технологий: анализ структуры страниц, машинное обучение, искусственный интеллект, обработка естественных языков и машинное зрение.
1	При O
2	этом O
3	используются O
4	широкий O
5	спектр O
6	технологий O
7	: O
8	анализ B-Task
9	структуры I-Task
10	страниц I-Task
11	, O
12	машинное B-Science
13	обучение I-Science
14	, O
15	искусственный B-Science
16	интеллект I-Science
17	, O
18	обработка B-Task
19	естественных I-Task
20	языков I-Task
21	и O
22	машинное B-Science
23	зрение I-Science
24	. O

# sent_id = 493
# text =   Решения, основанные на Deepomatic, позволяют находить информацию о фильме по его постеру, информацию о картине или скульптуре на выставке по ее фото, сделанному на камеру телефона, позволяют скачивать музыку, сфотографировав обложку альбома на диске и т.п.
1	Решения O
2	, O
3	основанные O
4	на O
5	Deepomatic B-Application
6	, O
7	позволяют O
8	находить B-Task
9	информацию I-Task
10	о I-Task
11	фильме I-Task
12	по O
13	его O
14	постеру O
15	, O
16	информацию B-Object
17	о I-Object
18	картине I-Object
19	или O
20	скульптуре O
21	на O
22	выставке O
23	по O
24	ее O
25	фото O
26	, O
27	сделанному O
28	на O
29	камеру O
30	телефона O
31	, O
32	позволяют O
33	скачивать O
34	музыку O
35	, O
36	сфотографировав O
37	обложку O
38	альбома O
39	на O
40	диске O
41	и O
42	т.п. O

# sent_id = 494
# text =   В нашем случае цель была сформулирована как повышение эффективности поиска кандидатов.
1	В O
2	нашем O
3	случае O
4	цель O
5	была O
6	сформулирована O
7	как O
8	повышение B-Task
9	эффективности I-Task
10	поиска I-Task
11	кандидатов I-Task
12	. O

# sent_id = 495
# text =   Основная задача здесь — найти эффективный способ отображения соответствия кандидатов и навыков.
1	Основная O
2	задача O
3	здесь O
4	— O
5	найти B-Task
6	эффективный I-Task
7	способ I-Task
8	отображения I-Task
9	соответствия I-Task
10	кандидатов I-Task
11	и I-Task
12	навыков I-Task
13	. O

# sent_id = 496
# text =   Кодирование в переменные — One-Hot Encoding (OHE) 
1	Кодирование O
2	в O
3	переменные O
4	— O
5	One B-Method
6	- I-Method
7	Hot I-Method
8	Encoding I-Method
9	( O
10	OHE B-ShortName
11	) O

# sent_id = 497
# text =   Для этого используют метод TF-IDF.
1	Для O
2	этого O
3	используют O
4	метод O
5	TF B-Metric
6	- I-Metric
7	IDF B-Metric
8	. O

# sent_id = 498
# text =   Соответственно, можно схлопнуть похожие навыки в некоторые факторы/компоненты/латентные признаки.
1	Соответственно O
2	, O
3	можно O
4	схлопнуть O
5	похожие O
6	навыки O
7	в O
8	некоторые O
9	факторы B-Object
10	/ O
11	компоненты B-Object
12	/ O
13	латентные B-Object
14	признаки I-Object
15	. O

# sent_id = 499
# text =   Одним из подходов, позволяющих находить такие компоненты, является группа методов матричной факторизации.
1	Одним O
2	из O
3	подходов O
4	, O
5	позволяющих O
6	находить O
7	такие O
8	компоненты B-Object
9	, O
10	является O
11	группа O
12	методов B-Method
13	матричной I-Method
14	факторизации I-Method
15	. O

# sent_id = 500
# text =   Полученные представления кандидатов и навыков называют эмбедингами.
1	Полученные O
2	представления O
3	кандидатов O
4	и O
5	навыков O
6	называют O
7	эмбедингами B-Object
8	. O

# sent_id = 501
# text =   При создании нашей системы рекомендации кандидатов на позиции мы использовали нейронную сеть — StarSpace.
1	При O
2	создании O
3	нашей O
4	системы O
5	рекомендации O
6	кандидатов O
7	на O
8	позиции O
9	мы O
10	использовали O
11	нейронную B-Method
12	сеть I-Method
13	— O
14	StarSpace B-Network
15	. O

# sent_id = 502
# text =  Другая группа методов, позволяющая решать задачи репрезентации сущностей — репрезентация графов.
1	Другая O
2	группа O
3	методов O
4	, O
5	позволяющая O
6	решать O
7	задачи B-Task
8	репрезентации I-Task
9	сущностей I-Task
10	— O
11	репрезентация B-Task
12	графов I-Task
13	. O

# sent_id = 503
# text =   Но большинство методов графовой репрезентации работает с одномодальными графами, поэтому обычно двухмодальные графы следует трансформировать в граф, где узлы представлены одним видом сущностей.
1	Но O
2	большинство O
3	методов B-Method
4	графовой I-Method
5	репрезентации I-Method
6	работает O
7	с O
8	одномодальными B-Object
9	графами I-Object
10	, O
11	поэтому O
12	обычно O
13	двухмодальные B-Object
14	графы I-Object
15	следует O
16	трансформировать O
17	в O
18	граф O
19	, O
20	где O
21	узлы O
22	представлены O
23	одним O
24	видом O
25	сущностей O
26	. O

# sent_id = 504
# text =   В первую очередь рассмотрим метод, основанный на графовой факторизации.
1	В O
2	первую O
3	очередь O
4	рассмотрим O
5	метод O
6	, O
7	основанный O
8	на O
9	графовой B-Method
10	факторизации I-Method
11	. O

# sent_id = 505
# text =   Это группа методов очень похожа на методы, применяемые для репрезентации текстов — w2v (skip-gram), doc2vec.
1	Это O
2	группа O
3	методов O
4	очень O
5	похожа O
6	на O
7	методы O
8	, O
9	применяемые O
10	для O
11	репрезентации O
12	текстов O
13	— O
14	w2v B-Method 
15	( O
16	skip B-Method
17	- I-Method
18	gram I-Method
19	) O
20	, O
21	doc2vec B-Method
22	. O

# sent_id = 506
# text =   Почитать подробнее про подобные методы графовой репрезентации можно, например, тут — DeepWalk, Node2vec, Graph2vec.
1	Почитать O
2	подробнее O
3	про O
4	подобные O
5	методы O
6	графовой O
7	репрезентации O
8	можно O
9	, O
10	например O
11	, O
12	тут O
13	— O
14	DeepWalk B-Method
15	, O
16	Node2vec B-Method
17	, O
18	Graph2vec B-Method
19	. O

# sent_id = 507
# text =   Сверточные сети на графах (Graph Convolutional Networks).
1	Сверточные O
2	сети O
3	на O
4	графах O
5	( O
6	Graph B-Method
7	Convolutional I-Method
8	Networks I-Method
9	) O
10	. O

# sent_id = 508
# text =   Для задачи репрезентации графов связей между сущностями мы использовали фреймворк PyTorch BigGraph — это ещё один фреймворк от Facebook Research.
1	Для O
2	задачи O
3	репрезентации B-Task
4	графов I-Task
5	связей I-Task
6	между I-Task
7	сущностями I-Task
8	мы O
9	использовали O
10	фреймворк O
11	PyTorch B-Application
12	BigGraph I-Application
13	— O
14	это O
15	ещё O
16	один O
17	фреймворк O
18	от O
19	Facebook B-Organization
20	Research I-Organization
21	. O

# sent_id = 509
# text =   Энкодер предложений (sentence encoder) – это модель, которая сопоставляет коротким текстам векторы в многомерном пространстве, причём так, что у текстов, похожих по смыслу, и векторы тоже похожи.
1	Энкодер B-Method
2	предложений I-Method
3	( O
4	sentence B-Method
5	encoder I-Method
6	) O
7	– O
8	это O
9	модель O
10	, O
11	которая O
12	сопоставляет O
13	коротким O
14	текстам O
15	векторы O
16	в O
17	многомерном O
18	пространстве O
19	, O
20	причём O
21	так O
22	, O
23	что O
24	у O
25	текстов O
26	, O
27	похожих O
28	по O
29	смыслу O
30	, O
31	и O
32	векторы O
33	тоже O
34	похожи O
35	. O

# sent_id = 510
# text =   Обычно для этой цели используются нейросети, а полученные векторы называются эмбеддингами.
1	Обычно O
2	для O
3	этой O
4	цели O
5	используются O
6	нейросети B-Method
7	, O
8	а O
9	полученные O
10	векторы O
11	называются O
12	эмбеддингами B-Object
13	. O

# sent_id = 511
# text =   Они полезны для кучи задач, например, few-shot классификации текстов, семантического поиска, или оценки качества перефразирования.
1	Они O
2	полезны O
3	для O
4	кучи O
5	задач O
6	, O
7	например O
8	, O
9	few B-Task
10	- I-Task
11	shot I-Task
12	классификации I-Task
13	текстов B-Task
14	, O
15	семантического B-Task
16	поиска I-Task
17	, O
18	или O
19	оценки B-Task
20	качества I-Task
21	перефразирования I-Task
22	. O

# sent_id = 512
# text =   Самой качественной моделью оказался mUSE, самой быстрой из предобученных – FastText, а по балансу скорости и качества победил rubert-tiny2.
1	Самой O
2	качественной O
3	моделью O
4	оказался O
5	mUSE B-Model
6	, O
7	самой O
8	быстрой O
9	из O
10	предобученных O
11	– O
12	FastText B-Model
13	, O
14	а O
15	по O
16	балансу O
17	скорости O
18	и O
19	качества O
20	победил O
21	rubert B-Model
22	- I-Model
23	tiny2 I-Model
24	. O

# sent_id = 513
# text =   Первой известной попыткой системно сравнить английские эмбеддинги предложений был SentEval, сочетающий чисто лингвистические задачи со вполне прикладными.
1	Первой O
2	известной O
3	попыткой O
4	системно O
5	сравнить O
6	английские O
7	эмбеддинги B-Object
8	предложений O
9	был O
10	SentEval B-Dataset
11	, O
12	сочетающий O
13	чисто O
14	лингвистические O
15	задачи O
16	со O
17	вполне O
18	прикладными O
19	. O

# sent_id = 514
# text =   Для русского языка тоже было создано немало разного рода бенчмарков NLU моделей:RussianSuperGLUE: бенчмарк "сложных" NLP задач; фокус на дообучаемых моделях.
1	Для O
2	русского B-Lang
3	языка O
4	тоже O
5	было O
6	создано O
7	немало O
8	разного O
9	рода O
10	бенчмарков O
11	NLU ShortName
12	моделей O
13	: O
14	RussianSuperGLUE B-Model
15	: O
16	бенчмарк O
17	" O
18	сложных O
19	" O
20	NLP B-Task
21	задач I-Task
22	; O
23	фокус O
24	на O
25	дообучаемых O
26	моделях O
27	. O

# sent_id = 515
# text = MOROCCO: RussianSuperGLUE + оценка производительности, довольно трудновоспроизводимый бенчмарк.
1	MOROCCO O
2	: O
3	RussianSuperGLUE B-Model
4	+ O
5	оценка O
6	производительности O
7	, O
8	довольно O
9	трудновоспроизводимый O
10	бенчмарк O
11	. O

# sent_id = 516
# text =   RuSentEval: бенчмарк BERT-подобных энкодеров предложений на лингвистических задачах.
1	RuSentEval B-Application
2	: O
3	бенчмарк O
4	BERT B-Model
5	- I-Model
6	подобных I-Model
7	энкодеров I-Model
8	предложений I-Model
9	на O
10	лингвистических O
11	задачах O
12	. O

# sent_id = 517
# text =   SentEvalRu и deepPavlovEval: два хороших, но давно не обновлявшихся прикладных бенчмарка.
1	SentEvalRu B-Application
2	и O
3	deepPavlovEval B-Application
4	: O
5	два O
6	хороших O
7	, O
8	но O
9	давно O
10	не O
11	обновлявшихся O
12	прикладных O
13	бенчмарка O
14	. O

# sent_id = 518
# text =   С тех пор появилось много новых русскоязычных моделей, включая rubert-tiny2, поэтому и бенчмарк пришло время обновить.
1	С O
2	тех O
3	пор O
4	появилось O
5	много O
6	новых O
7	русскоязычных O
8	моделей O
9	, O
10	включая O
11	rubert B-Model
12	- I-Model
13	tiny2 I-Model
14	, O
15	поэтому O
16	и O
17	бенчмарк O
18	пришло O
19	время O
20	обновить O
21	. O

# sent_id = 519
# text =   В основу бенчмарка легли BERT-подобные модели: sbert_large_nlu_ru, sbert_large_mt_nlu_ru, и ruRoberta-large от Сбера; rubert-base-cased-sentence, rubert-base-cased-conversational, distilrubert-tiny-cased-conversational, и distilrubert-base-cased-conversational от DeepPavlov; мои   rubert-tiny и rubert-tiny2; мультиязычные LaBSE (плюс урезанная версия LaBSE-en-ru) и старый добрый bert-base-multilingual-cased.
1	В O
2	основу O
3	бенчмарка O
4	легли O
5	BERT B-Model
6	- I-Model
7	подобные I-Model
8	модели I-Model
9	: O
10	sbert_large_nlu_ru B-Model
11	, O
12	sbert_large_mt_nlu_ru B-Model
13	, O
14	и O
15	ruRoberta B-Model
16	- I-Model
17	large I-Model
18	от O
19	Сбера B-Organization
20	; O
21	rubert B-Model
22	- I-Model
23	base I-Model
24	- I-Model
25	cased I-Model
26	- I-Model
27	sentence I-Model
28	, O
29	rubert B-Model
30	- I-Model
31	base I-Model
32	- I-Model
33	cased I-Model
34	- I-Model
35	conversational I-Model
36	, O
37	distilrubert B-Model
38	- I-Model
39	tiny I-Model
40	- I-Model
41	cased I-Model
42	- I-Model
43	conversational I-Model
44	, O
45	и O
46	distilrubert B-Model
47	- I-Model
48	base I-Model
49	- I-Model
50	cased I-Model
51	- I-Model
52	conversational I-Model
53	от O
54	DeepPavlov B-Organization
55	; O
56	мои O
57	rubert B-Model
58	- I-Model
59	tiny I-Model
60	и O
61	rubert B-Model
62	- I-Model
63	tiny2 I-Model
64	; O
65	мультиязычные O
66	LaBSE B-Model
67	( O
68	плюс O
69	урезанная O
70	версия O
71	LaBSE B-Model
72	- I-Model
73	en I-Model
74	- I-Model
75	ru I-Model
76	) O
77	и O
78	старый O
79	добрый O
80	bert B-Model
81	- I-Model
82	base I-Model
83	- I-Model
84	multilingual I-Model
85	- I-Model
86	cased I-Model
87	. O

# sent_id = 520
# text =   Кроме этого, я добавил в бенчмарк разные T5 модели, т.к. они тоже должны хорошо понимать тексты: мои rut5-small, rut5-base, rut5-base-multitask, и rut5-base-paraphraser, и Сберовские ruT5-base и ruT5-large.
1	Кроме O
2	этого O
3	, O
4	я O
5	добавил O
6	в O
7	бенчмарк O
8	разные O
9	T5 B-Model
10	модели I-Model
11	, O
12	т.к. O
13	они O
14	тоже O
15	должны O
16	хорошо O
17	понимать O
18	тексты O
19	: O
20	мои O
21	rut5-small B-Model
22	, O
23	rut5-base B-Model
24	, O
25	rut5-base B-Model
26	- I-Model
27	multitask I-Model
28	, O
29	и O
30	rut5-base B-Model
31	- I-Model
32	paraphraser I-Model
33	, O
34	и O
35	Сберовские B-Organization
36	ruT5-base B-Model
37	и O
38	ruT5-large B-Model
39	. O

# sent_id = 521
# text =   Помимо BERTов и T5, я включил в бенчмарк большие мультиязычные модели Laser от FAIR и USE-multilingual-large от Google.
1	Помимо O
2	BERTов O
3	и O
4	T5 O
5	, O
6	я O
7	включил O
8	в O
9	бенчмарк O
10	большие O
11	мультиязычные O
12	модели O
13	Laser B-Model
14	от O
15	FAIR B-Model
16	и O
17	USE B-Model
18	- I-Model
19	multilingual I-Model
20	- I-Model
21	large I-Model
22	от O
23	Google B-Organization
24	. O

# sent_id = 522
# text =   В качестве быстрого бейзлайна, я добавил FastText, а именно, geowac_tokens_none_fasttextskipgram_300_5_2020  с RusVectores, а также его сжатую версию.
1	В O
2	качестве O
3	быстрого O
4	бейзлайна O
5	, O
6	я O
7	добавил O
8	FastText B-Model
9	, O
10	а O
11	именно O
12	, O
13	geowac_tokens_none_fasttextskipgram_300_5_2020 B-Model
14	с O
15	RusVectores B-Model
16	, O
17	а O
18	также O
19	его O
20	сжатую O
21	версию O
22	. O

# sent_id = 523
# text =   Наконец, я добавил парочку "моделей", которые вообще не выучивают никаких параметров, а просто используют HashingVectorizer для превращения текста в вектор признаков.
1	Наконец O
2	, O
3	я O
4	добавил O
5	парочку O
6	" O
7	моделей O
8	" O
9	, O
10	которые O
11	вообще O
12	не O
13	выучивают O
14	никаких O
15	параметров O
16	, O
17	а O
18	просто O
19	используют O
20	HashingVectorizer B-Application
21	для O
22	превращения O
23	текста O
24	в O
25	вектор O
26	признаков O
27	. O

# sent_id = 524
# text =   Это доработанная версия rubert-tiny: я расширил словарь модели c 30К до 80К токенов, увеличил максимальную длину текста с 512 до 2048 токенов, и дообучил модель на комбинации задач masked language modelling, natural language inference, и аппроксимации эмбеддингов LaBSE.
1	Это O
2	доработанная O
3	версия O
4	rubert B-Model
5	- I-Model
6	tiny I-Model
7	: O
8	я O
9	расширил O
10	словарь O
11	модели O
12	c O
13	30К O
14	до O
15	80К O
16	токенов O
17	, O
18	увеличил O
19	максимальную O
20	длину O
21	текста O
22	с O
23	512 O
24	до O
25	2048 O
26	токенов O
27	, O
28	и O
29	дообучил O
30	модель O
31	на O
32	комбинации O
33	задач O
34	masked B-Task
35	language I-Task
36	modelling I-Task
37	, O
38	natural B-Task
39	language I-Task
40	inference I-Task
41	, O
42	и O
43	аппроксимации B-Task
44	эмбеддингов I-Task
45	LaBSE I-Task
46	. O

# sent_id = 525
# text =   В новой версии бенчмарка я оставил всё те же 10 задач, что и в прежней, но слегка изменил формат некоторых из них:Semantic text similarity (STS) на основе переведённого датасета STS-B; Paraphrase identification (PI) на основе датасета paraphraser.ru;Natural language inference (NLI) на датасете XNLI; Sentiment analysis (SA) на данных SentiRuEval2016.
1	В O
2	новой O
3	версии O
4	бенчмарка O
5	я O
6	оставил O
7	всё O
8	те O
9	же O
10	10 O
11	задач O
12	, O
13	что O
14	и O
15	в O
16	прежней O
17	, O
18	но O
19	слегка O
20	изменил O
21	формат O
22	некоторых O
23	из O
24	них O
25	: O
26	Semantic B-Task
27	text I-Task
28	similarity I-Task
29	( O
30	STS B-ShortName
31	) O
32	на O
33	основе O
34	переведённого O
35	датасета O
36	STS B-Dataset
37	- I-Dataset
38	B I-Dataset
39	; O
40	Paraphrase B-Task
41	identification I-Task
42	( O
43	PI B-ShortName
44	) O
45	на O
46	основе O
47	датасета O
48	paraphraser.ru B-Dataset
49	; O
50	Natural B-Task
51	language I-Task
52	inference I-Task
53	( O
54	NLI B-ShortName
55	) O
56	на O
57	датасете O
58	XNLI B-Dataset
59	; O
60	Sentiment B-Task
61	analysis I-Task
62	( O
63	SA B-ShortName
64	) O
65	на O
66	данных O
67	SentiRuEval2016 B-Dataset
68	. O

# sent_id = 526
# text =   В прошлой версии бенчмарка я собрал кривые тестовые выборки, поэтому этот датасет я переделал; Toxicity identification (TI) на датасете токсичных комментариев из OKMLCup; Inappropriateness identification (II) на датасете Сколтеха; Intent classification (IC) и её кросс-язычная версия ICX на датасете NLU-evaluation-data, который я автоматически перевёл на русский.
1	В O
2	прошлой O
3	версии O
4	бенчмарка O
5	я O
6	собрал O
7	кривые O
8	тестовые O
9	выборки O
10	, O
11	поэтому O
12	этот O
13	датасет O
14	я O
15	переделал O
16	; O
17	Toxicity B-Task
18	identification I-Task
19	( O
20	TI B-ShortName
21	) O
22	на O
23	датасете B-Dataset
24	токсичных I-Dataset
25	комментариев I-Dataset
26	из O
27	OKMLCup B-Application
28	; O
29	Inappropriateness B-Task
30	identification I-Task
31	( O
32	II B-ShortName
33	) O
34	на O
35	датасете B-Dataset
36	Сколтеха I-Dataset
37	; O
38	Intent B-Task
39	classification I-Task
40	( O
41	IC B-ShortName
42	) O
43	и O
44	её O
45	кросс O
46	- O
47	язычная O
48	версия O
49	ICX B-ShortName
50	на O
51	датасете O
52	NLU B-Dataset
53	- I-Dataset
54	evaluation I-Dataset
55	- I-Dataset
56	data I-Dataset
57	, O
58	который O
59	я O
60	автоматически O
61	перевёл O
62	на O
63	русский B-Lang
64	. O

# sent_id = 527
# text =   В IC классификатор обучается на русских данных, а в ICX – на английских, а тестируется в обоих случаях на русских.
1	В O
2	IC B-Model
3	классификатор I-Model
4	обучается O
5	на O
6	русских B-Lang
7	данных O
8	, O
9	а O
10	в O
11	ICX B-Model
12	– O
13	на O
14	английских B-Lang
15	, O
16	а O
17	тестируется O
18	в O
19	обоих O
20	случаях O
21	на O
22	русских O
23	. O

# sent_id = 528
# text =   Распознавание именованных сущностей () на датасетах factRuEval-2016E1) и RuDReC (NE2).
1	Распознавание B-Task
2	именованных I-Task
3	сущностей I-Task
4	( O
5	) O
6	на O
7	датасетах O
8	factRuEval-2016E1 B-Dataset
9	) O
10	и O
11	RuDReC B-Dataset
12	( O
13	NE2 B-Dataset
14	) O
15	. O

# sent_id = 529
# text =   Эти две задачи требуют получать эмбеддинги отдельных токенов, а не целых предложений; поэтому модели USE и Laser, не выдающие эмбеддинги токенов "из коробки", в оценке этих задач не участвовали.
1	Эти O
2	две O
3	задачи O
4	требуют O
5	получать O
6	эмбеддинги O
7	отдельных O
8	токенов O
9	, O
10	а O
11	не O
12	целых O
13	предложений O
14	; O
15	поэтому O
16	модели O
17	USE B-Model
18	и O
19	Laser B-Model
20	, O
21	не O
22	выдающие O
23	эмбеддинги O
24	токенов O
25	" O
26	из O
27	коробки O
28	" O
29	, O
30	в O
31	оценке O
32	этих O
33	задач O
34	не O
35	участвовали O
36	. O

# sent_id = 530
# text =   В задачах STS, PI и NLI оценивается степень связи двух текстов.
1	В O
2	задачах O
3	STS B-Task
4	, O
5	PI B-Task
6	и O
7	NLI B-Task
8	оценивается O
9	степень O
10	связи O
11	двух O
12	текстов O
13	. O

# sent_id = 531
# text =   Хороший энкодер предложений должен отражать эту степень в их косинусной близости, поэтому для STS и PI мы измеряем качество как Спирмановскую корреляцию косинусной близости и человеческих оценок сходства.
1	Хороший O
2	энкодер O
3	предложений O
4	должен O
5	отражать O
6	эту O
7	степень O
8	в O
9	их O
10	косинусной B-Metric
11	близости B-Metric
12	, O
13	поэтому O
14	для O
15	STS B-Task
16	и O
17	PI B-Task
18	мы O
19	измеряем O
20	качество O
21	как O
22	Спирмановскую B-Metric
23	корреляцию I-Metric
24	косинусной O
25	близости O
26	и O
27	человеческих O
28	оценок O
29	сходства O
30	. O

# sent_id = 532
# text =   Для NLI я обучил трёхклассовую (entail/contradict/neutral) логистическую регрессию поверх косинусной близости, и измеряю её точность (accuracy).
1	Для O
2	NLI B-Abbrev_Task
3	я O
4	обучил O
5	трёхклассовую O
6	( O
7	entail O
8	/ O
9	contradict O
10	/ O
11	neutral O
12	) O
13	логистическую B-Model
14	регрессию I-Model
15	поверх O
16	косинусной B-Metric
17	близости I-Metric
18	, O
19	и O
20	измеряю O
21	её O
22	точность B-Metric
23	( O
24	accuracy B-Metric
25	) O
26	. O

# sent_id = 533
# text =   Для задач бинарной классификации TI и II я измеряю ROC AUC, а в задачах многоклассовой классификации SA, IC и ICX – точность (accuracy).
1	Для O
2	задач O
3	бинарной O
4	классификации O
5	TI B-Abbrev_Task
6	и O
7	II B-Abbrev_Task
8	я O
9	измеряю O
10	ROC B-Metric
11	AUC I-Metric
12	, O
13	а O
14	в O
15	задачах O
16	многоклассовой O
17	классификации O
18	SA B-Abbrev_Task
19	, O
20	IC B-Abbrev_Task
21	и O
22	ICX B-Abbrev_Task
23	– O
24	точность B-Metric
25	( O
26	accuracy B-Metric
27	) O
28	. O

# sent_id = 534
# text =   Для всех задач классификации я обучаю логистическую регрессию либо KNN поверх эмбеддингов предложений, и выбираю лучшую модель из двух.
1	Для O
2	всех O
3	задач O
4	классификации B-Task
5	я O
6	обучаю O
7	логистическую B-Model
8	регрессию I-Model
9	либо O
10	KNN B-ShortName_Method
11	поверх O
12	эмбеддингов O
13	предложений B-Subject
14	, O
15	и O
16	выбираю O
17	лучшую O
18	модель B-Object
19	из O
20	двух O
21	. O

# sent_id = 535
# text =   Для задач NER я классифицировал токены логистической регрессией поверх их эмбеддингов, и измерял macro F1 по всем классам токенов, кроме О. 
1	Для O
2	задач O
3	NER B-Abbrev_Task
4	я O
5	классифицировал O
6	токены O
7	логистической B-Method
8	регрессией I-Method
9	поверх O
10	их O
11	эмбеддингов O
12	, O
13	и O
14	измерял O
15	macro B-Metric
16	F1 I-Metric
17	по O
18	всем O
19	классам O
20	токенов O
21	, O
22	кроме O
23	О O

# sent_id = 536
# text =  Поскольку разные модели токенизируют тексты по-разному, я токенизировал все тексты razdel'ом, и вычислял эмбеддинг слова как средний эмбеддинг его токенов.
1	Поскольку O
2	разные O
3	модели O
4	токенизируют O
5	тексты O
6	по O
7	- O
8	разному O
9	, O
10	я O
11	токенизировал O
12	все O
13	тексты O
14	razdel'ом B-Technology
15	, O
16	и O
17	вычислял O
18	эмбеддинг O
19	слова O
20	как O
21	средний O
22	эмбеддинг O
23	его O
24	токенов O
25	. O

# sent_id = 537
# text =   Единого победителя нет, но MUSE, sbert_large_mt_nlu_ru и rubert-base-cased-sentence взяли по многу призовых мест.
1	Единого O
2	победителя O
3	нет O
4	, O
5	но O
6	MUSE B-Model
7	, O
8	sbert_large_mt_nlu_ru B-Model
9	и O
10	rubert B-Model
11	- I-Model
12	base I-Model
13	- I-Model
14	cased I-Model
15	- I-Model
16	sentence I-Model
17	взяли O
18	по O
19	многу O
20	призовых O
21	мест O
22	. O

# sent_id = 538
# text =   Удивительно, но модели T5 очень хорошо показали себя на задачах NER.
1	Удивительно O
2	, O
3	но O
4	модели O
5	T5 B-Model
6	очень O
7	хорошо O
8	показали O
9	себя O
10	на O
11	задачах O
12	NER B-Abbrev_Task
13	. O

# sent_id = 539
# text =   Самыми качественными энкодерами предложений оказались мультиязычные MUSE, LaBSE и Laser.
1	Самыми O
2	качественными O
3	энкодерами O
4	предложений O
5	оказались O
6	мультиязычные O
7	MUSE B-Model
8	, O
9	LaBSE B-Model
10	и O
11	Laser B-Model
12	. O

# sent_id = 540
# text =   Но выбирать стоит из Парето-оптимальных моделей: таких, что ни одна другая модель не превосходит их по всем критериям.
1	Но O
2	выбирать O
3	стоит O
4	из O
5	Парето B-Object
6	- I-Object
7	оптимальных I-Object
8	моделей I-Object
9	: O
10	таких O
11	, O
12	что O
13	ни O
14	одна O
15	другая O
16	модель O
17	не O
18	превосходит O
19	их O
20	по O
21	всем O
22	критериям O
23	. O

# sent_id = 541
# text =   Из 25 моделей только 12 Парето-оптимальны:MUSE, rubert-tiny2, FT_geowac, Hashing_1000_char и Hashing_1000 обладают самым лучшим качеством для своей скорости на CPU; MUSE, LaBSE, rubert-tiny2, и distilbert-tiny обладают наилучшим качеством для своей скорости на GPU;MUSE, LaBSE, rubert-tiny2, rubert-tiny, FT_geowac_21mb, и Hashing_1000_char обладают наилучшим качеством для своего размера.
1	Из O
2	25 O
3	моделей O
4	только O
5	12 O
6	Парето O
7	- O
8	оптимальны O
9	: O
10	MUSE B-Model
11	, O
12	rubert B-Model
13	- I-Model
14	tiny2 I-Model
15	, O
16	FT_geowac B-Model
17	, O
18	Hashing_1000_char B-Model
19	и O
20	Hashing_1000 B-Model
21	обладают O
22	самым O
23	лучшим O
24	качеством O
25	для O
26	своей O
27	скорости O
28	на O
29	CPU O
30	; O
31	MUSE B-Model
32	, O
33	LaBSE B-Model
34	, O
35	rubert B-Model
36	- I-Model
37	tiny2 I-Model
38	, O
39	и O
40	distilbert B-Model
41	- I-Model
42	tiny I-Model
43	обладают O
44	наилучшим O
45	качеством O
46	для O
47	своей O
48	скорости O
49	на O
50	GPU O
51	; O
52	MUSE B-Model
53	, O
54	LaBSE B-Model
55	, O
56	rubert B-Model
57	- I-Model
58	tiny2 I-Model
59	, O
60	rubert B-Model
61	- I-Model
62	tiny I-Model
63	, O
64	FT_geowac_21 B-Model
65	mb O
66	, O
67	и O
68	Hashing_1000_char B-Model
69	обладают O
70	наилучшим O
71	качеством O
72	для O
73	своего O
74	размера O
75	. O

# sent_id = 542
# text =   Актуальный лидерборд смотрите в репозитории: https://github.com/avidale/encodechka
1	Актуальный O
2	лидерборд O
3	смотрите O
4	в O
5	репозитории O
6	: O
7	https://github.com/avidale/encodechka B-URL_InfoResource
8	. O

# sent_id = 543
# text =   Поддержка NlpCraft IDL добавлена в систему начиная с версии 0.7.5.
1	Поддержка O
2	NlpCraft B-ShortName_Environment
3	IDL I-ShortName_Environment
4	добавлена O
5	в O
6	систему O
7	начиная O
8	с O
9	версии O
10	0.7.5 O
11	. O

# sent_id = 544
# text =   Новая версия декларативного языка определения интентов, получившая название NlpCraft IDL (NlpCraft Intents Definition Language), значительно упростила процесс работы с интентами в диалоговых и поисковых системах, построенных на базе проекта Apache NlpCraft и вместе с тем расширила возможности системы.
1	Новая O
2	версия O
3	декларативного O
4	языка B-Object
5	определения B-Subject
6	интентов O
7	, O
8	получившая O
9	название O
10	NlpCraft B-ShortName_Environment
11	IDL I-ShortName_Environment
12	( O
13	NlpCraft B-Environment
14	Intents I-Environment
15	Definition B-Environment
16	Language B-Environment
17	) O
18	, O
19	значительно O
20	упростила O
21	процесс O
22	работы O
23	с O
24	интентами O
25	в O
26	диалоговых O
27	и O
28	поисковых O
29	системах O
30	, O
31	построенных O
32	на O
33	базе O
34	проекта O
35	Apache B-Project
36	NlpCraft I-Project
37	и O
38	вместе O
39	с O
40	тем O
41	расширила O
42	возможности O
43	системы O
44	. O

# sent_id = 545
# text =   NlpCraft IDL - это декларативный язык, позволяющий создавать определения интентов для их последующего использования в моделях Apache NlpCraft.
1	NlpCraft B-ShortName_Environment
2	IDL I-ShortName_Environment
3	- O
4	это O
5	декларативный O
6	язык O
7	, O
8	позволяющий O
9	создавать O
10	определения B-Subject
11	интентов O
12	для O
13	их O
14	последующего O
15	использования O
16	в O
17	моделях O
18	Apache B-Model
19	NlpCraft I-Model
20	. O

# sent_id = 546
# text =   Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
1	Чаще O
2	всего O
3	на O
4	практике O
5	в O
6	NLP B-ShortName_Science
7	приходится O
8	сталкиваться O
9	с O
10	задачей O
11	построения B-Task
12	эмбеддингов I-Task
13	. O

# sent_id = 547
# text =   Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
1	Для O
2	ее O
3	решения O
4	обычно O
5	используют O
6	один O
7	из O
8	следующих O
9	инструментов O
10	: O
11	Готовые O
12	векторы B-Object
13	/ O
14	эмбеддинги B-Object
15	слов I-Object
16	[ O
17	6 O
18	] O
19	; O
20	Внутренние O
21	состояния O
22	CNN B-ShortName_Method
23	, O
24	натренированных O
25	на O
26	таких O
27	задачах O
28	как O
29	, O
30	как O
31	определение B-Task
32	фальшивых I-Task
33	предложений I-Task
34	/ O
35	языковое B-Task
36	моделирование I-Task
37	/ O
38	классификация B-Task
39	[ O
40	7 O
41	] O
42	; O
43	Комбинация O
44	выше O
45	перечисленных O
46	методов O
47	; O
48	Кроме O
49	того O
50	, O
51	уже O
52	много O
53	раз O
54	было O
55	показано O
56	[ O
57	9 O
58	] O
59	, O
60	что O
61	в O
62	качестве O
63	хорошего O
64	бейслайна O
65	для O
66	эмбеддингов O
67	предложений O
68	можно O
69	взять O
70	и O
71	просто O
72	усредненные O
73	( O
74	с O
75	парой O
76	незначительных O
77	деталей O
78	, O
79	которые O
80	сейчас O
81	опустим O
82	) O
83	векторы O
84	слов O
85	. O

# sent_id = 548
# text =   Если для дальнейшей обработки не важен порядок слов, то текст упаковывают в Мешок слов (Bag-of-words).
1	Если O
2	для O
3	дальнейшей O
4	обработки O
5	не O
6	важен O
7	порядок O
8	слов O
9	, O
10	то O
11	текст O
12	упаковывают O
13	в O
14	Мешок B-Method
15	слов I-Method
16	( O
17	Bag B-Method
18	- I-Method
19	of I-Method
20	- I-Method
21	words I-Method
22	) O

# sent_id = 549
# text =   В обучающей выборке мы имеем письма с отметками спам/не спам, и скармливаем их в нейросеть: в полносвязную сеть и CNN подаем Bag-of-words, а в RNN уже можно учесть порядок слов, отправив ей Word Vector.
1	В O
2	обучающей O
3	выборке O
4	мы O
5	имеем O
6	письма O
7	с O
8	отметками O
9	спам O
10	/ O
11	не O
12	спам O
13	, O
14	и O
15	скармливаем O
16	их O
17	в O
18	нейросеть O
19	: O
20	в O
21	полносвязную O
22	сеть O
23	и O
24	CNN B-ShortName_Method
25	подаем O
26	Bag B-Method
27	- I-Method
28	of I-Method
29	- I-Method
30	words I-Method
31	, O
32	а O
33	в O
34	RNN B-ShortName_Method
35	уже O
36	можно O
37	учесть O
38	порядок O
39	слов O
40	, O
41	отправив O
42	ей O
43	Word B-Subject
44	Vector I-Subject
45	. O

# sent_id = 550
# text =   Сначала Яндекс распознаёт речь в текст с указанием таймингов и спикеров (за это отвечает голосовая биометрия).
1	Сначала O
2	Яндекс B-Organization
3	распознаёт B-Task
4	речь I-Task
5	в O
6	текст O
7	с O
8	указанием O
9	таймингов O
10	и O
11	спикеров O
12	( O
13	за O
14	это O
15	отвечает O
16	голосовая B-Task
17	биометрия I-Task
18	) O
19	. O

# sent_id = 551
# text =   Добавили поддержку немецкого, французского и испанского языков.
1	Добавили O
2	поддержку O
3	немецкого B-Lang
4	, O
5	французского B-Lang
6	и O
7	испанского B-Lang
8	языков O
9	. O

# sent_id = 552
# text =   Поэтому всё время существования как Яндекс Браузера, так и Яндекс Переводчика мы стараемся не просто переводить, но и помогать учить язык.
1	Поэтому O
2	всё O
3	время O
4	существования O
5	как O
6	Яндекс B-Technology
7	Браузера I-Technology
8	, O
9	так O
10	и O
11	Яндекс B-Technology
12	Переводчика I-Technology
13	мы O
14	стараемся O
15	не O
16	просто O
17	переводить O
18	, O
19	но O
20	и O
21	помогать O
22	учить O
23	язык O
24	. O

# sent_id = 553
# text =   С 27 по 30 мая в Российском государственном гуманитарном университете (РГГУ) пройдет международная научная конференция по компьютерной лингвистике «Диалог».
1	С O
2	27 O
3	по O
4	30 O
5	мая O
6	в O
7	Российском B-Organization
8	государственном I-Organization
9	гуманитарном I-Organization
10	университете I-Organization
11	( O
12	РГГУ B-Abbrev_Organization
13	) O
14	пройдет O
15	международная O
16	научная O
17	конференция O
18	по O
19	компьютерной B-Science
20	лингвистике I-Science
21	« O
22	Диалог O
23	» O
24	. O

# sent_id = 554
# text =   Подробно о том, что такое «Диалог» и почему ABBYY организует эту конференцию, мы писали здесь .
1	Подробно O
2	о O
3	том O
4	, O
5	что O
6	такое O
7	« O
8	Диалог O
9	» O
10	и O
11	почему O
12	ABBYY B-Organization
13	организует O
14	эту O
15	конференцию O
16	, O
17	мы O
18	писали O
19	здесь O
20	. O

# sent_id = 555
# text =   Главной задачей проведенных ранее тестирований был автоматический анализ тональности в целом небольших текстов – отзывов пользователей (о фильмах, книгах, цифровых фотокамерах) или мнений, выраженных в форме прямой или косвенной речи (новости).
1	Главной O
2	задачей O
3	проведенных O
4	ранее O
5	тестирований O
6	был O
7	автоматический B-Task
8	анализ I-Task
9	тональности I-Task
10	в O
11	целом O
12	небольших O
13	текстов O
14	– O
15	отзывов O
16	пользователей O
17	( O
18	о O
19	фильмах O
20	, O
21	книгах O
22	, O
23	цифровых O
24	фотокамерах O
25	) O
26	или O
27	мнений O
28	, O
29	выраженных O
30	в O
31	форме O
32	прямой O
33	или O
34	косвенной O
35	речи O
36	( O
37	новости O
38	) O
39	. O

# sent_id = 556
# text =   Основной целью нового цикла тестирований является автоматическая оценка тональности по отношению к заданному объекту и его конкретным свойствам.
1	Основной O
2	целью O
3	нового O
4	цикла O
5	тестирований O
6	является O
7	автоматическая B-Task
8	оценка I-Task
9	тональности I-Task
10	по O
11	отношению O
12	к O
13	заданному O
14	объекту O
15	и O
16	его O
17	конкретным O
18	свойствам O
19	. O

# sent_id = 557
# text =   Фишинговые электронные письма - это сообщения, которые кажутся очень похожими на настоящие, например, рассылку от вашего любимого интернет-магазина, но при этом они заманивают людей нажимать на прикрепленные вредоносные ссылки или документы.
1	Фишинговые B-Object
2	электронные I-Object
3	письма I-Object
4	- O
5	это O
6	сообщения O
7	, O
8	которые O
9	кажутся O
10	очень O
11	похожими O
12	на O
13	настоящие O
14	, O
15	например O
16	, O
17	рассылку O
18	от O
19	вашего O
20	любимого O
21	интернет O
22	- O
23	магазина O
24	, O
25	но O
26	при O
27	этом O
28	они O
29	заманивают O
30	людей O
31	нажимать O
32	на O
33	прикрепленные O
34	вредоносные O
35	ссылки O
36	или O
37	документы O
38	. O

# sent_id = 558
# text =   Поэтому в статье предлагается способ обнаружения фишинговых сообщений, называемый Federated Phish Bowl (далее FPB), использующий федеративное обучение и рекуррентную нейронную сеть с долгой краткосрочной памятью (LSTM).
1	Поэтому O
2	в O
3	статье O
4	предлагается O
5	способ O
6	обнаружения B-Task
7	фишинговых I-Task
8	сообщений I-Task
9	, O
10	называемый O
11	Federated B-Task
12	Phish I-Task
13	Bowl I-Task
14	( O
15	далее O
16	FPB ShortName_Task
17	) O
18	, O
19	использующий O
20	федеративное O
21	обучение O
22	и O
23	рекуррентную B-Network
24	нейронную I-Network
25	сеть I-Network
26	с I-Network
27	долгой I-Network
28	краткосрочной I-Network
29	памятью I-Network
30	( O
31	LSTM B-ShortName_Method
32	) O
33	. O

# sent_id = 559
# text =   Для работы с текстовыми последовательностями были придуманы рекуррентные нейронные сети (RNN, их улучшение - LSTM, которая бывает двунаправленной, когда последовательность обрабатывается в двух направлениях).
1	Для O
2	работы O
3	с O
4	текстовыми O
5	последовательностями O
6	были O
7	придуманы O
8	рекуррентные O
9	нейронные O
10	сети O
11	( O
12	RNN B-ShortName_Method
13	, O
14	их O
15	улучшение O
16	- O
17	LSTM B-ShortName_Method
18	, O
19	которая O
20	бывает O
21	двунаправленной O
22	, O
23	когда O
24	последовательность O
25	обрабатывается O
26	в O
27	двух O
28	направлениях O
29	) O
30	. O

# sent_id = 560
# text =   Например, сеть можно сделать двунаправленной (Bidirectional LSTM).
1	Например O
2	, O
3	сеть O
4	можно O
5	сделать O
6	двунаправленной O
7	( O
8	Bidirectional B-Method
9	LSTM I-Method
10	) O
11	. O

# sent_id = 561
# text =   FPB предлагает использовать подход, показанный на следующем изображении: 
1	FPB B-Method
2	предлагает O
3	использовать O
4	подход O
5	, O
6	показанный O
7	на O
8	следующем O
9	изображении O
10	: O

# sent_id = 562
# text =   Федеративное обучение - это метод машинного обучения, который обучает алгоритм на нескольких децентрализованных устройствах или серверах, содержащих локальные образцы данных, без обмена ими.
1	Федеративное B-Method
2	обучение I-Method
3	- O
4	это O
5	метод O
6	машинного O
7	обучения O
8	, O
9	который O
10	обучает O
11	алгоритм O
12	на O
13	нескольких O
14	децентрализованных O
15	устройствах O
16	или O
17	серверах O
18	, O
19	содержащих O
20	локальные O
21	образцы O
22	данных O
23	, O
24	без O
25	обмена O
26	ими O
27	. O

# sent_id = 563
# text =   Для обучения модели FPB с использованием федеративного обучения (FL) сервер параметров (PS) инициализирует глобальную модель (DL) на основе вышеупомянутых двунаправленных нейронных сетей LSTM и отправляет глобальную модель с глобальной матрицей преобразования слов в векторы всем клиентам на первом этапе обучения.
1	Для O
2	обучения O
3	модели O
4	FPB B-Method
5	с O
6	использованием O
7	федеративного B-Method
8	обучения I-Method
9	( O
10	FL B-ShortName_Method
11	) O
12	сервер B-Method
13	параметров I-Method
14	( O
15	PS B-ShortName_Method
16	) O
17	инициализирует O
18	глобальную B-Method
19	модель B-Method
20	( O
21	DL B-ShortName_Method
22	) O
23	на O
24	основе B-Subject
25	вышеупомянутых O
26	двунаправленных O
27	нейронных O
28	сетей O
29	LSTM B-ShortName_Method
30	и O
31	отправляет O
32	глобальную O
33	модель O
34	с O
35	глобальной O
36	матрицей O
37	преобразования O
38	слов O
39	в O
40	векторы O
41	всем O
42	клиентам O
43	на O
44	первом O
45	этапе O
46	обучения O
47	. O

# sent_id = 564
# text =   В рамках курса вы узнаете: Как латентные переменные применяются в задачах анализа текстов и как строить глубинные генеративные модели с латентными дискретными переменными.
1	В O
2	рамках O
3	курса O
4	вы O
5	узнаете O
6	: O
7	Как O
8	латентные B-Object
9	переменные I-Object
10	применяются O
11	в O
12	задачах O
13	анализа B-Task
14	текстов I-Task
15	и O
16	как O
17	строить O
18	глубинные B-Meodel
19	генеративные I-Meodel
20	модели I-Meodel
21	с O
22	латентными O
23	дискретными O
24	переменными O
25	. O

# sent_id = 565
# text =   Что такое semantic parsing: как строить формальные представления смысла текста, извлекая при этом неявные значения.
1	Что O
2	такое O
3	semantic B-Method
4	parsing I-Method
5	: O
6	как O
7	строить O
8	формальные O
9	представления O
10	смысла O
11	текста O
12	, O
13	извлекая O
14	при O
15	этом O
16	неявные O
17	значения O
18	. O

# sent_id = 566
# text =   Британские ученые обучили ИИ трансформировать устную речь в видео с виртуальным сурдопереводчиком.
1	Британские O
2	ученые O
3	обучили O
4	ИИ O
5	трансформировать O
6	устную B-Subject
7	речь I-Subject
8	в O
9	видео O
10	с O
11	виртуальным O
12	сурдопереводчиком B-Application
13	. O

# sent_id = 567
# text =   В Университете Суррея разработчики создали алгоритм сурдоперевода нового поколения.
1	В O
2	Университете B-Organization
3	Суррея I-Organization
4	разработчики O
5	создали O
6	алгоритм B-Method
7	сурдоперевода I-Method
8	нового O
9	поколения O
10	. O

# sent_id = 568
# text =   После этого последовательность поз подается сверточной нейросети U-Net.
1	После O
2	этого O
3	последовательность O
4	поз O
5	подается O
6	сверточной O
7	нейросети O
8	U B-Network
9	- I-Network
10	Net I-Network
11	. O

# sent_id = 569
# text =   Один из самых известных продуктов — анимированный виртуальный переводчик от IBM.
1	Один O
2	из O
3	самых O
4	известных O
5	продуктов O
6	— O
7	анимированный O
8	виртуальный O
9	переводчик O
10	от O
11	IBM B-Technology
12	. O

# sent_id = 570
# text =   Программа, придуманная учеными из Новосибирского академгородка, распознает речь, анализирует смысл и переводит на жестовый язык.
1	Программа O
2	, O
3	придуманная O
4	учеными O
5	из O
6	Новосибирского B-Organization
7	академгородка I-Organization
8	, O
9	распознает B-Task
10	речь I-Task
11	, O
12	анализирует B-Task
13	смысл I-Task
14	и O
15	переводит O
16	на O
17	жестовый B-Object
18	язык I-Object
19	. O

# sent_id = 571
# text =   В то время считали, что разработка станет такой же популярной, как Google Translator.
1	В O
2	то O
3	время O
4	считали O
5	, O
6	что O
7	разработка O
8	станет O
9	такой O
10	же O
11	популярной O
12	, O
13	как O
14	Google B-Technology
15	Translator I-Technology
16	. O

# sent_id = 572
# text =   Российские ученые из Института проблем управления им. В.А. Трапезникова РАН (ИПУ РАН) несколько лет назад начали разработку подобного ИИ.
1	Российские O
2	ученые O
3	из O
4	Института B-Organization
5	проблем I-Organization
6	управления I-Organization
7	В.А. I-Organization
8	Трапезникова I-Organization
9	РАН I-Organization
10	( O
11	ИПУ B-Abbrev_Organization
12	РАН I-Abbrev_Organization
13	) O
14	несколько O
15	лет O
16	назад O
17	начали O
18	разработку O
19	подобного O
20	ИИ O
21	. O

# sent_id = 573
# text =   Она несколько лет развивает сайт «Сурдосервер».
1	Она O
2	несколько O
3	лет O
4	развивает O
5	сайт O
6	« O
7	Сурдосервер B-Technology
8	» O
9	. O

# sent_id = 574
# text =   N-грамм это просто последовательности букв из слова.
1	N B-Object
2	- I-Object
3	грамм I-Object
4	это O
5	просто O
6	последовательности O
7	букв O
8	из O
9	слова O
10	. O

# sent_id = 575
# text =   Создаются лексические и синтаксические признаки токенов текста.
1	Создаются O
2	лексические B-Object
3	и O
4	синтаксические B-Object
5	признаки I-Object
6	токенов B-Subject
7	текста B-Object
8	. O

# sent_id = 576
# text =   В качестве классификатора намерений применяем Transformer.
1	В 
2	качестве O
3	классификатора B-Method
4	намерений O
5	применяем O
6	Transformer B-Method
7	. O

# sent_id = 577
# text =  Как отличить хороший ремонт от плохого, или как мы в SRG сделали из Томита-парсера многопоточную Java-библиотеку. 
1	Как O
2	отличить O
3	хороший O
4	ремонт O
5	от O
6	плохого O
7	, O
8	или O
9	как O
10	мы O
11	в O
12	SRG B-Abbrev_Organization
13	сделали O
14	из O
15	Томита B-Technology
16	- I-Technology
17	парсера I-Technology
18	многопоточную O
19	Java B-Library
20	- I-Library
21	библиотеку I-Library
22	. O

# sent_id = 578
# text =  В этой статье речь пойдет о том, как мы интегрировали разработанный Яндексом Томита-парсер в нашу систему, превратили его в динамическую библиотеку, подружили с Java, сделали многопоточной и решили с её помощью задачу классификации текста для оценки недвижимости.
1	В O
2	этой O
3	статье O
4	речь O
5	пойдет O
6	о O
7	том O
8	, O
9	как O
10	мы O
11	интегрировали O
12	разработанный O
13	Яндексом B-Organization
14	Томита B-Technology
15	- I-Technology
16	парсера I-Technology
17	в O
18	нашу O
19	систему O
20	, O
21	превратили O
22	его O
23	в O
24	динамическую O
25	библиотеку O
26	, O
27	подружили O
28	с O
29	Java B-Environment
30	, O
31	сделали O
32	многопоточной O
33	и O
34	решили O
35	с O
36	её O
37	помощью O
38	задачу O
39	классификации B-Task
40	текста O
41	для O
42	оценки B-Task
43	недвижимости I-Task
44	. O

# sent_id = 579
# text =   Итак, у нас есть текст объявления, который необходимо классифицировать в одну из категорий согласно состоянию ремонта в квартире (без отделки, чистовой, средний, хороший, отличный, эксклюзивный).
1	Итак O
2	, O
3	у O
4	нас O
5	есть O
6	текст B-Object
7	объявления I-Object
8	, O
9	который O
10	необходимо O
11	классифицировать O
12	в O
13	одну O
14	из O
15	категорий O
16	согласно O
17	состоянию O
18	ремонта O
19	в O
20	квартире O
21	( O
22	без O
23	отделки O
24	, O
25	чистовой O
26	, O
27	средний O
28	, O
29	хороший O
30	, O
31	отличный O
32	, O
33	эксклюзивный O
34	) O
35	. O

# sent_id = 580
# text =   Таким образом, по мере решения сформировалась вторая большая и интересная задача — научиться извлекать всю достаточную и необходимую информацию о ремонте из объявления, а именно обеспечить быстрый синтаксический и морфологический анализ текста, который сможет работать параллельно под нагрузкой в режиме библиотеки.
1	Таким O
2	образом O
3	, O
4	по O
5	мере O
6	решения O
7	сформировалась O
8	вторая O
9	большая O
10	и O
11	интересная O
12	задача O
13	— O
14	научиться O
15	извлекать B-Task
16	всю I-Task
17	достаточную I-Task
18	и I-Task
19	необходимую I-Task
20	информацию I-Task
21	о O
22	ремонте O
23	из O
24	объявления O
25	, O
26	а O
27	именно O
28	обеспечить O
29	быстрый O
30	синтаксический O
31	и O
32	морфологический B-Method
33	анализ I-Method
34	текста B-Object
35	, O
36	который O
37	сможет O
38	работать O
39	параллельно O
40	под O
41	нагрузкой O
42	в O
43	режиме O
44	библиотеки O
45	. O

# sent_id = 581
# text =   Из доступных средств для извлечения фактов из текста на основе контекстно-свободных грамматик, способных работать с русским языком, наше внимание привлекли Томита-парсер и библиотека Yagry на питоне.
1	Из O
2	доступных O
3	средств O
4	для O
5	извлечения B-Task
6	фактов I-Task
7	из O
8	текста B-Object
9	на O
10	основе O
11	контекстно B-Method
12	- I-Method
13	свободных I-Method
14	грамматик I-Method
15	, O
16	способных O
17	работать O
18	с O
19	русским O
20	языком O
21	, O
22	наше O
23	внимание O
24	привлекли O
25	Томита B-Technology
26	- I-Technology
27	парсер I-Technology
28	и O
29	библиотека O
30	Yagry B-Library
31	на O
32	питоне O
33	. O

# sent_id = 582
# text =   Многопоточный вариант Томиты — TomitaPooledParser использует для парсинга пул объектов TomitaParser, одинаковым образом сконфигурированных.
1	Многопоточный O
2	вариант O
3	Томиты B-Technology
4	— O
5	TomitaPooledParser B-Technology
6	использует O
7	для O
8	парсинга B-Task
9	пул O
10	объектов O
11	TomitaParser B-Technology
12	, O
13	одинаковым O
14	образом O
15	сконфигурированных O
16	. O

# sent_id = 583
# text =   Приведу только показатели качества классификации, которые были нами получены на тестах: Accuracy = 95% F1 score = 93%
1	Приведу O
2	только O
3	показатели O
4	качества O
5	классификации B-Task
6	, O
7	которые O
8	были O
9	нами O
10	получены O
11	на O
12	тестах O
13	: O
14	Accuracy B-Metric
15	= O
16	95 B-Value
17	% I-Value
18	F1 B-Metric
19	score I-Metric
20	= O
21	93 B-Value
22	% I-Value

# sent_id = 584
# text =  JavaScript-библиотека для обработки текстов на русском языке
1	JavaScript B-Library
2	- I-Library
3	библиотека I-Library
4	для O
5	обработки B-Task
6	текстов I-Task
7	на O
8	русском O
9	языке O

# sent_id = 585
# text =   Бессвязность текстов в нынешней версии «Генератора» вызвана тем, что на самом деле никакого анализа он производить не умеет.
1	Бессвязность O
2	текстов O
3	в O
4	нынешней O
5	версии O
6	« O
7	Генератора B-Technology
8	» O
9	вызвана O
10	тем O
11	, O
12	что O
13	на O
14	самом O
15	деле O
16	никакого O
17	анализа B-Method
18	он O
19	производить O
20	не O
21	умеет O
22	. O

# sent_id = 586
# text =   На данный момент библиотека умеет две вещи: токенизацию и анализ морфологии.
1	На O
2	данный O
3	момент O
4	библиотека O
5	умеет O
6	две O
7	вещи O
8	: O
9	токенизацию B-Method
10	и O
11	анализ B-Task
12	морфологии I-Task
13	. O

# sent_id = 587
# text =   Полный список граммем можно найти на странице проекта OpenCorpora.
1	Полный O
2	список O
3	граммем O
4	можно O
5	найти O
6	на O
7	странице O
8	проекта O
9	OpenCorpora B-InfoResource
10	. O

# sent_id = 588
# text =   Кроме того, для анализа используется словарь OpenCorpora, упакованный в специальном формате, но об этом ниже.
1	Кроме O
2	того O
3	, O
4	для O
5	анализа B-Task
6	используется O
7	словарь O
8	OpenCorpora B-InfoResource
9	, O
10	упакованный O
11	в O
12	специальном O
13	формате O
14	, O
15	но O
16	об O
17	этом O
18	ниже O
19	. O

# sent_id = 589
# text =   Вообще создатели проекта OpenCorpora большие молодцы и я вам рекомендую не только ознакомиться с ним, но и принять участие в коллаборативной разметке корпуса — это также поможет и другим опенсорсным проектам.
1	Вообще O
2	создатели O
3	проекта O
4	OpenCorpora B-InfoResource
5	большие O
6	молодцы O
7	и O
8	я O
9	вам O
10	рекомендую O
11	не O
12	только O
13	ознакомиться O
14	с O
15	ним O
16	, O
17	но O
18	и O
19	принять O
20	участие O
21	в O
22	коллаборативной O
23	разметке B-Task
24	корпуса I-Task
25	— O
26	это O
27	также O
28	поможет O
29	и O
30	другим O
31	опенсорсным O
32	проектам O
33	. O

# sent_id = 590
# text =   По сути эта часть библиотеки — порт замечательного морфологического анализатора pymorphy2 за авторством kmike (на Хабре была пара статей об этой библиотеке).
1	По O
2	сути O
3	эта O
4	часть O
5	библиотеки O
6	— O
7	порт O
8	замечательного O
9	морфологического B-Application
10	анализатора I-Application
11	pymorphy2 B-Technology
12	за O
13	авторством O
14	kmike B-Person
15	( O
16	на O
17	Хабре B-InfoResource
18	была O
19	пара O
20	статей O
21	об O
22	этой O
23	библиотеке O
24	) O
25	. O

# sent_id = 591
# text =   Анализируем тональность текстов с помощью Fast.ai
1	Анализируем O
2	тональность O
3	текстов O
4	с O
5	помощью O
6	Fast.ai B-Technology

# sent_id = 592
# text =  В статье пойдет речь о классификации тональности текстовых сообщений на русском языке (а по сути любой классификации текстов, используя те же технологии).
1	В O
2	статье O
3	пойдет O
4	речь O
5	о O
6	классификации B-Task
7	тональности I-Task
8	текстовых I-Task
9	сообщений I-Task
10	на O
11	русском B-Lang
12	языке O
13	( O
14	а O
15	по O
16	сути O
17	любой O
18	классификации O
19	текстов O
20	, O
21	используя O
22	те O
23	же O
24	технологии O
25	) O
26	. O

# sent_id = 593
# text =   За основу возьмем данную статью, в которой была рассмотрена классификация тональности на архитектуре CNN с использованием Word2vec модели.
1	За O
2	основу O
3	возьмем O
4	данную O
5	статью O
6	, O
7	в O
8	которой O
9	была O
10	рассмотрена O
11	классификация B-Task
12	тональности I-Task
13	на O
14	архитектуре O
15	CNN B-Method
16	с O
17	использованием O
18	Word2vec B-Model
19	модели o
20	. O

# sent_id = 594
# text =   В нашем примере будем решать ту же самую задачу разделения твитов на позитивные и негативные на том же самом датасете с использованием модели ULMFit.
1	В O
2	нашем O
3	примере O
4	будем O
5	решать O
6	ту O
7	же O
8	самую O
9	задачу O
10	разделения B-Task
11	твитов I-Task
12	на O
13	позитивные B-Object
14	и O
15	негативные B-Object
16	на O
17	том O
18	же O
19	самом O
20	датасете O
21	с O
22	использованием O
23	модели O
24	ULMFit B-Model
25	. O

# sent_id = 595
# text =   Результат из статьи (average F1-score = 0.78142) примем в качестве baseline.
1	Результат O
2	из O
3	статьи O
4	( O
5	average O
6	F1-score B-Metric
7	= O
8	0.78142 B-Value
9	) O
10	примем O
11	в O
12	качестве O
13	baseline O
14	. O

# sent_id = 596
# text =   Модель ULMFIT была представлена разработчиками fast.ai (Jeremy Howard, Sebastian Ruder) в 2018 году.
1	Модель O
2	ULMFIT B-Model
3	была O
4	представлена O
5	разработчиками O
6	fast.ai B-Organization
7	( O
8	Jeremy B-Person
9	Howard I-Person
10	, O
11	Sebastian B-Person
12	Ruder I-Person
13	) O
14	в O
15	2018 B-Date
16	году O
17	. O

# sent_id = 597
# text =   Суть подхода состоит в использовании transfer learning в задачах NLP, когда вы используете предобученные модели, сокращая время на обучение своих моделей и снижая требования к размерам размеченной тестовой выборки.
1	Суть O
2	подхода O
3	состоит O
4	в O
5	использовании O
6	transfer B-Method
7	learning I-Method
8	в O
9	задачах O
10	NLP B-ShortName_Science
11	, O
12	когда O
13	вы O
14	используете O
15	предобученные O
16	модели O
17	, O
18	сокращая O
19	время O
20	на O
21	обучение O
22	своих O
23	моделей O
24	и O
25	снижая O
26	требования O
27	к O
28	размерам O
29	размеченной O
30	тестовой O
31	выборки O
32	. O

# sent_id = 598
# text =   Для задачи моделирования языка ULMFit использует архитектуру AWD-LSTM, которая предполагает активное использование dropout везде, где только можно и имеет смысл.
1	Для O
2	задачи O
3	моделирования B-Task
4	языка O
5	ULMFit B-Model
6	использует O
7	архитектуру O
8	AWD B-Method
9	- I-Method
10	LSTM I-Method
11	, O
12	которая O
13	предполагает O
14	активное O
15	использование O
16	dropout O
17	везде O
18	, O
19	где O
20	только O
21	можно O
22	и O
23	имеет O
24	смысл O
25	. O

# sent_id = 599
# text =   Результат, показанный на тестовой выборке average F1-score = 0,80064.
1	Результат O
2	, O
3	показанный O
4	на O
5	тестовой O
6	выборке O
7	average O
8	F1-score B-Metric
9	= O
10	0,80064 B-Value
11	. O

# sent_id = 600
# text =   Добавьте возможности IBM Watson платформы в ваши приложения, разработанные на платформе IBM Cloud, или в сторонние приложения!
1	Добавьте O
2	возможности O
3	IBM B-Technology
4	Watson I-Technology
5	платформы O
6	в O
7	ваши O
8	приложения O
9	, O
10	разработанные O
11	на O
12	платформе O
13	IBM B-Technology
14	Cloud I-Technology
15	, O
16	или O
17	в O
18	сторонние O
19	приложения O
20	! O

# sent_id = 601
# text =   IBM Automation Platform для цифрового бизнеса — это интегрированная платформа с пятью возможностями автоматизации, которая помогает бизнесу быстро и масштабно управлять практически всеми типами проектов автоматизации — от повторяющихся и административных до работы на уровне экспертов.
1	IBM B-Technology
2	Automation I-Technology
3	Platform I-Technology
4	для O
5	цифрового B-Science
6	бизнеса I-Science
7	— O
8	это O
9	интегрированная O
10	платформа O
11	с O
12	пятью O
13	возможностями O
14	автоматизации O
15	, O
16	которая O
17	помогает O
18	бизнесу O
19	быстро O
20	и O
21	масштабно O
22	управлять O
23	практически O
24	всеми O
25	типами O
26	проектов O
27	автоматизации O
28	— O
29	от O
30	повторяющихся O
31	и O
32	административных O
33	до O
34	работы O
35	на O
36	уровне O
37	экспертов O
38	. O

# sent_id = 602
# text =   HuggingArtists | Генерируем текст песен с трансформером за 5 минут 
1	HuggingArtists B-Technology
2	| O
3	Генерируем B-Task
4	текст B-Object
5	песен I-Object
6	с O
7	трансформером O
8	за O
9	5 O
10	минут O

# sent_id = 603
# text =   В HuggingArtists, мы можем создавать тексты песен на основе конкретного исполнителя.
1	В O
2	HuggingArtists B-Technology
3	, O
4	мы O
5	можем O
6	создавать B-Task
7	тексты I-Task
8	песен O
9	на O
10	основе O
11	конкретного O
12	исполнителя O
13	. O

# sent_id = 604
# text =   Это было сделано путем fine-tune (точной настройки) предварительно обученного трансформера HuggingFace  на собранных данных Genius.
1	Это O
2	было O
3	сделано O
4	путем O
5	fine B-Method
6	- I-Method
7	tune I-Method
8	( O
9	точной B-Method
10	настройки I-Method
11	) O
12	предварительно O
13	обученного O
14	трансформера O
15	HuggingFace B-Model
16	на O
17	собранных O
18	данных O
19	Genius B-Technology
20	. O

# sent_id = 605
# text =   Кроме того, мы используем интеграцию Weights & Biases для автоматического учета производительности и прогнозов модели.
1	Кроме O
2	того O
3	, O
4	мы O
5	используем O
6	интеграцию O
7	Weights B-Library
8	& I-Library
9	Biases I-Library
10	для O
11	автоматического O
12	учета O
13	производительности O
14	и O
15	прогнозов O
16	модели O
17	. O

# sent_id = 606
# text =  Анализ тональности текста с использованием фреймворка Lightautoml 
1	Анализ B-Task
2	тональности I-Task
3	текста I-Task
4	с O
5	использованием O
6	фреймворка O
7	Lightautoml B-Technology

# sent_id = 607
# text =  Сентиментный анализ (анализ тональности) – это область компьютерной лингвистики, занимающаяся изучением эмоций в текстовых документах, в основе которой лежит машинное обучение.
1	Сентиментный B-Task
2	анализ I-Task
3	( O
4	анализ B-Task
5	тональности I-Task
6	) O
7	– O
8	это O
9	область O
10	компьютерной B-Science
11	лингвистики I-Science
12	, O
13	занимающаяся O
14	изучением O
15	эмоций O
16	в O
17	текстовых O
18	документах O
19	, O
20	в O
21	основе O
22	которой O
23	лежит O
24	машинное O
25	обучение O
26	. O

# sent_id = 608
# text =  В этой статье я покажу, как мы использовали для этих целей внутреннюю разработку компании – фреймворк LightAutoML, в котором имеется всё для решения поставленной задачи – предобученные готовые векторные представления слов FastText и готовые текстовые пресеты, в которых необходимо только указать гиперпараметры.
1	В O
2	этой O
3	статье O
4	я O
5	покажу O
6	, O
7	как O
8	мы O
9	использовали O
10	для O
11	этих O
12	целей O
13	внутреннюю O
14	разработку O
15	компании O
16	– O
17	фреймворк O
18	LightAutoML B-Technology
19	, O
20	в O
21	котором O
22	имеется O
23	всё O
24	для O
25	решения O
26	поставленной O
27	задачи O
28	– O
29	предобученные B-Subject
30	готовые I-Subject
31	векторные I-Subject
32	представления I-Subject
33	слов I-Subject
34	FastText B-Model
35	и O
36	готовые O
37	текстовые O
38	пресеты O
39	, O
40	в O
41	которых O
42	необходимо O
43	только O
44	указать O
45	гиперпараметры O
46	. O

# sent_id = 609
# text =   При обучении модели значение метрики F1-score достигло 0.894, соответственно можно сделать вывод о том, что модель хорошо справляется с задачей определения нейтральных и негативных обращений.
1	При O
2	обучении O
3	модели O
4	значение O
5	метрики O
6	F1-score B-Metric
7	достигло O
8	0.894 B-Value
9	, O
10	соответственно O
11	можно O
12	сделать O
13	вывод O
14	о O
15	том O
16	, O
17	что O
18	модель O
19	хорошо O
20	справляется O
21	с O
22	задачей O
23	определения B-Task
24	нейтральных I-Task
25	и I-Task
26	негативных I-Task
27	обращений I-Task
28	. O

# sent_id = 610
# text =   Также одним из способов оценить работу модели в целом можно по кривой ROC-AUC, которая описывает площадь под кривой (Area Under Curve – Receiver Operating Characteristic).
1	Также O
2	одним O
3	из O
4	способов O
5	оценить O
6	работу O
7	модели O
8	в O
9	целом O
10	можно O
11	по O
12	кривой O
13	ROC B-Metric
14	- I-Metric
15	AUC I-Metric
16	, O
17	которая O
18	описывает O
19	площадь O
20	под O
21	кривой O
22	( O
23	Area B-Metric
24	Under I-Metric
25	Curve I-Metric
26	– I-Metric
27	Receiver I-Metric
28	Operating I-Metric
29	Characteristic I-Metric
30	) O
31	. O

# sent_id = 611
# text =   В качестве подтверждения вышесказанного можно привести работу встроенного в LAMA модуля – LIME, который раскрывает работу модели окрашивая слова в тот или иной цвет, в зависимости от их эмоционального окраса.
1	В O
2	качестве O
3	подтверждения O
4	вышесказанного O
5	можно O
6	привести O
7	работу O
8	встроенного O
9	в O
10	LAMA B-Application
11	модуля O
12	– O
13	LIME B-Technology
14	, O
15	который O
16	раскрывает O
17	работу O
18	модели O
19	окрашивая O
20	слова B-Subject
21	в O
22	тот O
23	или O
24	иной O
25	цвет O
26	, O
27	в O
28	зависимости O
29	от O
30	их O
31	эмоционального O
32	окраса O
33	. O

# sent_id = 612
# text =  Также фреймворк может решать задачи регрессионного анализа, целью которого является определение зависимости между переменными и оценкой функции регрессии.
1	Также O
2	фреймворк O
3	может O
4	решать O
5	задачи O
6	регрессионного B-Task
7	анализа I-Task
8	, O
9	целью O
10	которого O
11	является O
12	определение B-Task
13	зависимости I-Task
14	между I-Task
15	переменными I-Task
16	и O
17	оценкой B-Task
18	функции I-Task
19	регрессии I-Task
20	. O

# sent_id = 613
# text =   Работа с текстомВ LightAutoML имеется большое количество вариантов разработки той или иной модели, работающей с текстом.
1	Работа O
2	с O
3	текстомВ O
4	LightAutoML B-Technology
5	имеется O
6	большое O
7	количество O
8	вариантов O
9	разработки O
10	той O
11	или O
12	иной O
13	модели O
14	, O
15	работающей O
16	с O
17	текстом O
18	. O

# sent_id = 614
# text =   Библиотека предоставляет не только получение стандартных признаков на основе TF-IDF, но и на основе эмбеддингов:1) На основе встроенного FastText, который можно тренировать на том или ином корпусе2) Предобученных моделей Gensim3) Любой другой объект, который имеет вид словаря, где на вход подается слово, а на выходе его эмбеддинги
1	Библиотека O
2	предоставляет O
3	не O
4	только O
5	получение O
6	стандартных O
7	признаков O
8	на O
9	основе O
10	TF B-Metric
11	- I-Metric
12	IDF I-Metric
13	, O
14	но O
15	и O
16	на O
17	основе O
18	эмбеддингов O
19	: O
20	1 O
21	) O
22	На O
23	основе O
24	встроенного O
25	FastText B-Model
26	, O
27	который O
28	можно O
29	тренировать O
30	на O
31	том O
32	или O
33	ином O
34	корпусе O
35	2 O
36	) O
37	Предобученных O
38	моделей O
39	Gensim3 B-Model
40	) O
41	Любой O
42	другой O
43	объект O
44	, O
45	который O
46	имеет O
47	вид O
48	словаря O
49	, O
50	где O
51	на O
52	вход O
53	подается O
54	слово O
55	, O
56	а O
57	на O
58	выходе O
59	его O
60	эмбеддинги O

# sent_id = 615
# text = Среди используемых стратегий извлечения представлений текстов из эмбеддингов слов, можно выделить:1) Weighted Average Transformer (WAT) – взвешивается каждое слово с некоторым весом
1	Среди O
2	используемых O
3	стратегий O
4	извлечения O
5	представлений O
6	текстов O
7	из O
8	эмбеддингов O
9	слов O
10	, O
11	можно O
12	выделить O
13	: O
14	1 O
15	) O
16	Weighted B-Method
17	Average I-Method
18	Transformer I-Method
19	( O
20	WAT B-ShortName_Method
21	) O
22	– O
23	взвешивается O
24	каждое O
25	слово O
26	с O
27	некоторым O
28	весом O

# sent_id = 616
# text =  Bag of Random Embedding Projections (BOREP) – строится линейная модель со случайными весами  
1	Bag B-Method
2	of I-Method
3	Random I-Method
4	Embedding I-Method
5	Projections I-Method
6	( O
7	BOREP B-ShortName_Method
8	) O
9	– O
10	строится O
11	линейная O
12	модель O
13	со O
14	случайными O
15	весами O

# sent_id = 617
# text =  Bert Pooling – получение эмбеддинга с последнего выхода модели Transformer  
1	Bert B-Method
2	Pooling I-Method
3	– O
4	получение O
5	эмбеддинга O
6	с O
7	последнего O
8	выхода O
9	модели O
10	Transformer B-Model

# sent_id = 618
# text =  За препроцессинг текста отвечает класс токенайзера, по умолчанию применяется только для TF-IDF.
1	За O
2	препроцессинг O
3	текста O
4	отвечает O
5	класс O
6	токенайзера O
7	, O
8	по O
9	умолчанию O
10	применяется O
11	только O
12	для O
13	TF B-Metric
14	- I-Metric
15	IDF I-Metric
16	. O

# sent_id = 619
# text =  Подводя итоги стоит сказать, что LightAutoML благодаря встроенному инструментарию способен показывать достаточно хорошие результаты в задачах бинарной или мультиклассовой классификации и регрессии.
1	Подводя O
2	итоги O
3	стоит O
4	сказать O
5	, O
6	что O
7	LightAutoML B-Technology
8	благодаря O
9	встроенному O
10	инструментарию O
11	способен O
12	показывать O
13	достаточно O
14	хорошие O
15	результаты O
16	в O
17	задачах O
18	бинарной O
19	или O
20	мультиклассовой B-Task
21	классификации I-Task
22	и O
23	регрессии O
24	. O

# sent_id = 620
# text =  Конкретно в нашем случае нам удалось создать модель сентиментного анализа, которая с 89% точностью определяет эмоциональный окрас обращения и слова, которые оказывают на это наибольшее влияние.
1	Конкретно O
2	в O
3	нашем O
4	случае O
5	нам O
6	удалось O
7	создать O
8	модель O
9	сентиментного O
10	анализа O
11	, O
12	которая O
13	с O
14	89 B-Value
15	% I-Value
16	точностью B-Metric
17	определяет O
18	эмоциональный O
19	окрас O
20	обращения O
21	и O
22	слова O
23	, O
24	которые O
25	оказывают O
26	на O
27	это O
28	наибольшее O
29	влияние O
30	. O

# sent_id = 621
# text =   Яндекс открывает датасеты Беспилотных автомобилей, Погоды и Переводчика, чтобы помочь решить проблему сдвига данных в ML       
1	Яндекс B-Organization
2	открывает O
3	датасеты O
4	Беспилотных O
5	автомобилей O
6	, O
7	Погоды O
8	и O
9	Переводчика O
10	, O
11	чтобы O
12	помочь O
13	решить O
14	проблему O
15	сдвига B-Task
16	данных I-Task
17	в O
18	ML B-ShortName_Science

# sent_id = 622
# text =   Для современных моделей, которые используются в машинном переводе, такой язык представляет серьезную проблему, так как большинство переводчиков обучаются на чуть более формальном языке: классической литературе, юридических документах или статьях Википедии.
1	Для O
2	современных O
3	моделей O
4	, O
5	которые O
6	используются O
7	в O
8	машинном O
9	переводе O
10	, O
11	такой O
12	язык O
13	представляет O
14	серьезную O
15	проблему O
16	, O
17	так O
18	как O
19	большинство O
20	переводчиков O
21	обучаются O
22	на O
23	чуть O
24	более O
25	формальном O
26	языке O
27	: O
28	классической B-InfoResource
29	литературе I-InfoResource
30	, O
31	юридических B-InfoResource
32	документах I-InfoResource
33	или O
34	статьях B-InfoResource
35	Википедии B-InfoResource
36	. O

# sent_id = 623
# text =   В треке перевода мы использовали для обучения англо-русский корпус WMT’20, который в основном состоит из государственных и новостных текстов.
1	В O
2	треке O
3	перевода O
4	мы O
5	использовали O
6	для O
7	обучения O
8	англо O
9	- O
10	русский O
11	корпус O
12	WMT’20 B-Corpus
13	, O
14	который O
15	в O
16	основном O
17	состоит O
18	из O
19	государственных O
20	и O
21	новостных B-InfoResource
22	текстов I-InfoResource
23	. O

# sent_id = 624
# text =   Данные без сдвига взяты из англо-русского корпуса Newstest’19, а также из корпуса новостных текстов, собранных службой Global Voices и переведенных Яндексом.
1	Данные O
2	без O
3	сдвига O
4	взяты O
5	из O
6	англо B-Lang
7	- O
8	русского B-Lang
9	корпуса O
10	Newstest’19 B-Corpus
11	, O
12	а O
13	также O
14	из O
15	корпуса O
16	новостных O
17	текстов B-Object
18	, O
19	собранных O
20	службой O
21	Global B-Organization
22	Voices I-Organization
23	и O
24	переведенных O
25	Яндексом B-Organization
26	. O

# sent_id = 625
# text =   Данные со сдвигом для отладки взяты из подготовленного для WMT Robustness Challenge корпуса Reddit и также переведены Яндексом.
1	Данные O
2	со O
3	сдвигом O
4	для O
5	отладки O
6	взяты O
7	из O
8	подготовленного O
9	для O
10	WMT O
11	Robustness O
12	Challenge O
13	корпуса O
14	Reddit B-Corpus
15	и O
16	также O
17	переведены O
18	Яндексом B-Organization
19	. O

# sent_id = 626
# text =   Для проверки модели на данных со сдвигом мы также собрали, перевели и разметили дополнительные данные с Reddit.
1	Для O
2	проверки O
3	модели O
4	на O
5	данных O
6	со O
7	сдвигом O
8	мы O
9	также O
10	собрали O
11	, O
12	перевели O
13	и O
14	разметили O
15	дополнительные O
16	данные O
17	с O
18	Reddit B-Corpus
19	. O

# sent_id = 627
# text =   Парсить комментарии мы будем с помощью официального API ВКонтакте для Python
1	Парсить O
2	комментарии O
3	мы O
4	будем O
5	с O
6	помощью O
7	официального O
8	API B-Application
9	ВКонтакте I-Application
10	для O
11	Python B-Environment

# sent_id = 628
# text =   Необходимо убрать из комментария направление, чтобы при поиске расстояния Левенштейна меньше ошибаться.
1	Необходимо O
2	убрать O
3	из O
4	комментария O
5	направление O
6	, O
7	чтобы O
8	при O
9	поиске O
10	расстояния B-Metric
11	Левенштейна I-Metric
12	меньше O
13	ошибаться O
14	. O

# sent_id = 629
# text =   Небольшая справка: расстояние Левенштейна — минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую.
1	Небольшая O
2	справка O
3	: O
4	расстояние B-Metric
5	Левенштейна I-Metric
6	— O
7	минимальное O
8	количество O
9	операций O
10	вставки O
11	одного O
12	символа O
13	, O
14	удаления O
15	одного O
16	символа O
17	и O
18	замены O
19	одного O
20	символа O
21	на O
22	другой O
23	, O
24	необходимых O
25	для O
26	превращения O
27	одной O
28	строки O
29	в O
30	другую O
31	. O

# sent_id = 630
# text =   Его мы будем находить с помощью библиотеки fuzzywuzzy.
1	Его O
2	мы O
3	будем O
4	находить O
5	с O
6	помощью O
7	библиотеки O
8	fuzzywuzzy B-Library
9	. O

# sent_id = 631
# text =   Для ускорения работы авторы библиотеки советуют также установить библиотеку python-Levenshtein.
1	Для O
2	ускорения O
3	работы O
4	авторы O
5	библиотеки O
6	советуют O
7	также O
8	установить O
9	библиотеку O
10	python B-Library
11	- I-Library
12	Levenshtein I-Library
13	. O

# sent_id = 632
# text =   Его мне любезно предоставил разработчик приложения GoTrans, Александр Козлов.
1	Его O
2	мне O
3	любезно O
4	предоставил O
5	разработчик O
6	приложения O
7	GoTrans B-Technology
8	, O
9	Александр B-Person
10	Козлов I-Person
11	. O

# sent_id = 633
# text =   Самый сложный кроссворд, составленный компьютером
1	Самый O
2	сложный O
3	кроссворд B-Object
4	, O
5	составленный O
6	компьютером O

# sent_id = 634
# text =   Пример Deep Blue показывает, что программы ИИ могут участвовать в викторинах и обыгрывать людей.
1	Пример O
2	Deep B-Technology
3	Blue I-Technology
4	показывает O
5	, O
6	что O
7	программы O
8	ИИ O
9	могут O
10	участвовать O
11	в O
12	викторинах B-Object
13	и O
14	обыгрывать O
15	людей O
16	. O

# sent_id = 635
# text =   Американский разработчик Мэтью Гинсберг (Matthew Ginsberg) создал программу под названием Dr Fill, которая справляется с кроссвордами гораздо лучше, чем абсолютное большинство людей, пишет New Scientist.
1	Американский O
2	разработчик O
3	Мэтью B-Person
4	Гинсберг I-Person
5	( O
6	Matthew B-Person
7	Ginsberg I-Person
8	) O
9	создал O
10	программу O
11	под O
12	названием O
13	Dr B-Technology
14	Fill I-Technology
15	, O
16	которая O
17	справляется O
18	с O
19	кроссвордами B-Object
20	гораздо O
21	лучше O
22	, O
23	чем O
24	абсолютное O
25	большинство O
26	людей O
27	, O
28	пишет O
29	New B-Organization
30	Scientist I-Organization
31	. O

# sent_id = 636
# text =   Анализ тональности текстов с помощью сверточных нейронных сетей 
1	Анализ B-Task
2	тональности I-Task
3	текстов I-Task
4	с O
5	помощью O
6	сверточных B-Method
7	нейронных I-Method
8	сетей I-Method

# sent_id = 637
# text =   Есть много способов решать такую задачу, и один из них — свёрточные нейронные сети (Convolutional Neural Networks).
1	Есть O
2	много O
3	способов O
4	решать O
5	такую O
6	задачу O
7	, O
8	и O
9	один O
10	из O
11	них O
12	— O
13	свёрточные B-Method
14	нейронные I-Method
15	сети I-Method
16	( O
17	Convolutional B-Method
18	Neural I-Method
19	Networks I-Method
20	) O
21	. O

# sent_id = 638
# text =   CNN изначально были разработаны для обработки изображений, однако они успешно справляются с решением задач в сфере автоматической обработки текстов.
1	CNN B-Method
2	изначально O
3	были O
4	разработаны O
5	для O
6	обработки O
7	изображений O
8	, O
9	однако O
10	они O
11	успешно O
12	справляются O
13	с O
14	решением O
15	задач O
16	в O
17	сфере O
18	автоматической B-Science
19	обработки I-Science
20	текстов I-Science
21	. O

# sent_id = 639
# text =   Я познакомлю вас с бинарным анализом тональности русскоязычных текстов с помощью свёрточной нейронной сети, для которой векторные представления слов были сформированы на основе обученной Word2Vec модели.
1	Я O
2	познакомлю O
3	вас O
4	с O
5	бинарным O
6	анализом B-Task
7	тональности I-Task
8	русскоязычных B-Lang
9	текстов B-Object
10	с O
11	помощью O
12	свёрточной B-Method
13	нейронной I-Method
14	сети I-Method
15	, O
16	для O
17	которой O
18	векторные B-Object
19	представления I-Object
20	слов I-Object
21	были O
22	сформированы O
23	на O
24	основе O
25	обученной O
26	Word2Vec B-Model
27	модели O
28	. O

# sent_id = 640
# text =   Для обучения я выбрал корпус коротких текстов Юлии Рубцовой, сформированный на основе русскоязычных сообщений из Twitter [4].
1	Для O
2	обучения O
3	я O
4	выбрал O
5	корпус B-Corpus
6	коротких I-Corpus
7	текстов I-Corpus
8	Юлии B-Person
9	Рубцовой I-Person
10	, O
11	сформированный O
12	на O
13	основе O
14	русскоязычных B-Lang
15	сообщений B-Object
16	из O
17	Twitter B-Technology
18	[ O
19	4 O
20	] O
21	. O

# sent_id = 641
# text =   Визуализация кластеров похожих слов с использование t-SNE.
1	Визуализация O
2	кластеров B-Result
3	похожих O
4	слов O
5	с O
6	использование O
7	t B-Method
8	- I-Method
9	SNE I-Method
10	. O

# sent_id = 642
# text =   На следующем этапе каждый текст был отображен в массив идентификаторов токенов.
1	На O
2	следующем O
3	этапе O
4	каждый O
5	текст B-Object
6	был O
7	отображен O
8	в O
9	массив O
10	идентификаторов O
11	токенов B-Subject
12	. O

# sent_id = 643
# text =   Вот пусть комментаторы поправят, но кроме модуля LanguageTool для Open Office (о нём мы ещё поговорим) даже в голову ничего не приходит.
1	Вот O
2	пусть O
3	комментаторы O
4	поправят O
5	, O
6	но O
7	кроме O
8	модуля O
9	LanguageTool B-Technology
10	для O
11	Open B-Technology
12	Office I-Technology
13	( O
14	о O
15	нём O
16	мы O
17	ещё O
18	поговорим O
19	) O
20	даже O
21	в O
22	голову O
23	ничего O
24	не O
25	приходит O
26	. O

# sent_id = 644
# text =   Было бы здорово составить базу с инструкциями не для людей, а для роботов, подумали инженеры из Института искусственного интеллекта при Бременском университете (Германия), авторы проекта RoboHow.
1	Было O
2	бы O
3	здорово O
4	составить O
5	базу O
6	с O
7	инструкциями B-Object
8	не O
9	для O
10	людей O
11	, O
12	а O
13	для O
14	роботов O
15	, O
16	подумали O
17	инженеры O
18	из O
19	Института B-Organization
20	искусственного I-Organization
21	интеллекта I-Organization
22	при O
23	Бременском B-Organization
24	университете I-Organization
25	( O
26	Германия O
27	) O
28	, O
29	авторы O
30	проекта O
31	RoboHow B-Project
32	. O

# sent_id = 645
# text =   С такой базой wiki-инструкций роботы смогут передавать информацию друг другу.
1	С O
2	такой O
3	базой O
4	wiki B-Database
5	- I-Database
6	инструкций I-Database
7	роботы O
8	смогут O
9	передавать O
10	информацию O
11	друг O
12	другу O
13	. O

# sent_id = 646
# text =   Созданный в Бременском университете робот PR2 (на фото вверху) учится понимать и выполнять «человеческие» инструкции из базы WikiHow.
1	Созданный O
2	в O
3	Бременском B-Organization
4	университете I-Organization
5	робот O
6	PR2 O
7	( O
8	на O
9	фото O
10	вверху O
11	) O
12	учится O
13	понимать O
14	и O
15	выполнять O
16	« O
17	человеческие O
18	» O
19	инструкции B-Object
20	из O
21	базы O
22	WikiHow B-Database
23	. O

# sent_id = 647
# text =   Успешно выполнив задачу, то есть усвоив урок, робот делится приобретёнными знаниями со своими собратьями через онлайновую базу Open Ease.
1	Успешно O
2	выполнив O
3	задачу O
4	, O
5	то O
6	есть O
7	усвоив O
8	урок O
9	, O
10	робот O
11	делится O
12	приобретёнными O
13	знаниями O
14	со O
15	своими O
16	собратьями O
17	через O
18	онлайновую O
19	базу O
20	Open B-Database
21	Ease I-Database
22	. O

# sent_id = 648
# text =   Здесь инструкции записаны в машиночитаемом виде, на языке, похожем на язык Семантической сети.
1	Здесь O
2	инструкции B-Object
3	записаны O
4	в O
5	машиночитаемом O
6	виде O
7	, O
8	на O
9	языке O
10	, O
11	похожем O
12	на O
13	язык O
14	Семантической B-Application
15	сети I-Application
16	. O

# sent_id = 649
# text =   Это очень сложная задача, которая сочетает в себе тесную интеграцию распознавания речи, интерпретации команд на естественном языке, машинного зрения и планирования сложных действий через алгоритмы осуществления отдельных манипуляций.
1	Это O
2	очень O
3	сложная O
4	задача O
5	, O
6	которая O
7	сочетает O
8	в O
9	себе O
10	тесную O
11	интеграцию O
12	распознавания B-Task
13	речи I-Task
14	, O
15	интерпретации B-Task
16	команд I-Task
17	на I-Task
18	естественном I-Task
19	языке I-Task
20	, O
21	машинного O
22	зрения O
23	и O
24	планирования O
25	сложных O
26	действий O
27	через O
28	алгоритмы O
29	осуществления O
30	отдельных O
31	манипуляций O
32	. O

# sent_id = 650
# text =   «М.видео-Эльдорадо» внедряет нейросеть для ответов на вопросы покупателей 
1	« O
2	М.видео B-Organization
3	- I-Organization
4	Эльдорадо I-Organization
5	» O
6	внедряет O
7	нейросеть O
8	для O
9	ответов B-Task
10	на I-Task
11	вопросы I-Task
12	покупателей O

# sent_id = 651
# text =   Президент Ассоциации больших данных Анна Серебряникова отметила, что ИИ в ретейле может применяться для прогнозирования открытия новых торговых точек, а также для персонализации предложений для клиентов и создания чат-ботов для службы поддержки.
1	Президент O
2	Ассоциации B-Organization
3	больших I-Organization
4	данных I-Organization
5	Анна B-Person
6	Серебряникова I-Person
7	отметила O
8	, O
9	что O
10	ИИ O
11	в O
12	ретейле O
13	может O
14	применяться O
15	для O
16	прогнозирования O
17	открытия O
18	новых O
19	торговых O
20	точек O
21	, O
22	а O
23	также O
24	для O
25	персонализации B-Task
26	предложений I-Task
27	для O
28	клиентов O
29	и O
30	создания B-Task
31	чат I-Task
32	- I-Task
33	ботов I-Task
34	для O
35	службы O
36	поддержки O
37	. O

# sent_id = 652
# text =   В Facebook AI продемонстрировали прямой машинный перевод с одного языка на другой
1	В O
2	Facebook B-Organization
3	AI I-Organization
4	продемонстрировали O
5	прямой O
6	машинный O
7	перевод O
8	с O
9	одного O
10	языка O
11	на O
12	другой O

# sent_id = 653
# text =  Facebook AI представила новую систему машинного перевода M2M-100 с 15 млрд параметров.
1	Facebook B-Organization
2	AI I-Organization
3	представила O
4	новую O
5	систему B-Application
6	машинного I-Application
7	перевода I-Application
8	M2M-100 B-Model
9	с O
10	15 O
11	млрд O
12	параметров O
13	. O

# sent_id = 654
# text =   Она способна переводить с одного языка на другой напрямую, не используя английский в качестве промежуточного.
1	Она O
2	способна O
3	переводить B-Task
4	с I-Task
5	одного I-Task
6	языка I-Task
7	на I-Task
8	другой I-Task
9	напрямую O
10	, O
11	не O
12	используя O
13	английский O
14	в O
15	качестве O
16	промежуточного O
17	. O

# sent_id = 655
# text =   Она способна осуществлять переводы между парами из ста языков.
1	Она O
2	способна O
3	осуществлять O
4	переводы B-Task
5	между O
6	парами O
7	из O
8	ста O
9	языков O
10	. O

# sent_id = 656
# text =   Модель обучали на наборе данных из более чем 7,5 млрд предложений как из базы Facebook, так и из других источников.
1	Модель B-Object
2	обучали O
3	на O
4	наборе O
5	данных O
6	из O
7	более O
8	чем O
9	7,5 O
10	млрд O
11	предложений O
12	как O
13	из O
14	базы O
15	Facebook B-Organization
16	, O
17	так O
18	и O
19	из O
20	других O
21	источников O
22	. O

# sent_id = 657
# text =   При разработке использовали инструмент CommonCrawl, который поддерживает открытый репозиторий данных веб-сканирования, и систему классификации текстов FastText, которую в Facebook представили несколько лет назад.
1	При O
2	разработке O
3	использовали O
4	инструмент O
5	CommonCrawl B-Technology
6	, O
7	который O
8	поддерживает O
9	открытый O
10	репозиторий O
11	данных O
12	веб O
13	- O
14	сканирования O
15	, O
16	и O
17	систему O
18	классификации O
19	текстов O
20	FastText App_system
21	, O
22	которую O
23	в O
24	Facebook B-Organization
25	представили O
26	несколько O
27	лет O
28	назад O
29	. O

# sent_id = 658
# text =   Согласно метрикам BLEU, M2M-100 на 10 баллов опережает предшественника, где английский язык был промежуточным.
1	Согласно O
2	метрикам O
3	BLEU B-Metric
4	, O
5	M2M-100 B-Model
6	на O
7	10 O
8	баллов O
9	опережает O
10	предшественника O
11	, O
12	где O
13	английский O
14	язык  O
15	был O
16	промежуточным O
17	. O

# sent_id = 659
# text =   Facebook AI отмечает, что эта модель может быть полезной не только при машинном переводе, но и при изучении языков.
1	Facebook B-Organization
2	AI I-Organization
3	отмечает O
4	, O
5	что O
6	эта O
7	модель O
8	может O
9	быть O
10	полезной O
11	не O
12	только O
13	при O
14	машинном B-Task
15	переводе I-Task
16	, O
17	но O
18	и O
19	при O
20	изучении O
21	языков O
22	. O

# sent_id = 660
# text =   Я тестировала Google Translate на одних и тех же текстах в марте и декабре 2011, январе 2016 и декабре 2017 года.
1	Я O
2	тестировала O
3	Google B-Technology
4	Translate I-Technology
5	на O
6	одних O
7	и O
8	тех O
9	же O
10	текстах O
11	в O
12	марте O
13	и O
14	декабре O
15	2011 O
16	, O
17	январе O
18	2016 O
19	и O
20	декабре O
21	2017 O
22	года O
23	. O

# sent_id = 661
# text =   Брала одни и те же отрывки на английском, русском, немецком, французском, украинском и польском языках и переводила каждый на остальные пять языков из выборки.
1	Брала O
2	одни O
3	и O
4	те O
5	же O
6	отрывки O
7	на O
8	английском B-Lang
9	, O
10	русском B-Lang
11	, O
12	немецком B-Lang
13	, O
14	французском B-Lang
15	, O
16	украинском B-Lang
17	и O
18	польском B-Lang
19	языках O
20	и O
21	переводила O
22	каждый O
23	на O
24	остальные O
25	пять O
26	языков O
27	из O
28	выборки O
29	. O

# sent_id = 662
# text =   Результаты cross-verification в целом совпали с тенденциями в первоначальной выборке.
1	Результаты O
2	cross B-Method
3	- I-Method
4	verification I-Method
5	в O
6	целом O
7	совпали O
8	с O
9	тенденциями O
10	в O
11	первоначальной O
12	выборке O
13	. O

# sent_id = 663
# text =   С марта 2017 года нейросеть стали использовать для перевода на русский.
1	С O
2	марта O
3	2017 B-TERM
4	года O
5	нейросеть O
6	стали O
7	использовать O
8	для O
9	перевода B-Task
10	на O
11	русский B-Lang
12	. O

# sent_id = 664
# text =   Сервис не переводит дословно, результат стал более свободным: адекватная перефразировка, перегруппировка слов, перестановка слов из начала в конец предложения, если того требуют правила языка (в немецком это реализовано великолепно).
1	Сервис O
2	не O
3	переводит O
4	дословно O
5	, O
6	результат O
7	стал O
8	более O
9	свободным O
10	: O
11	адекватная O
12	перефразировка B-Task
13	, O
14	перегруппировка B-Task
15	слов I-Task
16	, O
17	перестановка B-Task
18	слов I-Task
19	из O
20	начала O
21	в O
22	конец O
23	предложения O
24	, O
25	если O
26	того O
27	требуют O
28	правила O
29	языка O
30	( O
31	в O
32	немецком B-Lang
33	это O
34	реализовано O
35	великолепно O
36	) O
37	. O

# sent_id = 665
# text =   В отличие от предыдущего уровня (phrase-based translation– однократное нахождение соответствий отдельных слов и фраз), нейронный переводчик в какой-то степени трансформирует предложения, анализирует их как единое целое и устанавливает соответствия «из конца в конец» в несколько стадий(end-to-end mapping – сквозное преобразование, полного цикла, непрерывная трансформация многообразия данных со входа на выход).
1	В O
2	отличие O
3	от O
4	предыдущего O
5	уровня O
6	( O
7	phrase B-Method
8	- I-Method
9	based I-Method
10	translation I-Method
11	– O
12	однократное B-Method
13	нахождение I-Method
14	соответствий I-Method
15	отдельных I-Method
16	слов I-Method
17	и O
18	фраз O
19	) O
20	, O
21	нейронный O
22	переводчик O
23	в O
24	какой O
25	- O
26	то O
27	степени O
28	трансформирует O
29	предложения O
30	, O
31	анализирует O
32	их O
33	как O
34	единое O
35	целое O
36	и O
37	устанавливает O
38	соответствия O
39	« O
40	из O
41	конца O
42	в O
43	конец O
44	» O
45	в O
46	несколько O
47	стадий O
48	( O
49	end B-Method
50	- I-Method
51	to I-Method
52	- I-Method
53	end I-Method
54	mapping I-Method
55	– O
56	сквозное B-Method
57	преобразование I-Method
58	, O
59	полного O
60	цикла O
61	, O
62	непрерывная O
63	трансформация B-Method
64	многообразия I-Method
65	данных I-Method
66	со O
67	входа O
68	на O
69	выход O
70	) O
71	. O

# sent_id = 666
# text =   Сейчас в Яндексе мой основной проект это Алиса, голосовой помощник, который Яндекс запустил в октябре прошлого года, и моя группа отвечает за то, что можно условно назвать мозгами Алисы.
1	Сейчас O
2	в O
3	Яндексе B-Organization
4	мой O
5	основной O
6	проект O
7	это O
8	Алиса B-Technology
9	, O
10	голосовой B-Object
11	помощник I-Object
12	, O
13	который O
14	Яндекс B-Organization
15	запустил O
16	в O
17	октябре O
18	прошлого O
19	года O
20	, O
21	и O
22	моя O
23	группа O
24	отвечает O
25	за O
26	то O
27	, O
28	что O
29	можно O
30	условно O
31	назвать O
32	мозгами O
33	Алисы B-Technology
34	. O

# sent_id = 667
# text =   Мы интерпретируем то, что сказал пользователь на естественном языке и превращаем это в некоторое структурированное представление.
1	Мы O
2	интерпретируем O
3	то O
4	, O
5	что O
6	сказал O
7	пользователь O
8	на O
9	естественном B-Object
10	языке I-Object
11	и O
12	превращаем O
13	это O
14	в O
15	некоторое O
16	структурированное B-Object
17	представление B-Object
18	. O

# sent_id = 668
# text =   Есть Siri, единственный голосовой помощник, который тоже понимает русский язык, но он работает только на iOS и MacOS, это как бы не самая популярная платформа в России, и к Siri как к продукту тоже есть определенные вопросы.
1	Есть O
2	Siri B-Technology
3	, O
4	единственный O
5	голосовой B-Object
6	помощник I-Object
7	, O
8	который O
9	тоже O
10	понимает O
11	русский B-Lang
12	язык O
13	, O
14	но O
15	он O
16	работает O
17	только O
18	на O
19	iOS B-Environment
20	и O
21	MacOS B-Environment
22	, O
23	это O
24	как O
25	бы O
26	не O
27	самая O
28	популярная O
29	платформа O
30	в O
31	России O
32	, O
33	и O
34	к O
35	Siri B-Technology
36	как O
37	к O
38	продукту O
39	тоже O
40	есть O
41	определенные O
42	вопросы O
43	. O

# sent_id = 669
# text =   На самом деле у нас уже есть модель которая оценивает градацию этой оскорбительности, и если бы возникла продуктовая необходимость, мы уже могли бы сделать такой ползунок который делает ответы более или менее дерзкими.
1	На O
2	самом O
3	деле O
4	у O
5	нас O
6	уже O
7	есть O
8	модель O
9	которая O
10	оценивает B-Task
11	градацию I-Task
12	этой O
13	оскорбительности I-Task
14	, O
15	и O
16	если O
17	бы O
18	возникла O
19	продуктовая O
20	необходимость O
21	, O
22	мы O
23	уже O
24	могли O
25	бы O
26	сделать O
27	такой O
28	ползунок O
29	который O
30	делает O
31	ответы O
32	более O
33	или O
34	менее O
35	дерзкими O
36	. O

# sent_id = 670
# text =   Это генеративная нейронная сеть, способная решать множество задач по обработке естествнного языка (NLP).
1	Это O
2	генеративная B-Method
3	нейронная I-Method
4	сеть I-Method
5	, O
6	способная O
7	решать O
8	множество O
9	задач O
10	по O
11	обработке B-Science
12	естествнного I-Science
13	языка I-Science
14	( O
15	NLP B-ShortName_Method
16	) O
17	. O

# sent_id = 671
# text =   Это такие задачи как суммаризация (сделать из большого текста его резюме), понимание текста (NLU), вопросно-ответные системы, генерация (например, стихов, — на Хабре была хорошая статья) и другие.
1	Это O
2	такие O
3	задачи O
4	как O
5	суммаризация B-Task
6	( O
7	сделать O
8	из O
9	большого O
10	текста O
11	его O
12	резюме O
13	) O
14	, O
15	понимание B-Task
16	текста I-Task
17	( O
18	NLU Abbrev_Task
19	) O
20	, O
21	вопросно B-App_system
22	- I-App_system
23	ответные I-App_system
24	системы I-App_system
25	, O
26	генерация B-Task
27	( O
28	например O
29	, O
30	стихов O
31	, O
32	— O
33	на O
34	Хабре B-Organization
35	была O
36	хорошая O
37	статья O
38	) O
39	и O
40	другие O
41	. O

# sent_id = 672
# text =   В Яндекс.Браузер внедрили машинный перевод видеороликов 
1	В O
2	Яндекс B-Technology
3	. I-Technology
4	Браузер I-Technology
5	внедрили O
6	машинный B-Science
7	перевод I-Science
8	видеороликов O

# sent_id = 673
# text =   Алгоритм отслеживает темп речи говорящего, за счет чего переводчик делает паузы, замедляет или ускоряет речь, чтобы закадровый голос совпадал с картинкой.Перевод доступен в Яндекс.Браузере для Windows и macOS.
1	Алгоритм O
2	отслеживает O
3	темп B-Object
4	речи I-Object
5	говорящего O
6	, O
7	за O
8	счет O
9	чего O
10	переводчик O
11	делает O
12	паузы B-Object
13	, O
14	замедляет O
15	или O
16	ускоряет O
17	речь B-Subject
18	, O
19	чтобы O
20	закадровый O
21	голос B-Subject
22	совпадал O
23	с O
24	картинкой O
25	. O
26	Перевод O
27	доступен O
28	в O
29	Яндекс B-Organization
30	. O
31	Браузере O
32	для O
33	Windows B-Environment
34	и O
35	macOS B-Environment
36	. O

# sent_id = 674
# text =   Тогда к статистической модели, которая была в «Переводчике» с момента запуска, добавили технологию перевода с помощью нейросети.
1	Тогда O
2	к O
3	статистической O
4	модели O
5	, O
6	которая O
7	была O
8	в O
9	« O
10	Переводчике B-Application
11	» O
12	с O
13	момента O
14	запуска O
15	, O
16	добавили O
17	технологию O
18	перевода B-Task
19	с O
20	помощью O
21	нейросети O
22	. O

# sent_id = 675
# text =   Компания объясняла, что ее технология не разбивает текст на отдельные слова, а рассматривает его целиком, чтобы лучше передать смысл.В июне Яндекс открыл доступ к нейросети «Балабоба» для всех пользователей.
1	Компания O
2	объясняла O
3	, O
4	что O
5	ее O
6	технология O
7	не O
8	разбивает O
9	текст B-Object
10	на O
11	отдельные O
12	слова B-Subject
13	, O
14	а O
15	рассматривает O
16	его O
17	целиком O
18	, O
19	чтобы O
20	лучше O
21	передать O
22	смысл O
23	. O
24	В O
25	июне O
26	Яндекс B-Organization
27	открыл O
28	доступ O
29	к O
30	нейросети O
31	« O
32	Балабоба B-Technology
33	» O
34	для O
35	всех O
36	пользователей O
37	. O

# sent_id = 676
# text =   Она работает на языковой модели из семейства YaLM (Yet another Language Model).
1	Она O
2	работает O
3	на O
4	языковой O
5	модели O
6	из O
7	семейства O
8	YaLM B-ShortName_Model
9	( O
10	Yet B-Model
11	another I-Model
12	Language I-Model
13	Model I-Model
14	) O
15	. O

# sent_id = 677
# text =   Эта модель помогает нейросети запоминать правила языка, выбирать подходящие слова и связывать их по смыслу.
1	Эта O
2	модель O
3	помогает O
4	нейросети O
5	запоминать B-Task
6	правила I-Task
7	языка I-Task
8	, O
9	выбирать B-Task
10	подходящие I-Task
11	слова I-Task
12	и O
13	связывать O
14	их O
15	по O
16	смыслу O
17	. O

# sent_id = 678
# text =   У «Балабобы» нет своего мнения, она выдает случайные продолжения и может закончить историю, придумать подпись или написать небольшой рассказ.
1	У O
2	« O
3	Балабобы B-Technology
4	» O
5	нет O
6	своего O
7	мнения O
8	, O
9	она O
10	выдает O
11	случайные O
12	продолжения O
13	и O
14	может O
15	закончить O
16	историю O
17	, O
18	придумать O
19	подпись O
20	или O
21	написать O
22	небольшой O
23	рассказ O
24	. O

# sent_id = 679
# text =   AntiToxicBot — бот, распознающий токсичных пользователей в телеграм чатах.
1	AntiToxicBot B-Technology
2	— O
3	бот O
4	, O
5	распознающий B-Task
6	токсичных I-Task
7	пользователей I-Task
8	в O
9	телеграм O
10	чатах O
11	. O

# sent_id = 680
# text =   Почему же выбрано CNN+GRU, а не просто GRU или CNN?
1	Почему O
2	же O
3	выбрано O
4	CNN+GRU B-Method
5	, O
6	а O
7	не O
8	просто O
9	GRU B-Method
10	или O
11	CNN B-Method
12	? O

# sent_id = 681
# text =   Нейросеть состоит из 3-х основных частей(CNN, GRU, Linear).
1	Нейросеть O
2	состоит O
3	из O
4	3-х O
5	основных O
6	частей O
7	( O
8	CNN B-Method
9	, O
10	GRU B-Method
11	, O
12	Linear B-Method
13	) O
14	. O

# sent_id = 682
# text =   Как и в классификации картинок, свёрточный слой выделяет “признаки”, но в нашем случае векторизированный текст.
1	Как O
2	и O
3	в O
4	классификации O
5	картинок O
6	, O
7	свёрточный O
8	слой O
9	выделяет O
10	“ O
11	признаки O
12	” O
13	, O
14	но O
15	в O
16	нашем O
17	случае O
18	векторизированный B-Object
19	текст I-Object
20	. O

# sent_id = 683
# text =   То-есть данная часть сети учится выделять признаки токсичных и позитивных сообщений.
1	То O
2	- O
3	есть O
4	данная O
5	часть O
6	сети O
7	учится O
8	выделять B-Task
9	признаки I-Task
10	токсичных I-Task
11	и I-Task
12	позитивных I-Task
13	сообщений I-Task
14	. O

# sent_id = 684
# text =   GRU - Recurrent Neural Network
1	GRU B-Method
2	- O
3	Recurrent B-Method
4	Neural I-Method
5	Network

# sent_id = 685
# text =   Чтобы обрабатывать последовательности произвольной длины, используют рекуррентные слои.
1	Чтобы O
2	обрабатывать B-Task
3	последовательности I-Task
4	произвольной I-Task
5	длины I-Task
6	, O
7	используют O
8	рекуррентные O
9	слои O
10	. O

# sent_id = 686
# text =   В архитектуре используется рекуррентный слой GRU.
1	В O
2	архитектуре O
3	используется O
4	рекуррентный O
5	слой O
6	GRU B-Method
7	. O

# sent_id = 687
# text =  Данный слой учится делать заключительное решение по определению тональности текста на основе предыдущих слоёв.
1	Данный O
2	слой O
3	учится O
4	делать O
5	заключительное O
6	решение O
7	по O
8	определению B-Task
9	тональности I-Task
10	текста I-Task
11	на O
12	основе O
13	предыдущих O
14	слоёв O
15	. O

# sent_id = 688
# text =   Датасет был взят с сайта kaggle.
1	Датасет O
2	был O
3	взят O
4	с O
5	сайта O
6	kaggle B-Organization
7	. O

# sent_id = 689
# text =   Около 14000 комментариев с разметкой токсичное сообщение или нет.
1	Около O
2	14000 O
3	комментариев O
4	с O
5	разметкой B-Method
6	токсичное B-Labeling
7	сообщение I-Labeling
8	или O
9	нет O
10	. O

# sent_id = 690
# text =   Для решения данной проблемы была использована библиотека Yandex Speller, которая исправляет орфографические ошибки.
1	Для O
2	решения O
3	данной O
4	проблемы O
5	была O
6	использована O
7	библиотека O
8	Yandex B-Library
9	Speller I-Library
10	, O
11	которая O
12	исправляет B-Task
13	орфографические I-Task
14	ошибки I-Task
15	. O

# sent_id = 691
# text =   Можно было обучить собственный Word2Vec на основе данного набора данных, но лучше взять уже обученный.
1	Можно O
2	было O
3	обучить O
4	собственный O
5	Word2Vec B-Model
6	на O
7	основе B-Subject
8	данного B-Object
9	набора O
10	данных O
11	, O
12	но O
13	лучше O
14	взять O
15	уже O
16	обученный O
17	. O

# sent_id = 692
# text =   Например: Navec.
1	Например O
2	: O
3	Navec B-Model
4	. O

# sent_id = 693
# text =   Модель обучали на русской литературе (~150gb), что говорит о качественной векторизации текста.
1	Модель B-Object
2	обучали O
3	на O
4	русской B-Science
5	литературе I-Science
6	( O
7	~150 O
8	gb O
9	) O
10	, O
11	что O
12	говорит O
13	о O
14	качественной O
15	векторизации O
16	текста O
17	. O

# sent_id = 694
# text =  Для классификации используется обыкновенная функция потерь – кросс энтропия.
1	Для O
2	классификации B-Task
3	используется O
4	обыкновенная O
5	функция B-Method
6	потерь I-Method
7	– I-Method
8	кросс I-Method
9	энтропия I-Method
10	. O

# sent_id = 695
# text =  При обучении сети надо обращать внимание на основные параметры такие, как loss, precision и accuracy.
1	При O
2	обучении O
3	сети O
4	надо O
5	обращать O
6	внимание O
7	на O
8	основные O
9	параметры O
10	такие O
11	, O
12	как O
13	loss B-Metric
14	, O
15	precision B-Metric
16	и O
17	accuracy B-Metric
18	. O

# sent_id = 696
# text =   В ~80% случаев нейросеть классифицирует тональность текста правильно.
1	В O
2	~80 B-Value
3	% O
4	случаев O
5	нейросеть O
6	классифицирует O
7	тональность O
8	текста O
9	правильно O
10	. O

# sent_id = 697
# text =   Теперь нейронная сеть указала конкретные сцены, написанные не Шекспиром, и определила, кто на самом деле их написал.
1	Теперь O
2	нейронная B-Method
3	сеть I-Method
4	указала O
5	конкретные O
6	сцены O
7	, O
8	написанные O
9	не O
10	Шекспиром O
11	, O
12	и O
13	определила O
14	, O
15	кто O
16	на O
17	самом O
18	деле O
19	их O
20	написал O
21	. O

# sent_id = 698
# text =   Плехач обучил алгоритм распознавать стиль Шекспира на пьесах «Кориолан», «Цимбелин», «Зимняя сказка» и «Буря».
1	Плехач B-Person
2	обучил O
3	алгоритм O
4	распознавать B-Task
5	стиль I-Task
6	Шекспира O
7	на O
8	пьесах B-Object
9	« O
10	Кориолан O
11	» O
12	, O
13	« O
14	Цимбелин O
15	» O
16	, O
17	« O
18	Зимняя O
19	сказка O
20	» O
21	и O
22	« O
23	Буря O
24	» O
25	. O

# sent_id = 699
# text =   В результате искусственный интеллект согласился с анализом Спеддинга.
1	В O
2	результате O
3	искусственный O
4	интеллект O
5	согласился O
6	с O
7	анализом B-Method
8	Спеддинга I-Method
9	. O

# sent_id = 700
# text =   В прошлом году учёные из Университета Торонто, Мельбурнского Университета и подразделения IBM в Австралии научили искусственный интеллект генерировать сонеты в шекспировском стиле.
1	В O
2	прошлом O
3	году O
4	учёные O
5	из O
6	Университета B-Organization
7	Торонто I-Organization
8	, O
9	Мельбурнского B-Organization
10	Университета I-Organization
11	и O
12	подразделения O
13	IBM B-Organization
14	в O
15	Австралии O
16	научили O
17	искусственный O
18	интеллект O
19	генерировать O
20	сонеты O
21	в O
22	шекспировском O
23	стиле O
24	. O

# sent_id = 701
# text =   Алгоритм под названием Deepspeare обучали на 2,7 тыс. сонетов Шекспира, после чего он научился писать собственные, придерживаясь похожего стиля.
1	Алгоритм O
2	под O
3	названием O
4	Deepspeare B-Method
5	обучали O
6	на O
7	2,7 O
8	сонетов B-Object
9	Шекспира O
10	, O
11	после O
12	чего O
13	он O
14	научился O
15	писать O
16	собственные O
17	, O
18	придерживаясь O
19	похожего O
20	стиля O
21	. O

# sent_id = 702
# text =   Как научить свою нейросеть генерировать стихи
1	Как O
2	научить O
3	свою O
4	нейросеть O
5	генерировать B-Task
6	стихи I-Task

# sent_id = 703
# text =   Языковые модели определяют вероятность появления последовательности слов  в данном языке: .
1	Языковые O
2	модели O
3	определяют B-Task
4	вероятность I-Task
5	появления I-Task
6	последовательности I-Task
7	слов I-Task
8	в O
9	данном O
10	языке B-Object
11	: O
12	. O

# sent_id = 704
# text =   Кажется, самым простым способом построить такую модель является использование N-граммной статистики.
1	Кажется O
2	, O
3	самым O
4	простым O
5	способом O
6	построить O
7	такую O
8	модель B-Object
9	является O
10	использование O
11	N B-Method
12	- I-Method
13	граммной I-Method
14	статистики I-Method
15	. O

# sent_id = 705
# text =   Для решения такой проблемы используют обычно сглаживание Kneser–Ney или Katz’s backing-off.
1	Для O
2	решения O
3	такой O
4	проблемы O
5	используют O
6	обычно O
7	сглаживание B-Method
8	Kneser I-Method
9	– I-Method
10	Ney I-Method
11	или O
12	Katz B-Method
13	’s I-Method
14	backing I-Method
15	- I-Method
16	off I-Method
17	. O

# sent_id = 706
# text =   За более подробной информацией про методы сглаживания N-грамм стоит обратиться к известной книге Кристофера Маннинга “Foundations of Statistical Natural Language Processing”.
1	За O
2	более O
3	подробной O
4	информацией O
5	про O
6	методы O
7	сглаживания B-Method
8	N I-Method
9	- I-Method
10	грамм I-Method
11	стоит O
12	обратиться O
13	к O
14	известной O
15	книге O
16	Кристофера B-Person
17	Маннинга I-Person
18	“ O
19	Foundations B-Publication
20	of I-Publication
21	Statistical I-Publication
22	Natural I-Publication
23	Language I-Publication
24	Processing I-Publication
25	” O
26	. O

# sent_id = 707
# text =   Хочу заметить, что 5-граммы слов я назвал не просто так: именно их (со сглаживанием, конечно) Google демонстрирует в статье “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling” — и показывает результаты, весьма сопоставимые с результатами у рекуррентных нейронных сетей — о которых, собственно, и пойдет далее речь.
1	Хочу O
2	заметить O
3	, O
4	что O
5	5-граммы O
6	слов O
7	я O
8	назвал O
9	не O
10	просто O
11	так O
12	: O
13	именно O
14	их O
15	( O
16	со O
17	сглаживанием O
18	, O
19	конечно O
20	) O
21	Google B-Organization
22	демонстрирует O
23	в O
24	статье O
25	“ B-Publication
26	One I-Publication
27	Billion I-Publication
28	Word I-Publication
29	Benchmark I-Publication
30	for I-Publication
31	Measuring I-Publication
32	Progress I-Publication
33	in I-Publication
34	Statistical I-Publication
35	Language I-Publication
36	Modeling I-Publication
37	” I-Publication
38	— O
39	и O
40	показывает O
41	результаты O
42	, O
43	весьма O
44	сопоставимые O
45	с O
46	результатами O
47	у O
48	рекуррентных B-Method
49	нейронных I-Method
50	сетей I-Method
51	— O
52	о O
53	которых O
54	, O
55	собственно O
56	, O
57	и O
58	пойдет O
59	далее O
60	речь O
61	. O

# sent_id = 708
# text =   Преимущество рекуррентных нейронных сетей — в возможности использовать неограниченно длинный контекст.
1	Преимущество O
2	рекуррентных B-Method
3	нейронных I-Method
4	сетей I-Method
5	— O
6	в O
7	возможности O
8	использовать O
9	неограниченно O
10	длинный O
11	контекст O
12	. O

# sent_id = 709
# text =   На практике классические RNN страдают от затухания градиента — по сути, отсутствия возможности помнить контекст дальше, чем на несколько слов.
1	На O
2	практике O
3	классические O
4	RNN B-ShortName_Method
5	страдают O
6	от O
7	затухания O
8	градиента O
9	— O
10	по O
11	сути O
12	, O
13	отсутствия O
14	возможности O
15	помнить O
16	контекст O
17	дальше O
18	, O
19	чем O
20	на O
21	несколько O
22	слов O
23	. O

# sent_id = 710
# text =   Самыми популярными являются LSTM и GRU.
1	Самыми O
2	популярными O
3	являются O
4	LSTM B-ShortName_Method
5	и O
6	GRU B-ShortName_Method
7	. O

# sent_id = 711
# text =   В дальнейшем, говоря о рекуррентном слое, я всегда буду подразумевать LSTM.
1	В O
2	дальнейшем O
3	, O
4	говоря O
5	о O
6	рекуррентном O
7	слое O
8	, O
9	я O
10	всегда O
11	буду O
12	подразумевать O
13	LSTM B-ShortName_Method
14	. O

# sent_id = 712
# text =   Вспомним теперь, что для нашей задачи языковая модель нужна для выбора наиболее подходящего следующего слова по уже сгенерированной последовательности.
1	Вспомним O
2	теперь O
3	, O
4	что O
5	для O
6	нашей O
7	задачи O
8	языковая O
9	модель B-Object
10	нужна O
11	для O
12	выбора B-Task
13	наиболее I-Task
14	подходящего I-Task
15	следующего I-Task
16	слова I-Task
17	по O
18	уже O
19	сгенерированной B-Object
20	последовательности I-Object
21	. O

# sent_id = 713
# text =   Метрические правила определяют последовательность ударных и безударных слогов в строке.
1	Метрические B-Object
2	правила I-Object
3	определяют O
4	последовательность O
5	ударных O
6	и O
7	безударных O
8	слогов B-Subject
9	в O
10	строке O
11	. O

# sent_id = 714
# text =   Для решения этой проблемы мы делаем лучевой поиск (beam search), выбирая на каждом шаге вместо одного сразу N путей с наивысшими вероятностями.
1	Для O
2	решения O
3	этой O
4	проблемы O
5	мы O
6	делаем O
7	лучевой B-Method
8	поиск I-Method
9	( O
10	beam B-Method
11	search I-Method
12	) O
13	, O
14	выбирая O
15	на O
16	каждом O
17	шаге O
18	вместо O
19	одного O
20	сразу O
21	N O
22	путей O
23	с O
24	наивысшими O
25	вероятностями O
26	. O

# sent_id = 715
# text =  Автоматическое определение эмоций в текстовых беседах с использованием нейронных сетей
1	Автоматическое B-Task
2	определение I-Task
3	эмоций I-Task
4	в O
5	текстовых B-Object
6	беседах I-Object
7	с O
8	использованием O
9	нейронных B-Method
10	сетей I-Method

# sent_id = 716
#text = Одна из основных задач диалоговых систем состоит не только в предоставлении нужной пользователю информации, но и в генерации как можно более человеческих ответов.
1	Одна O
2	из O
3	основных O
4	задач O
5	диалоговых O
6	систем O
7	состоит O
8	не O
9	только O
10	в O
11	предоставлении B-Task
12	нужной I-Task
13	пользователю I-Task
14	информации I-Task
15	, O
16	но O
17	и O
18	в O
19	генерации B-Task
20	как O
21	можно O
22	более O
23	человеческих O
24	ответов I-Task
25	. O

# sent_id = 717
# text =   В этой статье мы рассмотрим архитектуру рекуррентной нейросети для определения эмоций в текстовых беседах, которая принимала участие в SemEval-2019 Task 3 “EmoContext”, ежегодном соревновании по компьютерной лингвистике.
1	В O
2	этой O
3	статье O
4	мы O
5	рассмотрим O
6	архитектуру O
7	рекуррентной B-Method
8	нейросети I-Method
9	для O
10	определения B-Task
11	эмоций I-Task
12	в O
13	текстовых B-Object
14	беседах I-Object
15	, O
16	которая O
17	принимала O
18	участие O
19	в O
20	SemEval-2019 O
21	Task O
22	3 O
23	“ O
24	EmoContext O
25	” O
26	, O
27	ежегодном O
28	соревновании O
29	по O
30	компьютерной B-Science
31	лингвистике I-Science
32	. O

# sent_id = 718
# text =   Задача состояла в классификации эмоций (“happy”, “sad”, “angry” и “others”) в беседе из трех реплик, в которой участвовали чат-бот и человек.
1	Задача O
2	состояла O
3	в O
4	классификации B-Task
5	эмоций I-Task
6	( O
7	“ O
8	happy B-Labeling
9	” O
10	, O
11	“ O
12	sad B-Labeling
13	” O
14	, O
15	“ O
16	angry B-Labeling
17	” O
18	и O
19	“ O
20	others B-Labeling
21	” O
22	) O
23	в O
24	беседе O
25	из O
26	трех O
27	реплик O
28	, O
29	в O
30	которой O
31	участвовали O
32	чат O
33	- O
34	бот O
35	и O
36	человек O
37	. O

# sent_id = 719
# text =   В четвёртой части мы опишем архитектуру LSTM, которую мы использовали в соревновании.
1	В O
2	четвёртой O
3	части O
4	мы O
5	опишем O
6	архитектуру O
7	LSTM B-ShortName_Method
8	, O
9	которую O
10	мы O
11	использовали O
12	в O
13	соревновании O
14	. O

# sent_id = 720
# text =   Код написан на языке Python с использованием библиотеки Keras.
1	Код O
2	написан O
3	на O
4	языке O
5	Python B-Environment
6	с O
7	использованием O
8	библиотеки O
9	Keras B-Library
10	. O

# sent_id = 721
# text =   Подробное описание представлено здесь: (Chatterjee et al., 2019).
1	Подробное O
2	описание O
3	представлено O
4	здесь O
5	: O
6	( O
7	Chatterjee B-Publication
8	et I-Publication
9	al I-Publication
10	. I-Publication
11	, I-Publication
12	2019 I-Publication
13	) O
14	. O

# sent_id = 722
# text =   Примеры из датасета EmoContext (Chatterjee et al., 2019)
1	Примеры O
2	из O
3	датасета O
4	EmoContext B-Dataset
5	( O
6	Chatterjee B-Publication
7	et I-Publication
8	al I-Publication
9	. I-Publication
10	, I-Publication
11	2019 I-Publication
12	) O

# sent_id = 723
# text =   Данные предоставлены Microsoft, скачать их можно в официальной группе в LinkedIn.
1	Данные O
2	предоставлены O
3	Microsoft B-Organization
4	, O
5	скачать O
6	их O
7	можно O
8	в O
9	официальной O
10	группе O
11	в O
12	LinkedIn B-Application
13	. O

# sent_id = 724
# text =   В дополнение к этим данным мы собрали 900 тыс. англоязычных сообщений из Twitter, чтобы создать Distant-датасет (300 тыс. твитов на каждую эмоцию).
1	В O
2	дополнение O
3	к O
4	этим O
5	данным O
6	мы O
7	собрали O
8	900 O
9	тыс. O
10	англоязычных O
11	сообщений O
12	из O
13	Twitter B-Organization
14	, O
15	чтобы O
16	создать O
17	Distant B-Dataset
18	- O
19	датасет O
20	( O
21	300 O
22	тыс. O
23	твитов O
24	на O
25	каждую O
26	эмоцию O
27	) O
28	. O

# sent_id = 725
# text =   При его создании мы придерживались стратегии Go et al. (2009), в рамках которой просто ассоциировали сообщения с наличием относящихся к эмоциям слов, таких как #angry, #annoyed, #happy, #sad, #surprised и так далее.
1	При O
2	его O
3	создании O
4	мы O
5	придерживались O
6	стратегии O
7	Go B-Publication
8	et I-Publication
9	al I-Publication
10	. I-Publication
11	( I-Publication
12	2009 I-Publication
13	) I-Publication
14	, O
15	в O
16	рамках O
17	которой O
18	просто O
19	ассоциировали B-Task
20	сообщения I-Task
21	с I-Task
22	наличием I-Task
23	относящихся I-Task
24	к I-Task
25	эмоциям I-Task
26	слов I-Task
27	, O
28	таких O
29	как O
30	# O
31	angry B-Labeling
32	, O
33	# O
34	annoyed B-Labeling
35	, O
36	# O
37	happy B-Labeling
38	, O
39	# O
40	sad B-Labeling
41	, O
42	# O
43	surprised B-Labeling
44	и O
45	так O
46	далее O
47	. O

# sent_id = 726
# text =   Список терминов основан на терминах из SemEval-2018 AIT DISC (Duppada et al., 2018).
1	Список O
2	терминов B-Subject
3	основан O
4	на O
5	терминах B-Subject
6	из O
7	SemEval-2018 B-Dataset
8	AIT I-Dataset
9	DISC I-Dataset
10	( O
11	Duppada B-Publication
12	et I-Publication
13	al I-Publication
14	. I-Publication
15	, I-Publication
16	2018 I-Publication
17	) O
18	. O

# sent_id = 727
# text =   Главной метрикой качества в соревновании EmoContext является усредненная F1-мера для трёх классов эмоций, то есть для классов «happy», «sad» и «angry».
1	Главной O
2	метрикой O
3	качества O
4	в O
5	соревновании O
6	EmoContext O
7	является O
8	усредненная O
9	F1-мера B-Metric
10	для O
11	трёх O
12	классов B-Object
13	эмоций O
14	, O
15	то O
16	есть O
17	для O
18	классов O
19	« O
20	happy B-Labeling
21	» O
22	, O
23	« O
24	sad B-Labeling
25	» O
26	и O
27	« O
28	angry B-Labeling
29	» O
30	. O

# sent_id = 728
# text =   Перед обучением мы предварительно обработали тексты с помощью инструмента Ekphrasis (Baziotis et al., 2017).
1	Перед O
2	обучением O
3	мы O
4	предварительно O
5	обработали O
6	тексты B-Object
7	с O
8	помощью O
9	инструмента B-Object
10	Ekphrasis B-Application
11	( O
12	Baziotis B-Publication
13	et I-Publication
14	al I-Publication
15	. I-Publication
16	, I-Publication
17	2017 I-Publication
18	) O
19	. O

# sent_id = 729
# text =   Он помогает исправить орфографию, нормализовать слова, сегментировать, а также определить, какие токены следует отбросить, нормализовать или аннотировать с помощью специальных тегов.
1	Он O
2	помогает O
3	исправить B-Task
4	орфографию I-Task
5	, O
6	нормализовать B-Task
7	слова I-Task
8	, O
9	сегментировать B-Task
10	, O
11	а O
12	также O
13	определить O
14	, O
15	какие O
16	токены B-Subject
17	следует O
18	отбросить O
19	, O
20	нормализовать O
21	или O
22	аннотировать O
23	с O
24	помощью O
25	специальных O
26	тегов B-Subject
27	. O

# sent_id = 730
# text =   Кроме того, Emphasis содержит токенизатор, который может идентифицировать большинство эмодзи, эмотиконов и сложных выражений, а также даты, время, валюты и акронимы.
1	Кроме O
2	того O
3	, O
4	Emphasis B-Application
5	содержит O
6	токенизатор B-Method
7	, O
8	который O
9	может O
10	идентифицировать B-Task
11	большинство I-Task
12	эмодзи I-Task
13	, O
14	эмотиконов I-Task
15	и O
16	сложных I-Task
17	выражений I-Task
18	, O
19	а O
20	также O
21	даты I-Task
22	, O
23	время I-Task
24	, O
25	валюты I-Task
26	и O
27	акронимы I-Task
28	. O

