# text = В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3.
В O
этой O
статье O
мы O
научим O
вас O
генерировать B-TERM
текст I-TERM
с O
помощью O
предварительно O
обученного O
GPT-2 B-TERM
— O
более O
легкого O
предшественника O
GPT-3 B-TERM
. O

# text =  Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface.
Мы O
будем O
использовать O
именитую O
библиотеку O
Transformers B-TERM
, O
разработанную O
Huggingface B-TERM
. O

# text =  Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.
Модель O
по O
умолчанию O
для O
конвейера O
генерации O
текста O
— O
GPT-2 B-TERM
, O
самая O
популярная O
модель O
декодирующего O
трансформера B-TERM
для O
генерации B-TERM
языка I-TERM
. O

# text =  Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.
Эта O
модель O
GPT2 B-TERM
от O
CKIPLab B-TERM
предварительно O
обучена O
на O
китайском B-TERM
корпусе I-TERM
, O
поэтому O
мы O
можем O
использовать O
их O
модель O
без O
необходимости O
заниматься O
настройкой O
самостоятельно O
. O
