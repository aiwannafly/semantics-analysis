# text =  Известный учёный Алан Тьюринг в 1950 году усомнился в том, что машина не может мыслить, и для проверки предложил свой знаменитый тест.
Известный O
учёный O
Алан B-TERM
Тьюринг I-TERM
в O
1950 B-TERM
году O
усомнился O
в O
том O
, O
что O
машина O
не O
может O
мыслить O
, O
и O
для O
проверки O
предложил O
свой O
знаменитый O
тест B-TERM
. O

# text = В 1954 году прошёл Джорджтаунский эксперимент.
В O
1954 B-TERM
году O
прошёл O
Джорджтаунский B-TERM
эксперимент I-TERM
. O

# text =  В его рамках демонстрировалась система, которая автоматически перевела 60 предложений с русского языка на французский.
В O
его O
рамках O
демонстрировалась O
система B-TERM
, O
которая O
автоматически O
перевела O
60 O
предложений B-TERM
с O
русского B-TERM
языка B-TERM
на O
французский B-TERM
. O

# text =  В 1960-е годы появились первые чат-боты, очень примитивные: в основном они перефразировали то, что говорил им собеседник-человек.
В O
1960-е B-TERM
годы O
появились O
первые O
чат B-TERM
- I-TERM
боты I-TERM
, O
очень O
примитивные O
: O
в O
основном O
они O
перефразировали O
то O
, O
что O
говорил O
им O
собеседник O
- O
человек O
. O

# text =  Даже знаменитый чат-бот Женя Густман, который, как считается, прошёл одну из версий теста Тьюринга, сделал это не благодаря хитрым алгоритмам.
Даже O
знаменитый O
чат B-TERM
- I-TERM
бот I-TERM
Женя I-TERM
Густман I-TERM
, O
который O
, O
как O
считается O
, O
прошёл O
одну O
из O
версий O
теста B-TERM
Тьюринга I-TERM
, O
сделал O
это O
не O
благодаря O
хитрым O
алгоритмам O
. O

# text =  Учёные пытались всё формализовать, построить формальную модель, онтологию, понятия, связи, общие правила синтаксического разбора и универсальную грамматику.
Учёные O
пытались O
всё O
формализовать O
, O
построить O
формальную B-TERM
модель I-TERM
, O
онтологию B-TERM
, O
понятия B-TERM
, O
связи B-TERM
, O
общие O
правила O
синтаксического B-TERM
разбора I-TERM
и O
универсальную B-TERM
грамматику I-TERM
. O

# text =  Тогда возникла теория грамматик Хомского.
Тогда O
возникла O
теория B-TERM
грамматик I-TERM
Хомского I-TERM
. O

# text =  Поэтому в 1980-е годы внимание переключилось на систему другого класса: на алгоритмы машинного обучения и так называемую корпусную лингвистику.
Поэтому O
в O
1980-е B-TERM
годы O
внимание O
переключилось O
на O
систему O
другого O
класса B-TERM
: O
на O
алгоритмы B-TERM
машинного I-TERM
обучения I-TERM
и O
так O
называемую O
корпусную B-TERM
лингвистику I-TERM
. O

# text =  В 1990-е годы эта область получила очень мощный толчок благодаря развитию Всемирной паутины с большим количеством слабоструктурированного текста, по которому нужно было искать, его требовалось каталогизировать.
В O
1990-е B-TERM
годы O
эта O
область O
получила O
очень O
мощный O
толчок O
благодаря O
развитию O
Всемирной B-TERM
паутины I-TERM
с O
большим O
количеством O
слабоструктурированного B-TERM
текста I-TERM
, O
по O
которому O
нужно O
было O
искать O
, O
его O
требовалось O
каталогизировать O
. O

# text =  В 2000-е анализ естественных языков начал применяться уже не только для поиска в Интернете, но и для решения разнообразных задач.
В O
2000-е O
анализ B-TERM
естественных I-TERM
языков I-TERM
начал O
применяться O
уже O
не O
только O
для O
поиска O
в O
Интернете B-TERM
, O
но O
и O
для O
решения O
разнообразных O
задач O
. O

# text =  Возникли модели, основанные на краудсорсинге: мы не только пытаемся что-то понять с помощью машины, а подключаем людей, которые за небольшую плату определяют, на каком языке написан текст.
Возникли O
модели O
, O
основанные O
на O
краудсорсинге B-TERM
: O
мы O
не O
только O
пытаемся O
что O
- O
то O
понять O
с O
помощью O
машины O
, O
а O
подключаем O
людей O
, O
которые O
за O
небольшую O
плату O
определяют O
, O
на O
каком O
языке O
написан O
текст O
. O

# text =  В некотором смысле начали возрождаться идеи использования формальных онтологий, но теперь онтологии крутятся вокруг краудсорсинговых баз знаний, в частности баз на основе Linked Open Data.
В O
некотором O
смысле O
начали O
возрождаться O
идеи O
использования O
формальных B-TERM
онтологий I-TERM
, O
но O
теперь O
онтологии B-TERM
крутятся O
вокруг O
краудсорсинговых O
баз O
знаний O
, O
в O
частности O
баз O
на O
основе O
Linked B-TERM
Open I-TERM
Data I-TERM
. O

# text =  Это целый набор баз знаний, его центр — машиночитаемый вариант «Википедии» DBpedia, который тоже наполняется по краудсорсинговой модели.
Это O
целый O
набор O
баз O
знаний O
, O
его O
центр O
— O
машиночитаемый O
вариант O
« O
Википедии B-TERM
» O
DBpedia B-TERM
, O
который O
тоже O
наполняется O
по O
краудсорсинговой B-TERM
модели I-TERM
. O

# text =  В частности, семантический анализ (о чём документ?), генерация автоматической аннотации и автоматического summary, перевод и создание документов.
В O
частности O
, O
семантический B-TERM
анализ I-TERM
( O
о O
чём O
документ O
? O
) O 
, O
генерация B-TERM
автоматической I-TERM
аннотации I-TERM
и O
автоматического B-TERM
summary I-TERM
, O
перевод B-TERM
и O
создание B-TERM
документов I-TERM
. O

# text =  Все наверняка слышали об известном генераторе научных статей SCIgen, который создал статью «Корчеватель: Алгоритм типичной унификации точек доступа и избыточности».
Все O
наверняка O
слышали O
об O
известном O
генераторе O
научных O
статей O
SCIgen B-TERM
, O
который O
создал O
статью O
« O
Корчеватель O
: O
Алгоритм O
типичной O
унификации O
точек O
доступа O
и O
избыточности O
» O
. O

# text =  Но в случае с лентой такие рекомендации работают плохо: здесь постоянно возникает ситуация холодного старта.
Но O
в O
случае O
с O
лентой O
такие O
рекомендации O
работают O
плохо O
: O
здесь O
постоянно O
возникает O
ситуация B-TERM
холодного I-TERM
старта I-TERM
. O

# text =  Поэтому применим классический воркэраунд для задачи холодного старта и построим систему контентных рекомендаций: попробуем научить машину понимать, о чём написан пост.
Поэтому O
применим O
классический O
воркэраунд B-TERM
для O
задачи O
холодного B-TERM
старта I-TERM
и O
построим O
систему O
контентных O
рекомендаций O
: O
попробуем O
научить O
машину O
понимать O
, O
о O
чём O
написан O
пост O
. O

# text =  Соответственно, требуется метод семантического анализа.
Соответственно O
, O
требуется O
метод B-TERM
семантического I-TERM
анализа I-TERM
. O

# text =  Тут поможет анализ эмоциональной окраски.
Тут O
поможет O
анализ B-TERM
эмоциональной I-TERM
окраски I-TERM
. O

# text =  В частности, это Apache Tika, японская библиотека language-detection и одна из последних разработок — питоновский пакет Ldig, который как раз работает на инфинитиграммах.
В O
частности O
, O
это O
Apache B-TERM
Tika I-TERM
, O
японская O
библиотека O
language B-TERM
- I-TERM
detection I-TERM
и O
одна O
из O
последних O
разработок O
— O
питоновский O
пакет O
Ldig B-TERM
, O
который O
как O
раз O
работает O
на O
инфинитиграммах O
. O

# text =  Но если текст короткий, из одного предложения или нескольких слов, то классический подход, основанный на триграммах, очень часто ошибается.
Но O
если O
текст B-TERM
короткий O
, O
из O
одного O
предложения B-TERM
или O
нескольких O
слов O
, O
то O
классический O
подход O
, O
основанный O
на O
триграммах B-TERM
, O
очень O
часто O
ошибается O
. O

# text =  Исправить ситуацию могут инфинитиграммы, но это новая область, далеко не для всех языков уже есть обученные и готовые классификаторы.
Исправить O
ситуацию O
могут O
инфинитиграммы B-TERM
, O
но O
это O
новая O
область O
, O
далеко O
не O
для O
всех O
языков O
уже O
есть O
обученные O
и O
готовые O
классификаторы O
. O

# text =  Первый основан на так называемом фонетическом матчинге.
Первый O
основан O
на O
так O
называемом O
фонетическом B-TERM
матчинге I-TERM
. O

# text =  Альтернативный подход — так называемое редакционное расстояние, с помощью которого мы ищем в словаре максимально похожие слова-аналоги.
Альтернативный O
подход O
— O
так O
называемое O
редакционное B-TERM
расстояние I-TERM
, O
с O
помощью O
которого O
мы O
ищем O
в O
словаре O
максимально O
похожие O
слова O
- O
аналоги O
. O

# text =  Первая концепция — стемминг, мы пытаемся найти основу слова.
Первая O
концепция O
— O
стемминг B-TERM
, O
мы O
пытаемся O
найти O
основу B-TERM
слова I-TERM
. O

# text =  Здесь используется подход affix stripping.
Здесь O
используется O
подход O
affix B-TERM
stripping I-TERM
. O

# text =  Есть известная реализация, так называемый стеммер Портера, или проект Snowball.
Есть O
известная O
реализация O
, O
так O
называемый O
стеммер B-TERM
Портера I-TERM
, O
или O
проект O
Snowball I-TERM
. O

# text =  Самый распространённый, наверное, инструмент — реализация в пакете Apache Lucene.
Самый O
распространённый O
, O
наверное O
, O
инструмент O
— O
реализация O
в O
пакете O
Apache B-TERM
Lucene I-TERM
. O

# text =  Вторая концепция, альтернатива стемминга — лемматизация.
Вторая O
концепция O
, O
альтернатива O
стемминга B-TERM
— O
лемматизация B-TERM
. O

# text =  Она пытается привести слово не к основе или корню, а к базовой, словарной форме — т. е. лемме.
Она O
пытается O
привести O
слово B-TERM
не O
к O
основе B-TERM
или O
корню B-TERM
, O
а O
к O
базовой O
, O
словарной B-TERM
форме I-TERM
— O
т O
. O
  O
е O
. O
лемме B-TERM
. O

# text =  Существует множество реализаций, и тема очень хорошо проработана именно для user generated текстов, пользовательски зашумлённых текстов.
Существует O
множество O
реализаций O
, O
и O
тема B-TERM
очень O
хорошо O
проработана O
именно O
для O
user B-TERM
generated I-TERM
текстов I-TERM
, O
пользовательски O
зашумлённых O
текстов B-TERM
. O

# text =  Теперь отобразим это в векторном пространстве, потому что почти все математические модели работают в векторных пространствах больших размерностей.
Теперь O
отобразим O
это O
в O
векторном B-TERM
пространстве I-TERM
, O
потому O
что O
почти O
все O
математические B-TERM
модели I-TERM
работают O
в O
векторных B-TERM
пространствах I-TERM
больших O
размерностей O
. O

# text =  Базовый подход, который используют многие модели, — метод "мешка слов".
Базовый O
подход O
, O
который O
используют O
многие O
модели O
, O
— O
метод B-TERM
" I-TERM
мешка I-TERM
слов I-TERM
" I-TERM
. O

# text =  Доминирует так называемый TF-IDF.
Доминирует O
так O
называемый O
TF B-TERM
- I-TERM
IDF I-TERM
. O

# text =  Частоту слова (term frequency, TF) определяют по-разному.
Частоту B-TERM
слова I-TERM
( O
term B-TERM
frequency I-TERM
, O
TF B-TERM
) O
определяют O
по O
- O
разному O
. O

# text =  Определив TF в документе, мы перемножаем её с обратной частотой документа (inverse document frequency, IDF).
Определив O
TF B-TERM
в O
документе O
, O
мы O
перемножаем O
её O
с O
обратной B-TERM
частотой I-TERM
документа I-TERM
( O
inverse B-TERM
document I-TERM
frequency I-TERM
, O
IDF B-TERM
) O
. O

# text =  IDF обычно вычисляют как логарифм от числа документов в корпусе, разделённый на количество документов, где это слово представлено.
IDF B-TERM
обычно O
вычисляют O
как O
логарифм B-TERM
от O
числа O
документов O
в O
корпусе B-TERM
, O
разделённый O
на O
количество O
документов O
, O
где O
это O
слово B-TERM
представлено O
. O

# text =  Например, при анализе эмоциональной окраски очень важно, к чему относилось, условно говоря, слово «хороший» или «нет».
Например O
, O
при O
анализе B-TERM
эмоциональной I-TERM
окраски I-TERM
очень O
важно O
, O
к O
чему O
относилось O
, O
условно O
говоря O
, O
слово B-TERM
« O
хороший O
» O
или O
« O
нет O
» O
. O

# text =  Тогда наряду с мешком слов поможет мешок N-грамм: мы добавляем в словарь не только слова, но и словосочетания.
Тогда O
наряду O
с O
мешком B-TERM
слов Ш-TERM
поможет O
мешок B-TERM
N I-TERM
- I-TERM
грамм I-TERM
: O
мы O
добавляем O
в O
словарь O
не O
только O
слова O
, O
но O
и O
словосочетания B-TERM
. O

# text =  Мы не будем вносить все словосочетания, потому что это приведёт к комбинаторному взрыву, но часто используемые статистически значимые пары или пары, соответствующие именованным сущностям, можно добавить, и это повысит качество работы итоговой модели.
Мы O
не O
будем O
вносить O
все O
словосочетания B-TERM
, O
потому O
что O
это O
приведёт O
к O
комбинаторному O
взрыву O
, O
но O
часто O
используемые O
статистически O
значимые O
пары O
или O
пары O
, O
соответствующие O
именованным B-TERM
сущностям I-TERM
, O
можно O
добавить O
, O
и O
это O
повысит O
качество O
работы O
итоговой O
модели B-TERM
. O

# text =  Отчасти эти ситуации позволяют обработать методы построения "векторных представлений слов", например, знаменитый word2vec или более модные skip-gramm.
Отчасти O
эти O
ситуации O
позволяют O
обработать O
методы O
построения O
" O
векторных B-TERM
представлений B-TERM
слов B-TERM
" O
, O
например O
, O
знаменитый O
word2vec B-TERM
или O
более O
модные O
skip B-TERM
- I-TERM
gramm I-TERM
. O

# text =  Стандартные хеш-функции равномерно размазывают данные по пространству хешей.
Стандартные O
хеш B-TERM
- I-TERM
функции I-TERM
равномерно O
размазывают O
данные O
по O
пространству O
хешей O
. O

# text =  Локально-чувствительный хеш похожие объекты поместит в пространстве объектов близко.
Локально B-TERM
- I-TERM
чувствительный I-TERM
хеш I-TERM
похожие O
объекты O
поместит O
в O
пространстве O
объектов O
близко O
. O

# text =  Мы выбираем случайный базис из случайных векторов.
Мы O
выбираем O
случайный O
базис B-TERM
из O
случайных B-TERM
векторов I-TERM
. O

# text =  Задача семантического анализа достаточно старая.
Задача O
семантического B-TERM
анализа I-TERM
достаточно O
старая O
. O

# text =  Современный подход — анализ семантики без учителя, поэтому его называют анализом скрытой (латентной) семантики.
Современный O
подход O
— O
анализ B-TERM
семантики I-TERM
без I-TERM
учителя I-TERM
, O
поэтому O
его O
называют O
анализом B-TERM
скрытой I-TERM
( I-TERM
латентной I-TERM
) I-TERM
семантики I-TERM
. O

# text =  Исторически первый подход к латентно-семантическому анализу — это латентно-семантическое индексирование.
Исторически O
первый O
подход O
к O
латентно B-TERM
- I-TERM
семантическому I-TERM
анализу I-TERM
— O
это O
латентно B-TERM
- I-TERM
семантическое I-TERM
индексирование I-TERM
. O

# text =  Мы уже использовали для решения задач коллаборативных рекомендаций хорошо зарекомендовавшие себя техники факторизации матриц.
Мы O
уже O
использовали O
для O
решения O
задач B-TERM
коллаборативных I-TERM
рекомендаций I-TERM
хорошо O
зарекомендовавшие O
себя O
техники O
факторизации O
матриц O
. O

# text =  В чём суть факторизации?
В O
чём O
суть O
факторизации B-TERM
? O

# text =  Одной из альтернатив стал так называемый вероятностный латентно-семантический индекс.
Одной O
из O
альтернатив O
стал O
так O
называемый O
вероятностный B-TERM
латентно I-TERM
- I-TERM
семантический I-TERM
индекс I-TERM
. O

# text =  Важно понять, что техника вероятностного латентно-семантического индекса — это техника факторизации матрицы.
Важно O
понять O
, O
что O
техника B-TERM
вероятностного I-TERM
латентно I-TERM
- I-TERM
семантического I-TERM
индекса I-TERM
— O
это O
техника I-TERM
факторизации I-TERM
матрицы I-TERM
. O

# text =  По сравнению с классической факторизацией на основе сингулярного разложения у вероятностной генерирующей модели есть важное преимущество.
По O
сравнению O
с O
классической B-TERM
факторизацией I-TERM
на O
основе O
сингулярного B-TERM
разложения I-TERM
у O
вероятностной O
генерирующей O
модели O
есть O
важное O
преимущество O
. O

# text =  Для этого используется перплексия.
Для O
этого O
используется O
перплексия B-TERM
. O

# text =  Есть так называемый EM-алгоритм.
Есть O
так O
называемый O
EM B-TERM
- I-TERM
алгоритм I-TERM
. O
