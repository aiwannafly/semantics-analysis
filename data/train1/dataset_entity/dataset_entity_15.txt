# text =   Типичным методом обучения без учителя является кластеризация, благодаря которому обучающая выборка разбивается на устойчивые группы или кластеры.
Типичным O
методом O
обучения O
без O
учителя O
является O
кластеризация B-Method
, O
благодаря O
которому O
обучающая O
выборка B-Object
разбивается O
на O
устойчивые O
группы O
или O
кластеры B-Result
. O

# text =   Другой подход обучения без учителя для текстов называется тематическим моделированием (topic modeling), позволяющим выявить в неразмеченных текстах основные тематики.
Другой O
подход O
обучения O
без O
учителя O
для O
текстов O
называется O
тематическим B-Method
моделированием I-Method
( O
topic B-Method
modeling I-Method
) O
, O
позволяющим O
выявить O
в O
неразмеченных O
текстах O
основные O
тематики O
. O

# text =   Если отказываемся от методов unsupervised learning, то логично обратиться к методам обучения с учителем (supervised learning) и в частности к классификации.
Если O
отказываемся O
от O
методов O
unsupervised B-Method
learning I-Method
, O
то O
логично O
обратиться O
к O
методам B-Method
обучения I-Method
с I-Method
учителем I-Method
( O
supervised B-Method
learning I-Method
) O
и O
в O
частности O
к O
классификации B-Task
. O

# text =   Результатом работы языковой модели являются эмбеддинги — это отображение из пространства слов в пространство векторов конкретной фиксированной длины, причем векторы, соответствующие близким по смыслу словам, будут расположены в новом пространстве рядом, а далекие по смыслу — далеко.
Результатом O
работы O
языковой O
модели O
являются O
эмбеддинги B-Object
— O
это O
отображение O
из O
пространства O
слов B-Subject
в O
пространство O
векторов O
конкретной O
фиксированной O
длины O
, O
причем O
векторы O
, O
соответствующие O
близким O
по O
смыслу B-Subject
словам I-Subject
, O
будут O
расположены O
в O
новом O
пространстве O
рядом O
, O
а O
далекие O
по O
смыслу O
— O
далеко O
. O

# text =   При использовании TF-IDF (например, вот) подхода с фильтром по частотам и логистической регрессии уже можно получить прекрасные результаты: изначально в краулер отправлялись очень разные тексты, и модель прекрасно справляется.
При O
использовании O
TF B-Metric
- I-Metric
IDF I-Metric
( O
например O
, O
вот O
) O
подхода O
с O
фильтром O
по O
частотам O
и O
логистической B-Metric
регрессии I-Metric
уже O
можно O
получить O
прекрасные O
результаты O
: O
изначально O
в O
краулер O
отправлялись O
очень O
разные O
тексты O
, O
и O
модель O
прекрасно O
справляется O
. O

# text =   Для каждой из популяций рассчитаем word2vec расстояние до центра положительной обучающей выборки.
Для O
каждой O
из O
популяций O
рассчитаем O
word2vec B-Result
расстояние O
до O
центра O
положительной O
обучающей O
выборки B-Object
. O

# text =   Распределения можно разделить, и для оценки расстояния между распределениями в первую очередь логично обратиться к Дивергенции Кульбака-Лейблера (ДКЛ).
Распределения O
можно O
разделить O
, O
и O
для O
оценки O
расстояния O
между O
распределениями O
в O
первую O
очередь O
логично O
обратиться O
к O
Дивергенции B-Metric
Кульбака I-Metric
- I-Metric
Лейблера I-Metric
( O
ДКЛ B-ShortName_Metric
) O
. O
