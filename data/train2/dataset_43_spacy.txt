# text =  В данной статье мы будем использовать модель трансформера для бинарной классификации текста.
В O
данной O
статье O
мы O
будем O
использовать O
модель B-TERM
трансформера I-TERM
для O
бинарной B-TERM
классификации I-TERM
текста I-TERM
. O

# text =  Самая простая и популярная связка – TF-IDF + линейная модель.
Самая O
простая O
и O
популярная O
связка O
– O
TF B-TERM
- I-TERM
IDF I-TERM
+ O
линейная B-TERM
модель I-TERM
. O

# text =  В случае с BERT можно (даже нужно) опустить препроцессинг и сразу перейти к токенизации и обучению.
В O
случае O
с O
BERT B-TERM
можно O
( O
даже O
нужно O
) O
опустить O
препроцессинг B-TERM
и O
сразу O
перейти O
к O
токенизации B-TERM
и O
обучению O
. O

# text =  Необходимо обучить модель находить обращения с жалобой на сотрудника или другими словами – бинарная классификация.
Необходимо O
обучить O
модель O
находить O
обращения O
с O
жалобой O
на O
сотрудника O
или O
другими O
словами O
– O
бинарная B-TERM
классификация I-TERM
. O

# text = Для решения описанной задачи используется модель от DeepPavlov rubert-base-cased-sentence.
Для O
решения O
описанной O
задачи O
используется O
модель O
от O
DeepPavlov B-TERM
rubert I-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
- I-TERM
sentence I-TERM
. O

# text =  На выходе мы получаем метрику f1 = 0.91 Посмотрим, как модель классифицировала данные показанные в начале статьи.
На O
выходе O
мы O
получаем O
метрику O
f1 B-TERM
= O
0.91 B-TERM
Посмотрим O
, O
как O
модель O
классифицировала O
данные O
показанные O
в O
начале O
статьи O
. O

# text =  Обученные модели можно найти на сайтах HuggingFace и DeepPavlov.
Обученные O
модели O
можно O
найти O
на O
сайтах O
HuggingFace B-TERM
и O
DeepPavlov B-TERM
. O
