# text =  Анализируем тональность текстов с помощью Fast.ai
Анализируем O
тональность O
текстов O
с O
помощью O
Fast.ai B-TERM

# text = В статье пойдет речь о классификации тональности текстовых сообщений на русском языке (а по сути любой классификации текстов, используя те же технологии).
В O
статье O
пойдет O
речь O
о O
классификации B-TERM
тональности I-TERM
текстовых I-TERM
сообщений I-TERM
на O
русском B-TERM
языке O
( O
а O
по O
сути O
любой O
классификации O
текстов O
, O
используя O
те O
же O
технологии O
) O
. O

# text =  За основу возьмем данную статью, в которой была рассмотрена классификация тональности на архитектуре CNN с использованием Word2vec модели.
За O
основу O
возьмем O
данную O
статью O
, O
в O
которой O
была O
рассмотрена O
классификация B-TERM
тональности I-TERM
на O
архитектуре O
CNN B-TERM
с O
использованием O
Word2vec B-TERM
модели O
. O

# text =  В нашем примере будем решать ту же самую задачу разделения твитов на позитивные и негативные на том же самом датасете с использованием модели ULMFit.
В O
нашем O
примере O
будем O
решать O
ту O
же O
самую O
задачу O
разделения B-TERM
твитов I-TERM
на O
позитивные B-TERM
и O
негативные B-TERM
на O
том O
же O
самом O
датасете O
с O
использованием O
модели O
ULMFit B-TERM
. O

# text =  Результат из статьи (average F1-score = 0.78142) примем в качестве baseline.
Результат O
из O
статьи O
( O
average O
F1-score B-TERM
= O
0.78142 B-TERM
) O
примем O
в O
качестве O
baseline O
. O

# text =  Модель ULMFIT была представлена разработчиками fast.ai (Jeremy Howard, Sebastian Ruder) в 2018 году.
Модель O
ULMFIT B-TERM
была O
представлена O
разработчиками O
fast.ai B-TERM
( O
Jeremy B-TERM
Howard I-TERM
, O
Sebastian B-TERM
Ruder I-TERM
) O
в O
2018 B-TERM
году O
. O

# text =  Суть подхода состоит в использовании transfer learning в задачах NLP, когда вы используете предобученные модели, сокращая время на обучение своих моделей и снижая требования к размерам размеченной тестовой выборки.
Суть O
подхода O
состоит O
в O
использовании O
transfer B-TERM
learning I-TERM
в O
задачах O
NLP B-TERM
, O
когда O
вы O
используете O
предобученные O
модели O
, O
сокращая O
время O
на O
обучение O
своих O
моделей O
и O
снижая O
требования O
к O
размерам O
размеченной O
тестовой O
выборки O
. O

# text =  Для задачи моделирования языка ULMFit использует архитектуру AWD-LSTM, которая предполагает активное использование dropout везде, где только можно и имеет смысл.
Для O
задачи O
моделирования B-TERM
языка O
ULMFit B-TERM
использует O
архитектуру O
AWD B-TERM
- I-TERM
LSTM I-TERM
, O
которая O
предполагает O
активное O
использование O
dropout O
везде O
, O
где O
только O
можно O
и O
имеет O
смысл O
. O

# text =  Результат, показанный на тестовой выборке average F1-score = 0,80064.
Результат O
, O
показанный O
на O
тестовой O
выборке O
average O
F1-score B-TERM
= O
0,80064 B-TERM
. O
