# text =   1 January 2010 at 07:59 Заметки об NLP (часть 2) Artificial Intelligence Natural Language Processing.
1 O
January O
2010 O
at O
07:59 O
Заметки O
об O
NLP B-ShortName_Science
( O
часть O
2 O
) O
Artificial B-Science
Intelligence I-Science
Natural B-Science
Language I-Science
Processing I-Science
. O

# text =   Хотя в первой части я и говорил, что не собираюсь останавливаться на морфологии, видимо, совсем без неё не получится
Хотя O
в O
первой O
части O
я O
и O
говорил O
, O
что O
не O
собираюсь O
останавливаться O
на O
морфологии B-Science
, O
видимо O
, O
совсем O
без O
неё O
не O
получится O
. O

# text =   Всё-таки обработка предложений сильно завязана на предшествующий морфологический анализ.
Всё O
- O
таки O
обработка O
предложений B-Subject
сильно O
завязана O
на O
предшествующий O
морфологический B-Method
анализ I-Method
. O

# text =   Наш с вами родной русский язык очень хорош (для нас) и труден (для иностранцев) богатой фонетикой и разнообразием грамматических средств.
Наш O
с O
вами O
родной O
русский O
язык B-Object
очень O
хорош O
( O
для O
нас O
) O
и O
труден O
( O
для O
иностранцев O
) O
богатой O
фонетикой B-Science
и O
разнообразием O
грамматических B-Object
средств I-Object
. O

# text =   Во-первых, в них не так много незнакомых нам фонем.
Во O
- O
первых O
, O
в O
них O
не O
так O
много O
незнакомых O
нам O
фонем B-Subject
. O

# text =   Во-вторых, обилие грамматических явлений редко сталкивает нас с чем-либо непонятным.
Во O
- O
вторых O
, O
обилие O
грамматических B-Object
явлений I-Object
редко O
сталкивает O
нас O
с O
чем O
- O
либо O
непонятным O
. O

# text =   А для американца, например, само понятие рода или падежа совершенно неочевидно.
А O
для O
американца O
, O
например O
, O
само O
понятие O
рода B-Subject
или O
падежа B-Subject
совершенно O
неочевидно O
. O

# text =   Теперь о морфологии.
Теперь O
о O
морфологии B-Science
. O

# text =   Автоматические морфологические анализаторы работают хорошо.
Автоматические B-Subject
морфологические I-Subject
анализаторы I-Subject
работают O
хорошо O
. O

# text =   Если кому интересно посмотреть, как работает автоматический анализатор — можно поэкспериментировать на сайте С.А. Старостина.
Если O
кому O
интересно O
посмотреть O
, O
как O
работает O
автоматический B-Subject
анализатор B-Subject
— O
можно O
поэкспериментировать O
на O
сайте O
С.А. B-Person
Старостина I-Person
. O

# text =   Смею предположить, что едва ли не все морфологические анализаторы русского так или иначе опираются на Грамматический словарь Зализняка.
Смею O
предположить O
, O
что O
едва O
ли O
не O
все O
морфологические B-Subject
анализаторы I-Subject
русского O
так O
или O
иначе O
опираются O
на O
Грамматический B-InfoResource
словарь I-InfoResource
Зализняка I-InfoResource
. O

# text =   Сам я пользуюсь разработками Алексея Сокирко, «обёрнутыми» в удобный интерфейс на сайте Lemmatizer.
Сам O
я O
пользуюсь O
разработками O
Алексея B-Person
Сокирко I-Person
, O
« O
обёрнутыми O
» O
в O
удобный O
интерфейс O
на O
сайте O
Lemmatizer B-InfoResource
. O

# text =   Судите сами: упомянутый русский морфологический анализатор Алексея Сокирко оперирует базой данных в 18,5 мегабайт.
Судите O
сами O
: O
упомянутый O
русский B-Subject
морфологический B-Subject
анализатор I-Subject
Алексея I-Subject
Сокирко I-Subject
оперирует O
базой B-InfoResource
данных I-InfoResource
в O
18,5 O
мегабайт O
. O

# text =   На Грамоте предлагают относить их к «предикативам», но общепринятого подхода нет.
На O
Грамоте B-InfoResource
предлагают O
относить O
их O
к O
« O
предикативам O
» O
, O
но O
общепринятого O
подхода O
нет O
. O

# text =   Например, ещё одна «фича» анализатора Сокирко: он называет глаголы в личной форме («бегаю») глаголами, а в начальной форме («бегать») — инфинитивами.
Например O
, O
ещё O
одна O
« O
фича O
» O
анализатора B-InfoResource
Сокирко I-InfoResource
: O
он O
называет O
глаголы O
в O
личной O
форме O
( O
« O
бегаю O
» O
) O
глаголами O
, O
а O
в O
начальной O
форме O
( O
« O
бегать O
» O
) O
— O
инфинитивами O
. O

# text =   Tags: NLP, обработка текстовб, компьютерная лингвистика.
Tags O
: O
NLP B-ShortName_Science
, O
обработка B-Task
текстов B-Task
, O
компьютерная B-Science
лингвистика I-Science

# text =   Туториал по фреймворку для программирования датасетов MTS AI corporate blog.
Туториал O
по O
фреймворку O
для O
программирования O
датасетов O
MTS B-Organization
AI I-Organization
corporate B-InfoResource
blog I-InfoResource

# text =   Я Игорь Буянов, старший разработчик группы разметки данных MTS AI.
Я O
Игорь B-Person
Буянов I-Person
, O
старший O
разработчик O
группы O
разметки B-Method
данных O
MTS B-Organization
AI I-Organization
. O

# text =   Недавно рассказывал о том, как делать иерархически датасет из Википедии.
Недавно O
рассказывал O
о O
том O
, O
как O
делать O
иерархически O
датасет O
из O
Википедии B-InfoResource
. O

# text =   В этом посте хочу рассказать вам о Сноркеле - фреймворке для программирования данных (data programming).
В O
этом O
посте O
хочу O
рассказать O
вам O
о O
Сноркеле B-Application
- O
фреймворке B-Environment
для I-Environment
программирования I-Environment
данных I-Environment
( O
data B-Task
programming I-Task
) O
. O

# text =   Проект стартовал в Стэнфорде как инструмент для помощи в разметке датасетов для задачи information extraction, а сейчас разработчики делают платформу для пользования внешними заказчиками. 
Проект O
стартовал O
в O
Стэнфорде B-Organization
как O
инструмент B-Object
для O
помощи O
в O
разметке B-Method
датасетов O
для O
задачи O
information B-Task
extraction I-Task
, O
а O
сейчас O
разработчики O
делают O
платформу O
для O
пользования O
внешними O
заказчиками O
. O

# text =   В разметочные функции (labeling functions) закодированы все возможные правила, по которым можно поставить какую-либо метку каждому примеру из набора данных.
В O
разметочные B-Method
функции I-Method
( O
labeling B-Method
functions I-Method
) O
закодированы O
все O
возможные O
правила O
, O
по O
которым O
можно O
поставить O
какую O
- O
либо O
метку O
каждому O
примеру O
из O
набора O
данных O
. O

# text =   В качестве основы для таких функций используются:внешние базы данных, такие как WordNet или WikiBase.
В O
качестве O
основы B-Subject
для O
таких O
функций O
используются O
: O
внешние O
базы B-InfoResource
данных I-InfoResource
, O
такие O
как O
WordNet B-Result
или O
WikiBase B-InfoResource
. O

# text =   Генеративная модель, являющаяся сердцем Сноркеля, попытается учесть недостатки отдельных функций.
Генеративная B-Object
модель I-Object
, O
являющаяся O
сердцем O
Сноркеля B-InfoResource
, O
попытается O
учесть O
недостатки O
отдельных O
функций O
. O

# text =   Для наглядности оставляю здесь иллюстрацию с последовательностью работы со Снокрелем для задачи information extraction из оригинальной статьи.
Для O
наглядности O
оставляю O
здесь O
иллюстрацию O
с O
последовательностью O
работы O
со O
Снокрелем B-InfoResource
для O
задачи O
information B-Science
extraction I-Science
из O
оригинальной O
статьи O
. O

# text =   Авторы оригинальной статьи представляют ее как факторный граф, или графическую вероятностную модель.
Авторы O
оригинальной O
статьи O
представляют O
ее O
как O
факторный B-Object
граф I-Object
, O
или O
графическую B-Model
вероятностную I-Model
модель I-Model
. O

# text =   Тогда модель определяется так, чтобы обучить эту модель без доступа к истинным меткам, это нужно обучаться с помощью логарифмического негативного маргинализированного правдоподобия, зная матрицу Оптимизацию авторы проводили с помощью SGD с семплированием Гиббса.
Тогда O
модель B-Object
определяется O
, O
чтобы O
обучить O
эту O
модель O
без O
доступа O
к O
истинным O
меткам O
, O
это O
нужно O
обучаться O
с O
помощью O
логарифмического B-Method
негативного I-Method
маргинализированного I-Method
правдоподобия I-Method
, O
зная O
матрицу B-Object
Оптимизацию I-Object
авторы O
проводили O
с O
помощью O
SGD B-ShortName_Method
с O
семплированием B-Method
Гиббса I-Method
. O

# text =   Загрузим заранее обученную модель fastText, чей выбор объясняется наличием огромного количества опечаток в текстах.
Загрузим O
заранее O
обученную O
модель B-Object
fastText B-Model
, O
чей O
выбор O
объясняется O
наличием O
огромного O
количества O
опечаток O
в O
текстах B-Object
. O

# text =   Таким образом мы получили опорный вектор для класса "диарея".
Таким O
образом O
мы O
получили O
опорный B-Method
вектор I-Method
для O
класса B-Object
" O
диарея O
" O
. O

# text =   Зайдя на несколько из них я увидел что большая половина типа Wix используют технологию Искусственного Интеллекта, чтобы создать шаблон разметки страницы и далее её уже заполнить.
Зайдя O
на O
несколько O
из O
них O
я O
увидел O
что O
большая O
половина O
типа O
Wix B-Organization
используют O
технологию O
Искусственного B-Science
Интеллекта I-Science
, O
чтобы O
создать O
шаблон B-Object
разметки B-Method
страницы O
и O
далее O
её O
уже O
заполнить O
. O

# text =   В финале я могу его редактировать путем Drag & Drop.
В O
финале O
я O
могу O
его O
редактировать O
путем O
Drag B-Method
& I-Method
Drop I-Method
. O

# text =   Чатботы и искусственный интеллект для понимания естественного языка (NLU – Natural Language Understanding) тема достаточно горячая, про нее не раз говорилось на Хабре.
Чатботы O
и O
искусственный B-Science
интеллект I-Science
для O
понимания B-Task
естественного I-Task
языка I-Task
( O
NLU B-Abbrev_Task
– O
Natural B-Task
Language I-Task
Understanding I-Task
) O
тема O
достаточно O
горячая O
, O
про O
нее O
не O
раз O
говорилось O
на O
Хабре O
. O

# text =   Хотя AI — это достаточно широкая область, включающая в себя машинное зрение, предиктивный анализ, машинный перевод и другие области – понимание естественного языка (NLU) и его генерация (NLG) является значительной и быстрорастущей его частью.
Хотя O
AI B-ShortName_Science
— O
это O
достаточно O
широкая O
область O
, O
включающая O
в O
себя O
машинное B-Science
зрение I-Science
, O
предиктивный B-Method
анализ I-Method
, O
машинный B-Task
перевод I-Task
и O
другие O
области O
– O
понимание B-Task
естественного I-Task
языка i-Task
( O
NLU B-ShortName
) O
и O
его O
генерация B-Task
( O
NLG B-Abbrev_Task
) O
является O
значительной O
и O
быстрорастущей O
его O
частью O
. O

# text =   Опуская историю, начавшуюся еще в 50-е годы с Алана Тьюринга и программы Элиза в 60-е годы, а также научные исследования в области лингвистики и машинного обучения 90-х годов, значимым событием более новой истории стало появление языка разметки AIML (Artificial Intelligence Markup Language), разработанной в 2001-м году Ричардом Уэлсом (Richard Wallace) и созданным на его основе чатботом A.L.I.C.E.
Опуская O
историю O
, O
начавшуюся O
еще O
в O
50-е O
годы O
с O
Алана B-Person
Тьюринга I-Person
и O
программы O
Элиза B-Application
в O
60-е O
годы O
, O
а O
также O
научные O
исследования O
в O
области O
лингвистики O
и O
машинного B-Science
обучения I-Science
90-х O
годов O
, O
значимым O
событием O
более O
новой O
истории O
стало O
появление O
языка O
разметки O
AIML B-ShortName_Method
( O
Artificial B-Method
Intelligence I-Method
Markup I-Method
Language I-Method
) O
, O
разработанной O
в O
2001-м B-Date_Method
году O
Ричардом B-Person
Уэлсом I-Person
( O
Richard I-Person
Wallace I-Person
) O
и O
созданным O
на O
его O
основе O
чатботом O
A.L.I.C.E. B-Abberv_Application

# text =   В течение последующих десяти лет подходы к написанию чатботов во многом представляли из себя переработки или улучшения этой методологии, получившей название «rule-based подход» или «подход на основе формальных правил».
В O
течение O
последующих O
десяти O
лет O
подходы O
к O
написанию O
чатботов O
во O
многом O
представляли O
из O
себя O
переработки O
или O
улучшения O
этой O
методологии O
, O
получившей O
название O
« O
rule B-Method
- I-Method
based I-Method
подход O
» O
или O
« O
подход O
на O
основе O
формальных B-Object
правил I-Object
» O
. O

# text =   Именно эти технологии, вместе с заметным продвижением в области технологий синтеза и распознавания речи, а также распространением мессенджеров и вебчатов – обусловили стремительный рост количества внедрений NLU-технологий в 2015-2018-м годах.
Именно O
эти O
технологии O
, O
вместе O
с O
заметным O
продвижением O
в O
области O
технологий O
синтеза B-Task
и I-Task
распознавания B-Task
речи I-Task
, O
а O
также O
распространением O
мессенджеров O
и O
вебчатов O
– O
обусловили O
стремительный O
рост O
количества O
внедрений O
NLU B-Method
- I-Method
технологий I-Method
в O
2015 B-Date_Method
- I-Date_Method
2018-м I-Date_Method
годах O
. O

# text =   Голосовые ассистенты (IVA): Alexa от Amazon, Google Assistant от Google, Siri от Apple, Cortana от Microsoft, Алиса от Яндекса – они определяют интенты (намерения) пользователей и исполняют команды.
Голосовые O
ассистенты O
( O
IVA O
) O
: O
Alexa B-Application
от O
Amazon B-Organization
, O
Google B-Application
Assistant I-Application
от O
Google B-Organization
, O
Siri B-Application
от O
Apple B-Organization
, O
Cortana B-Application
от O
Microsoft B-Organization
, O
Алиса B-Application
от O
Яндекса B-Organization
– O
они O
определяют O
интенты B-Object
( O
намерения B-Object
) O
пользователей O
и O
исполняют O
команды O
. O

# text =   В качестве каналов могут выступать умные устройства, ассистенты, встроенные в устройства или мобильные телефоны, привычный звонок на номер телефона, мессенджеры или вебчаты, подобные популярным в России Livetex, Jivosite или Webim.
В O
качестве O
каналов O
могут O
выступать O
умные O
устройства O
, O
ассистенты O
, O
встроенные O
в O
устройства O
или O
мобильные O
телефоны O
, O
привычный O
звонок O
на O
номер O
телефона O
, O
мессенджеры O
или O
вебчаты O
, O
подобные O
популярным O
в O
России O
Livetex B-Application
, O
Jivosite B-Application
или O
Webim B-Application
. O

# text =   За эту конвертацию отвечают платформы ASR (распознавание речи), TTS (синтез речи), системы интеграции с телефонией.
За O
эту O
конвертацию O
отвечают O
платформы O
ASR B-Abbrev_Task
( O
распознавание B-Task
речи I-Task
) O
, O
TTS B-ShortName_Method
( O
синтез B-Method
речи I-Method
) O
, O
системы O
интеграции O
с O
телефонией O
. O

# text =   Наличие развитого rule-based синтаксиса может ускорить разработку чатботов в разы.
Наличие O
развитого O
rule B-Method
- I-Method
based I-Method
синтаксиса O
может O
ускорить O
разработку O
чатботов O
в O
разы O
. O

# text =   Анализ эмоций, богатая и глубокая аналитика, специальные фильтры (например, на использование ненормативной лексики), языковая поддержка, хранение контекста, как и собственно, точность работы используемых нейросетевых алгоритмов, а также производительность, масштабируемость и стабильность – все это также важные, хотя и не всегда очевидные со стороны, особенности диалоговых платформ.
Анализ B-Task
эмоций I-Task
, O
богатая O
и O
глубокая B-Task
аналитика I-Task
, O
специальные O
фильтры O
( O
например O
, O
на O
использование O
ненормативной O
лексики O
) O
, O
языковая O
поддержка O
, O
хранение O
контекста O
, O
как O
и O
собственно O
, O
точность O
работы O
используемых O
нейросетевых O
алгоритмов O
, O
а O
также O
производительность O
, O
масштабируемость O
и O
стабильность O
– O
все O
это O
также O
важные O
, O
хотя O
и O
не O
всегда O
очевидные O
со O
стороны O
, O
особенности O
диалоговых B-Application
платформ I-Application
. O

# text =   Алгоритм понимания естественного языка (Natural Language Understanding, NLU) Microsoft DeBERTa превзошел человеческие возможности в одном из самых сложных тестов для подобных алгоритмов SuperGLUE.
Алгоритм O
понимания O
естественного O
языка B-Object
( O
Natural B-Task
Language I-Task
Understanding I-Task
, O
NLU B-Abbrev_Task
) O
Microsoft B-Organization
DeBERTa B-Application
превзошел O
человеческие O
возможности O
в O
одном O
из O
самых O
сложных O
тестов O
для O
подобных O
алгоритмов O
SuperGLUE B-Method
. O

# text =   На данный момент модель занимает первое место в рейтинге с показателем в 90,3, в то время как среднее значение человеческих возможностей составляет 89,8 баллов.
На O
данный O
момент O
модель O
занимает O
первое O
место O
в O
рейтинге O
с O
показателем O
в O
90,3 B-Value
, O
в O
то O
время O
как O
среднее O
значение O
человеческих O
возможностей O
составляет O
89,8 B-Value
баллов O
. O

# text =  Тест SuperGLUE включает в себя ряд задач, которые разработаны для оценки способности ИИ-моделей распознавать и понимать естественный язык, например, дать правильный ответ на вопрос на базе прочитанного абзаца, определить, правильно ли используется многозначное слово в определенном контексте и т.д.
Тест O
SuperGLUE B-Method
включает O
в O
себя O
ряд O
задач O
, O
которые O
разработаны O
для O
оценки O
способности O
ИИ O
- O
моделей O
распознавать O
и O
понимать O
естественный O
язык O
, O
например O
, O
дать B-Task
правильный I-Task
ответ I-Task
на I-Task
вопрос I-Task
на I-Task
базе I-Task
прочитанного I-Task
абзаца I-Task
, O
определить O
, O
правильно O
ли O
используется O
многозначное B-Subject
слово I-Subject
в O
определенном O
контексте O
и O
т.д. O

# text =   Тест был разработан группой исследователей в 2019 году.
Тест O
был O
разработан O
группой O
исследователей O
в O
2019 B-Date
году O
. O

# text =  Для того чтобы добиться текущего результата в 90,3 балла, DeBERTa получила масштабное обновление архитектуры: теперь она состоит из 48 слоев и имеет 1,5 млрд параметров.
Для O
того O
чтобы O
добиться O
текущего O
результата O
в O
90,3 B-Value
балла O
, O
DeBERTa B-Application
получила O
масштабное O
обновление O
архитектуры O
: O
теперь O
она O
состоит O
из O
48 B-Value
слоев O
и O
имеет O
1,5 O
млрд O
параметров O
. O

# text =   Кроме того, DeBERTa будет интегрирована в следующую версию Тьюринговой модели Microsoft Turing (Turing NLRv4).
Кроме O
того O
, O
DeBERTa B-Application
будет O
интегрирована O
в O
следующую O
версию O
Тьюринговой B-Model
модели I-Model
Microsoft I-Model
Turing I-Model
( O
Turing B-Model
NLRv4 I-Model
) O
. O

# text =   Тьюринговые модели используются в таких продуктах Microsoft, как Bing, Office, Dynamics и Azure Cognitive Services, чтобы совершенствовать, к примеру, взаимодействие с чат-ботами, предоставление рекомендаций и ответов на вопросы, поиск, автоматизацию поддержки клиентов, создание контента и решение многих других задач на пользу сотен миллионов пользователей.
Тьюринговые B-Model
модели I-Model
используются O
в O
таких O
продуктах O
Microsoft B-Organization
, O
как O
Bing B-Application
, O
Office B-Application
, O
Dynamics B-Application
и O
Azure B-Application
Cognitive I-Application
Services I-Application
, O
чтобы O
совершенствовать O
, O
к O
примеру O
, O
взаимодействие O
с O
чат B-Application
- I-Application
ботами I-Application
, O
предоставление O
рекомендаций O
и O
ответов O
на O
вопросы O
, O
поиск O
, O
автоматизацию 
поддержки O
клиентов O
, O
создание B-Task
контента I-Task
и O
решение O
многих O
других O
задач O
на O
пользу O
сотен O
миллионов O
пользователей O
. O

# text =   В отличии от машин, люди хорошо умеют использовать знания, ранее полученные при выполнении различных задач, для решения новых – это называется композиционным обобщением (англ. compositional generalization).
В O
отличии O
от O
машин O
, O
люди O
хорошо O
умеют O
использовать O
знания O
, O
ранее O
полученные O
при O
выполнении O
различных O
задач O
, O
для O
решения O
новых O
– O
это O
называется O
композиционным B-Method
обобщением I-Method
( O
англ O
. O
compositional B-Method
generalization I-Method
) O
. O

# text =   Типичным методом обучения без учителя является кластеризация, благодаря которому обучающая выборка разбивается на устойчивые группы или кластеры.
Типичным O
методом O
обучения O
без O
учителя O
является O
кластеризация B-Method
, O
благодаря O
которому O
обучающая O
выборка B-Object
разбивается O
на O
устойчивые O
группы O
или O
кластеры B-Result
. O

# text =   Другой подход обучения без учителя для текстов называется тематическим моделированием (topic modeling), позволяющим выявить в неразмеченных текстах основные тематики.
Другой O
подход O
обучения O
без O
учителя O
для O
текстов O
называется O
тематическим B-Method
моделированием I-Method
( O
topic B-Method
modeling I-Method
) O
, O
позволяющим O
выявить O
в O
неразмеченных O
текстах O
основные O
тематики O
. O

# text =   Если отказываемся от методов unsupervised learning, то логично обратиться к методам обучения с учителем (supervised learning) и в частности к классификации.
Если O
отказываемся O
от O
методов O
unsupervised B-Method
learning I-Method
, O
то O
логично O
обратиться O
к O
методам B-Method
обучения I-Method
с I-Method
учителем I-Method
( O
supervised B-Method
learning I-Method
) O
и O
в O
частности O
к O
классификации B-Task
. O

# text =   Результатом работы языковой модели являются эмбеддинги — это отображение из пространства слов в пространство векторов конкретной фиксированной длины, причем векторы, соответствующие близким по смыслу словам, будут расположены в новом пространстве рядом, а далекие по смыслу — далеко.
Результатом O
работы O
языковой O
модели O
являются O
эмбеддинги B-Object
— O
это O
отображение O
из O
пространства O
слов B-Subject
в O
пространство O
векторов O
конкретной O
фиксированной O
длины O
, O
причем O
векторы O
, O
соответствующие O
близким O
по O
смыслу B-Subject
словам I-Subject
, O
будут O
расположены O
в O
новом O
пространстве O
рядом O
, O
а O
далекие O
по O
смыслу O
— O
далеко O
. O

# text =   При использовании TF-IDF (например, вот) подхода с фильтром по частотам и логистической регрессии уже можно получить прекрасные результаты: изначально в краулер отправлялись очень разные тексты, и модель прекрасно справляется.
При O
использовании O
TF B-Metric
- I-Metric
IDF I-Metric
( O
например O
, O
вот O
) O
подхода O
с O
фильтром O
по O
частотам O
и O
логистической B-Metric
регрессии I-Metric
уже O
можно O
получить O
прекрасные O
результаты O
: O
изначально O
в O
краулер O
отправлялись O
очень O
разные O
тексты O
, O
и O
модель O
прекрасно O
справляется O
. O

# text =   Для каждой из популяций рассчитаем word2vec расстояние до центра положительной обучающей выборки.
Для O
каждой O
из O
популяций O
рассчитаем O
word2vec B-Result
расстояние O
до O
центра O
положительной O
обучающей O
выборки B-Object
. O

# text =   Распределения можно разделить, и для оценки расстояния между распределениями в первую очередь логично обратиться к Дивергенции Кульбака-Лейблера (ДКЛ).
Распределения O
можно O
разделить O
, O
и O
для O
оценки O
расстояния O
между O
распределениями O
в O
первую O
очередь O
логично O
обратиться O
к O
Дивергенции B-Metric
Кульбака I-Metric
- I-Metric
Лейблера I-Metric
( O
ДКЛ B-ShortName_Metric
) O
. O

# text = Основатель компании Imagination Engines, Stephen L. Thaler продвигает свою нейронную сеть по имени DABUS (Device for the Autonomous Boot-strapping of Unified Sentience), указывая ее в качестве автора изобретения в заявках на патенты на разные изобретения, сгенерированные этой сетью.
Основатель O
компании O
Imagination B-Organization
Engines I-Organization
, O
Stephen B-Person
L. I-Person
Thaler I-Person
продвигает O
свою O
нейронную O
сеть O
по O
имени O
DABUS B-ShortName_Model
( O
Device B-Model
for I-Model
the I-Model
Autonomous I-Model
Boot I-Model
- I-Model
strapping I-Model
of I-Model
Unified I-Model
Sentience I-Model
) O
, O
указывая O
ее O
в O
качестве O
автора O
изобретения O
в O
заявках O
на O
патенты O
на O
разные O
изобретения O
, O
сгенерированные O
этой O
сетью O
. O

# text =   Специалисты Data Science часто применяют различные методы получения датасетов.
Специалисты O
Data B-Science
Science I-Science
часто O
применяют O
различные O
методы O
получения O
датасетов O
. O

# text =   Цель этой статьи — представить краткий обзор трех разных методов извлечения данных с использованием языка Python.
Цель O
этой O
статьи O
— O
представить O
краткий O
обзор O
трех O
разных O
методов O
извлечения O
данных O
с O
использованием O
языка B-Object
Python B-Environment
. O

# text =   Я расскажу, как делать это с помощью Jupyter Notebook.
Я O
расскажу O
, O
как O
делать O
это O
с O
помощью O
Jupyter B-Environment
Notebook I-Environment
. O

# text =   Библиотека SQLAlchemy позволит связать ваш код в ноутбуке с наиболее распространенными типами баз данных.
Библиотека O
SQLAlchemy B-Environment
позволит O
связать O
ваш O
код O
в O
ноутбуке O
с O
наиболее O
распространенными O
типами O
баз B-InfoResource
данных I-InfoResource
. O

# text =   Мы собираемся применить Beautiful Soup и библиотеку urllib, чтобы соскрапить названия отелей и цены на них с веб-сайта TripAdvisor.
Мы O
собираемся O
применить O
Beautiful B-Environment
Soup I-Environment
и O
библиотеку O
urllib B-Environment
, O
чтобы O
соскрапить O
названия O
отелей O
и O
цены O
на O
них O
с O
веб O
- O
сайта O
TripAdvisor B-InfoResource
. O

# text =   Я приведу простой пример извлечения данных о погоде с общедоступного API Dark Sky.
Я O
приведу O
простой O
пример O
извлечения O
данных O
о O
погоде O
с O
общедоступного O
API B-Environment
Dark I-Environment
Sky I-Environment
. O

# text =   Для доступа к данным из Dark Sky я воспользуюсь библиотекой requests.
Для O
доступа O
к O
данным O
из O
Dark B-Environment
Sky I-Environment
я O
воспользуюсь O
библиотекой O
requests B-Environment
. O

# text =   Речь шла о морфологической разметке (part of speech tagging) современных текстов на русском языке.
Речь B-Subject
шла O
о O
морфологической B-Labeling
разметке B-Method
( O
part B-Method
of I-Method
speech I-Method
tagging I-Method
) O
современных O
текстов B-Object
на O
русском B-Lang
языке I-Lang
. O

# text =   Как обычно, результат разметки будет опубликован на условиях лицензии Creative Commons.
Как O
обычно O
, O
результат O
разметки B-Method
будет O
опубликован O
на O
условиях O
лицензии O
Creative B-Environment
Commons I-Environment
. O

# text =   Извлечение именованных сущностей из текста — одна из востребованных функций текстовой аналитики.
Извлечение B-Task
именованных I-Task
сущностей I-Task
из O
текста O
— O
одна O
из O
востребованных O
функций O
текстовой O
аналитики B-Activity
. O

# text =   Известный учёный Алан Тьюринг в 1950 году усомнился в том, что машина не может мыслить, и для проверки предложил свой знаменитый тест.
Известный O
учёный O
Алан B-Person
Тьюринг I-Person
в O
1950 B-Date
году O
усомнился O
в O
том O
, O
что O
машина O
не O
может O
мыслить O
, O
и O
для O
проверки O
предложил O
свой O
знаменитый O
тест B-Object
. O

# text =  В 1954 году прошёл Джорджтаунский эксперимент.
В O
1954 B-Date_Experiment
году O
прошёл O
Джорджтаунский B-Experiment
эксперимент I-Experiment
. O

# text =   В его рамках демонстрировалась система, которая автоматически перевела 60 предложений с русского языка на французский.
В O
его O
рамках O
демонстрировалась O
система B-Object
, O
которая O
автоматически O
перевела O
60 O
предложений B-Subject
с O
русского B-Lang
языка B-Object
на O
французский B-Lang
. O

# text =   В 1960-е годы появились первые чат-боты, очень примитивные: в основном они перефразировали то, что говорил им собеседник-человек.
В O
1960-е B-Date_Result
годы O
появились O
первые O
чат B-Result
- I-Result
боты I-Result
, O
очень O
примитивные O
: O
в O
основном O
они O
перефразировали O
то O
, O
что O
говорил O
им O
собеседник O
- O
человек O
. O

# text =   Даже знаменитый чат-бот Женя Густман, который, как считается, прошёл одну из версий теста Тьюринга, сделал это не благодаря хитрым алгоритмам.
Даже O
знаменитый O
чат B-Result
- I-Result
бот I-Result
Женя I-Result
Густман I-Result
, O
который O
, O
как O
считается O
, O
прошёл O
одну O
из O
версий O
теста B-Result
Тьюринга I-Result
, O
сделал O
это O
не O
благодаря O
хитрым O
алгоритмам O
. O

# text =   Учёные пытались всё формализовать, построить формальную модель, онтологию, понятия, связи, общие правила синтаксического разбора и универсальную грамматику.
Учёные O
пытались O
всё O
формализовать O
, O
построить O
формальную B-Model
модель B-Model
, O
онтологию B-Result
, O
понятия B-Object
, O
связи B-Object
, O
общие O
правила O
синтаксического B-Method
разбора I-Method
и O
универсальную B-Result
грамматику I-Result
. O

# text =   Тогда возникла теория грамматик Хомского.
Тогда O
возникла O
теория B-Method
грамматик I-Method
Хомского I-Method
. O

# text =   Поэтому в 1980-е годы внимание переключилось на систему другого класса: на алгоритмы машинного обучения и так называемую корпусную лингвистику.
Поэтому O
в O
1980-е B-Date
годы O
внимание O
переключилось O
на O
систему O
другого O
класса B-Object
: O
на O
алгоритмы B-Method
машинного B-Method
обучения I-Method
и O
так O
называемую O
корпусную B-Science
лингвистику I-Science
. O

# text =   В 1990-е годы эта область получила очень мощный толчок благодаря развитию Всемирной паутины с большим количеством слабоструктурированного текста, по которому нужно было искать, его требовалось каталогизировать.
В O
1990-е B-Date
годы O
эта O
область O
получила O
очень O
мощный O
толчок O
благодаря O
развитию O
Всемирной B-Application
паутины I-Application
с O
большим O
количеством O
слабоструктурированного B-Object
текста I-Object
, O
по O
которому O
нужно O
было O
искать O
, O
его O
требовалось O
каталогизировать O
. O

# text =   В 2000-е анализ естественных языков начал применяться уже не только для поиска в Интернете, но и для решения разнообразных задач.
В O
2000-е O
анализ B-Method
естественных I-Method
языков B-Method
начал O
применяться O
уже O
не O
только O
для O
поиска O
в O
Интернете B-Application
, O
но O
и O
для O
решения O
разнообразных O
задач O
. O

# text =   Возникли модели, основанные на краудсорсинге: мы не только пытаемся что-то понять с помощью машины, а подключаем людей, которые за небольшую плату определяют, на каком языке написан текст.
Возникли O
модели B-Object
, O
основанные O
на O
краудсорсинге B-Method
: O
мы O
не O
только O
пытаемся O
что O
- O
то O
понять O
с O
помощью O
машины O
, O
а O
подключаем O
людей O
, O
которые O
за O
небольшую O
плату O
определяют O
, O
на O
каком O
языке O
написан O
текст O
. O

# text =   В некотором смысле начали возрождаться идеи использования формальных онтологий, но теперь онтологии крутятся вокруг краудсорсинговых баз знаний, в частности баз на основе Linked Open Data.
В O
некотором O
смысле O
начали O
возрождаться O
идеи O
использования O
формальных B-Object
онтологий I-Object
, O
но O
теперь O
онтологии B-Object
крутятся O
вокруг O
краудсорсинговых O
баз O
знаний O
, O
в O
частности O
баз O
на O
основе O
Linked B-InfoResource
Open I-InfoResource
Data I-InfoResource
. O

# text =   Это целый набор баз знаний, его центр — машиночитаемый вариант «Википедии» DBpedia, который тоже наполняется по краудсорсинговой модели.
Это O
целый O
набор O
баз O
знаний O
, O
его O
центр O
— O
машиночитаемый O
вариант O
« O
Википедии B-InfoResource
» O
DBpedia B-InfoResource
, O
который O
тоже O
наполняется O
по O
краудсорсинговой B-Model
модели I-Model
. O

# text =   В частности, семантический анализ (о чём документ?), генерация автоматической аннотации и автоматического summary, перевод и создание документов.
В O
частности O
, O
семантический B-Method
анализ I-Method
( O
о O
чём O
документ O
? O
) O
, O
генерация B-Task
автоматической I-Task
аннотации I-Task
и O
автоматического B-Object
summary I-Object
, O
перевод B-Task
и O
создание B-Task
документов I-Task
. O

# text =   Все наверняка слышали об известном генераторе научных статей SCIgen, который создал статью «Корчеватель: Алгоритм типичной унификации точек доступа и избыточности».
Все O
наверняка O
слышали O
об O
известном O
генераторе O
научных O
статей O
SCIgen B-Technology
, O
который O
создал O
статью O
« O
Корчеватель O
: O
Алгоритм O
типичной O
унификации O
точек O
доступа O
и O
избыточности O
» O
. O

# text =   Но в случае с лентой такие рекомендации работают плохо: здесь постоянно возникает ситуация холодного старта.
Но O
в O
случае O
с O
лентой O
такие O
рекомендации O
работают O
плохо O
: O
здесь O
постоянно O
возникает O
ситуация B-Object
холодного I-Object
старта I-Object
. O

# text =   Поэтому применим классический воркэраунд для задачи холодного старта и построим систему контентных рекомендаций: попробуем научить машину понимать, о чём написан пост.
Поэтому O
применим O
классический O
воркэраунд B-Method
для O
задачи O
холодного B-Task
старта I-Task
и O
построим O
систему O
контентных O
рекомендаций O
: O
попробуем O
научить O
машину O
понимать O
, O
о O
чём O
написан O
пост O
. O

# text =   Соответственно, требуется метод семантического анализа.
Соответственно O
, O
требуется O
метод B-Method
семантического I-Method
анализа I-Method
. O

# text =   Тут поможет анализ эмоциональной окраски.
Тут O
поможет O
анализ B-Method
эмоциональной I-Method
окраски I-Method
. O

# text =   В частности, это Apache Tika, японская библиотека language-detection и одна из последних разработок — питоновский пакет Ldig, который как раз работает на инфинитиграммах.
В O
частности O
, O
это O
Apache B-Library
Tika I-Library
, O
японская O
библиотека O
language B-Library
- I-Library
detection I-Library
и O
одна O
из O
последних O
разработок O
— O
питоновский O
пакет O
Ldig B-Library
, O
который O
как O
раз O
работает O
на O
инфинитиграммах O
. O

# text =   Но если текст короткий, из одного предложения или нескольких слов, то классический подход, основанный на триграммах, очень часто ошибается.
Но O
если O
текст B-Object
короткий O
, O
из O
одного O
предложения B-Subject
или O
нескольких O
слов B-Subject
, O
то O
классический O
подход O
, O
основанный O
на O
триграммах B-Subject
, O
очень O
часто O
ошибается O
. O

# text =   Исправить ситуацию могут инфинитиграммы, но это новая область, далеко не для всех языков уже есть обученные и готовые классификаторы.
Исправить O
ситуацию O
могут O
инфинитиграммы B-Object
, O
но O
это O
новая O
область O
, O
далеко O
не O
для O
всех O
языков O
уже O
есть O
обученные O
и O
готовые O
классификаторы O
. O

# text =   Первый основан на так называемом фонетическом матчинге.
Первый O
основан O
на O
так O
называемом O
фонетическом B-Method
матчинге I-Method
. O

# text =   Альтернативный подход — так называемое редакционное расстояние, с помощью которого мы ищем в словаре максимально похожие слова-аналоги.
Альтернативный O
подход O
— O
так O
называемое O
редакционное B-Metric
расстояние I-Metric
, O
с O
помощью O
которого O
мы O
ищем O
в O
словаре O
максимально O
похожие O
слова O
- O
аналоги O
. O

# text =   Первая концепция — стемминг, мы пытаемся найти основу слова.
Первая O
концепция O
— O
стемминг B-Method
, O
мы O
пытаемся O
найти O
основу B-Subject
слова I-Subject
. O

# text =   Здесь используется подход affix stripping.
Здесь O
используется O
подход O
affix B-Method
stripping I-Method
. O

# text =   Есть известная реализация, так называемый стеммер Портера, или проект Snowball.
Есть O
известная O
реализация O
, O
так O
называемый O
стеммер B-Object
Портера I-TERM
, O
или O
проект O
Snowball B-Project
. O

# text =   Самый распространённый, наверное, инструмент — реализация в пакете Apache Lucene.
Самый O
распространённый O
, O
наверное O
, O
инструмент O
— O
реализация O
в O
пакете O
Apache B-Library
Lucene I-Library
. O

# text =   Вторая концепция, альтернатива стемминга — лемматизация.
Вторая O
концепция O
, O
альтернатива O
стемминга B-Method
— O
лемматизация B-Method
. O

# text =   Она пытается привести слово не к основе или корню, а к базовой, словарной форме — т. е. лемме.
Она O
пытается O
привести O
слово B-Subject
не O
к O
основе B-Subject
или O
корню B-Object
, O
а O
к O
базовой O
, O
словарной B-Subject
форме I-Subject
— O
т O
. O
  O
е O
. O
лемме B-Subject
. O

# text =   Существует множество реализаций, и тема очень хорошо проработана именно для user generated текстов, пользовательски зашумлённых текстов.
Существует O
множество O
реализаций O
, O
и O
тема B-Object
очень O
хорошо O
проработана O
именно O
для O
user B-Object
generated I-Object
текстов I-Object
, O
пользовательски O
зашумлённых O
текстов B-Object
. O

# text =   Теперь отобразим это в векторном пространстве, потому что почти все математические модели работают в векторных пространствах больших размерностей.
Теперь O
отобразим O
это O
в O
векторном B-Object
пространстве I-Object
, O
потому O
что O
почти O
все O
математические B-Model
модели B-Model
работают O
в O
векторных B-Object
пространствах I-Object
больших O
размерностей O
. O

# text =   Базовый подход, который используют многие модели, — метод "мешка слов".
Базовый O
подход O
, O
который O
используют O
многие O
модели O
, O
— O
метод B-Method
" I-Method
мешка I-Method
слов I-Method
" I-Method
. O

# text =   Доминирует так называемый TF-IDF.
Доминирует O
так O
называемый O
TF B-Metric
- I-Metric
IDF I-Metric
. O

# text =   Частоту слова (term frequency, TF) определяют по-разному.
Частоту B-Metric
слова B-Metric
( O
term B-Metric
frequency I-Metric
, O
TF B-Metric
) O
определяют O
по O
- O
разному O
. O

# text =   Определив TF в документе, мы перемножаем её с обратной частотой документа (inverse document frequency, IDF).
Определив O
TF B-Metric
в O
документе O
, O
мы O
перемножаем O
её O
с O
обратной B-Metric
частотой I-Metric
документа I-Metric
( O
inverse B-Metric
document I-Metric
frequency I-Metric
, O
IDF B-Metric
) O
. O

# text =   IDF обычно вычисляют как логарифм от числа документов в корпусе, разделённый на количество документов, где это слово представлено.
IDF B-Metric
обычно O
вычисляют O
как O
логарифм B-Object
от O
числа O
документов O
в O
корпусе B-Object
, O
разделённый O
на O
количество O
документов O
, O
где O
это O
слово B-Subject
представлено O
. O

# text =   Например, при анализе эмоциональной окраски очень важно, к чему относилось, условно говоря, слово «хороший» или «нет».
Например O
, O
при O
анализе B-Method
эмоциональной I-Method
окраски I-Method
очень O
важно O
, O
к O
чему O
относилось O
, O
условно O
говоря O
, O
слово B-Subject
« O
хороший O
» O
или O
« O
нет O
» O
. O

# text =   Тогда наряду с мешком слов поможет мешок N-грамм: мы добавляем в словарь не только слова, но и словосочетания.
Тогда O
наряду O
с O
мешком B-Method
слов I-Method
поможет O
мешок B-Method
N I-Method
- I-Method
грамм I-Method
: O
мы O
добавляем O
в O
словарь O
не O
только O
слова B-Subject
, O
но O
и O
словосочетания B-Subject
. O

# text =   Мы не будем вносить все словосочетания, потому что это приведёт к комбинаторному взрыву, но часто используемые статистически значимые пары или пары, соответствующие именованным сущностям, можно добавить, и это повысит качество работы итоговой модели.
Мы O
не O
будем O
вносить O
все O
словосочетания B-Subject
, O
потому O
что O
это O
приведёт O
к O
комбинаторному O
взрыву O
, O
но O
часто O
используемые O
статистически O
значимые O
пары O
или O
пары O
, O
соответствующие O
именованным B-Subject
сущностям I-Subject
, O
можно O
добавить O
, O
и O
это O
повысит O
качество O
работы O
итоговой O
модели B-Object
. O

# text =   Отчасти эти ситуации позволяют обработать методы построения "векторных представлений слов", например, знаменитый word2vec или более модные skip-gramm.
Отчасти O
эти O
ситуации O
позволяют O
обработать O
методы O
построения O
" O
векторных B-Subject
представлений I-Subject
слов I-Subject
" O
, O
например O
, O
знаменитый O
word2vec B-Model
или O
более O
модные O
skip B-Model
- I-Model
gramm I-Model
. O

# text =   Стандартные хеш-функции равномерно размазывают данные по пространству хешей.
Стандартные O
хеш B-Method
- I-Method
функции I-Method
равномерно O
размазывают O
данные O
по O
пространству O
хешей O
. O

# text =   Локально-чувствительный хеш похожие объекты поместит в пространстве объектов близко.
Локально B-Method
- I-Method
чувствительный I-Method
хеш I-Method
похожие O
объекты O
поместит O
в O
пространстве O
объектов O
близко O
. O

# text =   Мы выбираем случайный базис из случайных векторов.
Мы O
выбираем O
случайный O
базис B-Object
из O
случайных B-Object
векторов I-Object
. O

# text =   Задача семантического анализа достаточно старая.
Задача O
семантического B-Task
анализа B-Task
достаточно O
старая O
. O

# text =   Современный подход — анализ семантики без учителя, поэтому его называют анализом скрытой (латентной) семантики.
Современный O
подход O
— O
анализ B-Method
семантики I-Method
без I-Method
учителя I-Method
, O
поэтому O
его O
называют O
анализом B-Method
скрытой I-Method
( I-Method
латентной I-Method
) I-Method
семантики I-Method
. O

# text =   Исторически первый подход к латентно-семантическому анализу — это латентно-семантическое индексирование.
Исторически O
первый O
подход O
к O
латентно B-Method
- I-Method
семантическому I-Method
анализу B-Method
— O
это O
латентно B-Method
- I-Method
семантическое I-Method
индексирование I-Method
. O

# text =   Мы уже использовали для решения задач коллаборативных рекомендаций хорошо зарекомендовавшие себя техники факторизации матриц.
Мы O
уже O
использовали O
для O
решения O
задач B-Task
коллаборативных I-Task
рекомендаций I-Task
хорошо O
зарекомендовавшие O
себя O
техники O
факторизации O
матриц O
. O

# text =   В чём суть факторизации?
В O
чём O
суть O
факторизации B-Method
? O

# text =   Одной из альтернатив стал так называемый вероятностный латентно-семантический индекс.
Одной O
из O
альтернатив O
стал O
так O
называемый O
вероятностный B-Metric
латентно I-Metric
- I-Metric
семантический I-Metric
индекс B-Metric
. O

# text =   Важно понять, что техника вероятностного латентно-семантического индекса — это техника факторизации матрицы.
Важно O
понять O
, O
что O
техника B-Method
вероятностного I-Method
латентно I-Method
- I-Method
семантического I-Method
индекса B-Method
— O
это O
техника I-Method
факторизации I-Method
матрицы I-Method
. O

# text =   По сравнению с классической факторизацией на основе сингулярного разложения у вероятностной генерирующей модели есть важное преимущество.
По O
сравнению O
с O
классической B-Method
факторизацией I-Method
на O
основе O
сингулярного B-Object
разложения I-Object
у O
вероятностной O
генерирующей O
модели O
есть O
важное O
преимущество O
. O

# text =   Для этого используется перплексия.
Для O
этого O
используется O
перплексия B-Method
. O

# text =   Есть так называемый EM-алгоритм.
Есть O
так O
называемый O
EM B-Method
- I-Method
алгоритм I-Method
. O

# text =  Как поясняет сам Томас Димсон, This Word Does Not Exist является вариацией нейросети GPT-2.
Как O
поясняет O
сам O
Томас B-Person
Димсон I-Person
, O
This B-Model
Word I-Model
Does I-Model
Not I-Model
Exist I-Model
является O
вариацией O
нейросети O
GPT-2 B-Model
. O

# text =   Существует также твиттер-бот проекта.
Существует O
также O
твиттер B-Technology
- I-Technology
бот I-Technology
проекта O
. O

# text =   Чтобы натренировать свою нейросеть на основе загруженных файлов, Димсон рекомендует воспользоваться контентом Apple Dictionary или Urban Dictionary.
Чтобы O
натренировать O
свою O
нейросеть O
на O
основе O
загруженных O
файлов O
, O
Димсон B-Person
рекомендует O
воспользоваться O
контентом O
Apple B-InfoResource
Dictionary I-InfoResource
или O
Urban B-InfoResource
Dictionary I-InfoResource
. O

# text =  Правда, пользователи YCombinator уже заметили, что This Word Does Not Exist иногда предлагает уже существующие слова — например, refactoring.
Правда O
, O
пользователи O
YCombinator B-Application
уже O
заметили O
, O
что O
This B-Model
Word I-Model
Does I-Model
Not I-Model
Exist I-Model
иногда O
предлагает O
уже O
существующие O
слова O
— O
например O
, O
refactoring O
. O

# text =   После выхода учебника я читал курс на его основе в УрФУ, ШАДе, ИТМО и СПбГУ и убедился, что наличие перевода очень помогает.
После O
выхода O
учебника O
я O
читал O
курс O
на O
его O
основе O
в O
УрФУ B-Abbrev_Organization
, O
ШАДе B-Abbrev_Organization
, O
ИТМО B-Abbrev_Organization
и O
СПбГУ B-Abbrev_Organization
и O
убедился O
, O
что O
наличие O
перевода O
очень O
помогает O
. O

# text =   В случае NLP потребность в «локализованных» учебных материалах еще заметнее, чем в информационном поиске.
В O
случае O
NLP B-ShortName_Science
потребность O
в O
« O
локализованных O
» O
учебных O
материалах O
еще O
заметнее O
, O
чем O
в O
информационном B-Task
поиске I-Task
. O

# text =   Слушателям предлагается самостоятельно реализовать методы морфологического анализа, определения тональности текста, автоматического реферирования документов, извлечения именованных сущностей и машинного перевода.
Слушателям O
предлагается O
самостоятельно O
реализовать O
методы B-Method
морфологического I-Method
анализа I-Method
, O
определения B-Task
тональности I-Task
текста I-Task
, O
автоматического B-Task
реферирования I-Task
документов I-Task
, O
извлечения B-Task
именованных I-Task
сущностей I-Task
и O
машинного B-Task
перевода I-Task
. O

# text =   Желательно, чтобы слушатели обладали базовыми знаниями линейной алгебры, теории вероятностей, математической статистики и машинного обучения, а также навыками программирования (необходимы для решения практических заданий).
Желательно O
, O
чтобы O
слушатели O
обладали O
базовыми O
знаниями O
линейной B-Science
алгебры I-Science
, O
теории B-Science
вероятностей I-Science
, O
математической B-Science
статистики I-Science
и O
машинного B-Method
обучения I-Method
, O
а O
также O
навыками O
программирования B-Science
( O
необходимы O
для O
решения O
практических O
заданий O
) O
. O

# text =   Он опубликовал программу (репозиторий на гитхабе), которая делает именно это: генерирует политические речи, удивительно похожие на настоящие.
Он O
опубликовал O
программу O
( O
репозиторий O
на O
гитхабе O
) O
, O
которая O
делает O
именно O
это O
: O
генерирует B-Task
политические I-Task
речи B-Task
, O
удивительно O
похожие O
на O
настоящие O
. O
# text =  Ученые Новосибирского государственного технического университета НЭТИ завершают разработку системы распознавания русского жестового языка.
Ученые O
Новосибирского B-Organization
государственного I-Organization
технического I-Organization
университета I-Organization
НЭТИ B-ShortName_Organization
завершают O
разработку O
системы O
распознавания O
русского B-Lang
жестового B-Object
языка I-Object
. O

# text =   Точность распознавания составляет 92%.
Точность B-Metric
распознавания O
составляет O
92 B-Value
% I-Value
. O

# text =   «Мы также вели работу над выделением эпентезы (межжестовое движение).
« O
Мы O
также O
вели O
работу O
над O
выделением B-Task
эпентезы I-Task
( O
межжестовое B-Object
движение I-Object
) O
. O

# text =   Сейчас точность выделения жестов в видеопотоке составляет 85—90%.
Сейчас O
точность B-Metric
выделения B-Task
жестов I-Task
в O
видеопотоке O
составляет O
85—90 B-Value
% I-Value
. O

# text =   С моделью от OpenAI связано сразу несколько новостей — хорошая и не очень.
С O
моделью B-Object
от O
OpenAI B-Organization
связано O
сразу O
несколько O
новостей O
— O
хорошая O
и O
не O
очень O
. O

# text =   Сделка OpenAI и Microsoft. Начать придется с менее приятной — компания Майкрософт завладела эксклюзивными правами на GPT-3.
Сделка O
OpenAI B-Organization
и O
Microsoft B-Organization
. O
Начать O
придется O
с O
менее O
приятной O
— O
компания O
Майкрософт B-Organization
завладела O
эксклюзивными O
правами O
на O
GPT-3 B-Model
. O

# text =   Сделка предсказуемо вызвала негодование — Элон Маск, основатель OpenAI, а ныне бывший член совета директоров компании, заявил, что Майкрософт по сути захватили OpenAI.
Сделка O
предсказуемо O
вызвала O
негодование O
— O
Элон B-Person
Маск I-Person
, O
основатель O
OpenAI B-Organization
, O
а O
ныне O
бывший O
член O
совета O
директоров O
компании O
, O
заявил O
, O
что O
Майкрософт B-Organization
по O
сути O
захватили O
OpenAI B-Organization
. O

# text =   Дело в том, что OpenAI изначально создавалась как некоммерческая организация с высокой миссией — не позволить искусственному интеллекту оказаться в руках отдельного государства или корпорации.
Дело O
в O
том O
, O
что O
OpenAI B-Organization
изначально O
создавалась O
как O
некоммерческая O
организация O
с O
высокой O
миссией O
— O
не O
позволить O
искусственному B-Object
интеллекту I-Object
оказаться O
в O
руках O
отдельного O
государства O
или O
корпорации O
. O

# text =   ruGPT3 от Сбера. Теперь к более приятной новости — исследователи из Сбера выложили в открытый доступ модель, которая повторяет архитектуру GPT-3 и основана на коде GPT-2 и, самое главное, обучена на русскоязычном корпусе.
ruGPT3 B-Model
от O
Сбера B-Organization
. O
Теперь O
к O
более O
приятной O
новости O
— O
исследователи O
из O
Сбера B-Organization
выложили O
в O
открытый O
доступ O
модель B-Object
, O
которая O
повторяет O
архитектуру O
GPT-3 B-Model
и O
основана O
на O
коде O
GPT-2 B-Model
и O
, O
самое O
главное O
, O
обучена O
на O
русскоязычном O
корпусе B-Object
. O

# text =   Если коммерческие организации можно оправдать тем, что код часто вплетен в инфраструктуру проектов, то что говорить про исследовательские институты и некоммерческие компании вроде DeepMind и OpenAI?
Если O
коммерческие O
организации O
можно O
оправдать O
тем O
, O
что O
код O
часто O
вплетен O
в O
инфраструктуру O
проектов O
, O
то O
что O
говорить O
про O
исследовательские O
институты O
и O
некоммерческие O
компании O
вроде O
DeepMind B-Organization
и O
OpenAI B-Organization
? O

# text =   Платформа для видеозвонков Maxine объединяет в себе целый зоопарк ML-алгоритмов.
Платформа O
для O
видеозвонков O
Maxine B-Application
объединяет O
в O
себе O
целый O
зоопарк O
ML O
- O
алгоритмов O
. O

# text =   Google Meet поделились кейсом создания своего алгоритма для качественного удаления фона на основе фреймворка от Mediapipe (который умеет отслеживание движение глаз, головы и рук).
Google B-Organization
Meet I-Organization
поделились O
кейсом O
создания O
своего O
алгоритма O
для O
качественного O
удаления O
фона B-Subject
на O
основе B-Subject
фреймворка O
от O
Mediapipe B-Organization
( O
который O
умеет O
отслеживание O
движение O
глаз O
, O
головы O
и O
рук O
) O
. O

# text =   Google также запустил новую функцию для сервиса YouTube Stories на iOS, который позволяет улучшать качество речи.
Google B-Organization
также O
запустил O
новую O
функцию O
для O
сервиса O
YouTube B-Technology
Stories I-Technology
на O
iOS B-Environment
, O
который O
позволяет O
улучшать O
качество O
речи B-Subject
. O

# text =   Прорывы #DeepPavlov в 2019 году: обзор и итоги года Московский физико-технический институт (МФТИ).
Прорывы O
# O
DeepPavlov B-Model
в O
2019 B-Date
году O
: O
обзор O
и O
итоги O
года O
Московский B-Organization
физико I-Organization
- I-Organization
технический I-Organization
институт I-Organization
( O
МФТИ B-ShortName_Organization
) O
. O

# text =   Библиотеке #DeepPavlov, на минуточку, уже два года, и мы рады, что наше сообщество с каждым днем растет.
Библиотеке O
# O
DeepPavlov B-Library
, O
на O
минуточку O
, O
уже O
два O
года O
, O
и O
мы O
рады O
, O
что O
наше O
сообщество O
с O
каждым O
днем O
растет O
. O

# text =   Увеличилось количество коммерческих решений за счет state-of-art технологий, реализованных в DeepPavlov, в разных отраслях от ритейла до промышленности.
Увеличилось O
количество O
коммерческих O
решений O
за O
счет O
state B-Method
- I-Method
of I-Method
- I-Method
art I-Method
технологий O
, O
реализованных O
в O
DeepPavlov B-Model
, O
в O
разных O
отраслях O
от O
ритейла O
до O
промышленности O
. O

# text =   Вышел первый релиз DeepPavlov Agent.
Вышел O
первый O
релиз O
DeepPavlov B-Application
Agent B-Application
. O

# text =   DeepPavlov решает проблемы такие как: классификация текста, исправление опечаток, распознавание именованных сущностей, ответы на вопросы по базе знаний и многие другие.
DeepPavlov B-Model
решает O
проблемы O
такие O
как O
: O
классификация B-Task
текста B-Task
, O
исправление B-Task
опечаток I-Task
, O
распознавание B-Task
именованных I-Task
сущностей I-Task
, O
ответы B-Task
на I-Task
вопросы I-Task
по O
базе O
знаний O
и O
многие O
другие O
. O

# text =   Библиотека поддерживает платформы Linux и Windows.
Библиотека O
поддерживает O
платформы O
Linux B-Environment
и O
Windows B-Environment
. O

# text =   В настоящее время современные результаты во многих задачах были достигнуты благодаря применению моделей на основе BERT.
В O
настоящее O
время O
современные O
результаты O
во O
многих O
задачах O
были O
достигнуты O
благодаря O
применению O
моделей O
на O
основе O
BERT B-ShortName_Model
. O

# text =   Команда DeepPavlov интегрировала BERT в три последующие задачи: классификация текста, распознавание именованных сущностей и ответы на вопросы.
Команда O
DeepPavlov B-Organization
интегрировала O
BERT B-ShortName_Model
в O
три O
последующие O
задачи O
: O
классификация B-Task
текста B-Task
, O
распознавание B-Task
именованных I-Task
сущностей I-Task
и O
ответы B-Task
на I-Task
вопросы I-Task
. O

# text =   Модель классификации текста на основе BERT DeepPavlov служит, например, для решения проблемы обнаружения оскорблений.
Модель O
классификации O
текста O
на O
основе O
BERT B-ShortName_Model
DeepPavlov B-Model
служит O
, O
например O
, O
для O
решения O
проблемы O
обнаружения B-Task
оскорблений I-Task
. O

# text =   В дополнение к моделям классификации текста DeepPavlov содержит модель на основе BERT для распознавания именованных сущностей (NER).
В O
дополнение O
к O
моделям O
классификации B-Task
текста I-Task
DeepPavlov B-Model
содержит O
модель O
на O
основе O
BERT B-ShortName_Model
для O
распознавания B-Task
именованных I-Task
сущностей I-Task
( O
NER B-Abbrev_Task
) O
. O

# text =   Например, модель может извлечь важную информацию из резюме, чтобы облегчить работу специалистов по кадрам.
Например O
, O
модель B-Object
может O
извлечь B-Task
важную I-Task
информацию I-Task
из O
резюме O
, O
чтобы O
облегчить O
работу O
специалистов O
по O
кадрам O
. O

# text =   Кроме того, NER может использоваться для идентификации соответствующих объектов в запросах клиентов, таких как спецификации продуктов, названия компаний или данные о филиалах компании.
Кроме O
того O
, O
NER B-Abbrev_Task
может O
использоваться O
для O
идентификации B-Task
соответствующих I-Task
объектов I-Task
в O
запросах O
клиентов O
, O
таких O
как O
спецификации O
продуктов O
, O
названия O
компаний O
или O
данные O
о O
филиалах O
компании O
. O

# text =   Команда DeepPavlov обучила модель NER на англоязычном корпусе OntoNotes, который имеет 19 типов разметки, включая PER (человек), LOC (местоположение), ORG (организация) и многие другие.
Команда O
DeepPavlov B-Organization
обучила O
модель B-Model
NER I-Model
на O
англоязычном O
корпусе B-Corpus
OntoNotes I-Corpus
, O
который O
имеет O
19 O
типов O
разметки B-Method
, O
включая O
PER Labeling
( O
человек O
) O
, O
LOC Labeling
( O
местоположение O
) O
, O
ORG Labeling
( O
организация O
) O
и O
многие O
другие O
. O

# text =   Одним из основных переломных моментов в этой области стал выпуск Стэнфордского набора данных для ответов на вопросы (SQuAD).
Одним O
из O
основных O
переломных O
моментов O
в O
этой O
области O
стал O
выпуск O
Стэнфордского B-InfoResource
набора I-InfoResource
данных I-InfoResource
для I-InfoResource
ответов I-InfoResource
на I-InfoResource
вопросы I-InfoResource
( O
SQuAD B-Abbrev_InfoResource
) O
. O

# text =   Набор данных SQuAD привел к появлению бесчисленных подходов к решению задачи вопросно-ответных систем.
Набор O
данных O
SQuAD B-Abbrev_InfoResource
привел O
к O
появлению O
бесчисленных O
подходов O
к O
решению O
задачи O
вопросно B-Task
- I-Task
ответных I-Task
систем I-Task
. O

# text =   Одной из наиболее успешных является модель DeepPavlov BERT.
Одной O
из O
наиболее O
успешных O
является O
модель O
DeepPavlov B-Model
BERT I-Model
. O

# text =   Чтобы использовать модель QA на основе BERT с DeepPavlov, необходимо следующее.
Чтобы O
использовать O
модель O
QA B-Model
на O
основе O
BERT B-ShortName_Model
с O
DeepPavlov B-TERM
, O
необходимо O
следующее O
. O

# text =   DeepPavlov Agent — платформа для создания многозадачных чат-ботов.
DeepPavlov B-Technology
Agent I-Technology
— O
платформа O
для O
создания O
многозадачных O
чат O
- O
ботов O
. O

# text =   При разработке разговорных агентов в основном применяется модульная архитектура для целенаправленного диалога, при котором разворачивается сценарий.
При O
разработке O
разговорных B-Technology
агентов I-Technology
в O
основном O
применяется O
модульная B-Object
архитектура I-Object
для O
целенаправленного B-Task
диалога B-Task
, O
при O
котором O
разворачивается O
сценарий O
. O

# text =   Для решения этой задачи в октябре 2019 года вышел первый релиз DeepPavlov Agent 1.0 — платформы для создания многозадачных чат-ботов.
Для O
решения O
этой O
задачи O
в O
октябре O
2019 B-Date_Application
года I-Date_Application
вышел O
первый O
релиз O
DeepPavlov B-Technology
Agent I-Technology
1.0 I-Technology
— O
платформы O
для O
создания O
многозадачных O
чат O
- O
ботов O
. O

# text =   Агент помогает разработчикам производственных чатботов организовать несколько NLP моделей в одном конвейере.
Агент O
помогает O
разработчикам O
производственных O
чатботов O
организовать B-Task
несколько I-Task
NLP I-Task
моделей I-Task
в O
одном O
конвейере O
. O

# text =   Чтобы упростить работу с предобученными NLP моделями из DeepPavlov, в сентябрь 2019 года был запущен SaaS сервис.
Чтобы O
упростить O
работу O
с O
предобученными O
NLP O
моделями O
из O
DeepPavlov B-Model
, O
в O
сентябрь O
2019 B-Date_Application
года I-Date_Application
был O
запущен O
SaaS B-Technology
сервис I-Technology
. O

# text =   DeepPavlov Cloud позволяет анализировать текст, а также хранить документы в облачном хранилище.
DeepPavlov B-Technology
Cloud I-Technology
позволяет O
анализировать B-Task
текст I-Task
, O
а O
также O
хранить B-Task
документы I-Task
в O
облачном O
хранилище O
. O

# text =   Оценка состояния диалога (DST — Dialogue State Traking) является основным компонентом в таких диалоговых системах.
Оценка B-Task
состояния I-Task
диалога I-Task
( O
DST B-Task
— O
Dialogue B-Task
State I-Task
Traking I-Task
) O
является O
основным O
компонентом O
в O
таких O
диалоговых O
системах O
. O

# text =   DST отвечает за перевод высказываний на человеческом языке в семантическое представление языка, в частности, за извлечение намерений (intets) и пар слот-значение (slot, value), соответствующих цели пользователя.
DST B-Task
отвечает O
за O
перевод B-Task
высказываний I-Task
на O
человеческом O
языке O
в O
семантическое O
представление O
языка O
, O
в O
частности O
, O
за O
извлечение O
намерений O
( O
intets O
) O
и O
пар O
слот O
- O
значение O
( O
slot O
, O
value O
) O
, O
соответствующих O
цели O
пользователя O
. O

# text =   В ходе участия команды в DSTC8 была разработана модель GOLOMB (GOaL-Oriented Multi-task BERT-based dialogue state tracker) — целеориентированная мультизадачная модель на базе BERT для отслеживания состояния диалога.
В O
ходе O
участия O
команды O
в O
DSTC8 O
была O
разработана O
модель O
GOLOMB B-ShortName_Model
( O
GOaL B-Model
- I-Model
Oriented I-Model
Multi I-Model
- I-Model
task I-Model
BERT I-Model
- I-Model
based I-Model
dialogue I-Model
state I-Model
tracker I-Model
) O
— O
целеориентированная O
мультизадачная O
модель O
на O
базе O
BERT B-ShortName_Model
для O
отслеживания B-Task
состояния I-Task
диалога I-Task
. O

# text =   Для предсказания состояния диалога модель решает несколько классификационных задач и задачу поиска подстроки.
Для O
предсказания O
состояния O
диалога O
модель O
решает O
несколько O
классификационных B-Task
задач I-Task
и O
задачу B-Task
поиска I-Task
подстроки I-Task
. O

# text =   В скором времени данная модель появится библиотеке DeepPavlov.
В O
скором O
времени O
данная O
модель O
появится O
библиотеке O
DeepPavlov B-Model
. O

# text =   Как было сказано ранее, DeepPavlov поставляется с несколькими предобученными компонентами, работающими на TensorFlow и Keras.
Как O
было O
сказано O
ранее O
, O
DeepPavlov B-Model
поставляется O
с O
несколькими O
предобученными O
компонентами O
, O
работающими O
на O
TensorFlow B-Application
и O
Keras B-Application
. O

# text =   На основании триггера на определенные ключевые слова она сможет определять, к примеру, признаки обмана, мошенничества.
На O
основании O
триггера O
на O
определенные O
ключевые B-Subject
слова I-Subject
она O
сможет O
определять O
, O
к O
примеру O
, O
признаки O
обмана O
, O
мошенничества O
. O

# text =   То есть, сформировав некоторый корпус слов-триггеров, вполне возможно классифицировать сайты по их текстовому содержанию.
То O
есть O
, O
сформировав O
некоторый O
корпус B-InfoResource
слов I-InfoResource
- I-InfoResource
триггеров I-InfoResource
, O
вполне O
возможно O
классифицировать O
сайты O
по O
их O
текстовому O
содержанию O
. O

# text =   Задача распознавания текста относится к сфере обработки естественного языка или NLP (natural language processing).
Задача B-Task
распознавания I-Task
текста I-Task
относится O
к O
сфере O
обработки B-Science
естественного I-Science
языка I-Science
или O
NLP B-ShortName_Science
( O
natural B-Science
language I-Science
processing I-Science
) O
. O

# text =   NLP — направление искусственного интеллекта, нацеленное на обработку и анализ данных на естественном языке и обучение машин взаимодействию с людьми [1].
NLP B-Science
— O
направление O
искусственного B-Science
интеллекта I-Science
, O
нацеленное O
на O
обработку O
и O
анализ B-Method
данных I-Method
на O
естественном O
языке O
и O
обучение O
машин O
взаимодействию O
с O
людьми O
[ O
1 O
] O
. O

# text =   Такой подход называется методом вложения слов (word embedding).
Такой O
подход O
называется O
методом B-Method
вложения I-Method
слов I-Method
( O
word B-Subject
embedding I-Subject
) O
. O

# text =   Используя данные, состоящие из таких векторов, мы можем применять различные методы Machine Learning.
Используя O
данные O
, O
состоящие O
из O
таких O
векторов O
, O
мы O
можем O
применять O
различные O
методы O
Machine B-Science
Learning I-Science
. O

# text =   И поскольку искусственные нейронные сети лучшим образом справляются с векторно-матричными вычислениями, то выбор в их пользу становиться очевидным.
И O
поскольку O
искусственные B-Method
нейронные I-Method
сети I-Method
лучшим O
образом O
справляются O
с O
векторно B-Method
- I-Method
матричными I-Method
вычислениями I-Method
, O
то O
выбор O
в O
их O
пользу O
становиться O
очевидным O
. O

# text =   Искусственная нейронная сеть — это математическая модель, а также ее программное или аппаратное воплощение, построенные по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.
Искусственная B-Method
нейронная I-Method
сеть I-Method
— O
это O
математическая B-Model
модель I-Model
, O
а O
также O
ее O
программное O
или O
аппаратное O
воплощение O
, O
построенные O
по O
принципу O
организации O
и O
функционирования O
биологических O
нейронных O
сетей O
— O
сетей O
нервных O
клеток O
живого O
организма O
. O

# text =   Современные модели представляют собой так называемые глубокие модели.
Современные O
модели O
представляют O
собой O
так O
называемые O
глубокие B-Model
модели I-Model
. O

# text =   И в ее решении наилучшие метрики точности были достигнуты рекуррентными нейронными сетями, LSTM (сети с долгой краткосрочной памятью).
И O
в O
ее O
решении O
наилучшие O
метрики O
точности B-Metric
были O
достигнуты O
рекуррентными B-Model
нейронными I-Model
сетями I-Model
, O
LSTM B-ShortName_Model
( O
сети B-Model
с I-Model
долгой I-Model
краткосрочной I-Model
памятью I-Model
) O
. O

# text =   Позже свое превосходство в этой нише обрели NLP-модели-трансформеры.
Позже O
свое O
превосходство O
в O
этой O
нише O
обрели O
NLP B-Model
- I-Model
модели B-Model
- I-Model
трансформеры I-Model
. O

# text =  Описание упомянутых рекуррентных нейросетей (RNN), LSTM и GRU выходит за рамки темы статьи.
Описание O
упомянутых O
рекуррентных B-Model
нейросетей I-Model
( O
RNN B-ShortName_Model
) O
, O
LSTM B-ShortName_Model
и O
GRU B-ShortName_Model
выходит O
за O
рамки O
темы O
статьи O
. O

# text =   Однако RNN способны фиксировать зависимости только в одном направлении языка.
Однако O
RNN B-ShortName_Model
способны O
фиксировать B-Task
зависимости I-Task
только O
в O
одном O
направлении O
языка O
. O

# text =   Кроме этого, RNN не очень хороши в захвате долгосрочных зависимостей.
Кроме O
этого O
, O
RNN B-ShortName_Model
не O
очень O
хороши O
в O
захвате B-Task
долгосрочных I-Task
зависимостей I-Task
. O

# text =  LSTM избегают проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.
LSTM B-ShortName_Model
избегают O
проблемы O
долговременной O
зависимости O
, O
запоминая O
значения O
как O
на O
короткие O
, O
так O
и O
на O
длинные O
промежутки O
времени O
. O

# text =   Это объясняется тем, что LSTM не использует функцию активации внутри своих рекуррентных компонентов.
Это O
объясняется O
тем O
, O
что O
LSTM B-ShortName_Model
не O
использует O
функцию B-Method
активации I-Method
внутри O
своих O
рекуррентных O
компонентов O
. O

# text =   LSTM часто используются в машинном переводе и в задачах генерирования текстов на естественном языке.
LSTM B-ShortName_Model
часто O
используются O
в O
машинном B-Science
переводе I-Science
и O
в O
задачах O
генерирования B-Task
текстов I-Task
на O
естественном O
языке O
. O

# text =   Прежде чем использовать такой мощный и в то же время сложный инструмент, наша команда протестировала и более простые NLP-методы машинного обучения, в том числе «наивный байесовский классификатор», алгоритмы, использующие bag-of-words («мешок слов» — метод представления слов) и tf-idf (метрика определения частоты вхождения слов), а также простейшие модели нейронных сетей, состоящие из небольшого количества скрытых слоев.
Прежде O
чем O
использовать O
такой O
мощный O
и O
в O
то O
же O
время O
сложный O
инструмент O
, O
наша O
команда O
протестировала O
и O
более O
простые O
NLP O
- O
методы O
машинного O
обучения O
, O
в O
том O
числе O
« O
наивный B-Method
байесовский I-Method
классификатор I-Method
» O
, O
алгоритмы O
, O
использующие O
bag B-Method
- I-Method
of I-Method
- I-Method
words I-Method
( O
« O
мешок B-Method
слов I-Method
» O
— O
метод B-Method
представления I-Method
слов I-Method
) O
и O
tf B-Metric
- I-Metric
idf B-Metric
( O
метрика B-TERM
определения B-Subject
частоты I-TERM
вхождения I-TERM
слов O
) O
, O
а O
также O
простейшие O
модели O
нейронных O
сетей O
, O
состоящие O
из O
небольшого O
количества O
скрытых O
слоев O
. O

# text =   BERT, или Bidirectional Encoder Representations from Transformers, — нейросетевая модель-трансформер от Google, на которой сегодня строится большинство инструментов автоматической обработки языка.
BERT B-ShortName_Model
, O
или O
Bidirectional B-TEModelRM
Encoder I-Model
Representations I-Model
from I-Model
Transformers I-Model
, O
— O
нейросетевая O
модель O
- O
трансформер O
от O
Google B-Organization
, O
на O
которой O
сегодня O
строится O
большинство O
инструментов O
автоматической O
обработки O
языка O
. O

# text =  Релиз BERT в 2018 году стал некоторой переломной точкой в развитии NLP-моделей.
Релиз O
BERT B-ShortName_Model
в O
2018 B-Date
году I-Date
стал O
некоторой O
переломной O
точкой O
в O
развитии O
NLP B-Model
- I-Model
моделей I-Model
. O

# text =   Его появлению предшествовал ряд недавних разработок в области обработки естественного языка (BERT, ELMO и Ко в картинках — как в NLP пришло трансферное обучение): Semi-supervised Sequence learning (Andrew Dai и Quoc Le), ELMo (Matthew Peters и исследователи из AI2 и UW CSE), ULMFiT (Jeremy Howard и Sebastian Ruder), OpenAI Transformer (исследователи OpenAI Radford, Narasimhan, Salimans, и Sutskever) и Трансформер (Vaswani et al).
Его O
появлению O
предшествовал O
ряд O
недавних O
разработок O
в O
области O
обработки O
естественного O
языка O
( O
BERT B-ShortName_Model
, O
ELMO B-ShortName_Model
и O
Ко B-ShortName_Model
в O
картинках O
— O
как O
в O
NLP B-ShortName_Science
пришло O
трансферное B-Method
обучение I-Method
): O
Semi B-Method
- I-Method
supervised I-Method
Sequence I-Method
learning I-Method
( O
Andrew B-Person
Dai I-Person
и O
Quoc B-Person
Le I-Person
) O
, O
ELMo B-Model
( O
Matthew B-Person
Peters I-Person
и O
исследователи O
из O
AI2 B-Organization
и O
UW B-Organization
CSE B-Organization
) O
, O
ULMFiT B-Model
( O
Jeremy B-Person
Howard I-Person
и O
Sebastian B-Person
Ruder I-Person
) O
, O
OpenAI B-TERM
Transformer I-TERM
( O
исследователи O
OpenAI B-Organization
Radford I-Organization
, O
Narasimhan B-Person
, O
Salimans B-Person
, O
и O
Sutskever B-Person
) O
и O
Трансформер B-Model
( O
Vaswani B-Person
et O
al O
)O 
. O

# text =  Трансформеры в машинном обучении — это семейство архитектур нейронных сетей, общая идея которых основана на так называемом «самовнимании» (self-attention).
Трансформеры B-Model
в O
машинном B-Science
обучении I-Science
— O
это O
семейство O
архитектур O
нейронных O
сетей O
, O
общая O
идея O
которых O
основана O
на O
так O
называемом O
« O
самовнимании B-Method
» O
( O
self B-Method
- I-Method
attention I-Method
) O
. O

# text =   Однако алгоритм Self-attention не сразу поймет смысл предложения.
Однако O
алгоритм O
Self B-Method
- I-Method
attention I-Method
не O
сразу O
поймет O
смысл O
предложения O
. O

# text =   Потом результаты сетей объединяется.По своей сути BERT — это обученный стек энкодеров Трансформера.
Потом O
результаты O
сетей O
объединяется O
. O
По O
своей O
сути O
BERT B-ShortName_Model
— O
это O
обученный O
стек O
энкодеров B-Method
Трансформера I-Method
. O

# text =   Разработкой и обучением модели BERT занималась целая группа исследователей Google AI Language на многомиллионном наборе слов на разных языках (более 100).
Разработкой O
и O
обучением O
модели O
BERT B-ShortName_Model
занималась O
целая O
группа O
исследователей O
Google B-Organization
AI I-Organization
Language I-Organization
на O
многомиллионном O
наборе O
слов O
на O
разных O
языках O
( O
более O
100 O
) O
. O

# text =   И мы дообучили BERT распознавать текстовое содержимое сайтов по 63 категориям (медицина, здоровье, видео, интернет-магазины, юмористические сайты, эротика, оружие, секты, криминал и пр.).
И O
мы O
дообучили O
BERT B-ShortName_Model
распознавать O
текстовое O
содержимое O
сайтов O
по O
63 O
категориям O
( O
медицина B-Science
, O
здоровье O
, O
видео O
, O
интернет O
- O
магазины O
, O
юмористические O
сайты O
, O
эротика O
, O
оружие O
, O
секты O
, O
криминал O
) O
. O

# text =   Smart-Cat на первом этапе проводит их предобработку.
Smart B-App_system
- I-App_system
Cat I-App_system
на O
первом O
этапе O
проводит O
их O
предобработку O
. O

# text =   Для удобства работы с категоризатором Smart-Cat мы создали специальный Telegram-бот.
Для O
удобства O
работы O
с O
категоризатором O
Smart B-App_system
- I-App_system
Cat I-App_system
мы O
создали O
специальный O
Telegram B-Technology
- I-Technology
бот I-Technology
. O

# text =   После своей работы BERT-bot отправит CSV-таблицу.
После O
своей O
работы O
BERT B-Technology
- I-Technology
bot I-Technology
отправит O
CSV O
- O
таблицу O
. O

# text =   Но, как я уже сказал ранее, такие модели могут применяться не только в задачах классификации текста.
Но O
, O
как O
я O
уже O
сказал O
ранее O
, O
такие O
модели O
могут O
применяться O
не O
только O
в O
задачах O
классификации B-Task
текста I-Task
. O

# text =   В современном мире всё большую популярность приобретает методика под названием customer development для тестирования идей и гипотез о будущем продукте.
В O
современном O
мире O
всё O
большую O
популярность O
приобретает O
методика O
под O
названием O
customer B-Method
development I-Method
для O
тестирования O
идей O
и O
гипотез O
о O
будущем O
продукте O
. O

# text =   В данном решении была использована готовая нейросеть от сервиса RusVectores, обученная на корпусе НКРЯ с использованием алгоритма word2vec CBOW с длиной вектора 300.
В O
данном O
решении O
была O
использована O
готовая O
нейросеть O
от O
сервиса O
RusVectores B-Technology
, O
обученная O
на O
корпусе O
НКРЯ B-ShortName
с O
использованием O
алгоритма O
word2vec B-Model
CBOW B-Method
с O
длиной O
вектора O
300.

# text =  НКРЯ – это совокупность русскоязычных текстов, Национальный Корпус Русского Языка в полном объёме.
НКРЯ B-Abbrev_InfoResource
– O
это O
совокупность O
русскоязычных O
текстов O
, O
Национальный B-Result
Корпус I-Result
Русского I-Result
Языка I-Result
в O
полном O
объёме O
. O

# text =  Word2vec CBOW — алгоритм, благодаря которому слово на естественном языке представляется в виде числового вектора.
word2vec B-Model
CBOW B-Method
— O
алгоритм O
, O
благодаря O
которому O
слово O
на O
естественном O
языке O
представляется O
в O
виде O
числового O
вектора O
. O

# text =   CBOW – это аббревиатура Continuous Bag of Words.
CBOW B-ShortName_Method
– O
это O
аббревиатура O
Continuous B-Method
Bag I-Method
of I-Method
Words I-Method
. O

# text =  Она обозначает алгоритм, который есть в word2vec.
Она O
обозначает O
алгоритм O
, O
который O
есть O
в O
word2vec B-Model
. O

# text =   Данный алгоритм называют моделью «мешка слов», он предсказывает слово по контексту.
Данный O
алгоритм O
называют O
моделью O
« O
мешка B-Method
слов I-Method
» O
, O
он O
предсказывает B-Task
слово I-Task
по I-Task
контексту I-Task
. O

# text =   Ещё один алгоритм в word2vec - Skip-gram предсказывает контекст по слову.
Ещё O
один O
алгоритм O
в O
word2vec B-Model
- O
Skip B-Method
- I-Method
gram I-Method
предсказывает O
контекст O
по O
слову O
. O

# text =  С помощью данных алгоритмов генерируют близкие по смыслу слова при запросе в поисковой системе, сравнивают документы по смыслу, определяют смысловую близость слов и предложений.
С O
помощью O
данных O
алгоритмов O
генерируют O
близкие O
по O
смыслу O
слова O
при O
запросе O
в O
поисковой O
системе O
, O
сравнивают B-Task
документы I-Task
по I-Task
смыслу I-Task
, O
определяют B-Task
смысловую I-Task
близость I-Task
слов I-Task
и O
предложений O
. O

# text =  Более подробно о word2vec можно почитать в статье "Немного про word2vec: полезная теория".
Более O
подробно O
о O
word2vec B-Application
можно O
почитать O
в O
статье O
" O
Немного O
про O
word2vec O
: O
полезная O
теория" O
. O

# text =  О векторном представлении слов (эмбеддинге) хорошо и с примерами описано в статье "Что такое эмбеддинги и как они помогают машинам понимать тексты".
О O
векторном B-Object
представлении I-Object
слов I-Object
( O
эмбеддинге B-Object
) O
хорошо O
и O
с O
примерами O
описано O
в O
статье O
" O
Что O
такое O
эмбеддинги O
и O
как O
они O
помогают O
машинам O
понимать O
тексты" O
. O

# text =   Т.к. у меня таких мощностей нет, я воспользовался доступным онлайн сервисом RusVectores.
Т.к O
. O
у O
меня O
таких O
мощностей O
нет O
, O
я O
воспользовался O
доступным O
онлайн O
сервисом O
RusVectores B-Model
. O

# text =   Эти модели всегда ищут синонимы — даже для устоявшихся словосочетаний.
Эти O
модели O
всегда O
ищут B-Task
синонимы I-Task
— O
даже O
для O
устоявшихся O
словосочетаний B-Subject
. O

# text =   Перед переходом к самим метрикам необходимо ввести важную концепцию для описания этих метрик в терминах ошибок классификации — confusion matrix (матрица ошибок).
Перед O
переходом O
к O
самим O
метрикам O
необходимо O
ввести O
важную O
концепцию O
для O
описания O
этих O
метрик O
в O
терминах B-Subject
ошибок O
классификации O
— O
confusion B-TERM
matrix I-TERM
( O
матрица B-TERM
ошибок I-TERM
) O
. O

# text =   Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма.
Интуитивно O
понятной O
, O
очевидной O
и O
почти O
неиспользуемой O
метрикой O
является O
accuracy B-TERM
— O
доля O
правильных O
ответов O
алгоритма O
. O

# text =   Для оценки качества работы алгоритма на каждом из классов по отдельности введем метрики precision (точность) и recall (полнота).
Для O
оценки O
качества O
работы O
алгоритма O
на O
каждом O
из O
классов B-Object
по O
отдельности O
введем O
метрики O
precision B-Metric
( O
точность B-Metric
) O
и O
recall B-Metric
( O
полнота B-Metric
) O
. O

# text =   Precision можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными, а recall показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.
Precision B-Metric
можно O
интерпретировать O
как O
долю O
объектов O
, O
названных O
классификатором O
положительными O
и O
при O
этом O
действительно O
являющимися O
положительными O
, O
а O
recall B-Metric
показывает O
, O
какую O
долю O
объектов O
положительного O
класса O
из O
всех O
объектов O
положительного O
класса O
нашел O
алгоритм O
. O

# text =   F-мера достигает максимума при полноте и точности, равными единице, и близка к нулю, если один из аргументов близок к нулю.
F B-Metric
- I-Metric
мера I-Metric
достигает O
максимума O
при O
полноте B-Metric
и O
точности B-Metric
, O
равными O
единице O
, O
и O
близка O
к O
нулю O
, O
если O
один O
из O
аргументов B-Object
близок O
к O
нулю O
. O

# text =   Одним из способов оценить модель в целом, не привязываясь к конкретному порогу, является AUC-ROC (или ROC AUC) — площадь (Area Under Curve) под кривой ошибок (Receiver Operating Characteristic curve ).
Одним O
из O
способов O
оценить O
модель B-Object
в O
целом O
, O
не O
привязываясь O
к O
конкретному O
порогу O
, O
является O
AUC B-TERM
- I-TERM
ROC I-TERM
( O
или O
ROC B-TERM
AUC I-TERM
) O
— O
площадь B-TERM
( O
Area B-TERM
Under I-TERM
Curve I-TERM
) O
под O
кривой O
ошибок O
( O
Receiver B-TERM
Operating I-TERM
Characteristic I-TERM
curve I-TERM
) O
. O

# text =   Интуитивно можно представить минимизацию logloss как задачу максимизации accuracy путем штрафа за неверные предсказания.
Интуитивно O
можно O
представить O
минимизацию O
logloss B-TERM
как O
задачу B-TERM
максимизации I-TERM
accuracy I-TERM
путем O
штрафа O
за O
неверные O
предсказания O
. O

# text =   Вчера OpenAI выпустили Whisper.
Вчера O
OpenAI B-Organization
выпустили O
Whisper B-Application
. O

# text =   По сути авторы попытались: Исключить транскрипты других систем ASR из датасета; Привести пунктуации к некому стандарту.
По O
сути O
авторы O
попытались O
: O
Исключить O
транскрипты O
других O
систем O
ASR B-Abberv_Application
из O
датасета; O
Привести O
пунктуации O
к O
некому O
стандарту O
. O

# text =   Серьезной нормализации или денормализации текста не делалось.
Серьезной O
нормализации B-Method
или O
денормализации B-Method
текста O
не O
делалось O
. O

# text =   Под капотом же seq2seq модель, глядишь сама всё и так выучит.
Под O
капотом O
же O
seq2seq B-Model
модель O
, O
глядишь O
сама O
всё O
и O
так O
выучит O
. O

# text =   Ведь исходя из названий FAIR, OpenAI и прочие же FOSS - альтруисты, борющиеся за наше будущее, они же выложили код для тренировки (а повторить смогут лишь GAFA компании) и все датасеты, не так ли?
Ведь O
исходя O
из O
названий O
FAIR B-Organization
, O
OpenAI B-Organization
и O
прочие O
же O
FOSS B-Organization
- O
альтруисты O
, O
борющиеся O
за O
наше O
будущее O
, O
они O
же O
выложили O
код O
для O
тренировки O
( O
а O
повторить O
смогут O
лишь O
GAFA B-Organization
компании O
) O
и O
все O
датасеты O
, O
не O
так O
ли O
? O

# text =   На практике OpenAI уже давно не Open, а недавняя история с DALLE-2 / Midjourney / Stable Diffusion скорее иллюстрируют тренд.
На O
практике O
OpenAI B-Organization
уже O
давно O
не O
Open O
, O
а O
недавняя O
история O
с O
DALLE-2 B-Model
/ O
Midjourney B-Model
/ O
Stable B-Model
Diffusion I-Model
скорее O
иллюстрируют O
тренд O
. O

# text =   Наверное стоит только сказать, что это sequence-to-sequence encoder-decoder трансформерная модель, без особого снижения длины инпута с довольном стандартным окном в 25 миллисекунд и шагом в 10 миллисекунд, работающая на аудио в 16 килогерц.
Наверное O
стоит O
только O
сказать O
, O
что O
это O
sequence B-Model
- I-Model
to I-Model
- I-Model
sequence I-Model
encoder I-Model
- I-Model
decoder I-Model
трансформерная O
модель O
, O
без O
особого O
снижения O
длины O
инпута O
с O
довольном O
стандартным O
окном O
в O
25 O
миллисекунд O
и O
шагом O
в O
10 O
миллисекунд O
, O
работающая O
на O
аудио O
в O
16 O
килогерц O
. O

# text =   Решать конечно вам для вашего конкретного приложения, но если сравнивать только саму модель распознавания, а не весь обвес в виде сервиса (понятно, что тут VAD и детектор языка запихали тоже в модель), например с древними бенчмарками из silero-models, то самые маленькие модели на CPU в расчете на 1 ядро (1 ядро = 2 потока) отличаются по скорости … на два порядка.
Решать O
конечно O
вам O
для O
вашего O
конкретного O
приложения O
, O
но O
если O
сравнивать O
только O
саму O
модель O
распознавания O
, O
а O
не O
весь O
обвес O
в O
виде O
сервиса O
( O
понятно O
, O
что O
тут O
VAD B-ShortName_Method
и O
детектор O
языка O
запихали O
тоже O
в O
модель O
) O
, O
например O
с O
древними O
бенчмарками O
из O
silero B-Method
- I-Method
models I-Method
, O
то O
самые O
маленькие O
модели O
на O
CPU O
в O
расчете O
на O
1 O
ядро O
( O
1 O
ядро O
= O
2 O
потока O
) O
отличаются O
по O
скорости O
… O
на O
два O
порядка O
. O

# text =   Для наших моделей из прошлого релиза, многие из этих датасетов тоже как бы "zero-shot" (то есть у нас нет соответствующего большого тренировочного датасета).
Для O
наших O
моделей O
из O
прошлого O
релиза O
, O
многие O
из O
этих O
датасетов O
тоже O
как O
бы O
" O
zero B-Method
- I-Method
shot I-Method
" O
( O
то O
есть O
у O
нас O
нет O
соответствующего O
большого O
тренировочного O
датасета O
) O
. O

# text =   В течение четырех лет вышло несколько версий модели, способных транскрибировать лекции, телефонные разговоры, телевизионные программы, радиошоу и другие прямые трансляции с «человеческой точностью».
В O
течение O
четырех O
лет O
вышло O
несколько O
версий O
модели O
, O
способных O
транскрибировать B-Task
лекции I-Task
, O
телефонные O
разговоры O
, O
телевизионные O
программы O
, O
радиошоу O
и O
другие O
прямые O
трансляции O
с O
« O
человеческой O
точностью O
» O
. O

# text =   Модель DeepSpeech представляет собой сквозную обучаемую архитектуру на уровне символов, которая может транскрибировать аудио на различных языках.
Модель O
DeepSpeech B-Model
представляет O
собой O
сквозную O
обучаемую O
архитектуру O
на O
уровне O
символов O
, O
которая O
может O
транскрибировать B-Task
аудио I-Task
на O
различных O
языках O
. O

# text =   Вдохновленная этими усилиями по сбору данных, исследовательская группа Mozilla в сотрудничестве с группой открытых инноваций запустила проект Common Voice, цель которого заключалась в сборе и проверке речевых данных.
Вдохновленная O
этими O
усилиями O
по O
сбору O
данных O
, O
исследовательская O
группа O
Mozilla B-Organization
в O
сотрудничестве O
с O
группой O
открытых O
инноваций O
запустила O
проект O
Common B-Project
Voice I-Project
, O
цель B-Object
которого O
заключалась O
в O
сборе O
и O
проверке B-Task
речевых I-Task
данных I-Task
. O

# text =   Common Voice включает не только речевые записи, но и из добровольно предоставленные метаданные, такие как возраст, пол и акцент говорящего.
Common B-Project
Voice I-Project
включает O
не O
только O
речевые B-Object
записи I-Object
, O
но O
и O
из O
добровольно O
предоставленные O
метаданные B-Object
, O
такие O
как O
возраст O
, O
пол O
и O
акцент B-Subject
говорящего O
. O

# text =   Сегодня Common Voice является одним из крупнейших в мире мультиязычных корпусов, являющихся общественным достоянием, с более чем 9 тысячами часов голосовых данных на 60 различных языках, включая такие редкие языки, как валлийский и киньяруанда.
Сегодня O
Common B-Project
Voice I-Project
является O
одним O
из O
крупнейших O
в O
мире O
мультиязычных B-Object
корпусов I-Object
, O
являющихся O
общественным O
достоянием O
, O
с O
более O
чем O
9 O
тысячами O
часов O
голосовых O
данных O
на O
60 O
различных O
языках O
, O
включая O
такие O
редкие O
языки O
, O
как O
валлийский B-Lang
и O
киньяруанда B-Lang
. O

# text =  В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3.
В O
этой O
статье O
мы O
научим O
вас O
генерировать B-Task
текст I-Task
с O
помощью O
предварительно O
обученного O
GPT-2 B-Model
— O
более O
легкого O
предшественника O
GPT-3 B-Model
. O

# text =   Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface.
Мы O
будем O
использовать O
именитую O
библиотеку O
Transformers B-Library
, O
разработанную O
Huggingface B-Organization
. O

# text =   Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.
Модель B-Object
по O
умолчанию O
для O
конвейера O
генерации B-Task
текста I-Task
— O
GPT-2 B-Model
, O
самая O
популярная O
модель O
декодирующего O
трансформера B-Model
для O
генерации B-Task
языка I-Task
. O

# text =   Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.
Эта O
модель O
GPT2 B-Model
от O
CKIPLab B-Organization
предварительно O
обучена O
на O
китайском B-Lang
корпусе B-Object
, O
поэтому O
мы O
можем O
использовать O
их O
модель O
без O
необходимости O
заниматься O
настройкой O
самостоятельно O
. O

# text =   Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком.
Однажды O
нам O
понадобилось O
выбрать O
синтаксический B-Application
парсер I-Application
для O
работы O
с O
русским O
языком O
. O

# text =   Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение.
Для O
этого O
мы O
углубились O
в O
дебри O
морфологии B-Science
и O
токенизации B-Method
, O
протестировали O
разные O
варианты O
и O
оценили O
их O
применение O
. O

# text =   В первой строке предложение разобрано в рамках грамматики зависимостей.
В O
первой O
строке O
предложение O
разобрано O
в O
рамках O
грамматики B-Method
зависимостей I-Method
. O

# text =   Во второй строке разбор идет в соответствии с грамматикой непосредственно составляющих.
Во O
второй O
строке O
разбор O
идет O
в O
соответствии O
с O
грамматикой B-Method
непосредственно I-Method
составляющих I-Method
. O

# text =   Поэтому в автоматическом парсинге русского языка принято работать исходя из грамматики зависимостей.
Поэтому O
в O
автоматическом B-Task
парсинге I-Task
русского B-Lang
языка O
принято O
работать O
исходя O
из O
грамматики B-Method
зависимостей I-Method
. O

# text =   Чтобы облегчить себе выбор парсера, мы обратили свой взгляд на проект Universal Dependencies и недавно прошедшее в его рамках соревнование CoNLL Shared Task.
Чтобы O
облегчить O
себе O
выбор O
парсера O
, O
мы O
обратили O
свой O
взгляд O
на O
проект O
Universal B-Project
Dependencies I-Project
и O
недавно O
прошедшее O
в O
его O
рамках O
соревнование O
CoNLL O
Shared O
Task O
. O

# text =   Universal Dependencies — это проект по унификации разметки синтаксических корпусов (трибанков) в рамках грамматики зависимостей.
Universal B-Project
Dependencies I-Project
— O
это O
проект O
по O
унификации B-Task
разметки I-Task
синтаксических B-Object
корпусов I-Object
( O
трибанков B-Object
) O
в O
рамках O
грамматики B-Method
зависимостей I-Method
. O

# text =   Мы можем оценивать, правильно ли нашли вершину слова — метрика UAS (Unlabeled attachment score).
Мы O
можем O
оценивать O
, O
правильно O
ли O
нашли O
вершину O
слова O
— O
метрика O
UAS B-ShortName
( O
Unlabeled B-Metric
attachment I-Metric
score I-Metric
) O
. O

# text =   Или оценивать, правильно ли найдена как вершина, так и тип зависимости — метрика LAS (Labeled attachment score).
Или O
оценивать O
, O
правильно O
ли O
найдена O
как O
вершина O
, O
так O
и O
тип O
зависимости O
— O
метрика O
LAS B-ShortName_Metric
( O
Labeled B-Metric
attachment I-Metric
score I-Metric
) O
. O

# text =   Казалось бы, здесь напрашивается оценка точности (accuracy) — считаем, сколько раз мы попали из общего количества случаев.
Казалось O
бы O
, O
здесь O
напрашивается O
оценка O
точности B-Metric
( O
accuracy B-Metric
) O
— O
считаем O
, O
сколько O
раз O
мы O
попали O
из O
общего O
количества O
случаев O
. O

# text =   Разработчики, решающие задачи автоматического парсинга, часто берут на вход сырой текст, который в соответствии с пирамидой анализа проходит этапы токенизации и морфологического анализа.
Разработчики O
, O
решающие O
задачи O
автоматического B-Task
парсинга I-Task
, O
часто O
берут O
на O
вход O
сырой O
текст O
, O
который O
в O
соответствии O
с O
пирамидой O
анализа O
проходит O
этапы O
токенизации B-Method
и O
морфологического B-Method
анализа I-Method
. O

# text =   Поэтому формулой оценки в данном случае является ф-мера, где точность (precision) — доля точных попаданий относительно общего числа предсказаний, а полнота — доля точных попаданий относительно числа связей в размеченных данных.
Поэтому O
формулой O
оценки O
в O
данном O
случае O
является O
ф B-Metric
- I-Metric
мера I-Metric
, O
где O
точность B-Metric
( O
precision B-Metric
) O
— O
доля O
точных O
попаданий O
относительно O
общего O
числа O
предсказаний O
, O
а O
полнота B-Metric
— O
доля O
точных O
попаданий O
относительно O
числа O
связей O
в O
размеченных O
данных O
. O

# text =   Очевидно, что все эксперименты проводятся на SynTagRus (разработка ИППИ РАН), в котором более миллиона токенов.
Очевидно O
, O
что O
все O
эксперименты O
проводятся O
на O
SynTagRus B-Dataset
( O
разработка O
ИППИ B-Organization
РАН I-Organization
) O
, O
в O
котором O
более O
миллиона O
токенов B-Subject
. O

# text =   По итогам соревнования прошлого года модели, которые обучались на одном и том же SynTagRus, достигли следующих показателей LAS:
По O
итогам O
соревнования O
прошлого O
года O
модели O
, O
которые O
обучались O
на O
одном O
и O
том O
же O
SynTagRus B-Dataset
, O
достигли O
следующих O
показателей O
LAS B-ShortName_Metric
: O

# text =   Забегая вперед, заметим, что новая версия UDPipe (Future) оказалась еще выше в этом году.
Забегая O
вперед O
, O
заметим O
, O
что O
новая O
версия O
UDPipe B-Application
( O
Future B-Organization
) O
оказалась O
еще O
выше O
в O
этом O
году O
. O

# text =   В список не вошел Syntaxnet — парсер Google.
В O
список O
не O
вошел O
Syntaxnet B-Application
— O
парсер O
Google B-Organization
. O

# text =   Ответ прост: Syntaxnet начинался лишь с этапа морфологического анализа.
Ответ O
прост O
: O
Syntaxnet B-Application
начинался O
лишь O
с O
этапа O
морфологического B-Method
анализа I-Method
. O

# text =   В качестве начальных данных у нас есть табличка выше с лидирующим Syntaxnet и с UDPipe 2.0 где-то на 7 месте.
В O
качестве O
начальных O
данных O
у O
нас O
есть O
табличка O
выше O
с O
лидирующим O
Syntaxnet B-Application
и O
с O
UDPipe B-Application
2.0 I-Application
где O
- O
то O
на O
7 O
месте O
. O

# text =   Синтаксис, разумеется, далеко не единственный модуль «под капотом» real-time системы, поэтому тратить на него больше десятка миллисекунд не стоит.
Синтаксис B-Science
, O
разумеется O
, O
далеко O
не O
единственный O
модуль O
« O
под O
капотом O
» O
real B-Application
- I-Application
time I-Application
системы O
, O
поэтому O
тратить O
на O
него O
больше O
десятка O
миллисекунд O
не O
стоит O
. O

# text =   Для русского языка у нас есть достаточно хорошие морфологические анализаторы, которые могут встроиться в нашу пирамиду.
Для O
русского B-Lang
языка O
у O
нас O
есть O
достаточно O
хорошие O
морфологические B-Subject
анализаторы I-Subject
, O
которые O
могут O
встроиться O
в O
нашу O
пирамиду O
. O

# text =   Затем начинает работу теггер — штука, которая предсказывает морфологические свойства токена: в каком падеже слово стоит, в каком числе.
Затем O
начинает O
работу O
теггер B-Subject
— O
штука O
, O
которая O
предсказывает O
морфологические B-Subject
свойства I-Subject
токена I-Subject
: O
в O
каком O
падеже O
слово O
стоит O
, O
в O
каком O
числе O
. O

# text =   В UDPipe есть еще лемматизатор, который подбирает для слов начальную форму.
В O
UDPipe B-Application
есть O
еще O
лемматизатор B-Application
, O
который O
подбирает O
для O
слов O
начальную B-Subject
форму I-Subject
. O

# text =   UDPipe — это transition-based архитектура: она работает быстро, за линейное время проходя по всем токенам один раз.
UDPipe B-App_system
— O
это O
transition B-Model
- I-Model
based I-Model
архитектура O
: O
она O
работает O
быстро O
, O
за O
линейное O
время O
проходя O
по O
всем O
токенам B-Subject
один O
раз O
. O

# text =   RightArc — то же самое, но зависимость строится в другую сторону, и отбрасывается верхушка.
RightArc B-Model
— O
то O
же O
самое O
, O
но O
зависимость O
строится O
в O
другую O
сторону O
, O
и O
отбрасывается O
верхушка O
. O

# text =   У классических transition-based parser возможны три операции, перечисленные выше: стрелка в одну сторону, стрелка в другую сторону и шифт.
У O
классических O
transition B-Application
- I-Application
based I-Application
parser I-Application
возможны O
три O
операции O
, O
перечисленные O
выше O
: O
стрелка O
в O
одну O
сторону O
, O
стрелка O
в O
другую O
сторону O
и O
шифт O
. O

# text =   Анализатор Mystem (разработка яндекса) в определении частей речи достигает лучших результатов, чем UDPipe.
Анализатор O
Mystem B-Application
( O
разработка O
яндекса B-Organization
) O
в O
определении O
частей O
речи O
достигает O
лучших O
результатов O
, O
чем O
UDPipe B-App_system
. O

# text =   Многие знают, что Mystem не полностью понимает морфологическую омонимию.
Многие O
знают O
, O
что O
Mystem B-Application
не O
полностью O
понимает O
морфологическую B-Object
омонимию I-Object
. O

# text =   При помощи анализатора RNNMorph.
При O
помощи O
анализатора O
RNNMorph B-Application
. O

# text =   Про него мало кто слышал, но в прошлом году он выиграл соревнование среди морфологических анализаторов, проводившееся в рамках конференции «Диалог».
Про O
него O
мало O
кто O
слышал O
, O
но O
в O
прошлом O
году O
он O
выиграл O
соревнование O
среди O
морфологических B-Subject
анализаторов I-Subject
, O
проводившееся O
в O
рамках O
конференции O
« O
Диалог O
» O
. O

# text =   Хотя если сравнивать их чисто на уровне качества морфологической разметки (данные с MorphoRuEval-2017), то проигрыш получается значительный — порядка 15%, если считать accuracy по словам.
Хотя O
если O
сравнивать O
их O
чисто O
на O
уровне O
качества O
морфологической B-Labeling
разметки B-Method
( O
данные O
с O
MorphoRuEval-2017 O
) O
, O
то O
проигрыш O
получается O
значительный O
— O
порядка O
15 O
% O
, O
если O
считать O
accuracy B-Metric
по O
словам O
. O

# text =   Дальше буду сравнивать нас с Syntaxnet и остальными алгоритмами.
Дальше O
буду O
сравнивать O
нас O
с O
Syntaxnet B-App_system
и O
остальными O
алгоритмами O
. O

# text =   Интересно, что мы почти дотянулись по метрике LAS до версии Syntaxnet.
Интересно O
, O
что O
мы O
почти O
дотянулись O
по O
метрике O
LAS B-ShortName_Metric
до O
версии O
Syntaxnet B-App_system
. O

# text =   В архитектуре стенфордского парсера и Syntaxnet заложена другая концепия: сначала они генерируют полный ориентированный граф, и дальше работа алгоритма состоит в том, чтобы оставить тот скелет (минимальное остовное дерево), который будет наиболее вероятным.
В O
архитектуре O
стенфордского O
парсера O
и O
Syntaxnet B-App_system
заложена O
другая O
концепия O
: O
сначала O
они O
генерируют O
полный O
ориентированный B-Object
граф I-Object
, O
и O
дальше O
работа O
алгоритма O
состоит O
в O
том O
, O
чтобы O
оставить O
тот O
скелет O
( O
минимальное B-Object
остовное I-Object
дерево I-Object
) O
, O
который O
будет O
наиболее O
вероятным O
. O

# text =   Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
Чаще O
всего O
на O
практике O
в O
NLP B-ShortName_Science
приходится O
сталкиваться O
с O
задачей O
построения B-Task
эмбеддингов I-Task
. O

# text =   Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
Для O
ее O
решения O
обычно O
используют O
один O
из O
следующих O
инструментов O
: O
Готовые O
векторы B-Object
/ O
эмбеддинги B-Subject
слов I-Subject
[ O
6 O
] O
; O
Внутренние O
состояния O
CNN B-ShortName_Method
, O
натренированных O
на O
таких O
задачах O
как O
, O
как O
определение B-Task
фальшивых I-Task
предложений I-Task
/ O
языковое B-Task
моделирование I-Task
/ O
классификация B-Task
[ O
7 O
] O
; O
Комбинация O
выше O
перечисленных O
методов O
; O
Кроме O
того O
, O
уже O
много O
раз O
было O
показано O
[ O
9 O
] O
, O
что O
в O
качестве O
хорошего O
бейслайна O
для O
эмбеддингов O
предложений O
можно O
взять O
и O
просто O
усредненные O
( O
с O
парой O
незначительных O
деталей O
, O
которые O
сейчас O
опустим O
) O
векторы O
слов O
. O

# text =   Выраженная в тексте эмоциональная оценка называется тональностью или сентиментом (от англ. sentiment — чувство; мнение, настроение) текста.
Выраженная O
в O
тексте O
эмоциональная B-Object
оценка I-Object
называется O
тональностью B-Object
или O
сентиментом B-Object
( O
от O
англ O
sentiment O
— O
чувство O
; O
мнение O
, O
настроение O
) O
текста B-Object
. O

# text =   Исторически сложилось так, что традиционный подход к сентимент анализу представляет собой задачу классификации текста (части текста) на две-три категории (негативный, позитивный, нейтральный или просто: негативный или позитивный) [Pang & Lee; Turney ].
Исторически O
сложилось O
так O
, O
что O
традиционный O
подход O
к O
сентимент B-Method
анализу I-Method
представляет O
собой O
задачу O
классификации B-Task
текста O
( O
части O
текста O
) O
на O
две O
- O
три O
категории O
( O
негативный O
, O
позитивный O
, O
нейтральный O
или O
просто O
: O
негативный O
или O
позитивный O
) O
[ O
Pang B-Person
& O
Lee B-Person
; O
Turney B-Person
] O
. O

# text =   Такой вид сентимент анализа называется объектной тональностью (object-based).
Такой O
вид O
сентимент B-Method
анализа I-Method
называется O
объектной B-Object
тональностью I-Object
( O
object B-Object
- I-Object
based I-Object
) O
. O

# text =   Таким образом, тональность высказывания определяется тремя компонентами: субъектом тональности (кто высказал оценку), объектом тональности (о ком или о чём высказана оценка) и собственно тональной оценкой (как оценили).
Таким O
образом O
, O
тональность B-Subject
высказывания I-Subject
определяется O
тремя O
компонентами O
: O
субъектом B-Object
тональности I-Object
( O
кто O
высказал O
оценку O
) O
, O
объектом B-Object
тональности I-Object
( O
о O
ком O
или O
о O
чём O
высказана O
оценка O
) O
и O
собственно O
тональной B-Object
оценкой I-Object
( O
как O
оценили O
) O
. O

# text =   Еще одним направлением сентимент анализа является выявление негативности / позитивности атрибутов объекта тональности (feature-based / aspect-based sentiment analysis).
Еще O
одним O
направлением O
сентимент B-Method
анализа I-Method
является O
выявление B-Task
негативности I-Task
/ I-Task
позитивности I-Task
атрибутов I-Task
объекта I-Task
тональности I-Task
( O
feature B-Method
- I-Method
based I-Method
/ I-Method
aspect I-Method
- I-Method
based I-Method
sentiment B-Method
analysis I-Method
) O
. O

# text =   При статистическом подходе для решения задачи общей классификации текстов на классы тональности широко используют метод опорных векторов (SVM), Байесовы модели, различного рода регрессии [Chetviorkin & Loukachevitch — описание соревнования ROMIP-2011 по сентимент анализу данных, практически все участники использовали SVM или Байес].
При O
статистическом O
подходе O
для O
решения O
задачи O
общей O
классификации B-Task
текстов O
на O
классы O
тональности O
широко O
используют O
метод B-Method
опорных I-Method
векторов I-Method
( O
SVM B-ShortName_Method
) O
, O
Байесовы B-Model
модели I-Model
, O
различного O
рода O
регрессии B-Model
[ O
Chetviorkin B-Person
& I-TERM
Loukachevitch I-Person
— O
описание O
соревнования O
ROMIP-2011 O
по O
сентимент B-Method
анализу I-Method
данных O
, O
практически O
все O
участники O
использовали O
SVM B-ShortName_Method
или O
Байес B-Method
] O
. O

# text =   Если же целью является определение тональности у определенного, заранее заданного объекта (нескольких объектов), то применяют более сложные статистические алгоритмы, такие как CRF [Антонова и Соловьев], алгоритмы семантической близости (например, латентно-семантический анализ – LSA, латентное размещение Дирихле — LDA) и др., а также методы, основанные на правилах [Пазельская и Соловьев].
Если O
же O
целью O
является O
определение B-Task
тональности I-Task
у O
определенного O
, O
заранее O
заданного O
объекта O
( O
нескольких O
объектов O
) O
, O
то O
применяют O
более O
сложные O
статистические B-Method
алгоритмы I-Method
, O
такие O
как O
CRF B-ShortName_Method
[ O
Антонова B-Person
и O
Соловьев B-Person
] O
, O
алгоритмы B-Method
семантической I-Method
близости I-Method
( O
например O
, O
латентно B-TERM
- I-TERM
семантический I-TERM
анализ B-Method
– O
LSA B-ShortName_Method
, O
латентное B-Method
размещение I-Method
Дирихле I-Method
— O
LDA B-ShortName_Method
) O
, O
а O
также O
методы O
, O
основанные O
на O
правилах O
[ O
Пазельская B-Person
и O
Соловьев B-Person
] O
. O

# text =   Удалось найти лишь это упоминание про систему Deepgram.
Удалось O
найти O
лишь O
это O
упоминание O
про O
систему O
Deepgram B-App_system
. O

# text =   Также очень похожая функциональность есть у Microsoft в Streams, но нигде не нашел упоминания про поддержку русского языка, судя по всему, ее тоже нет.
Также O
очень O
похожая O
функциональность O
есть O
у O
Microsoft B-Organization
в O
Streams B-App_system
, O
но O
нигде O
не O
нашел O
упоминания O
про O
поддержку O
русского O
языка O
, O
судя O
по O
всему O
, O
ее O
тоже O
нет O
. O

# text =   Нейросети, которые могут преобразовывать речь в текст называются (сюрприз-сюрприз), speech-to-text.
Нейросети O
, O
которые O
могут O
преобразовывать B-Task
речь I-Task
в I-Task
текст I-Task
называются O
( O
сюрприз O
- O
сюрприз O
) O
, O
speech B-Task
- I-Task
to I-Task
- I-Task
text I-Task
. O

# text =   Если получится найти публичный сервис speech-to-text, то его можно использовать, чтобы «оцифровать» речь во всех вебинарах, а сделать потом нечеткий поиск по тексту – более простая задача.
Если O
получится O
найти O
публичный O
сервис O
speech B-Task
- I-Task
to I-Task
- I-Task
text I-Task
, O
то O
его O
можно O
использовать O
, O
чтобы O
« O
оцифровать O
» O
речь O
во O
всех O
вебинарах O
, O
а O
сделать O
потом O
нечеткий O
поиск O
по O
тексту O
– O
более O
простая O
задача O
. O

# text =   Поиск сервисов, способных делать speech-to-text показал, что таких систем масса, в том числе и разработанных в России, есть среди них также глобальные облачные провайдеры вроде Google, Amazon, MS Azure.
Поиск O
сервисов O
, O
способных O
делать O
speech B-Task
- I-Task
to I-Task
- I-Task
text I-Task
показал O
, O
что O
таких O
систем O
масса O
, O
в O
том O
числе O
и O
разработанных O
в O
России O
, O
есть O
среди O
них O
также O
глобальные O
облачные O
провайдеры O
вроде O
Google B-Organization
, O
Amazon B-Organization
, O
MS B-Organization
Azure I-Organization
. O

# text =   Custom Vocabularies – позволяет создать «словарь» из тех, слов, которые должна «выучить» нейросеть перед тем, как приступить к распознаванию.
Custom B-App_system
Vocabularies I-App_system
– O
позволяет O
создать O
« O
словарь O
» O
из O
тех O
, O
слов O
, O
которые O
должна O
« O
выучить O
» O
нейросеть O
перед O
тем O
, O
как O
приступить O
к O
распознаванию B-Task
. O

# text =   Можно попробовать прикрутить к итоговому набору текстов алгоритм BERT (Bi-directional Encoder Representation from Transformer), описание есть тут.
Можно O
попробовать O
прикрутить O
к O
итоговому O
набору O
текстов O
алгоритм O
BERT B-ShortName_Model
( O
Bi B-Model
- I-Model
directional I-Model
Encoder I-Model
Representation I-Model
from I-Model
Transformer I-Model
) O
, O
описание O
есть O
тут O
. O

# text =  В этой статье мы расскажем о методе Propensity Score Adjustment, который применим для коррекции смещений и улучшения данных, полученных на онлайн-панелях.
В O
этой O
статье O
мы O
расскажем O
о O
методе B-Method
Propensity I-Method
Score I-Method
Adjustment I-Method
, O
который O
применим O
для O
коррекции O
смещений O
и O
улучшения O
данных O
, O
полученных O
на O
онлайн O
- O
панелях O
. O

# text =  Итоговые коэффициенты, корректирующие смещение онлайн-выборки, можно рассчитать по методу Хорвица-Томпсона.
Итоговые O
коэффициенты O
, O
корректирующие O
смещение O
онлайн O
- O
выборки O
, O
можно O
рассчитать O
по O
методу B-Method
Хорвица I-Method
- I-Method
Томпсона I-Method
. O

# text =  Взвешивание (Weighting) - метод предназначен для коррекции известных перекосов выборок по социально-демографическим атрибутам.
Взвешивание B-Method
( O
Weighting B-Method
) O
метод O
предназначен O
для O
коррекции O
известных O
перекосов O
выборок O
по O
социально O
- O
демографическим O
атрибутам O
. O

# text =  Авторы оригинального исследования Pew Research рекомендуют использовать для корректировки онлайн-опросов модели случайных лесов (random forest).
Авторы O
оригинального O
исследования O
Pew B-Research
Research I-Research
рекомендуют O
использовать O
для O
корректировки O
онлайн O
- O
опросов O
модели O
случайных B-Method
лесов I-Method
( O
random B-Method
forest I-Method
) O
. O


# text =  Сейчас стандарт коррекции онлайн-выборок находится на стадии обсуждения и разработки и метод Propensity Score Adjustment, который мы рассмотрели, может стать общепринятым способом коррекции онлайн-панелей.
Сейчас O
стандарт O
коррекции O
онлайн O
- O
выборок O
находится O
на O
стадии O
обсуждения O
и O
разработки O
и O
метод B-Method
Propensity I-Method
Score I-Method
Adjustment I-Method
, O
который O
мы O
рассмотрели O
, O
может O
стать O
общепринятым O
способом O
коррекции O
онлайн O
- O
панелей O
. O

# text =   Некоторое время назад к нам обратился заказчик с не совсем обычной задачей — воспроизвести сервис IBM Watson Personality Insights, который анализировал текст, написанный человеком и определял по нему ряд личностных характеристик.
Некоторое O
время O
назад O
к O
нам O
обратился O
заказчик O
с O
не O
совсем O
обычной O
задачей O
— O
воспроизвести O
сервис O
IBM B-Technology
Watson I-Technology
Personality I-Technology
Insights I-Technology
, O
который O
анализировал B-Task
текст I-Task
, O
написанный O
человеком O
и O
определял O
по O
нему O
ряд O
личностных B-Object
характеристик I-Object
. O

# text =   Основная идея данного сервиса состояла в том, что он получает на вход текст написанный определенным человеком и определяет по этому тексту четыре группы характеристик личности.
Основная O
идея O
данного O
сервиса O
состояла O
в O
том O
, O
что O
он O
получает O
на O
вход O
текст O
написанный O
определенным O
человеком O
и O
определяет B-Task
по O
этому O
тексту O
четыре O
группы I-Task
характеристик I-Task
личности I-Task
. O

# text =   Например, Personality Insights использовался в психотерапии для оценки состояния пациентов [5], в искусстве (оценка личности персонажей пьес Шекспира) [6], определении спама [7] а также в научных исследованиях.
Например O
, O
Personality B-Technology
Insights I-Technology
использовался O
в O
психотерапии B-Science
для O
оценки B-Task
состояния I-Task
пациентов I-Task
[ O
5 O
] O
, O
в O
искусстве O
( O
оценка B-Task
личности I-Task
персонажей I-Task
пьес I-Task
Шекспира I-Task
) O
[ O
6 O
] O
, O
определении B-Task
спама I-Task
[ O
7 O
] O
а O
также O
в O
научных O
исследованиях O
. O

# text =   На сайте Personality Insights качество моделей Watson оценивалось с помощью двух показателей — средней абсолютной ошибки (MAE) и коэффициента корреляции.
На O
сайте O
Personality B-Technology
Insights I-Technology
качество O
моделейO
Watson B-Technology
оценивалось O
с O
помощью O
двух O
показателей O
— O
средней B-Metric
абсолютной I-Metric
ошибки I-Metric
( O
MAE B-ShortName_Metric
) O
и O
коэффициента B-Metric
корреляции I-Metric
. O

# text =   В литературе для предсказания характеристик Big 5 использовались различные методы линейная регрессия с использованием признаков полученных латентным семантическим анализом [11], ридж-регрессия по большому набору собранных вручную признаков [12], SVM с признаками TF/IDF [13], word2vec и doc2vec [14].
В O
литературе O
для O
предсказания O
характеристик O
Big O
5 O
использовались O
различные O
методы O
линейная B-Method
регрессия I-Method
с O
использованием O
признаков O
полученных O
латентным O
семантическим O
анализом O
[ O
11 O
] O
, O
ридж B-Method
- I-Method
регрессия I-Method
по O
большому O
набору O
собранных O
вручную O
признаков O
[ O
12 O
] O
, O
SVM B-ShortName_Method
с O
признаками O
TF B-Metric
/ O
IDF B-Metric
[ O
13 O
] O
, O
word2vec B-Model
и O
doc2vec B-Model
[ O
14 O
] O
. O

# text =   В более современных работах присутствуют сверточные нейронные сети [15, 16], а также предобученные модели BERT [17]
В O
более O
современных O
работах O
присутствуют O
сверточные B-Method
нейронные B-Method
сети I-Method
[ O
15 O
, O
16 O
] O
, O
а O
также O
предобученные O
модели O
BERT B-Model
[ O
17 O
] O

# text =  Модель, которую построил заказчик использовала вектора слов word2vec и рекуррентную нейронную сеть на базе GRU (gated recurrent unit) (Рис 1а).
Модель O
, O
которую O
построил O
заказчик O
использовала O
вектора O
слов O
word2vec B-Model
и O
рекуррентную O
нейронную O
сеть O
на O
базе O
GRU B-ShortName_Method
( O
gated B-Method
recurrent I-Method
unit I-Method
) O
( O
Рис O
1а O
) O
. O

# text =   Обучалась модель с функцией ошибки MSE (среднеквадратичное отклонение).
Обучалась O
модель B-Object
с O
функцией O
ошибки O
MSE B-ShortName_Method
( O
среднеквадратичное B-Method
отклонение I-Method
) O
. O

# text =   Сигмоидная функция активации обычно не очень хорошо подходит для задачи регрессии.
Сигмоидная B-Method
функция I-Method
активации I-Method
обычно O
не O
очень O
хорошо O
подходит O
для O
задачи O
регрессии B-Task
. O

# text =   В литературе для регрессии рекомендуют использовать линейную активацию или RelU.
В O
литературе O
для O
регрессии B-Task
рекомендуют O
использовать O
линейную B-Method
активацию I-Method
или O
RelU B-Method
. O

# text =   Вычислив MAE отдельно для характеристики личности и отдельно для потребительских предпочтений получили значения 0.11 и 0.148 соответственно, т. е. потребительские предпочтения сильно портят общую картину.
Вычислив O
MAE B-Metric
отдельно O
для O
характеристики O
личности O
и O
отдельно O
для O
потребительских O
предпочтений O
получили O
значения O
0.11 B-Value
и O
0.148 B-Value
соответственно O
, O
т O
. O
  O
е O
. O
потребительские O
предпочтения O
сильно O
портят O
общую O
картину O
. O

# text =   Замена BERT на более современную модель XLM RoBERTa large позволило улучшить результаты (эта модель более ресурсозатратная и медленная, но заказчик сказал, что скорость работы не критична).
Замена O
BERT O
на O
более O
современную O
модель O
XLM B-Model
RoBERTa I-Model
large I-Model
позволило O
улучшить O
результаты O
( O
эта O
модель O
более O
ресурсозатратная O
и O
медленная O
, O
но O
заказчик O
сказал O
, O
что O
скорость O
работы O
не O
критична O
) O
. O

# text =   Итоговый MAE составил 0.073 для характеристик личности и 0.098 для потребительских предпочтений.
Итоговый O
MAE B-Metric
составил O
0.073 B-Value
для O
характеристик O
личности O
и O
0.098 B-Value
для O
потребительских O
предпочтений O
. O

# text =   Получились немного разные цифры, но средний коэффициент корреляции по всем параметрам составил 0.68, что говорит о том, что характеристики, выдаваемые с разных переводов одного текста должны быть весьма похожи.
Получились O
немного O
разные O
цифры O
, O
но O
средний O
коэффициент B-Metric
корреляции I-Metric
по O
всем O
параметрам O
составил O
0.68 B-Value
, O
что O
говорит O
о O
том O
, O
что O
характеристики O
, O
выдаваемые O
с O
разных O
переводов O
одного O
текста O
должны O
быть O
весьма O
похожи O
. O

# text =   Надо сказать, что признаки, формируемые верхними слоями подобных моделей не всегда являются самыми лучшими, точнее даже сказать, как правило, не являются — в классических задачах, таких как поиск именованных сущностей или ответы на вопросы по тексту, признаки верхних уровней работают хуже, чем признаки промежуточных [18].
Надо O
сказать O
, O
что O
признаки O
, O
формируемые O
верхними O
слоями O
подобных O
моделей O
не O
всегда O
являются O
самыми O
лучшими O
, O
точнее O
даже O
сказать O
, O
как O
правило O
, O
не O
являются O
— O
в O
классических O
задачах O
, O
таких O
как O
поиск B-Task
именованных I-Task
сущностей I-Task
или O
ответы O
на O
вопросы O
по O
тексту O
, O
признаки O
верхних O
уровней O
работают O
хуже O
, O
чем O
признаки O
промежуточных O
[ O
18 O
] O
. O

# text =   Veridical Data Science — программная статья о методологии верификации моделей.
Veridical B-Publication
Data I-Publication
Science I-Publication
— O
программная O
статья O
о O
методологии O
верификации B-Task
моделей I-Task
. O

# text =  Чтобы обеспечить надежную проверку и разработать механизмы проверки и пополнения знаний, нужны специалисты смежных областей, одновременно обладающие компетенциями в ML и в предметной области (медицине, лингвистике, нейробиологии, образовании и т.д.).
Чтобы O
обеспечить O
надежную O
проверку O
и O
разработать O
механизмы O
проверки O
и O
пополнения O
знаний O
, O
нужны O
специалисты O
смежных O
областей O
, O
одновременно O
обладающие O
компетенциями O
в O
ML O
и O
в O
предметной O
области O
( O
медицине B-Science
, O
лингвистике B-Science
, O
нейробиологии B-Science
, O
образовании B-Science
и O
т.д. O
) O
. O

# text =   В частности, развивается causal inference и commonsense reasoning.
В O
частности O
, O
развивается O
causal B-Method
inference I-Method
и O
commonsense B-Method
reasoning I-Method
. O

# text =   Часть докладов посвящена мета-обучению (о том, как учиться учиться) и соединению DL-технологий с логикой 1 и 2 порядка — термин Artificial General Intelligence (AGI) становится обычным термином в выступлениях спикеров.
Часть O
докладов O
посвящена O
мета O
- O
обучению O
( O
о O
том O
, O
как O
учиться O
учиться O
) O
и O
соединению O
DL B-Technology
- I-Technology
технологий I-Technology
с O
логикой O
1 O
и O
2 O
порядка O
— O
термин O
Artificial B-Application
General I-Application
Intelligence I-Application
( O
AGI B-Abberv_Application
) O
становится O
обычным O
термином o
в O
выступлениях O
спикеров O
. O

# text =   Google запускает Coral ai – аналог raspberry pi, мини-компьютер для внедрения нейросетей в экспериментальные установки.
Google B-Organization
запускает O
Coral B-Technology
ai I-Technology
– O
аналог O
raspberry B-Technology
pi I-Technology
, O
мини O
- O
компьютер O
для O
внедрения O
нейросетей O
в O
экспериментальные O
установки O
. O

# text =   Federated learning – направление ML, в котором отдельные модели учатся независимо друг от друга, а затем объединяются в единую модель (без централизации исходных данных), с поправками на редкие события, аномалии, персонализацию и т.д.
Federated B-Method
learning I-Method
– O
направление O
ML B-ShortName_Method
, O
в O
котором O
отдельные O
модели O
учатся O
независимо O
друг O
от O
друга O
, O
а O
затем O
объединяются O
в O
единую O
модель O
( O
без O
централизации O
исходных O
данных O
) O
, O
с O
поправками O
на O
редкие O
события O
, O
аномалии O
, O
персонализацию O
и O
т.д. O

# text =   Генеративные модели на основании federated learning – будущее перспективное направление по мнению Google, которое находится “в ранних стадиях экспоненциального роста”.
Генеративные B-Model
модели I-Model
на O
основании O
federated B-Method
learning I-Method
– O
будущее O
перспективное O
направление O
по O
мнению O
Google B-Organization
, O
которое O
находится O
“ O
в O
ранних O
стадиях O
экспоненциального O
роста O
” O
. O

# text =   МТИ Technology Review протестировали два инструмента — MyInterview и Curious Thing.
МТИ B-Organization
Technology I-Organization
Review I-Organization
протестировали O
два O
инструмента O
— O
MyInterview B-Technology
и O
Curious B-Technology
Thing I-Technology
. O

# text =   Потом появилась нейросеть GPT-2, которая была как минимум в 10 раз мощнее и была способна обрабатывать 1,5 миллиарда параметров (переменных, определяющих возможности машинного обучения).
Потом O
появилась O
нейросеть O
GPT-2 B-Model
, O
которая O
была O
как O
минимум O
в O
10 O
раз O
мощнее O
и O
была O
способна O
обрабатывать O
1,5 O
миллиарда O
параметров O
( O
переменных O
, O
определяющих O
возможности O
машинного B-Science
обучения I-Science
) O
. O

# text =   Используя новый алгоритм упаковки, в Graphcore ускорили обработку естественного языка более чем в 2 раза при обучении BERT-Large.
Используя O
новый O
алгоритм B-Method
упаковки I-Method
, O
в O
Graphcore B-Organization
ускорили O
обработку O
естественного O
языка O
более O
чем O
в O
2 O
раза O
при O
обучении O
BERT B-Model
- I-Model
Large I-Model
. O

# text =   В Graphcore предполагают, что алгоритм упаковки также может применяться в геномике, в моделях фолдинга белков и других моделях с перекошенным распределением длины, оказывая гораздо более широкое влияние на различные отрасли и приложения.
В O
Graphcore B-Organization
предполагают O
, O
что O
алгоритм B-Method
упаковки I-Method
также O
может O
применяться O
в O
геномике B-Science
, O
в O
моделях B-Model
фолдинга I-Model
белков O
и O
других O
моделях O
с O
перекошенным O
распределением O
длины O
, O
оказывая O
гораздо O
более O
широкое O
влияние O
на O
различные O
отрасли O
и O
приложения O
. O

# text =   В новой работе Graphcore представили высокоэффективный алгоритм гистограммной упаковки с неотрицательными наименьшими квадратами (или NNLSHP), а также алгоритм BERT, применяемый к упакованным последовательностям.
В O
новой O
работе O
Graphcore B-Organization
представили O
высокоэффективный O
алгоритм B-Method
гистограммной I-Method
упаковки I-Method
с O
неотрицательными O
наименьшими O
квадратами O
( O
или O
, O
а O
также O
алгоритм O
BERT B-Model
, O
применяемый O
к O
упакованным O
последовательностям O
. O

# text =   Сбер создал и опубликовал в открытом доступе программную библиотеку PyTorch-LifeStream, содержащую несколько алгоритмов построения эмбеддингов событийных данных.
Сбер B-Organization
создал O
и O
опубликовал O
в O
открытом O
доступе O
программную O
библиотеку O
PyTorch B-Application
- I-Application
LifeStream I-Application
, O
содержащую O
несколько O
алгоритмов O
построения O
эмбеддингов O
событийных O
данных O
. O

# text =   Эмбеддинг (Embedding) – преобразования сложноструктурированных данных,  например слов, текстов, атрибутов событий, событий и их последовательностей, в машинно-читаемый набор чисел – числовой вектор.Событийные данные – разные последовательности.
Эмбеддинг B-Object
( O
Embedding B-Object
) O
– O
преобразования O
сложноструктурированных O
данных O
, O
например O
слов O
, O
текстов O
, O
атрибутов O
событий O
, O
событий O
и O
их O
последовательностей O
, O
в O
машинно O
- O
читаемый O
набор O
чисел O
– O
числовой O
вектор O
. O
Событийные O
данные O
– O
разные O
последовательности O
. O

# text =   Самой популярной из трёх задач соревнования стала главная – Matching.
Самой O
популярной O
из O
трёх O
задач O
соревнования O
стала O
главная O
– O
Matching B-Task
. O

# text =   Стоит отметить, что и для них всё было непросто – конкурсная задача матчинга позволила удачно применить разработанный в Лаборатории ИИ метод генерации эмбеддингов транзакционных данных одновременно для двух разных доменов событийных данных (транзакции и кликстрим – атрибуты посещения веб-страниц). 
Стоит O
отметить O
, O
что O
и O
для O
них O
всё O
было O
непросто O
– O
конкурсная O
задача O
матчинга O
позволила O
удачно O
применить O
разработанный O
в O
Лаборатории B-Organization
ИИ I-Organization
метод B-Method
генерации I-Method
эмбеддингов I-Method
транзакционных I-Method
данных I-Method
одновременно O
для O
двух O
разных O
доменов O
событийных O
данных O
( O
транзакции B-Object
и O
кликстрим B-Object
– O
атрибуты O
посещения O
веб O
- O
страниц O
) O
. O

# text =   Победители создали лучшее решение благодаря применению собственной библиотеки PyTorch-LifeStream, которая позволила ускорить разработку решения, так как содержит много готовых инструментов для работы с событийными данными, и дала возможность стать фаворитом престижного конкурса. 
Победители O
создали O
лучшее O
решение O
благодаря O
применению O
собственной O
библиотеки O
PyTorch B-Application
- I-Application
LifeStream I-Application
, O
которая O
позволила O
ускорить O
разработку O
решения O
, O
так O
как O
содержит O
много O
готовых O
инструментов O
для O
работы O
с O
событийными O
данными O
, O
и O
дала O
возможность O
стать O
фаворитом O
престижного O
конкурса O

# text =   Фичи для транзакций и кликов объединялись и подавались в алгоритм бустинга.
Фичи O
для O
транзакций O
и O
кликов O
объединялись O
и O
подавались O
в O
алгоритм O
бустинга B-Method
. O

# text =   Алгоритм обучался как задача бинарной классификации.
Алгоритм O
обучался O
как O
задача B-Task
бинарной I-Task
классификации I-Task
. O

# text =   Команда решила использовать схему обучения, похожую на сиамскую сеть.
Команда O
решила O
использовать O
схему O
обучения O
, O
похожую O
на O
сиамскую B-Method
сеть I-Method
. O

# text =   Метки использованы для выборки положительных и отрицательных образцов для функции потерь Softmax Loss.
Метки O
использованы O
для O
выборки O
положительных O
и O
отрицательных O
образцов O
для O
функции O
потерь O
Softmax B-Method
Loss I-Method
. O

# text =  SequenceEncoder – рекурентно-нейронная сеть (RNN), совместно используемая для транзакций и кликов.
SequenceEncoder B-Method
– O
рекурентно B-Network
- I-Network
нейронная B-Network
сеть I-Network
( O
RNN B-ShortName_Method
) O
, O
совместно O
используемая O
для O
транзакций O
и O
кликов O
. O

# text =   В итоге это дало самый большой прирост качества: для ансамбля из пяти моделей метрика качества R1 выросла с 0.2819 до 0.2949.
В O
итоге O
это O
дало O
самый O
большой O
прирост O
качества O
: O
для O
ансамбля O
из O
пяти O
моделей O
метрика O
качества O
R1 B-Metric
выросла O
с O
0.2819 B-Value
до O
0.2949 B-Value
. O

# text =   Решать задачу будем с использованием нейронных сетей, но оптимизируемых генетическим алгоритмом (ГА) – такой процесс называют нейроэволюцией.
Решать O
задачу O
будем O
с O
использованием O
нейронных O
сетей O
, O
но O
оптимизируемых O
генетическим B-Method
алгоритмом I-Method
( O
ГА B-ShortName
) O
– O
такой O
процесс O
называют O
нейроэволюцией O
. O

# text =   Мы воспользовались методом NEAT (NeuroEvolution of Augmenting Topologies), изобретенным Кеннетом Стенли и Ристо Мииккулайненом в начале века [1]: во-первых, он хорошо зарекомендовал себя в важных для народного хозяйства проблемах, во-вторых, к началу работы над проектом у нас уже был свой фреймворк, реализующий NEAT.
Мы O
воспользовались O
методом O
NEAT B-ShortName
( O
NeuroEvolution B-Method
of I-Method
Augmenting I-Method
Topologies I-Method
) O
, O
изобретенным O
Кеннетом B-Person
Стенли I-Person
и O
Ристо B-Person
Мииккулайненом I-Person
в O
начале O
века O
[ O
1 O
] O
: O
во O
- O
первых O
, O
он O
хорошо O
зарекомендовал O
себя O
в O
важных O
для O
народного O
хозяйства O
проблемах O
, O
во O
- O
вторых O
, O
к O
началу O
работы O
над O
проектом O
у O
нас O
уже O
был O
свой O
фреймворк O
, O
реализующий O
NEAT B-Method
. O

# text =   Строго говоря, нам нужно получить параллельный корпус из двух текстов.
Строго O
говоря O
, O
нам O
нужно O
получить O
параллельный B-Object
корпус I-Object
из O
двух O
текстов O
. O

# text =   Для выравнивания воспользуемся библиотекой lingtrain-aligner, над которой я работаю около года и которая родилась из кучи скриптов на python, часть из которых еще ждет своего часа.
Для O
выравнивания B-Method
воспользуемся O
библиотекой O
lingtrain B-Application
- I-Application
aligner I-Application
, O
над O
которой O
я O
работаю O
около O
года O
и O
которая O
родилась O
из O
кучи O
скриптов O
на O
python O
, O
часть O
из O
которых O
еще O
ждет O
своего O
часа O
. O

# text =   Хорошим решением мне видится регрессия на координаты строк при выравнивании батча и сдвиг окна на конец потока при выравнивании следующего.
Хорошим O
решением O
мне O
видится O
регрессия B-Method
на O
координаты O
строк O
при O
выравнивании B-Method
батча O
и O
сдвиг O
окна O
на O
конец O
потока O
при O
выравнивании O
следующего O
. O

# text =   Facebook представила систему распознавания речи wav2vec-U.
Facebook B-Organization
представила O
систему O
распознавания B-Task
речи I-Task
wav2vec B-TERM
- O
U O

# text =   Система разбивает запись на речевые единицы, которые приблизительно соответствуют отдельным звукам.
Система O
разбивает O
запись O
на O
речевые B-Object
единицы I-Object
, O
которые O
приблизительно O
соответствуют O
отдельным O
звукам O
. O

# text =   Чтобы научиться распознавать слова в аудиозаписи, Facebook обучила генеративную состязательную сеть (GAN).
Чтобы O
научиться O
распознавать O
слова O
в O
аудиозаписи O
, O
Facebook B-Organization
обучила O
генеративную B-Model
состязательную I-Model
сеть I-Model
( O
GAN B-ShortName
) O
. O

# text =   Генератор берет каждый аудиосегмент и предсказывает фонему, соответствующую звуку на языке.
Генератор O
берет O
каждый O
аудиосегмент O
и O
предсказывает O
фонему B-Subject
, O
соответствующую O
звуку O
на O
языке B-Object
. O

# text =   Новая модель распознавания речи Facebook AI — это последняя разработка за несколько лет работы над моделями распознавания речи.
Новая O
модель O
распознавания B-Task
речи I-Task
Facebook B-Organization
AI I-Organization
— O
это O
последняя O
разработка O
за O
несколько O
лет O
работы O
над O
моделями O
распознавания O
речи O
. O

# text =   Ее предшественниками стали wav2letter, wav2vec, Librilight, wav2vec 2.0, XLSR и wav2vec 2.0.
ее O
предшественниками O
стали O
wav2letter B-Model
, O
wav2vec B-Model
, O
Librilight B-Model
, O
wav2vec B-Model
2.0 I-Model
, O
XLSR B-Model
и O
wav2vec B-Model
2.0 I-Model
. O

# text =   В полку LLM прибыло: недавно специалисты из Французского национального центра научных исследований (French National Center for Scientific Research) объявили о релизе новой большой языковой модели под названием BLOOM (расшифровывается как BigScience Large Open-science Open-access Multilingual Language Model).
В O
полку O
LLM B-Model
прибыло O
: O
недавно O
специалисты O
из O
Французского B-Organization
национального I-Organization
центра I-Organization
научных I-Organization
исследований I-Organization
( O
French B-Organization
National I-Organization
Center I-Organization
for I-Organization
Scientific I-Organization
Research I-Organization
) O
объявили O
о O
релизе O
новой O
большой O
языковой O
модели O
под O
названием O
BLOOM B-ShortName
( O
расшифровывается O
как O
BigScience B-Model
Large I-Model
Open I-Model
- I-Model
science I-Model
Open I-Model
- I-Model
access I-Model
Multilingual I-Model
Language B-Model
Model I-Model
) O
. O

# text =   Большие языковые модели или LLM (Large Language Models) — это алгоритмы глубокого обучения, которые обучаются на огромных объемах данных.
Большие O
языковые O
модели O
или O
LLM B-ShortName
( O
Large B-Model
Language I-Model
Models I-Model
) O
— O
это O
алгоритмы O
глубокого O
обучения O
, O
которые O
обучаются O
на O
огромных O
объемах O
данных O
. O

# text =   Их можно использовать в качестве чат-ботов, для поиска информации, модерации онлайн-контента, анализа литературы или для создания совершенно новых фрагментов текста на основе подсказок (чем занимается, например, «Порфирьевич», который способен генерировать весьма забавные короткие рассказы).
Их O
можно O
использовать O
в O
качестве O
чат B-Object
- I-Object
ботов I-Object
, O
для O
поиска B-Task
информации I-Task
, O
модерации B-Task
онлайн I-Task
- I-Task
контента I-Task
, O
анализа B-Task
литературы I-Task
или O
для O
создания B-Task
совершенно I-Task
новых I-Task
фрагментов I-Task
текста I-Task
на O
основе B-Subject
подсказок O
( O
чем O
занимается O
, O
например O
, O
« O
Порфирьевич B-Application
» O
, O
который O
способен O
генерировать O
весьма O
забавные O
короткие O
рассказы O
) O
. O

# text =   Новая LLM с открытым исходным кодом в отличие от таких известных LLM, как GPT-3 от OpenAI и LaMDA от Google, BLOOM является открытой языковой моделью, а исследователи охотно делятся подробностями о тех данных, на которых она обучалась, рассказывают о проблемах в ее разработке и о том, как они оценивали производительность BLOOM.
Новая O
LLM B-ShortName
с O
открытым O
исходным O
кодом O
в O
отличие O
от O
таких O
известных O
LLM B-ShortName
, O
как O
GPT-3 B-Model
от O
OpenAI B-Organization
и O
LaMDA B-Model
от O
Google B-Organization
, O
BLOOM B-Model
является O
открытой O
языковой O
моделью O
, O
а O
исследователи O
охотно O
делятся O
подробностями O
о O
тех O
данных O
, O
на O
которых O
она O
обучалась O
, O
рассказывают O
о O
проблемах O
в O
ее O
разработке O
и O
о O
том O
, O
как O
они O
оценивали O
производительность O
BLOOM B-Model
. O

# text =   OpenAI и Google не делились своим кодом и не делали свои модели общедоступными.
OpenAI B-Organization
и O
Google B-Organization
не O
делились O
своим O
кодом O
и O
не O
делали O
свои O
модели O
общедоступными O
. O

# text =   И уже сейчас над BLOOM работают более тысячи исследователей-добровольцев в рамках проекта под названием BigScience, который координирует стартап Hugging Face, существующий за счет финансовой поддержки французского правительства.
И O
уже O
сейчас O
над O
BLOOM B-Model
работают O
более O
тысячи O
исследователей O
- O
добровольцев O
в O
рамках O
проекта O
под O
названием O
BigScience B-Project
, O
который O
координирует O
стартап O
Hugging B-Organization
Face I-Organization
, O
существующий O
за O
счет O
финансовой O
поддержки O
французского O
правительства O
. O

# text =   BLOOM может обрабатывать 46 языков, включая французский, испанский, арабский, вьетнамский, китайский, индонезийский, каталанский, целых 13 языков Индии (хинди, бенгали, маратхи и ряд других) и аж 20 африканских.
BLOOM B-Model
может O
обрабатывать O
46 O
языков O
, O
включая O
французский B-Lang
, O
испанский B-Lang
, O
арабский B-Lang
, O
вьетнамский B-Lang
, O
китайский B-Lang
, O
индонезийский B-Lang
, O
каталанский B-Lang
, O
целых O
13 O
языков O
Индии O
( O
хинди B-Lang
, O
бенгали B-Lang
, O
маратхи B-Lang
и O
ряд O
других O
) O
и O
аж O
20 O
африканских O
. O

# text =   На русском BLOOM тоже пишет, но пока довольно вяло.
На O
русском B-Lang
тоже O
пишет O
, O
но O
пока O
довольно O
вяло O
. O

# text =   Почти треть обучающих данных была введена в модель BLOOM на английском языке: следствие того, что именно английский является наиболее часто используемым языком в интернете.
Почти O
треть O
обучающих O
данных O
была O
введена O
в O
модель O
BLOOM B-Model
на O
английском B-Lang
языке O
: O
следствие O
того O
, O
что O
именно O
английский B-Lang
является O
наиболее O
часто O
используемым O
языком O
в O
интернете O
. O

# text =   В июне текущего года инженер Google Блейк Лемуан (он на фото выше) ошарашил мировую общественность заявлением, что LLM LaMDA, над которой он работал вместе с другими программистами, может обладать некоторым подобием разума.
В O
июне O
текущего O
года O
инженер O
Google B-Organization
Блейк B-Person
Лемуан I-Person
( O
он O
на O
фото O
выше O
) O
ошарашил O
мировую O
общественность O
заявлением O
, O
что O
LLM B-ShortName
LaMDA I-ShortName
, O
над O
которой O
он O
работал O
вместе O
с O
другими O
программистами O
, O
может O
обладать O
некоторым O
подобием O
разума O
. O

# text =   Американский ученый и исследователь ИИ Гэри Маркус еще до появления в сети откровений Лемуана опубликовал на портале Scientific American материал под названием «Общий ИИ не так неизбежен, как вы думаете».
Американский O
ученый O
и O
исследователь O
ИИ B-ShortName
Гэри B-Person
Маркус I-Person
еще O
до O
появления O
в O
сети O
откровений O
Лемуана B-Person
опубликовал O
на O
портале O
Scientific B-InfoResource
American I-InfoResource
материал O
под O
названием O
« B-Publication
Общий I-Publication
ИИ I-Publication
не I-Publication
так I-Publication
неизбежен I-Publication
, I-Publication
как I-Publication
вы I-Publication
думаете I-Publication
» I-Publication
. O

# text =   В частности, DALL-E 2 от OpenAI провалил тест на различение изображений астронавтов, едущих на лошадях, перепутав их с лошадьми, оседлавшими астронавтов.
В O
частности O
, O
DALL B-Model
- I-Model
E I-Model
2 I-Model
от O
OpenAI B-Organization
провалил O
тест O
на O
различение O
изображений O
астронавтов O
, O
едущих O
на O
лошадях O
, O
перепутав O
их O
с O
лошадьми O
, O
оседлавшими O
астронавтов O
. O

# text =   «Сбер» представил mGPT — версию нейросети GPT-3, способную генерировать тексты на 61 языке Open source *Machine learning *Artificial Intelligence IT-companies       
« O
Сбер B-Organization
» O
представил O
mGPT B-Model
— O
версию O
нейросети O
GPT-3 B-Model
, O
способную O
генерировать O
тексты O
на O
61 O
языке O

# text =   21 апреля 2022 года команда разработчиков SberDevices представила многоязычную версию нейросети GPT-3 под названием mGPT.
21 B-Data
апреля I-Data
2022 I-Data
года O
команда O
разработчиков O
SberDevices B-Organization
представила O
многоязычную O
версию O
нейросети O
GPT-3 B-Model
под O
названием O
mGPT B-Model
. O

# text =   «Сбер» рассказал, что модель mGPT может использоваться как просто для генерации текста, так и для решения различных задач в области обработки естественного языка на одном из поддерживаемых языков путем дообучения или в составе ансамблей моделей.
« O
Сбер B-Organization
» O
рассказал O
, O
что O
модель O
mGPT B-Model
может O
использоваться O
как O
просто O
для O
генерации B-Task
текста I-Task
, O
так O
и O
для O
решения O
различных O
задач O
в O
области O
обработки B-Science
естественного I-Science
языка I-Science
на O
одном O
из O
поддерживаемых O
языков O
путем O
дообучения O
или O
в O
составе O
ансамблей B-Method
моделей I-Method
. O

# text =   Разработчики уточнили, что модель mGPT показывает выдающиеся результаты на многих задачах few-shot и zero-shot learning: в этой области машинного обучения не требуется отдельно доучивать модель, достаточно сформулировать задачу текстом и привести несколько примеров, после чего mGPT научится выполнять новую задачу.
Разработчики O
уточнили O
, O
что O
модель O
mGPT B-Model
показывает O
выдающиеся O
результаты O
на O
многих O
задачах O
few B-Task
- I-Task
shot I-Task
и O
zero B-Task
- I-Task
shot I-Task
learning I-Task
: O
в O
этой O
области O
машинного B-Science
обучения I-Science
не O
требуется O
отдельно O
доучивать O
модель O
, O
достаточно O
сформулировать O
задачу O
текстом O
и O
привести O
несколько O
примеров O
, O
после O
чего O
mGPT B-Model
научится O
выполнять O
новую O
задачу O
. O

# text =   Это может использоваться для того, чтобы научить автоматизированную систему отвечать на вопросы, определять эмоциональную окраску текста, извлекать из текста имена, фамилии, названия компаний и тому подобное.
Это O
может O
использоваться O
для O
того O
, O
чтобы O
научить O
автоматизированную O
систему O
отвечать O
на O
вопросы O
, O
определять B-Task
эмоциональную I-Task
окраску I-Task
текста I-Task
, O
извлекать O
из O
текста O
имена O
, O
фамилии O
, O
названия O
компаний O
и O
тому O
подобное O
. O

# text =   «Сбер» раскрыл, что модель mGPT может также использоваться как компонент различных речевых технологий — например, для улучшения качества распознавания речи, генерации сценариев диалоговых систем и других задачах.
« O
Сбер B-Organization
» O
раскрыл O
, O
что O
модель O
mGPT B-Model
может O
также O
использоваться O
как O
компонент O
различных O
речевых B-Science
технологий I-Science
— O
например O
, O
для O
улучшения B-Task
качества I-Task
распознавания I-Task
речи I-Task
, O
генерации B-Task
сценариев I-Task
диалоговых I-Task
систем I-Task
и O
других O
задачах O
. O

# text =   Полный перечень языков, доступный в модели mGPT: азербайджанский, английский, арабский, армянский, африкаанс, баскский, башкирский, белорусский, бенгали, бирманский, болгарский, бурятский, венгерский, вьетнамский, голландский, греческий, грузинский, датский, иврит, индонезийский, испанский, итальянский, йоруба, казахский, калмыцкий, киргизский, китайский, корейский, латышский, литовский, малайский, малаялам, маратхи, молдавский, монгольский, немецкий, осетинский, персидский, польский, португальский, румынский, русский, суахили, таджикский, тайский, тамильский, татарский, телугу, тувинский, турецкий, туркменский, узбекский, украинский, урду, финский, французский, хинди, чувашский, шведский, якутский, японский.
Полный O
перечень O
языков O
, O
доступный O
в O
модели O
mGPT B-Model
: O
азербайджанский B-Lang
, O
английский B-Lang
, O
арабский B-Lang
, O
армянский B-Lang
, O
африкаанс B-Lang
, O
баскский B-Lang
, O
башкирский B-Lang
, O
белорусский B-Lang
, O
бенгали B-Lang
, O
бирманский B-Lang
, O
болгарский B-Lang
, O
бурятский B-Lang
, O
венгерский B-Lang
, O
вьетнамский B-Lang
, O
голландский B-Lang
, O
греческий B-Lang
, O
грузинский B-Lang
, O
датский B-Lang
, O
иврит B-Lang
, O
индонезийский B-Lang
, O
испанский B-Lang
, O
итальянский B-Lang
, O
йоруба B-Lang
, O
казахский B-Lang
, O
калмыцкий B-Lang
, O
киргизский B-Lang
, O
китайский B-Lang
, O
корейский B-Lang
, O
латышский B-Lang
, O
литовский B-Lang
, O
малайский B-Lang
, O
малаялам B-Lang
, O
маратхи B-Lang
, O
молдавский B-Lang
, O
монгольский B-Lang
, O
немецкий B-Lang
, O
осетинский B-Lang
, O
персидский B-Lang
, O
польский B-Lang
, O
португальский B-Lang
, O
румынский B-Lang
, O
русский B-Lang
, O
суахили B-Lang
, O
таджикский B-Lang
, O
тайский B-Lang
, O
тамильский B-Lang
, O
татарский B-Lang
, O
телугу B-Lang
, O
тувинский B-Lang
, O
турецкий B-Lang
, O
туркменский B-Lang
, O
узбекский B-Lang
, O
украинский B-Lang
, O
урду B-Lang
, O
финский B-Lang
, O
французский B-Lang
, O
хинди B-Lang
, O
чувашский B-Lang
, O
шведский B-Lang
, O
якутский B-Lang
, O
японский B-Lang
. O

# text =   В 2020 году «Сбер» представил русскоязычную версию нейросети GPT-3, именно она используется в двух виртуальных ассистентах семейства «Салют» от «Сбера».
В O
2020 B-Date
году O
« O
Сбер B-Organization
» O
представил O
русскоязычную B-Lang
версию O
нейросети O
GPT-3 B-Model
, O
именно O
она O
используется O
в O
двух O
виртуальных O
ассистентах O
семейства O
« O
Салют B-Application
» O
от O
« O
Сбера B-Organization
» O
. O

# text =   Русскоязычная версия GPT-3, разработанная «Сбером», доступна на платформе SmartMarket.
Русскоязычная O
версия O
GPT-3 B-Model
, O
разработанная O
« O
Сбером B-Organization
» O
, O
доступна O
на O
платформе O
SmartMarket B-Application
. O

# text =   В ноябре 2021 года «Сбер» обучил нейросеть ruGPT-3 автоматически писать код и назвал эту функцию JARVIS.
В O
ноябре O
2021 B-Date
года O
« O
Сбер B-Organization
» O
обучил O
нейросеть O
ruGPT-3 B-Model
автоматически O
писать O
код O
и O
назвал O
эту O
функцию O
JARVIS B-Method
. O

# text =   Не заблокированы: Sber AI — на GitHub; ruDALL-E — на GitHub; Russian GPT-3 models — GitHub.
Не O
заблокированы O
: O
Sber B-Model
AI I-Model
— O
на O
GitHub B-InfoResource
; O
ruDALL B-Model
- I-Model
E I-Model
— O
на O
GitHub B-InfoResource
; O
Russian B-Model
GPT-3 I-Model
models I-Model
— O
GitHub B-InfoResource
. O

# text =   Заблокированы: большая часть ссылок на открытом портале Open Source от разработчиков «Сбера»; SberDevices; Sberbank AI Lab; Open source software developed by Sberbank-Technology.
Заблокированы O
: O
большая O
часть O
ссылок O
на O
открытом O
портале O
Open B-InfoResource
Source B-InfoResource
от O
разработчиков O
« O
Сбера B-Organization
» O
; O
SberDevices B-Application
; O
Sberbank B-Organization
AI I-Organization
Lab I-Organization
; O
Open B-Application
source I-Application
software I-Application
developed I-Application
by I-Application
Sberbank I-Application
- I-Application
Technology I-Application
. O

# text =   В данной статье мы будем использовать модель трансформера для бинарной классификации текста.
В O
данной O
статье O
мы O
будем O
использовать O
модель B-Model
трансформера I-Model
для O
бинарной B-Task
классификации I-Task
текста I-Task
. O

# text =   Самая простая и популярная связка – TF-IDF + линейная модель.
Самая O
простая O
и O
популярная O
связка O
– O
TF B-Metric
- I-Metric
IDF I-Metric
+ O
линейная B-Model
модель I-Model
. O

# text =   В случае с BERT можно (даже нужно) опустить препроцессинг и сразу перейти к токенизации и обучению.
В O
случае O
с O
BERT B-Model
можно O
( O
даже O
нужно O
) O
опустить O
препроцессинг B-Method
и O
сразу O
перейти O
к O
токенизации B-Method
и O
обучению O
. O

# text =   Необходимо обучить модель находить обращения с жалобой на сотрудника или другими словами – бинарная классификация.
Необходимо O
обучить O
модель O
находить O
обращения O
с O
жалобой O
на O
сотрудника O
или O
другими O
словами O
– O
бинарная B-Task
классификация I-Task
. O

# text =  Для решения описанной задачи используется модель от DeepPavlov rubert-base-cased-sentence.
Для O
решения O
описанной O
задачи O
используется O
модель O
от O
DeepPavlov B-Model
rubert I-Model
- I-Model
base I-Model
- I-Model
cased I-Model
- I-Model
sentence I-Model
. O

# text =   На выходе мы получаем метрику f1 = 0.91 Посмотрим, как модель классифицировала данные показанные в начале статьи.
На O
выходе O
мы O
получаем O
метрику O
f1 B-Metric
= O
0.91 B-Value
Посмотрим O
, O
как O
модель O
классифицировала O
данные O
показанные O
в O
начале O
статьи O
. O

# text =   Обученные модели можно найти на сайтах HuggingFace и DeepPavlov.
Обученные O
модели O
можно O
найти O
на O
сайтах O
HuggingFace B-InfoResource
и O
DeepPavlov B-InfoResource
. O

# text =   Соответственно, мы приходим к стандартной задаче Machine Learning (ML) – «многоклассовая классификация».
Соответственно O
, O
мы O
приходим O
к O
стандартной O
задаче O
Machine B-Science
Learning I-Science
( O
ML B-ShortName
) O
– O
« O
многоклассовая B-Task
классификация I-Task
» O
. O

# text =   В результате данного анализа решается задача — сбор сводной аналитики по организации.
В O
результате O
данного O
анализа O
решается O
задача O
— O
сбор B-Task
сводной I-Task
аналитики I-Task
по I-Task
организации I-Task
. O

# text =   В случае многоклассовой классификации число классов должно быть более 2 и может достигать даже многих тысяч.
В O
случае O
многоклассовой B-Task
классификации I-Task
число O
классов B-Object
должно O
быть O
более O
2 O
и O
может O
достигать O
даже O
многих O
тысяч O
. O

# text =   Во-вторых, такого разброса тематик, связанных с техническими текстами, у нас еще не было: нейросети, переиспользование контента, автоматическое тестирование документации, встраивание текста в интерфейс, мастерство технических коммуникаций, построение процессов перевода и принципы написания документов с расчетом на их последующую локализацию.
Во O
- O
вторых O
, O
такого O
разброса O
тематик O
, O
связанных O
с O
техническими B-Object
текстами I-Object
, O
у O
нас O
еще O
не O
было O
: O
нейросети O
, O
переиспользование B-Task
контента I-Task
, O
автоматическое B-Task
тестирование I-Task
документации I-Task
, O
встраивание B-Task
текста I-Task
в I-Task
интерфейс I-Task
, O
мастерство B-Task
технических I-Task
коммуникаций I-Task
, O
построение B-Task
процессов I-Task
перевода I-Task
и O
принципы B-Task
написания I-Task
документов I-Task
с O
расчетом O
на O
их O
последующую O
локализацию O
. O

# text =   Зачастую суммаризация предполагает работу с большими генеративными текстовыми моделями, куда надо «положить» все отзывы.
Зачастую O
суммаризация B-Task
предполагает O
работу O
с O
большими O
генеративными B-Object
текстовыми I-Object
моделями I-Object
, O
куда O
надо O
« O
положить O
» O
все O
отзывы O
. O

# text =   То есть какие аспекты искать; выделять эти аспекты в отзывах; оценивать тональность высказываний.
То O
есть O
какие O
аспекты O
искать O
; O
выделять O
эти O
аспекты O
в O
отзывах O
; O
оценивать O
тональность B-Subject
высказываний I-Subject
. O

# text =   Мы начали со внутреннего инструмента Яндекса — библиотеки регулярных выражений под названием Remorph.
Мы O
начали O
со O
внутреннего O
инструмента O
Яндекса B-Organization
— O
библиотеки B-Application
регулярных I-Application
выражений I-Application
под O
названием O
Remorph B-Application
. O


# text =   Исследователи Массачусетского технологического университета разработали систему искусственного интеллекта, которая способна переписывать устаревшие предложения в статьях «Википедии».
Исследователи O
Массачусетского B-Organization
технологического I-Organization
университета I-Organization
разработали O
систему B-Application
искусственного I-Application
интеллекта I-Application
, O
которая O
способна O
переписывать O
устаревшие O
предложения O
в O
статьях O
« O
Википедии B-InfoResource
» O
. O

# text =   Расширение статей, серьезные переписывания или другие рутинные изменения, такие как обновление номеров, дат, имен и местоположений в настоящее время добровольно выполняются пользователями из разных стран.
Расширение B-Task
статей I-Task
, O
серьезные O
переписывания O
или O
другие O
рутинные O
изменения O
, O
такие O
как O
обновление O
номеров O
, O
дат O
, O
имен O
и O
местоположений O
в O
настоящее O
время O
добровольно O
выполняются O
пользователями O
из O
разных O
стран O
. O

# text =   Если она видит какие-либо противоречия между этими двумя высказываниями, то использует «маску нейтральности», чтобы определить те противоречивые слова, которые нужно удалить, и те, которые обязательно нужно сохранить.
Если O
она O
видит O
какие O
- O
либо O
противоречия O
между O
этими O
двумя O
высказываниями O
, O
то O
использует O
« O
маску B-Object
нейтральности I-Object
» O
, O
чтобы O
определить O
те O
противоречивые O
слова O
, O
которые O
нужно O
удалить O
, O
и O
те O
, O
которые O
обязательно O
нужно O
сохранить O
. O

# text =   Отмечается, что систему также можно использовать для дополнения наборов данных, предназначенных для обучения детекторов фейкньюс, что потенциально снижает предвзятость и повышает точность информации.
Отмечается O
, O
что O
систему O
также O
можно O
использовать O
для O
дополнения B-Task
наборов I-Task
данных I-Task
, O
предназначенных O
для O
обучения B-Task
детекторов I-Task
фейкньюс I-Task
, O
что O
потенциально O
снижает O
предвзятость O
и O
повышает O
точность O
информации O
. O

# text =  Наше выработанное решение – обучить нейронную сеть, которая способна по тексту обращения автоматически распознавать заранее ранжированные по классам проблемы, извлекать сущность (номер заказа и телефон клиента) и по определённым классам сделать автоматизацию решения.
Наше O
выработанное O
решение O
– O
обучить O
нейронную B-Method
сеть I-Method
, O
которая O
способна O
по O
тексту O
обращения O
автоматически B-Task
распознавать I-Task
заранее I-Task
ранжированные I-Task
по I-Task
классам I-Task
проблемы I-Task
, O
извлекать O
сущность O
( O
номер O
заказа O
и O
телефон O
клиента O
) O
и O
по O
определённым O
классам O
сделать O
автоматизацию O
решения O
. O

# text =   На самом деле уже существуют продвинутые и проверенные методы ее обработки, использующие нейронные сети, с распознаванием смысла и контекста – BERT (Bidirectional Encoder Representations from Transformers).
На O
самом O
деле O
уже O
существуют O
продвинутые O
и O
проверенные O
методы O
ее O
обработки O
, O
использующие O
нейронные B-Method
сети I-Method
, O
с O
распознаванием O
смысла O
и O
контекста O
– O
BERT B-ShortName
( O
Bidirectional B-Method
Encoder I-Method
Representations I-Method
from I-Method
Transformers I-Method
) O
. O

# text =   Перед тем как выбрать нейронные сети, мы протестировали несколько более стандартных архитектур, случайные леса и бустинг.
Архитектура O
модели B-Object
Перед O
тем O
как O
выбрать O
нейронные B-Method
сети I-Method
, O
мы O
протестировали O
несколько O
более O
стандартных O
архитектур O
, O
случайные B-Method
леса I-Method
и O
бустинг B-Method
. O

# text =   Эта модель была обучена на огромном корпусе русскоязычного текста с двумя задачами – предсказать замаскированное слово в предложениях и предсказать, если одно из предложений следует по смыслу за вторым.
Эта O
модель O
была O
обучена O
на O
огромном O
корпусе O
русскоязычного O
текста O
с O
двумя O
задачами O
– O
предсказать B-Task
замаскированное I-Task
слово I-Task
в I-Task
предложениях I-Task
и O
предсказать O
, O
если O
одно O
из O
предложений O
следует O
по O
смыслу O
за O
вторым O
. O

# text =   Наша задача – дообучить эту языковую модель для нашего приложения (одна модель для классификации и одна – для извлечения сущности).
Наша O
задача O
– O
дообучить O
эту O
языковую O
модель O
для O
нашего O
приложения O
( O
одна O
модель B-Object
для O
классификации B-Task
и O
одна O
– O
для O
извлечения B-Task
сущности I-Task
) O
. O

# text =   результат первой модели – точность 77%
результат O
первой O
модели O
– O
точность B-Metric
77% B-Value

# text =   Чтобы определить, какие ещё есть потенциальные классы, мы повели так называемое тематическое моделирование, используя несколько подходов: начиная от пробалистических моделей (латентное распределение Дирихле, ARTM) и всё те же нейронные сети (BERT).
Чтобы O
определить O
, O
какие O
ещё O
есть O
потенциальные O
классы O
, O
мы O
повели O
так O
называемое O
тематическое B-Method
моделирование I-Method
, O
используя O
несколько O
подходов O
: O
начиная O
от O
пробалистических B-Model
моделей B-Model
( O
латентное B-Method
распределение I-Method
Дирихле I-Method
, O
ARTM B-ShortName
) O
и O
всё O
те O
же O
нейронные B-Method
сети I-Method
( O
BERT B-Model
) O
. O

# text =   Теперь нам нужно было использовать некоторые технические способы, чтобы сделать максимально высоким качество модели, которая на новых классах давала точность 72%.
Теперь O
нам O
нужно O
было O
использовать O
некоторые O
технические O
способы O
, O
чтобы O
сделать O
максимально O
высоким O
качество O
модели O
, O
которая O
на O
новых O
классах O
давала O
точность B-Metric
72 B-Value
% I-Value
. O

# text =   Второе, мы стандартно провели экстенсивный тюнинг гиперпараметров и изменили нашу метрику с точности на F1, чтобы ставить больше акцента на точность по каждому классу, так как общая точность предвзято относится к доминирующим классам.
Второе O
, O
мы O
стандартно O
провели O
экстенсивный B-Method
тюнинг I-Method
гиперпараметров I-Method
и O
изменили O
нашу O
метрику O
с O
точности B-Metric
на O
F1 B-Metric
, O
чтобы O
ставить O
больше O
акцента O
на O
точность B-Metric
по O
каждому O
классу O
, O
так O
как O
общая O
точность B-Metric
предвзято O
относится O
к O
доминирующим O
классам O
. O

# text =   Изменение оптимизирующей метрики на F1 позволило алгоритму обучения дольше обучаться, так как почти на каждом этапе происходило улучшение по F1, когда метрика была точность, мы достигали плато гораздо быстрее.
Изменение O
оптимизирующей O
метрики O
на O
F1 B-Metric
позволило O
алгоритму O
обучения O
дольше O
обучаться O
, O
так O
как O
почти O
на O
каждом O
этапе O
происходило O
улучшение O
по O
F1 B-Metric
, O
когда O
метрика O
была O
точность B-Metric
, O
мы O
достигали O
плато O
гораздо O
быстрее O
. O

# text =   Изначально на этапе MVP (minimum viable product) мы применяли регулярные выражения для извлечения сущности.
Изначально O
на O
этапе O
MVP B-ShortName
( O
minimum B-Object
viable I-Object
product I-Object
) O
мы O
применяли O
регулярные B-Object
выражения I-Object
для O
извлечения B-Task
сущности I-Task
. O

# text =   Протестировав поведение модели на продовских данных, мы обнаружили, что точность извлечения была около 50%.
Протестировав O
поведение O
модели O
на O
продовских O
данных O
, O
мы O
обнаружили O
, O
что O
точность B-Metric
извлечения O
была O
около O
50 B-Value
% I-Value
. O

# text =   Мы поняли, что даже извлечение сущности зависит от контекста, и решили использовать BERT.
Мы O
поняли O
, O
что O
даже O
извлечение B-Task
сущности I-Task
зависит O
от O
контекста O
, O
и O
решили O
использовать O
BERT B-Model
. O

# text =   На вход необходимо представить размеченные данные с маркировкой BIO (beginning, intermediate, O – пустота).
На O
вход O
необходимо O
представить O
размеченные O
данные O
с O
маркировкой B-Object
BIO B-ShortName
( O
beginning O
, O
intermediate O
, O
O O
– O
пустота O
) O
. O

# text =   Мы производили разметку 800 обращений на DataTurcks: Точность подхода BERT – 94% на этапе обучения, она валидирована на тестовых данных.
Мы O
производили O
разметку B-Method
800 O
обращений O
на O
DataTurcks B-Dataset
: O
Точность B-Metric
подхода O
BERT O
– O
94 B-Value
% I-Value
на O
этапе O
обучения O
, O
она O
валидирована O
на O
тестовых O
данных O
. O

# text =   Это постобработка увеличила точность до 98%.
Это O
постобработка O
увеличила O
точность B-Metric
до O
98% B-Value

# text =   В иностранной литературе можно встретить термин Continuous Learning (CL), который объединяет различные методы использования новых данных для поддержания эффективности моделей.
В O
иностранной O
литературе O
можно O
встретить O
термин O
Continuous B-Method
Learning I-Method
( O
CL B-ShortName
) O
, O
который O
объединяет O
различные O
методы O
использования O
новых O
данных O
для O
поддержания O
эффективности O
моделей O
. O

# text =   Методы CL были положены в основу пайплайна переобучения.
Методы O
CL B-ShortName
были O
положены O
в O
основу O
пайплайна O
переобучения O
. O

# text =   Для решения этой проблемы требуется архитектура, которая позволяет GPT-3 анализировать содержание письма и оценивать, какая информация актуальна для ответа.
Для O
решения O
этой O
проблемы O
требуется O
архитектура O
, O
которая O
позволяет O
GPT-3 B-Model
анализировать B-Task
содержание I-Task
письма I-Task
и O
оценивать B-Task
, I-Task
какая I-Task
информация I-Task
актуальна I-Task
для O
ответа O
. O

# text =   OpenAI представила модель машинного обучения GPT-3, обученную на 175 млрд параметров, в июне 2020 года.
OpenAI B-Organization
представила O
модель O
машинного O
обучения O
GPT-3 B-Model
, O
обученную O
на O
175 O
млрд O
параметров O
, O
в O
июне B-Date
2020 I-Date
года I-Date
. O

# text =   В отличие от предшественников GPT-2 и GPT-1 ее исходный код или обучающий набор данных решили не открывать.
В O
отличие O
от O
предшественников O
GPT-2 B-Model
и O
GPT-1 B-Model
ее O
исходный O
код O
или O
обучающий O
набор O
данных O
решили O
не O
открывать O
. O

# text =   Модель уже попытались применить в медицинской сфере для общения с пациентами, но результаты эксперимента оказались неутешительными.
Модель O
уже O
попытались O
применить O
в O
медицинской B-Science
сфере I-Science
для O
общения O
с O
пациентами O
, O
но O
результаты O
эксперимента O
оказались O
неутешительными O
. O

# text =  Между тем создатели проекта GPT-Neo от EleutherAI решили воссоздать аналог GPT-3, но с открытым исходным кодом.
Между O
тем O
создатели O
проекта O
GPT B-Project
- B-Project
Neo B-Project
от O
EleutherAI B-Organization
решили O
воссоздать O
аналог O
GPT-3 B-Model
, O
но O
с O
открытым O
исходным O
кодом O
. O

# text =   Однако только сейчас мы немного приблизились к сюжетам фантастических фильмов: можем попросить Алису убавить громкость, Google Assistant — заказать такси или Siri — завести будильник.
Однако O
только O
сейчас O
мы O
немного O
приблизились O
к O
сюжетам O
фантастических O
фильмов O
: O
можем O
попросить O
Алису B-Application
убавить B-Task
громкость I-Task
, O
Google B-Application
Assistant I-Application
— O
заказать B-Task
такси I-Task
или O
Siri B-Application
— O
завести B-Task
будильник I-Task
. O

# text =   Технологии языкового процессинга востребованы в разработках, связанных с построением искусственного интеллекта: в поисковых системах, для извлечения фактов, оценки тональности текста, машинного перевода и диалога.
Технологии B-Method
языкового I-Method
процессинга I-Method
востребованы O
в O
разработках O
, O
связанных O
с O
построением O
искусственного O
интеллекта O
: O
в O
поисковых B-Application
системах I-Application
, O
для O
извлечения B-Task
фактов I-Task
, O
оценки B-Task
тональности I-Task
текста I-Task
, O
машинного B-Science
перевода I-Science
и O
диалога O
. O

# text =   Первые разговоры об обработке естественного языка компьютером начались еще в 30-е годы XX-го века с философских рассуждений Айера — он предлагал отличать разумного человека от глупой машины с помощью эмпирического теста.
Первые O
разговоры O
об O
обработке B-Task
естественного I-Task
языка I-Task
компьютером O
начались O
еще O
в O
30-е B-Date
годы I-Date
XX I-Date
- I-Date
го I-Date
века I-Date
с O
философских O
рассуждений O
Айера B-Person
— O
он O
предлагал O
отличать O
разумного O
человека O
от O
глупой O
машины O
с O
помощью O
эмпирического O
теста O
. O

# text =   В 1950 году Алан Тьюринг в философском журнале Mind предложил такой тест, где судья должен определить, с кем он ведет диалог: с человеком или компьютером.
В O
1950 B-Date
году O
Алан B-Person
Тьюринг I-Person
в O
философском O
журнале O
Mind B-Publication
предложил O
такой O
тест B-Object
, O
где O
судья O
должен O
определить O
, O
с O
кем O
он O
ведет O
диалог O
: O
с O
человеком O
или O
компьютером O
. O

# text =   В 1954 году Джорджтаунский университет совместно с компанией IBM продемонстрировали программу машинного перевода с русского на английский, которая работала на базе словаря из 250 слов и набора из 6 грамматических правил.
В O
1954 B-Date
году O
Джорджтаунский B-Organization
университет I-Organization
совместно O
с O
компанией O
IBM B-TERM
продемонстрировали O
программу B-Application
машинного I-Application
перевода I-Application
с O
русского B-Lang
на O
английский B-Lang
, O
которая O
работала O
на O
базе O
словаря O
из O
250 O
слов O
и O
набора O
из O
6 O
грамматических O
правил O
. O

# text =   Параллельно с попытками научить компьютер переводить текст, ученые и целые университеты думали над созданием робота, способного имитировать речевое поведение человека.
Параллельно O
с O
попытками O
научить O
компьютер O
переводить O
текст O
, O
ученые O
и O
целые O
университеты O
думали O
над O
созданием O
робота O
, O
способного O
имитировать B-Task
речевое I-Task
поведение I-Task
человека I-Task
. O

# text =   Первой успешной реализацией чат-бота стал виртуальный собеседник ELIZA, написанный в 1966 году Джозефом Вейценбаумом.
Первой O
успешной O
реализацией O
чат O
- O
бота O
стал O
виртуальный B-Result
собеседник I-Result
ELIZA B-Result
, O
написанный O
в O
1966 B-Date
году I-Date
Джозефом B-Person
Вейценбаумом I-Person
. O

# text =   Элиза пародировала поведение психотерапевта, выделяя значимые слова из фразы собеседника и задавая встречный вопрос.
Элиза B-Result
пародировала O
поведение O
психотерапевта O
, O
выделяя O
значимые O
слова O
из O
фразы O
собеседника O
и O
задавая O
встречный O
вопрос O
. O

# text =   Можно считать, что это был первый чат-бот, построенный на правилах (rule-based bot), и он положил начало целому классу таких систем.
Можно O
считать O
, O
что O
это O
был O
первый O
чат O
- O
бот O
, O
построенный O
на O
правилах O
( O
rule B-Application
- I-Application
based I-Application
bot I-Application
) O
, O
и O
он O
положил O
начало O
целому O
классу O
таких O
систем O
. O

# text =   Без Элизы не появились бы такие программы-собеседники, как Cleverbot, WeChat Xiaoice, Eugene Goostman — формально прошедший тест Тьюринга в 2014 году, — и даже Siri, Jarvis и Alexa.
Без O
Элизы B-Result
не O
появились O
бы O
такие O
программы O
- O
собеседники O
, O
как O
Cleverbot B-Result
, O
WeChat B-Result
Xiaoice I-Result
, O
Eugene B-Result
Goostman I-Result
— O
формально O
прошедший O
тест B-Method
Тьюринга I-Method
в O
2014 B-Date
году O
, O
— O
и O
даже O
Siri B-Application
, O
Jarvis B-Application
и O
Alexa B-Application
. O

# text =   В 1968 году Терри Виноградом на языке LISP была разработана программа SHRDLU.
В O
1968 B-Date
году O
Терри B-Person
Виноградом I-Person
на O
языке O
LISP B-Lang
была O
разработана O
программа O
SHRDLU B-Result
. O

# text =   Следующим шагом в развитии чат-ботов стала программа A.L.I.C.E., для которой Ричард Уоллес разработал специальный язык разметки — AIML (англ. Artificial Intelligence Markup Language).
Следующим O
шагом O
в O
развитии O
чат O
- O
ботов O
стала O
программа O
A.L.I.C.E. B-Application
, O
для O
которой O
Ричард B-Person
Уоллес I-Person
разработал O
специальный O
язык O
разметки B-Method
— O
AIML B-ShortName
( O
англ O
. O
Artificial B-Lang
Intelligence I-Lang
Markup I-Lang
Language I-Lang
) O
. O

# text =   Разговоры о нейронных сетях и глубоком обучении ходили уже в 90-е годы, а первый нейрокомпьютер «Марк-1» появился вообще в 1958 году.
Разговоры O
о O
нейронных B-Method
сетях I-Method
и O
глубоком O
обучении O
ходили O
уже O
в O
90-е O
годы O
, O
а O
первый O
нейрокомпьютер B-Object
« O
Марк-1 B-Application
» O
появился O
вообще O
в O
1958 B-Date
году I-Date
. O

# text =   1970 г. Машинный перевод на основе правил (англ. RBMT) был первой попыткой научить машину переводить.
1970 O
г. O
Машинный B-Science
перевод I-Science
на I-Science
основе I-Science
правил I-Science
( O
англ O
RBMT B-ShortName
. O
) O
был O
первой O
попыткой O
научить O
машину O
переводить O
. O

# text =   1984 г. Машинный перевод на основе примеров (англ. EBMT) был способен переводить даже совсем не похожие друг на друга языки, где задавать какие-то правила было бесполезно.
1984 B-TERM
г. I-TERM
Машинный B-Science
перевод I-Science
на I-Science
основе I-Science
примеров I-Science
( O
англ O
. O
EBMT B-ShortName
) O
был O
способен O
переводить O
даже O
совсем O
не O
похожие O
друг O
на O
друга O
языки O
, O
где O
задавать O
какие O
- O
то O
правила O
было O
бесполезно O
. O

# text =   1990 г. Статистический машинный перевод (англ. SMT) в эпоху развития интернета позволил использовать не только готовые языковые корпуса, но даже книги и вольно переведенные статьи.
1990 B-Date
г. O
Статистический B-Science
машинный I-Science
перевод I-Science
( O
англ I-TERM
SMT B-ShortName
) O
в O
эпоху O
развития O
интернета O
позволил O
использовать O
не O
только O
готовые O
языковые B-Object
корпуса I-Object
, O
но O
даже O
книги O
и O
вольно O
переведенные O
статьи O
. O

# text =   Статистические методы и сейчас активно используются в языковом процессинге.
Статистические B-Method
методы I-Method
и O
сейчас O
активно O
используются O
в O
языковом B-Method
процессинге I-Method
. O

# text =   По мере развития обработки естественного языка множество задач решалось классическими статистическими методами и множеством правил, однако проблему нечеткости и неоднозначности в языке это не решало.
По O
мере O
развития O
обработки B-Task
естественного I-Task
языка I-Task
множество O
задач O
решалось O
классическими O
статистическими B-Method
методами I-Method
и O
множеством O
правил O
, O
однако O
проблему B-Task
нечеткости I-Task
и I-Task
неоднозначности I-Task
в O
языке O
это O
не O
решало O
. O

# text =   Так родился статистический метод анализа текста word2vec (англ. Word to vector).
Так O
родился O
статистический B-Method
метод I-Method
анализа I-Method
текста I-Method
word2vec B-Result
( O
англ O
. O
Word B-Result
to I-Result
vector I-Result
) O
. O

# text =   Под эти критерии отлично подходит рекуррентная нейронная сеть (RNN), однако по мере увеличения расстояния между связанными частями текста необходимо увеличивать и размер RNN, из-за чего падает качество обработки информации.
Под O
эти O
критерии O
отлично O
подходит O
рекуррентная B-Network
нейронная I-Network
сеть I-Network
( O
RNN B-ShortName
) O
, O
однако O
по O
мере O
увеличения O
расстояния O
между O
связанными O
частями O
текста O
необходимо O
увеличивать O
и O
размер O
RNN B-ShortName
, O
из O
- O
за O
чего O
падает O
качество O
обработки O
информации O
. O

# text =   Эту проблему решает сеть LSTM (англ. Long short-term memory).
Эту O
проблему O
решает O
сеть O
LSTM B-ShortName
( O
англ O
Long B-Method
short I-Method
- I-Method
term I-Method
memory I-Method
) O
. O

# text =   Если говорить о языке Python, который часто используется для анализа данных, то это NLTK и Spacy.
Если O
говорить O
о O
языке O
Python B-Environment
, O
который O
часто O
используется O
для O
анализа B-Method
данных I-Method
, O
то O
это O
NLTK B-Application
и O
Spacy B-Application
. O

# text =   Крупные компании также принимают участие в разработке библиотек для NLP, как например NLP Architect от Intel или PyTorch от исследователей из Facebook и Uber.
Крупные O
компании O
также O
принимают O
участие O
в O
разработке O
библиотек O
для O
NLP B-ShortName
, O
как O
например O
NLP B-Application
Architect I-Application
от O
Intel B-Organization
или O
PyTorch B-Application
от O
исследователей O
из O
Facebook B-Organization
и O
Uber B-Organization
. O

# text =   Направление b2c не единственное, где можно применять чат-ботов.
Направление O
b2c B-ShortName
не O
единственное O
, O
где O
можно O
применять O
чат O
- O
ботов O
. O

# text =   Участникам предлагалось определить потенциальные заболевания коров по реальным жалобам людей из открытых источников, а также научиться выделять из текстов симптомы заболеваний (NER - Named Entity Recognition).
Участникам O
предлагалось O
определить O
потенциальные O
заболевания O
коров O
по O
реальным O
жалобам O
людей O
из O
открытых O
источников O
, O
а O
также O
научиться O
выделять O
из O
текстов O
симптомы O
заболеваний O
( O
NER B-ShortName
- O
Named B-Science
Entity I-Science
Recognition I-Science
) O
. O

# text =   Эта статья будет интересна не только тем, кто специализируется в NLP (Natural Language Processing), но и начинающим исследователям данных.
Эта O
статья O
будет O
интересна O
не O
только O
тем O
, O
кто O
специализируется O
в O
NLP B-ShortName
( O
Natural B-Science
Language I-Science
Processing I-Science
) O
, O
но O
и O
начинающим O
исследователям O
данных O
. O

# text =   Спаны - это участки текста, которые содержат в себе определенный смысл.
Спаны B-Object
  O
- O
  O
это O
участки O
текста B-Object
, O
которые O
содержат O
в O
себе O
определенный O
смысл O
. O

# text =   Программа для разметки YEDDA и процесс разметки.
Программа O
для O
разметки B-Method
YEDDA B-Application
и O
процесс O
разметки O
. O

# text =   Так как задача является составной, то и метрика состояла из двух компонентов с весом 0.8 для задачи классификации и 0.2 для задачи NER.
Так O
как O
задача O
является O
составной O
, O
то O
и O
метрика O
состояла O
из O
двух O
компонентов O
с O
весом B-Metric
0.8 B-Value
для O
задачи O
классификации B-Task
и O
0.2 B-Value
для O
задачи O
NER B-Science
. O

# text =   В задаче классификации использовался logloss, вычисляемый как среднее значение метрики sklearn.metrics.log_loss по классам болезней.
В O
задаче O
классификации B-Task
использовался O
logloss B-Method
, O
вычисляемый O
как O
среднее O
значение O
метрики O
sklearn.metrics.log_loss B-Metric
по O
классам O
болезней O
. O

# text =  В задаче NER использовался span-based F1-score, рассчитываемый следующим образом: для каждого текста берутся предсказанные индексы начала и конца размеченных признаков болезни, по ним выделяются из текста токены (отдельные слова, разделенные пробелом) и сравниваются с истинной (экспертной) разметкой.
В O
задаче O
NER B-ShortName
использовался O
span B-Metric
- I-Metric
based I-Metric
F1-score I-Metric
, O
рассчитываемый O
следующим O
образом O
: O
для O
каждого O
текста B-Object
берутся O
предсказанные O
индексы B-Object
начала O
и O
конца O
размеченных O
признаков O
болезни O
, O
по O
ним O
выделяются O
из O
текста O
токены B-Subject
( O
отдельные O
слова B-Subject
, O
разделенные O
пробелом O
) O
и O
сравниваются O
с O
истинной O
( O
экспертной O
) O
разметкой B-Method
. O

# text =   Код для подсчета метрики span-based F1-score.
Код O
для O
подсчета O
метрики O
span B-Metric
- I-Metric
based I-Metric
F1-score I-Metric

# text =   Этим решением стало использование классификатора CatBoost, который прямо из коробки может обрабатывать текстовые фичи.
Этим O
решением O
стало O
использование O
классификатора O
CatBoost B-Method
, O
который O
прямо O
из O
коробки O
может O
обрабатывать O
текстовые O
фичи O
. O

# text =   Решение для задачи распознавания симптомов мы давать не стали, чтобы участники Data Science чемпионата могли покреативить.
Решение O
для O
задачи B-Task
распознавания I-Task
симптомов I-Task
мы O
давать O
не O
стали O
, O
чтобы O
участники O
Data B-Science
Science I-Science
чемпионата O
могли O
покреативить O
. O

# text =   Во-первых, конкретно для этого соревнования наиболее эффективный подход - это доразметка спанов тренировочных данных для задачи NER.
Во O
- O
первых O
, O
конкретно O
для O
этого O
соревнования O
наиболее O
эффективный O
подход O
- O
это O
доразметка B-Method
спанов I-Method
тренировочных I-Method
данных I-Method
для O
задачи O
NER B-Task
. O

# text =   Во-вторых, участники использовали базовые подходы для NLP-задач: удаление стоп-слов и знаков пунктуации, приведение к нижнему регистру, стемминг и лемматизация.
Во O
- O
вторых O
, O
участники O
использовали O
базовые O
подходы O
для O
NLP B-ShortName
- O
задач O
: O
удаление O
стоп O
- O
слов O
и O
знаков O
пунктуации O
, O
приведение O
к O
нижнему O
регистру O
, O
стемминг B-Method
и O
лемматизация B-Method
. O

# text =  Более же продвинутым подходом является аугментация данных.
Более O
же O
продвинутым O
подходом O
является O
аугментация B-Method
данных I-Method
. O

# text =   Один из возможных способов аугментации текста - перифраз текста.
Один O
из O
возможных O
способов O
аугментации B-Task
текста I-Task
- O
перифраз B-Method
текста I-Method
. O

# text =   Примером данного решения является использование парафрайзера на основе “rut5-base-paraphraser” из библиотеки huggingface.
Примером O
данного O
решения O
является O
использование O
парафрайзера B-Object
на O
основе O
“ O
rut5-base B-Model
- I-Model
paraphraser I-Model
” O
из O
библиотеки O
huggingface B-Application
. O

# text =   Реализуется данный метод аналогично с предыдущим, как модель можно использовать “LaBSE-en-ru”.
Реализуется O
данный O
метод O
аналогично O
с O
предыдущим O
, O
как O
модель O
можно O
использовать O
“ O
LaBSE B-Model
- I-Model
en I-Model
- I-Model
ru I-Model
” O
. O

# text =   Сначала решается задача выделения симптомов (NER), после чего в текстах убираются все слова, не являющиеся симптомами.
Сначала O
решается O
задача B-Task
выделения I-Task
симптомов I-Task
( O
NER B-ShortName
) O
, O
после O
чего O
в O
текстах B-Object
убираются O
все O
слова B-Subject
, O
не O
являющиеся O
симптомами O
. O

# text =   Базовым вариантом эмбеддингов является TF-IDF, который зависит от частоты употребления слова в документе.
Базовым O
вариантом O
эмбеддингов B-TERM
является O
TF B-Metric
- I-Metric
IDF I-Metric
, O
который O
зависит O
от O
частоты B-Metric
употребления I-Metric
слова I-Metric
в O
документе O
. O

# text =   И чтобы его улучшить, можно использовать эмбеддинги предобученных моделей, таких как Word2Vec, FastText и тд.
И O
чтобы O
его O
улучшить O
, O
можно O
использовать O
эмбеддинги B-Object
предобученных I-Object
моделей I-Object
, O
таких O
как O
Word2Vec B-Application
, O
FastText B-Application
и O
тд O
. O

# text =   В частности, в одном из лучших решений использовался необычный FastText, предобученный на корпусе текстов RuDReC, который содержит отзывы потребителей на русском языке о фармацевтической продукции.
В O
частности O
, O
в O
одном O
из O
лучших O
решений O
использовался O
необычный O
FastText B-Model
, O
предобученный O
на O
корпусе O
текстов O
RuDReC B-Dataset
, O
который O
содержит O
отзывы O
потребителей O
на O
русском B-Lang
языке O
о O
фармацевтической O
продукции O
. O

# text =   Напомним, что алгоритм работы с трансформерами можно представить следующим образом: сначала тексты преобразовываются токенизатором, далее обучается модель-трансформер.
Напомним O
, O
что O
алгоритм O
работы O
с O
трансформерами B-Object
можно O
представить O
следующим O
образом O
: O
сначала O
тексты O
преобразовываются O
токенизатором B-Method
, O
далее O
обучается O
модель B-Model
- I-Model
трансформер I-Model
. O

# text =   Если же говорить о выборе моделей, то наилучшие результаты были получены следующими из них: RuBERT-base, RuBERT-large, LaBSE-en-ru.
Если O
же O
говорить O
о O
выборе O
моделей O
, O
то O
наилучшие O
результаты O
были O
получены O
следующими O
из O
них O
: O
RuBERT B-Model
- I-Model
base I-Model
, O
RuBERT B-Model
- I-Model
large I-Model
, O
LaBSE B-Model
- I-Model
en I-Model
- I-Model
ru I-Model
. O

# text =   Предположим, что вы и так слышали о моделях семейства BERT (в предыдущей статье мы описывали, как применяем BERT в других задачах), а вот LaBSE - выбор совершенно неочевидный.
Предположим O
, O
что O
вы O
и O
так O
слышали O
о O
моделях O
семейства O
BERT B-Model
( O
в O
предыдущей O
статье O
мы O
описывали O
, O
как O
применяем O
BERT B-Model
в O
других O
задачах O
) O
, O
а O
вот O
LaBSE B-Model
- O
выбор O
совершенно O
неочевидный O
. O

# text = Далее слова в тестовом наборе текстов также приводятся к векторам и сравниваются со словами из тренировочной разметки при помощи косинусной близости.
Далее O
слова O
в O
тестовом O
наборе O
текстов O
также O
приводятся O
к O
векторам O
и O
сравниваются O
со O
словами O
из O
тренировочной O
разметки O
при O
помощи O
косинусной B-Metric
близости I-Metric
. O

# text =   Архитектура в свою очередь может содержать LSTM, BiLSTM, RNN или GRU слои.
Архитектура O
в O
свою O
очередь O
может O
содержать O
LSTM B-ShortName
, O
BiLSTM B-ShortName
, O
RNN B-ShortName
или O
GRU B-ShortName
слои O
. O

# text =   Из интересных решений один из участников представил BiLSTM-сеть с CRF слоем.
Из O
интересных O
решений O
один O
из O
участников O
представил O
BiLSTM B-Model
- I-Model
сеть I-Model
с O
CRF B-Model
слоем O
. O

# text =   Используются те же модели, поэтому расскажем о различии в подготовке данных для моделей.Для задачи NER тексты преобразовываются с помощью токенизатора и теггинга.
Используются O
те O
же O
модели O
, O
поэтому O
расскажем O
о O
различии O
в O
подготовке O
данных O
для O
моделей O
. O
Для O
задачи O
NER B-Task
тексты O
преобразовываются O
с O
помощью O
токенизатора B-Method
и O
теггинга B-Method
. O

# text =   Сначала тексты при помощи токенизатора переводятся в вектора - это то, на чем обучается модель.
Сначала O
тексты O
при O
помощи O
токенизатора B-Method
переводятся O
в O
вектора B-Object
- O
это O
то O
, O
на O
чем O
обучается O
модель O
. O

# text =   Далее создаются таргеты при помощи теггинга.
Далее O
создаются O
таргеты B-Object
при O
помощи O
теггинга B-Method
. O

# text =   Самым распространенным алгоритмом теггинга является “Inside–outside–beginning”.
Самым O
распространенным O
алгоритмом O
теггинга O
является O
“ O
Inside B-Method
– I-Method
outside I-Method
– I-Method
beginning I-Method
” 
. O

# text =   Тег указывает на то, что слово находится внутри спана.
Тег B-Object
указывает O
на O
то O
, O
что O
слово O
находится O
внутри O
спана B-Object
. O

# text =   Среди решений были как кастомный код для обучения и инференса, так и код от huggingface, который можно использовать из коробки.
Среди O
решений O
были O
как O
кастомный O
код O
для O
обучения O
и O
инференса O
, O
так O
и O
код O
от O
huggingface B-Organization
, O
который O
можно O
использовать O
из O
коробки O
. O

# text =   Безусловно, основной метрикой оценивания являлся лидерборд.
Безусловно O
, O
основной O
метрикой O
оценивания O
являлся O
лидерборд B-Metric
. O

# text =   Для решения ситуации мы можем искусственно сгенерировать данные с помощью языка программирования.
Для O
решения O
ситуации O
мы O
можем O
искусственно O
сгенерировать B-Task
данные I-Task
с O
помощью O
языка O
программирования O
. O

# text =   Пересмотрев множество примеров и статей, была найдена англоязычная статья, в которой рассмотрены три самых интересных, в плане функциональности и простоты использования, способа генерации синтетических данных с помощью пакетов Python.
Пересмотрев O
множество O
примеров O
и O
статей O
, O
была O
найдена O
англоязычная O
статья O
, O
в O
которой O
  O
рассмотрены O
три O
самых O
интересных O
, O
в O
плане O
функциональности O
и O
простоты O
использования O
, O
способа O
генерации B-Task
синтетических I-Task
данных I-Task
с O
помощью O
пакетов O
  O
Python B-Environment
. O

# text =   Faker - это пакет Python, разработанный для упрощения генерации синтетических данных.
Faker B-Application
- O
это O
пакет O
Python B-Environment
, O
разработанный O
для O
упрощения O
генерации B-Task
синтетических I-Task
данных I-Task
. O

# text =   SDV или Synthetic Data Vault - это пакет Python для генерации синтетических данных на основе предоставленного набора данных.
SDV B-ShortName
или O
Synthetic B-Application
Data I-Application
Vault I-Application
- O
это O
пакет O
Python B-Environment
для O
генерации B-Task
синтетических I-Task
данных I-Task
на O
основе O
предоставленного O
набора O
данных O
. O

# text =   SDV генерирует данные, применяя математические методы и модели машинного обучения.
SDV B-ShortName
генерирует O
данные O
, O
применяя O
математические B-Method
методы I-Method
и O
модели B-Model
машинного I-Model
обучения I-Model
. O

# text =   С помощью SVD можно обработать данные, даже если они содержат несколько типов данных и отсутствующие значения.
С O
помощью O
SVD B-ShortName
можно O
обработать B-Task
данные I-Task
, O
даже O
если O
они O
содержат O
несколько O
типов O
данных O
и O
отсутствующие O
значения O
. O

# text =   Используем для этого одну из доступных моделей SVD Singular Table GaussianCopula.
Используем O
для O
этого O
одну O
из O
доступных O
моделей O
SVD B-ShortName
Singular B-Model
Table I-Model
GaussianCopula I-Model
. O

# text =   Воспользуемся функцией evaluate из SDV.
Воспользуемся O
функцией O
evaluate B-Method
из O
SDV B-ShortName
. O

# text =   Возьмем для примера статистические метрики (критерии Колмогорова–Смирнова и Хи-квадрат) и метрику обнаружения, основанную на классификаторе логистической регрессии.
Возьмем O
для O
примера O
статистические B-Metric
метрики I-Metric
( O
критерии B-Metric
Колмогорова I-Metric
– I-Metric
Смирнова I-Metric
и O
Хи B-Metric
- I-Metric
квадрат I-Metric
) O
и O
метрику B-Metric
обнаружения I-Metric
, O
основанную O
на O
классификаторе B-Method
логистической I-Method
регрессии I-Method
. O

# text =   KSTest используется для сравнения столбцов с непрерывными данными, а CSTest с дискретными данными.
KSTest B-Metric
используется O
для O
сравнения O
столбцов O
с O
непрерывными O
данными O
, O
а O
CSTest B-Metric
с O
дискретными O
данными O
. O

# text =   Метрика LogisticDetection при помощи машинного обучения позволяет оценить насколько сложно отличить синтетические данные от исходных.
Метрика O
LogisticDetection B-Metric
при O
помощи O
машинного O
обучения O
позволяет O
оценить O
насколько O
сложно O
отличить B-Task
синтетические I-Task
данные I-Task
от I-Task
исходных I-Task
. O

# text =   Gretel или Gretel Synthetics – это пакет Python с открытым исходным кодом, основанный на рекуррентной нейронной сети для создания структурированных и не структурированных данных.
Gretel B-Application
или O
Gretel B-Application
Synthetics I-Application
– O
это O
пакет O
Python B-Environment
с O
открытым O
исходным O
кодом O
, O
основанный O
на O
рекуррентной B-Network
нейронной I-Network
сети I-Network
для O
создания O
структурированных O
и O
не O
структурированных O
данных O
. O

# text =   Этот модуль работает непосредственно с датафреймами данных Pandas и позволяет автоматически разбивать датафрейм на более мелкие датафреймы (по кластерам столбцов), выполнять обучение модели и генерацию для каждого фрейма независимо.
Этот O
модуль O
работает O
  O
непосредственно O
с O
датафреймами O
данных O
Pandas B-Application
и O
позволяет O
автоматически O
разбивать O
датафрейм O
на O
более O
мелкие O
датафреймы O
( O
по O
кластерам O
столбцов O
) O
, O
выполнять O
обучение O
модели O
и O
генерацию O
для O
каждого O
фрейма O
независимо O
. O

# text =   Теперь с помощью пакета Gretel cгенерируем синтетические данные для Stroke Prediction Dataset и проанализируем их относительно данных полученных с помощью пакета SVD из пункта 2.
Теперь O
с O
помощью O
пакета O
Gretel B-Application
cгенерируем O
синтетические O
данные O
для O
Stroke B-Dataset
Prediction I-Dataset
Dataset I-Dataset
и O
проанализируем O
их O
относительно O
данных O
полученных O
с O
помощью O
пакета O
SVD B-ShortName
из O
пункта O
2 O
. O

# text =   Метрикой оценки качества является ROC-AUC.
Метрикой O
оценки O
качества O
является O
ROC B-Metric
- I-Metric
AUC I-Metric
. O

# text =   Разработанный подход для решения задачи кредитного скоринга в дальнейшем легко переносим и на прочие банковские задачи: модели склонности, оттока и дохода.
Разработанный O
подход O
для O
решения O
задачи B-Task
кредитного I-Task
скоринга I-Task
в O
дальнейшем O
легко O
переносим O
и O
на O
прочие O
банковские O
задачи O
: O
модели O
склонности O
, O
оттока O
и O
дохода O
. O

# text =   Токены, относящиеся к ФИО, мы выделяем с помощью клиентской базы и проверки с помощью библиотек для морфологического анализа.
Токены B-Subject
, O
относящиеся O
к O
ФИО O
, O
мы O
выделяем O
с O
помощью O
клиентской O
базы O
и O
проверки O
с O
помощью O
библиотек O
для O
морфологического B-Method
анализа I-Method
. O

# text =  Лемматизация оставшихся токенов.
Лемматизация B-Method
оставшихся O
токенов B-Subject
. O

# text =   Для этого корпуса мы обучили word2vec-модель, где для каждого токена выучили эмбеддинг размера 50.
Для O
этого O
корпуса O
мы O
обучили O
word2vec B-Model
- I-Model
модель B-Model
, O
где O
для O
каждого O
токена O
выучили O
эмбеддинг B-Object
размера O

# text =   Благодаря богатому набору данных бустинг индивидуально имеет приличное качество.
Благодаря O
богатому O
набору O
данных O
бустинг B-Method
индивидуально O
имеет O
приличное O
качество O
. O

# text =   Одной из первых практических задач было определение авторства политических текстов The Federalist Papers, написанных в США в 1780 годах.
Одной O
из O
первых O
практических O
задач O
было O
определение B-Task
авторства I-Task
политических O
текстов O
The O
Federalist O
Papers O
, O
написанных O
в O
США O
в O
1780 B-Date
годах I-Date
. O

# text =   Я рассмотрю простейший способ анализа с помощью несложных расчетов и пакета Natural Language Toolkit, что в совокупности с matplotlib позволяет получить интересные результаты буквально в несколько строк кода.
Я O
рассмотрю O
простейший O
способ O
анализа O
с O
помощью O
несложных O
расчетов O
и O
пакета O
Natural B-Application
Language I-Application
Toolkit I-Application
, O
что O
в O
совокупности O
с O
matplotlib B-Application
позволяет O
получить O
интересные O
результаты O
буквально O
в O
несколько O
строк O
кода O
. O

# text =   К этой группе относятся решения от крупнейших компаний: Amazon Machine Learning, Microsoft Azure Machine Learning и Microsoft Cognitive Services, Google Cloud Prediction API и Google Cloud Machine Learning, IBM Watson Cloud и AlchemyAPI, BigML и другие.
К O
этой O
группе O
относятся O
решения O
от O
крупнейших O
компаний O
: O
Amazon B-Application
Machine I-Application
Learning I-Application
, O
Microsoft B-Application
Azure I-Application
Machine I-Application
Learning I-Application
и O
Microsoft B-Application
Cognitive I-Application
Services I-Application
, O
Google B-Application
Cloud I-Application
Prediction I-Application
API I-Application
и O
Google B-Application
Cloud I-Application
Machine I-Application
Learning I-Application
, O
IBM B-Application
Watson I-Application
Cloud I-Application
и O
AlchemyAPI B-Application
, O
BigML B-Application
и O
другие O
. O

# text =   Возможности этого сервиса в области анализа речи и естественного языка пока ограничиваются английским языком, однако многие другие сервисы поддерживают русский язык, например, полностью бесплатный wit.ai, приобретённый Facebook, и его российский конкурент api.ai (понимание текстовых и голосовых команд и вопросов на естественных языках, преобразование речи в текст), IBM AlchemyAPI (анализ тональности текста, выявление сущностей и ключевых слов), Google Natural Language API (классификация текстов, графы связей, извлечение информации из текстов, анализ тональности, намерений, извлечение инсайтов; поддерживает русский язык с помощью технологии машинного перевода Google Translate, использует глубокое обучение и word2vec).
Возможности O
этого O
сервиса O
в O
области O
анализа B-Task
речи I-Task
и O
естественного O
языка O
пока O
ограничиваются O
английским O
языком O
, O
однако O
многие O
другие O
сервисы O
поддерживают O
русский O
язык O
, O
например O
, O
полностью O
бесплатный O
wit.ai B-Application
, O
приобретённый O
Facebook B-Organization
, O
и O
его O
российский O
конкурент O
api.ai B-Application
( O
понимание B-Task
текстовых I-Task
и I-Task
голосовых I-Task
команд I-Task
и I-Task
вопросов I-Task
на O
естественных O
языках O
, O
преобразование B-Task
речи I-Task
в I-Task
текст I-Task
) O
, O
IBM B-Application
AlchemyAPI I-Application
( O
анализ B-Task
тональности I-Task
текста I-Task
, O
выявление B-Task
сущностей I-Task
и O
ключевых B-Subject
слов I-Subject
) O
, O
Google B-Application
Natural I-Application
Language I-Application
API I-TERM
( O
классификация B-Task
текстов O
, O
графы B-Object
связей I-Object
, O
извлечение B-Task
информации I-Task
из I-Task
текстов I-Task
, O
анализ B-Task
тональности I-Task
, O
намерений O
, O
извлечение O
инсайтов O
; O
поддерживает O
русский O
язык O
с O
помощью O
технологии O
машинного B-Science
перевода I-Science
Google B-Application
Translate I-Application
, O
использует O
глубокое O
обучение O
и O
word2vec B-Application
) O
. O

# text =   Например, IBM Watson предлагает инструмент Personality Insights, позволяющий определять черты личности человека, его потребности и ценности, намерения и другие характеристики по его записям в Твиттере, социальных сетях или по другим текстовым источникам.
Например O
, O
IBM B-Application
Watson I-Application
предлагает O
инструмент O
Personality B-Application
Insights I-Application
, O
позволяющий O
определять O
черты O
личности O
человека O
, O
его O
потребности O
и O
ценности O
, O
намерения O
и O
другие O
характеристики O
по O
его O
записям O
в O
Твиттере B-Application
, O
социальных O
сетях O
или O
по O
другим O
текстовым O
источникам O
. O

# text =   Например, Diffbot позволяет автоматически сканировать страницы сайтов, извлекать из них нужную информацию: тексты, изображения, видео, информацию о продуктах, комментарии и др., в очищенном в структурированном виде, а также позволяет классифицировать страницы.
Например O
, O
Diffbot B-Application
позволяет O
автоматически O
сканировать B-Task
страницы I-Task
сайтов I-Task
, O
извлекать O
из O
них O
нужную O
информацию O
: O
тексты O
, O
изображения O
, O
видео O
, O
информацию O
о O
продуктах O
, O
комментарии O
, O
в O
очищенном O
в O
структурированном O
виде O
, O
а O
также O
позволяет O
классифицировать B-TERM
страницы I-TERM
. O

# text =   При этом используются широкий спектр технологий: анализ структуры страниц, машинное обучение, искусственный интеллект, обработка естественных языков и машинное зрение.
При O
этом O
используются O
широкий O
спектр O
технологий O
: O
анализ B-Task
структуры I-Task
страниц I-Task
, O
машинное B-Science
обучение I-Science
, O
искусственный B-Science
интеллект I-Science
, O
обработка B-Task
естественных I-Task
языков I-Task
и O
машинное B-Science
зрение I-Science
. O

# text =   Решения, основанные на Deepomatic, позволяют находить информацию о фильме по его постеру, информацию о картине или скульптуре на выставке по ее фото, сделанному на камеру телефона, позволяют скачивать музыку, сфотографировав обложку альбома на диске и т.п.
Решения O
, O
основанные O
на O
Deepomatic B-Application
, O
позволяют O
находить B-Task
информацию I-Task
о I-Task
фильме I-Task
по O
его O
постеру O
, O
информацию B-Object
о I-Object
картине I-Object
или O
скульптуре O
на O
выставке O
по O
ее O
фото O
, O
сделанному O
на O
камеру O
телефона O
, O
позволяют O
скачивать O
музыку O
, O
сфотографировав O
обложку O
альбома O
на O
диске O
и O
т.п. O

# text =   В нашем случае цель была сформулирована как повышение эффективности поиска кандидатов.
В O
нашем O
случае O
цель O
была O
сформулирована O
как O
повышение B-Task
эффективности I-Task
поиска I-Task
кандидатов I-Task
. O

# text =   Основная задача здесь — найти эффективный способ отображения соответствия кандидатов и навыков.
Основная O
задача O
здесь O
— O
найти B-Task
эффективный I-Task
способ I-Task
отображения I-Task
соответствия I-Task
кандидатов I-Task
и I-Task
навыков I-Task
. O

# text =   Кодирование в переменные — One-Hot Encoding (OHE) 
Кодирование O
в O
переменные O
— O
One B-Method
- I-Method
Hot I-Method
Encoding I-Method
( O
OHE B-ShortName
) O

# text =   Для этого используют метод TF-IDF.
Для O
этого O
используют O
метод O
TF B-Metric
- I-Metric
IDF B-Metric
. O

# text =   Соответственно, можно схлопнуть похожие навыки в некоторые факторы/компоненты/латентные признаки.
Соответственно O
, O
можно O
схлопнуть O
похожие O
навыки O
в O
некоторые O
факторы B-Object
/ O
компоненты B-Object
/ O
латентные B-Object
признаки I-Object
. O

# text =   Одним из подходов, позволяющих находить такие компоненты, является группа методов матричной факторизации.
Одним O
из O
подходов O
, O
позволяющих O
находить O
такие O
компоненты B-Object
, O
является O
группа O
методов B-Method
матричной I-Method
факторизации I-Method
. O

# text =   Полученные представления кандидатов и навыков называют эмбедингами.
Полученные O
представления O
кандидатов O
и O
навыков O
называют O
эмбедингами B-Object
. O

# text =   При создании нашей системы рекомендации кандидатов на позиции мы использовали нейронную сеть — StarSpace.
При O
создании O
нашей O
системы O
рекомендации O
кандидатов O
на O
позиции O
мы O
использовали O
нейронную B-Method
сеть I-Method
— O
StarSpace B-Network
. O

# text =  Другая группа методов, позволяющая решать задачи репрезентации сущностей — репрезентация графов.
Другая O
группа O
методов O
, O
позволяющая O
решать O
задачи B-Task
репрезентации I-Task
сущностей I-Task
— O
репрезентация B-Task
графов I-Task
. O

# text =   Но большинство методов графовой репрезентации работает с одномодальными графами, поэтому обычно двухмодальные графы следует трансформировать в граф, где узлы представлены одним видом сущностей.
Но O
большинство O
методов B-Method
графовой I-Method
репрезентации I-Method
работает O
с O
одномодальными B-Object
графами I-Object
, O
поэтому O
обычно O
двухмодальные B-Object
графы I-Object
следует O
трансформировать O
в O
граф O
, O
где O
узлы O
представлены O
одним O
видом O
сущностей O
. O

# text =   В первую очередь рассмотрим метод, основанный на графовой факторизации.
В O
первую O
очередь O
рассмотрим O
метод O
, O
основанный O
на O
графовой B-Method
факторизации I-Method
. O

# text =   Это группа методов очень похожа на методы, применяемые для репрезентации текстов — w2v (skip-gram), doc2vec.
Это O
группа O
методов O
очень O
похожа O
на O
методы O
, O
применяемые O
для O
репрезентации O
текстов O
— O
w2v B-Method 
( O
skip B-Method
- I-Method
gram I-Method
) O
, O
doc2vec B-Method
. O

# text =   Почитать подробнее про подобные методы графовой репрезентации можно, например, тут — DeepWalk, Node2vec, Graph2vec.
Почитать O
подробнее O
про O
подобные O
методы O
графовой O
репрезентации O
можно O
, O
например O
, O
тут O
— O
DeepWalk B-Method
, O
Node2vec B-Method
, O
Graph2vec B-Method
. O

# text =   Сверточные сети на графах (Graph Convolutional Networks).
Сверточные O
сети O
на O
графах O
( O
Graph B-Method
Convolutional I-Method
Networks I-Method
) O
. O

# text =   Для задачи репрезентации графов связей между сущностями мы использовали фреймворк PyTorch BigGraph — это ещё один фреймворк от Facebook Research.
Для O
задачи O
репрезентации B-Task
графов I-Task
связей I-Task
между I-Task
сущностями I-Task
мы O
использовали O
фреймворк O
PyTorch B-Application
BigGraph I-Application
— O
это O
ещё O
один O
фреймворк O
от O
Facebook B-Organization
Research I-Organization
. O

# text =   Энкодер предложений (sentence encoder) – это модель, которая сопоставляет коротким текстам векторы в многомерном пространстве, причём так, что у текстов, похожих по смыслу, и векторы тоже похожи.
Энкодер B-Method
предложений I-Method
( O
sentence B-Method
encoder I-Method
) O
– O
это O
модель O
, O
которая O
сопоставляет O
коротким O
текстам O
векторы O
в O
многомерном O
пространстве O
, O
причём O
так O
, O
что O
у O
текстов O
, O
похожих O
по O
смыслу O
, O
и O
векторы O
тоже O
похожи O
. O

# text =   Обычно для этой цели используются нейросети, а полученные векторы называются эмбеддингами.
Обычно O
для O
этой O
цели O
используются O
нейросети B-Method
, O
а O
полученные O
векторы O
называются O
эмбеддингами B-Object
. O

# text =   Они полезны для кучи задач, например, few-shot классификации текстов, семантического поиска, или оценки качества перефразирования.
Они O
полезны O
для O
кучи O
задач O
, O
например O
, O
few B-Task
- I-Task
shot I-Task
классификации I-Task
текстов B-Task
, O
семантического B-Task
поиска I-Task
, O
или O
оценки B-Task
качества I-Task
перефразирования I-Task
. O

# text =   Самой качественной моделью оказался mUSE, самой быстрой из предобученных – FastText, а по балансу скорости и качества победил rubert-tiny2.
Самой O
качественной O
моделью O
оказался O
mUSE B-Model
, O
самой O
быстрой O
из O
предобученных O
– O
FastText B-Model
, O
а O
по O
балансу O
скорости O
и O
качества O
победил O
rubert B-Model
- I-Model
tiny2 I-Model
. O

# text =   Первой известной попыткой системно сравнить английские эмбеддинги предложений был SentEval, сочетающий чисто лингвистические задачи со вполне прикладными.
Первой O
известной O
попыткой O
системно O
сравнить O
английские O
эмбеддинги B-Object
предложений O
был O
SentEval B-Dataset
, O
сочетающий O
чисто O
лингвистические O
задачи O
со O
вполне O
прикладными O
. O

# text =   Для русского языка тоже было создано немало разного рода бенчмарков NLU моделей:RussianSuperGLUE: бенчмарк "сложных" NLP задач; фокус на дообучаемых моделях.
Для O
русского B-Lang
языка O
тоже O
было O
создано O
немало O
разного O
рода O
бенчмарков O
NLU ShortName
моделей O
: O
RussianSuperGLUE B-Model
: O
бенчмарк O
" O
сложных O
" O
NLP B-Task
задач I-Task
; O
фокус O
на O
дообучаемых O
моделях O
. O

# text = MOROCCO: RussianSuperGLUE + оценка производительности, довольно трудновоспроизводимый бенчмарк.
MOROCCO O
: O
RussianSuperGLUE B-Model
+ O
оценка O
производительности O
, O
довольно O
трудновоспроизводимый O
бенчмарк O
. O

# text =   RuSentEval: бенчмарк BERT-подобных энкодеров предложений на лингвистических задачах.
RuSentEval B-Application
: O
бенчмарк O
BERT B-Model
- I-Model
подобных I-Model
энкодеров I-Model
предложений I-Model
на O
лингвистических O
задачах O
. O

# text =   SentEvalRu и deepPavlovEval: два хороших, но давно не обновлявшихся прикладных бенчмарка.
SentEvalRu B-Application
и O
deepPavlovEval B-Application
: O
два O
хороших O
, O
но O
давно O
не O
обновлявшихся O
прикладных O
бенчмарка O
. O

# text =   С тех пор появилось много новых русскоязычных моделей, включая rubert-tiny2, поэтому и бенчмарк пришло время обновить.
С O
тех O
пор O
появилось O
много O
новых O
русскоязычных O
моделей O
, O
включая O
rubert B-Model
- I-Model
tiny2 I-Model
, O
поэтому O
и O
бенчмарк O
пришло O
время O
обновить O
. O

# text =   В основу бенчмарка легли BERT-подобные модели: sbert_large_nlu_ru, sbert_large_mt_nlu_ru, и ruRoberta-large от Сбера; rubert-base-cased-sentence, rubert-base-cased-conversational, distilrubert-tiny-cased-conversational, и distilrubert-base-cased-conversational от DeepPavlov; мои   rubert-tiny и rubert-tiny2; мультиязычные LaBSE (плюс урезанная версия LaBSE-en-ru) и старый добрый bert-base-multilingual-cased.
В O
основу O
бенчмарка O
легли O
BERT B-Model
- I-Model
подобные I-Model
модели I-Model
: O
sbert_large_nlu_ru B-Model
, O
sbert_large_mt_nlu_ru B-Model
, O
и O
ruRoberta B-Model
- I-Model
large I-Model
от O
Сбера B-Organization
; O
rubert B-Model
- I-Model
base I-Model
- I-Model
cased I-Model
- I-Model
sentence I-Model
, O
rubert B-Model
- I-Model
base I-Model
- I-Model
cased I-Model
- I-Model
conversational I-Model
, O
distilrubert B-Model
- I-Model
tiny I-Model
- I-Model
cased I-Model
- I-Model
conversational I-Model
, O
и O
distilrubert B-Model
- I-Model
base I-Model
- I-Model
cased I-Model
- I-Model
conversational I-Model
от O
DeepPavlov B-Organization
; O
мои O
rubert B-Model
- I-Model
tiny I-Model
и O
rubert B-Model
- I-Model
tiny2 I-Model
; O
мультиязычные O
LaBSE B-Model
( O
плюс O
урезанная O
версия O
LaBSE B-Model
- I-Model
en I-Model
- I-Model
ru I-Model
) O
и O
старый O
добрый O
bert B-Model
- I-Model
base I-Model
- I-Model
multilingual I-Model
- I-Model
cased I-Model
. O

# text =   Кроме этого, я добавил в бенчмарк разные T5 модели, т.к. они тоже должны хорошо понимать тексты: мои rut5-small, rut5-base, rut5-base-multitask, и rut5-base-paraphraser, и Сберовские ruT5-base и ruT5-large.
Кроме O
этого O
, O
я O
добавил O
в O
бенчмарк O
разные O
T5 B-Model
модели I-Model
, O
т.к. O
они O
тоже O
должны O
хорошо O
понимать O
тексты O
: O
мои O
rut5-small B-Model
, O
rut5-base B-Model
, O
rut5-base B-Model
- I-Model
multitask I-Model
, O
и O
rut5-base B-Model
- I-Model
paraphraser I-Model
, O
и O
Сберовские B-Organization
ruT5-base B-Model
и O
ruT5-large B-Model
. O

# text =   Помимо BERTов и T5, я включил в бенчмарк большие мультиязычные модели Laser от FAIR и USE-multilingual-large от Google.
Помимо O
BERTов O
и O
T5 O
, O
я O
включил O
в O
бенчмарк O
большие O
мультиязычные O
модели O
Laser B-Model
от O
FAIR B-Model
и O
USE B-Model
- I-Model
multilingual I-Model
- I-Model
large I-Model
от O
Google B-Organization
. O

# text =   В качестве быстрого бейзлайна, я добавил FastText, а именно, geowac_tokens_none_fasttextskipgram_300_5_2020  с RusVectores, а также его сжатую версию.
В O
качестве O
быстрого O
бейзлайна O
, O
я O
добавил O
FastText B-Model
, O
а O
именно O
, O
geowac_tokens_none_fasttextskipgram_300_5_2020 B-Model
с O
RusVectores B-Model
, O
а O
также O
его O
сжатую O
версию O
. O

# text =   Наконец, я добавил парочку "моделей", которые вообще не выучивают никаких параметров, а просто используют HashingVectorizer для превращения текста в вектор признаков.
Наконец O
, O
я O
добавил O
парочку O
" O
моделей O
" O
, O
которые O
вообще O
не O
выучивают O
никаких O
параметров O
, O
а O
просто O
используют O
HashingVectorizer B-Application
для O
превращения O
текста O
в O
вектор O
признаков O
. O

# text =   Это доработанная версия rubert-tiny: я расширил словарь модели c 30К до 80К токенов, увеличил максимальную длину текста с 512 до 2048 токенов, и дообучил модель на комбинации задач masked language modelling, natural language inference, и аппроксимации эмбеддингов LaBSE.
Это O
доработанная O
версия O
rubert B-Model
- I-Model
tiny I-Model
: O
я O
расширил O
словарь O
модели O
c O
30К O
до O
80К O
токенов O
, O
увеличил O
максимальную O
длину O
текста O
с O
512 O
до O
2048 O
токенов O
, O
и O
дообучил O
модель O
на O
комбинации O
задач O
masked B-Task
language I-Task
modelling I-Task
, O
natural B-Task
language I-Task
inference I-Task
, O
и O
аппроксимации B-Task
эмбеддингов I-Task
LaBSE I-Task
. O

# text =   В новой версии бенчмарка я оставил всё те же 10 задач, что и в прежней, но слегка изменил формат некоторых из них:Semantic text similarity (STS) на основе переведённого датасета STS-B; Paraphrase identification (PI) на основе датасета paraphraser.ru;Natural language inference (NLI) на датасете XNLI; Sentiment analysis (SA) на данных SentiRuEval2016.
В O
новой O
версии O
бенчмарка O
я O
оставил O
всё O
те O
же O
10 O
задач O
, O
что O
и O
в O
прежней O
, O
но O
слегка O
изменил O
формат O
некоторых O
из O
них O
: O
Semantic B-Task
text I-Task
similarity I-Task
( O
STS B-ShortName
) O
на O
основе O
переведённого O
датасета O
STS B-Dataset
- I-Dataset
B I-Dataset
; O
Paraphrase B-Task
identification I-Task
( O
PI B-ShortName
) O
на O
основе O
датасета O
paraphraser.ru B-Dataset
; O
Natural B-Task
language I-Task
inference I-Task
( O
NLI B-ShortName
) O
на O
датасете O
XNLI B-Dataset
; O
Sentiment B-Task
analysis I-Task
( O
SA B-ShortName
) O
на O
данных O
SentiRuEval2016 B-Dataset
. O

# text =   В прошлой версии бенчмарка я собрал кривые тестовые выборки, поэтому этот датасет я переделал; Toxicity identification (TI) на датасете токсичных комментариев из OKMLCup; Inappropriateness identification (II) на датасете Сколтеха; Intent classification (IC) и её кросс-язычная версия ICX на датасете NLU-evaluation-data, который я автоматически перевёл на русский.
В O
прошлой O
версии O
бенчмарка O
я O
собрал O
кривые O
тестовые O
выборки O
, O
поэтому O
этот O
датасет O
я O
переделал O
; O
Toxicity B-Task
identification I-Task
( O
TI B-ShortName
) O
на O
датасете B-Dataset
токсичных I-Dataset
комментариев I-Dataset
из O
OKMLCup B-Application
; O
Inappropriateness B-Task
identification I-Task
( O
II B-ShortName
) O
на O
датасете B-Dataset
Сколтеха I-Dataset
; O
Intent B-Task
classification I-Task
( O
IC B-ShortName
) O
и O
её O
кросс O
- O
язычная O
версия O
ICX B-ShortName
на O
датасете O
NLU B-Dataset
- I-Dataset
evaluation I-Dataset
- I-Dataset
data I-Dataset
, O
который O
я O
автоматически O
перевёл O
на O
русский B-Lang
. O

# text =   В IC классификатор обучается на русских данных, а в ICX – на английских, а тестируется в обоих случаях на русских.
В O
IC B-Model
классификатор I-Model
обучается O
на O
русских B-Lang
данных O
, O
а O
в O
ICX B-Model
– O
на O
английских B-Lang
, O
а O
тестируется O
в O
обоих O
случаях O
на O
русских O
. O

# text =   Распознавание именованных сущностей () на датасетах factRuEval-2016E1) и RuDReC (NE2).
Распознавание B-Task
именованных I-Task
сущностей I-Task
( O
) O
на O
датасетах O
factRuEval-2016E1 B-Dataset
) O
и O
RuDReC B-Dataset
( O
NE2 B-Dataset
) O
. O

# text =   Эти две задачи требуют получать эмбеддинги отдельных токенов, а не целых предложений; поэтому модели USE и Laser, не выдающие эмбеддинги токенов "из коробки", в оценке этих задач не участвовали.
Эти O
две O
задачи O
требуют O
получать O
эмбеддинги O
отдельных O
токенов O
, O
а O
не O
целых O
предложений O
; O
поэтому O
модели O
USE B-Model
и O
Laser B-Model
, O
не O
выдающие O
эмбеддинги O
токенов O
" O
из O
коробки O
" O
, O
в O
оценке O
этих O
задач O
не O
участвовали O
. O

# text =   В задачах STS, PI и NLI оценивается степень связи двух текстов.
В O
задачах O
STS B-Task
, O
PI B-Task
и O
NLI B-Task
оценивается O
степень O
связи O
двух O
текстов O
. O

# text =   Хороший энкодер предложений должен отражать эту степень в их косинусной близости, поэтому для STS и PI мы измеряем качество как Спирмановскую корреляцию косинусной близости и человеческих оценок сходства.
Хороший O
энкодер O
предложений O
должен O
отражать O
эту O
степень O
в O
их O
косинусной B-Metric
близости B-Metric
, O
поэтому O
для O
STS B-Task
и O
PI B-Task
мы O
измеряем O
качество O
как O
Спирмановскую B-Metric
корреляцию I-Metric
косинусной O
близости O
и O
человеческих O
оценок O
сходства O
. O

# text =   Для NLI я обучил трёхклассовую (entail/contradict/neutral) логистическую регрессию поверх косинусной близости, и измеряю её точность (accuracy).
Для O
NLI B-Abbrev_Task
я O
обучил O
трёхклассовую O
( O
entail O
/ O
contradict O
/ O
neutral O
) O
логистическую B-Model
регрессию I-Model
поверх O
косинусной B-Metric
близости I-Metric
, O
и O
измеряю O
её O
точность B-Metric
( O
accuracy B-Metric
) O
. O

# text =   Для задач бинарной классификации TI и II я измеряю ROC AUC, а в задачах многоклассовой классификации SA, IC и ICX – точность (accuracy).
Для O
задач O
бинарной O
классификации O
TI B-Abbrev_Task
и O
II B-Abbrev_Task
я O
измеряю O
ROC B-Metric
AUC I-Metric
, O
а O
в O
задачах O
многоклассовой O
классификации O
SA B-Abbrev_Task
, O
IC B-Abbrev_Task
и O
ICX B-Abbrev_Task
– O
точность B-Metric
( O
accuracy B-Metric
) O
. O

# text =   Для всех задач классификации я обучаю логистическую регрессию либо KNN поверх эмбеддингов предложений, и выбираю лучшую модель из двух.
Для O
всех O
задач O
классификации B-Task
я O
обучаю O
логистическую B-Model
регрессию I-Model
либо O
KNN B-ShortName_Method
поверх O
эмбеддингов O
предложений B-Subject
, O
и O
выбираю O
лучшую O
модель B-Object
из O
двух O
. O

# text =   Для задач NER я классифицировал токены логистической регрессией поверх их эмбеддингов, и измерял macro F1 по всем классам токенов, кроме О. 
Для O
задач O
NER B-Abbrev_Task
я O
классифицировал O
токены O
логистической B-Method
регрессией I-Method
поверх O
их O
эмбеддингов O
, O
и O
измерял O
macro B-Metric
F1 I-Metric
по O
всем O
классам O
токенов O
, O
кроме O
О O

# text =  Поскольку разные модели токенизируют тексты по-разному, я токенизировал все тексты razdel'ом, и вычислял эмбеддинг слова как средний эмбеддинг его токенов.
Поскольку O
разные O
модели O
токенизируют O
тексты O
по O
- O
разному O
, O
я O
токенизировал O
все O
тексты O
razdel'ом B-Technology
, O
и O
вычислял O
эмбеддинг O
слова O
как O
средний O
эмбеддинг O
его O
токенов O
. O

# text =   Единого победителя нет, но MUSE, sbert_large_mt_nlu_ru и rubert-base-cased-sentence взяли по многу призовых мест.
Единого O
победителя O
нет O
, O
но O
MUSE B-Model
, O
sbert_large_mt_nlu_ru B-Model
и O
rubert B-Model
- I-Model
base I-Model
- I-Model
cased I-Model
- I-Model
sentence I-Model
взяли O
по O
многу O
призовых O
мест O
. O

# text =   Удивительно, но модели T5 очень хорошо показали себя на задачах NER.
Удивительно O
, O
но O
модели O
T5 B-Model
очень O
хорошо O
показали O
себя O
на O
задачах O
NER B-Abbrev_Task
. O

# text =   Самыми качественными энкодерами предложений оказались мультиязычные MUSE, LaBSE и Laser.
Самыми O
качественными O
энкодерами O
предложений O
оказались O
мультиязычные O
MUSE B-Model
, O
LaBSE B-Model
и O
Laser B-Model
. O

# text =   Но выбирать стоит из Парето-оптимальных моделей: таких, что ни одна другая модель не превосходит их по всем критериям.
Но O
выбирать O
стоит O
из O
Парето B-Object
- I-Object
оптимальных I-Object
моделей I-Object
: O
таких O
, O
что O
ни O
одна O
другая O
модель O
не O
превосходит O
их O
по O
всем O
критериям O
. O

# text =   Из 25 моделей только 12 Парето-оптимальны:MUSE, rubert-tiny2, FT_geowac, Hashing_1000_char и Hashing_1000 обладают самым лучшим качеством для своей скорости на CPU; MUSE, LaBSE, rubert-tiny2, и distilbert-tiny обладают наилучшим качеством для своей скорости на GPU;MUSE, LaBSE, rubert-tiny2, rubert-tiny, FT_geowac_21mb, и Hashing_1000_char обладают наилучшим качеством для своего размера.
Из O
25 O
моделей O
только O
12 O
Парето O
- O
оптимальны O
: O
MUSE B-Model
, O
rubert B-Model
- I-Model
tiny2 I-Model
, O
FT_geowac B-Model
, O
Hashing_1000_char B-Model
и O
Hashing_1000 B-Model
обладают O
самым O
лучшим O
качеством O
для O
своей O
скорости O
на O
CPU O
; O
MUSE B-Model
, O
LaBSE B-Model
, O
rubert B-Model
- I-Model
tiny2 I-Model
, O
и O
distilbert B-Model
- I-Model
tiny I-Model
обладают O
наилучшим O
качеством O
для O
своей O
скорости O
на O
GPU O
; O
MUSE B-Model
, O
LaBSE B-Model
, O
rubert B-Model
- I-Model
tiny2 I-Model
, O
rubert B-Model
- I-Model
tiny I-Model
, O
FT_geowac_21 B-Model
mb O
, O
и O
Hashing_1000_char B-Model
обладают O
наилучшим O
качеством O
для O
своего O
размера O
. O

# text =   Актуальный лидерборд смотрите в репозитории: https://github.com/avidale/encodechka
Актуальный O
лидерборд O
смотрите O
в O
репозитории O
: O
https://github.com/avidale/encodechka B-URL_InfoResource
. O

# text =   Поддержка NlpCraft IDL добавлена в систему начиная с версии 0.7.5.
Поддержка O
NlpCraft B-ShortName_Environment
IDL I-ShortName_Environment
добавлена O
в O
систему O
начиная O
с O
версии O
0.7.5 O
. O

# text =   Новая версия декларативного языка определения интентов, получившая название NlpCraft IDL (NlpCraft Intents Definition Language), значительно упростила процесс работы с интентами в диалоговых и поисковых системах, построенных на базе проекта Apache NlpCraft и вместе с тем расширила возможности системы.
Новая O
версия O
декларативного O
языка B-Object
определения B-Subject
интентов O
, O
получившая O
название O
NlpCraft B-ShortName_Environment
IDL I-ShortName_Environment
( O
NlpCraft B-Environment
Intents I-Environment
Definition B-Environment
Language B-Environment
) O
, O
значительно O
упростила O
процесс O
работы O
с O
интентами O
в O
диалоговых O
и O
поисковых O
системах O
, O
построенных O
на O
базе O
проекта O
Apache B-Project
NlpCraft I-Project
и O
вместе O
с O
тем O
расширила O
возможности O
системы O
. O

# text =   NlpCraft IDL - это декларативный язык, позволяющий создавать определения интентов для их последующего использования в моделях Apache NlpCraft.
NlpCraft B-ShortName_Environment
IDL I-ShortName_Environment
- O
это O
декларативный O
язык O
, O
позволяющий O
создавать O
определения B-Subject
интентов O
для O
их O
последующего O
использования O
в O
моделях O
Apache B-Model
NlpCraft I-Model
. O

# text =   Чаще всего на практике в NLP приходится сталкиваться с задачей построения эмбеддингов.
Чаще O
всего O
на O
практике O
в O
NLP B-ShortName_Science
приходится O
сталкиваться O
с O
задачей O
построения B-Task
эмбеддингов I-Task
. O

# text =   Для ее решения обычно используют один из следующих инструментов: Готовые векторы / эмбеддинги слов [6]; Внутренние состояния CNN, натренированных на таких задачах как, как определение фальшивых предложений / языковое моделирование / классификация [7]; Комбинация выше перечисленных методов; Кроме того, уже много раз было показано [9], что в качестве хорошего бейслайна для эмбеддингов предложений можно взять и просто усредненные (с парой незначительных деталей, которые сейчас опустим) векторы слов.
Для O
ее O
решения O
обычно O
используют O
один O
из O
следующих O
инструментов O
: O
Готовые O
векторы B-Object
/ O
эмбеддинги B-Object
слов I-Object
[ O
6 O
] O
; O
Внутренние O
состояния O
CNN B-ShortName_Method
, O
натренированных O
на O
таких O
задачах O
как O
, O
как O
определение B-Task
фальшивых I-Task
предложений I-Task
/ O
языковое B-Task
моделирование I-Task
/ O
классификация B-Task
[ O
7 O
] O
; O
Комбинация O
выше O
перечисленных O
методов O
; O
Кроме O
того O
, O
уже O
много O
раз O
было O
показано O
[ O
9 O
] O
, O
что O
в O
качестве O
хорошего O
бейслайна O
для O
эмбеддингов O
предложений O
можно O
взять O
и O
просто O
усредненные O
( O
с O
парой O
незначительных O
деталей O
, O
которые O
сейчас O
опустим O
) O
векторы O
слов O
. O

# text =   Если для дальнейшей обработки не важен порядок слов, то текст упаковывают в Мешок слов (Bag-of-words).
Если O
для O
дальнейшей O
обработки O
не O
важен O
порядок O
слов O
, O
то O
текст O
упаковывают O
в O
Мешок B-Method
слов I-Method
( O
Bag B-Method
- I-Method
of I-Method
- I-Method
words I-Method
) O

# text =   В обучающей выборке мы имеем письма с отметками спам/не спам, и скармливаем их в нейросеть: в полносвязную сеть и CNN подаем Bag-of-words, а в RNN уже можно учесть порядок слов, отправив ей Word Vector.
В O
обучающей O
выборке O
мы O
имеем O
письма O
с O
отметками O
спам O
/ O
не O
спам O
, O
и O
скармливаем O
их O
в O
нейросеть O
: O
в O
полносвязную O
сеть O
и O
CNN B-ShortName_Method
подаем O
Bag B-Method
- I-Method
of I-Method
- I-Method
words I-Method
, O
а O
в O
RNN B-ShortName_Method
уже O
можно O
учесть O
порядок O
слов O
, O
отправив O
ей O
Word B-Subject
Vector I-Subject
. O

# text =   Сначала Яндекс распознаёт речь в текст с указанием таймингов и спикеров (за это отвечает голосовая биометрия).
Сначала O
Яндекс B-Organization
распознаёт B-Task
речь I-Task
в O
текст O
с O
указанием O
таймингов O
и O
спикеров O
( O
за O
это O
отвечает O
голосовая B-Task
биометрия I-Task
) O
. O

# text =   Добавили поддержку немецкого, французского и испанского языков.
Добавили O
поддержку O
немецкого B-Lang
, O
французского B-Lang
и O
испанского B-Lang
языков O
. O

# text =   Поэтому всё время существования как Яндекс Браузера, так и Яндекс Переводчика мы стараемся не просто переводить, но и помогать учить язык.
Поэтому O
всё O
время O
существования O
как O
Яндекс B-Technology
Браузера I-Technology
, O
так O
и O
Яндекс B-Technology
Переводчика I-Technology
мы O
стараемся O
не O
просто O
переводить O
, O
но O
и O
помогать O
учить O
язык O
. O

# text =   С 27 по 30 мая в Российском государственном гуманитарном университете (РГГУ) пройдет международная научная конференция по компьютерной лингвистике «Диалог».
С O
27 O
по O
30 O
мая O
в O
Российском B-Organization
государственном I-Organization
гуманитарном I-Organization
университете I-Organization
( O
РГГУ B-Abbrev_Organization
) O
пройдет O
международная O
научная O
конференция O
по O
компьютерной B-Science
лингвистике I-Science
« O
Диалог O
» O
. O

# text =   Подробно о том, что такое «Диалог» и почему ABBYY организует эту конференцию, мы писали здесь .
Подробно O
о O
том O
, O
что O
такое O
« O
Диалог O
» O
и O
почему O
ABBYY B-Organization
организует O
эту O
конференцию O
, O
мы O
писали O
здесь O
. O

# text =   Главной задачей проведенных ранее тестирований был автоматический анализ тональности в целом небольших текстов – отзывов пользователей (о фильмах, книгах, цифровых фотокамерах) или мнений, выраженных в форме прямой или косвенной речи (новости).
Главной O
задачей O
проведенных O
ранее O
тестирований O
был O
автоматический B-Task
анализ I-Task
тональности I-Task
в O
целом O
небольших O
текстов O
– O
отзывов O
пользователей O
( O
о O
фильмах O
, O
книгах O
, O
цифровых O
фотокамерах O
) O
или O
мнений O
, O
выраженных O
в O
форме O
прямой O
или O
косвенной O
речи O
( O
новости O
) O
. O

# text =   Основной целью нового цикла тестирований является автоматическая оценка тональности по отношению к заданному объекту и его конкретным свойствам.
Основной O
целью O
нового O
цикла O
тестирований O
является O
автоматическая B-Task
оценка I-Task
тональности I-Task
по O
отношению O
к O
заданному O
объекту O
и O
его O
конкретным O
свойствам O
. O

# text =   Фишинговые электронные письма - это сообщения, которые кажутся очень похожими на настоящие, например, рассылку от вашего любимого интернет-магазина, но при этом они заманивают людей нажимать на прикрепленные вредоносные ссылки или документы.
Фишинговые B-Object
электронные I-Object
письма I-Object
- O
это O
сообщения O
, O
которые O
кажутся O
очень O
похожими O
на O
настоящие O
, O
например O
, O
рассылку O
от O
вашего O
любимого O
интернет O
- O
магазина O
, O
но O
при O
этом O
они O
заманивают O
людей O
нажимать O
на O
прикрепленные O
вредоносные O
ссылки O
или O
документы O
. O

# text =   Поэтому в статье предлагается способ обнаружения фишинговых сообщений, называемый Federated Phish Bowl (далее FPB), использующий федеративное обучение и рекуррентную нейронную сеть с долгой краткосрочной памятью (LSTM).
Поэтому O
в O
статье O
предлагается O
способ O
обнаружения B-Task
фишинговых I-Task
сообщений I-Task
, O
называемый O
Federated B-Task
Phish I-Task
Bowl I-Task
( O
далее O
FPB ShortName_Task
) O
, O
использующий O
федеративное O
обучение O
и O
рекуррентную B-Network
нейронную I-Network
сеть I-Network
с I-Network
долгой I-Network
краткосрочной I-Network
памятью I-Network
( O
LSTM B-ShortName_Method
) O
. O

# text =   Для работы с текстовыми последовательностями были придуманы рекуррентные нейронные сети (RNN, их улучшение - LSTM, которая бывает двунаправленной, когда последовательность обрабатывается в двух направлениях).
Для O
работы O
с O
текстовыми O
последовательностями O
были O
придуманы O
рекуррентные O
нейронные O
сети O
( O
RNN B-ShortName_Method
, O
их O
улучшение O
- O
LSTM B-ShortName_Method
, O
которая O
бывает O
двунаправленной O
, O
когда O
последовательность O
обрабатывается O
в O
двух O
направлениях O
) O
. O

# text =   Например, сеть можно сделать двунаправленной (Bidirectional LSTM).
Например O
, O
сеть O
можно O
сделать O
двунаправленной O
( O
Bidirectional B-Method
LSTM I-Method
) O
. O

# text =   FPB предлагает использовать подход, показанный на следующем изображении: 
FPB B-Method
предлагает O
использовать O
подход O
, O
показанный O
на O
следующем O
изображении O
: O

# text =   Федеративное обучение - это метод машинного обучения, который обучает алгоритм на нескольких децентрализованных устройствах или серверах, содержащих локальные образцы данных, без обмена ими.
Федеративное B-Method
обучение I-Method
- O
это O
метод O
машинного O
обучения O
, O
который O
обучает O
алгоритм O
на O
нескольких O
децентрализованных O
устройствах O
или O
серверах O
, O
содержащих O
локальные O
образцы O
данных O
, O
без O
обмена O
ими O
. O

# text =   Для обучения модели FPB с использованием федеративного обучения (FL) сервер параметров (PS) инициализирует глобальную модель (DL) на основе вышеупомянутых двунаправленных нейронных сетей LSTM и отправляет глобальную модель с глобальной матрицей преобразования слов в векторы всем клиентам на первом этапе обучения.
Для O
обучения O
модели O
FPB B-Method
с O
использованием O
федеративного B-Method
обучения I-Method
( O
FL B-ShortName_Method
) O
сервер B-Method
параметров I-Method
( O
PS B-ShortName_Method
) O
инициализирует O
глобальную B-Method
модель B-Method
( O
DL B-ShortName_Method
) O
на O
основе B-Subject
вышеупомянутых O
двунаправленных O
нейронных O
сетей O
LSTM B-ShortName_Method
и O
отправляет O
глобальную O
модель O
с O
глобальной O
матрицей O
преобразования O
слов O
в O
векторы O
всем O
клиентам O
на O
первом O
этапе O
обучения O
. O

# text =   В рамках курса вы узнаете: Как латентные переменные применяются в задачах анализа текстов и как строить глубинные генеративные модели с латентными дискретными переменными.
В O
рамках O
курса O
вы O
узнаете O
: O
Как O
латентные B-Object
переменные I-Object
применяются O
в O
задачах O
анализа B-Task
текстов I-Task
и O
как O
строить O
глубинные B-Meodel
генеративные I-Meodel
модели I-Meodel
с O
латентными O
дискретными O
переменными O
. O

# text =   Что такое semantic parsing: как строить формальные представления смысла текста, извлекая при этом неявные значения.
Что O
такое O
semantic B-Method
parsing I-Method
: O
как O
строить O
формальные O
представления O
смысла O
текста O
, O
извлекая O
при O
этом O
неявные O
значения O
. O

# text =   Британские ученые обучили ИИ трансформировать устную речь в видео с виртуальным сурдопереводчиком.
Британские O
ученые O
обучили O
ИИ O
трансформировать O
устную B-Subject
речь I-Subject
в O
видео O
с O
виртуальным O
сурдопереводчиком B-Application
. O

# text =   В Университете Суррея разработчики создали алгоритм сурдоперевода нового поколения.
В O
Университете B-Organization
Суррея I-Organization
разработчики O
создали O
алгоритм B-Method
сурдоперевода I-Method
нового O
поколения O
. O

# text =   После этого последовательность поз подается сверточной нейросети U-Net.
После O
этого O
последовательность O
поз O
подается O
сверточной O
нейросети O
U B-Network
- I-Network
Net I-Network
. O

# text =   Один из самых известных продуктов — анимированный виртуальный переводчик от IBM.
Один O
из O
самых O
известных O
продуктов O
— O
анимированный O
виртуальный O
переводчик O
от O
IBM B-Technology
. O

# text =   Программа, придуманная учеными из Новосибирского академгородка, распознает речь, анализирует смысл и переводит на жестовый язык.
Программа O
, O
придуманная O
учеными O
из O
Новосибирского B-Organization
академгородка I-Organization
, O
распознает B-Task
речь I-Task
, O
анализирует B-Task
смысл I-Task
и O
переводит O
на O
жестовый B-Object
язык I-Object
. O

# text =   В то время считали, что разработка станет такой же популярной, как Google Translator.
В O
то O
время O
считали O
, O
что O
разработка O
станет O
такой O
же O
популярной O
, O
как O
Google B-Technology
Translator I-Technology
. O

# text =   Российские ученые из Института проблем управления им. В.А. Трапезникова РАН (ИПУ РАН) несколько лет назад начали разработку подобного ИИ.
Российские O
ученые O
из O
Института B-Organization
проблем I-Organization
управления I-Organization
В.А. I-Organization
Трапезникова I-Organization
РАН I-Organization
( O
ИПУ B-Abbrev_Organization
РАН I-Abbrev_Organization
) O
несколько O
лет O
назад O
начали O
разработку O
подобного O
ИИ O
. O

# text =   Она несколько лет развивает сайт «Сурдосервер».
Она O
несколько O
лет O
развивает O
сайт O
« O
Сурдосервер B-Technology
» O
. O

# text =   N-грамм это просто последовательности букв из слова.
N B-Object
- I-Object
грамм I-Object
это O
просто O
последовательности O
букв O
из O
слова O
. O

# text =   Создаются лексические и синтаксические признаки токенов текста.
Создаются O
лексические B-Object
и O
синтаксические B-Object
признаки I-Object
токенов B-Subject
текста B-Object
. O

# text =   В качестве классификатора намерений применяем Transformer.
В 
качестве O
классификатора B-Method
намерений O
применяем O
Transformer B-Method
. O

# text =  Как отличить хороший ремонт от плохого, или как мы в SRG сделали из Томита-парсера многопоточную Java-библиотеку. 
Как O
отличить O
хороший O
ремонт O
от O
плохого O
, O
или O
как O
мы O
в O
SRG B-Abbrev_Organization
сделали O
из O
Томита B-Technology
- I-Technology
парсера I-Technology
многопоточную O
Java B-Library
- I-Library
библиотеку I-Library
. O

# text =  В этой статье речь пойдет о том, как мы интегрировали разработанный Яндексом Томита-парсер в нашу систему, превратили его в динамическую библиотеку, подружили с Java, сделали многопоточной и решили с её помощью задачу классификации текста для оценки недвижимости.
В O
этой O
статье O
речь O
пойдет O
о O
том O
, O
как O
мы O
интегрировали O
разработанный O
Яндексом B-Organization
Томита B-Technology
- I-Technology
парсера I-Technology
в O
нашу O
систему O
, O
превратили O
его O
в O
динамическую O
библиотеку O
, O
подружили O
с O
Java B-Environment
, O
сделали O
многопоточной O
и O
решили O
с O
её O
помощью O
задачу O
классификации B-Task
текста O
для O
оценки B-Task
недвижимости I-Task
. O

# text =   Итак, у нас есть текст объявления, который необходимо классифицировать в одну из категорий согласно состоянию ремонта в квартире (без отделки, чистовой, средний, хороший, отличный, эксклюзивный).
Итак O
, O
у O
нас O
есть O
текст B-Object
объявления I-Object
, O
который O
необходимо O
классифицировать O
в O
одну O
из O
категорий O
согласно O
состоянию O
ремонта O
в O
квартире O
( O
без O
отделки O
, O
чистовой O
, O
средний O
, O
хороший O
, O
отличный O
, O
эксклюзивный O
) O
. O

# text =   Таким образом, по мере решения сформировалась вторая большая и интересная задача — научиться извлекать всю достаточную и необходимую информацию о ремонте из объявления, а именно обеспечить быстрый синтаксический и морфологический анализ текста, который сможет работать параллельно под нагрузкой в режиме библиотеки.
Таким O
образом O
, O
по O
мере O
решения O
сформировалась O
вторая O
большая O
и O
интересная O
задача O
— O
научиться O
извлекать B-Task
всю I-Task
достаточную I-Task
и I-Task
необходимую I-Task
информацию I-Task
о O
ремонте O
из O
объявления O
, O
а O
именно O
обеспечить O
быстрый O
синтаксический O
и O
морфологический B-Method
анализ I-Method
текста B-Object
, O
который O
сможет O
работать O
параллельно O
под O
нагрузкой O
в O
режиме O
библиотеки O
. O

# text =   Из доступных средств для извлечения фактов из текста на основе контекстно-свободных грамматик, способных работать с русским языком, наше внимание привлекли Томита-парсер и библиотека Yagry на питоне.
Из O
доступных O
средств O
для O
извлечения B-Task
фактов I-Task
из O
текста B-Object
на O
основе O
контекстно B-Method
- I-Method
свободных I-Method
грамматик I-Method
, O
способных O
работать O
с O
русским O
языком O
, O
наше O
внимание O
привлекли O
Томита B-Technology
- I-Technology
парсер I-Technology
и O
библиотека O
Yagry B-Library
на O
питоне O
. O

# text =   Многопоточный вариант Томиты — TomitaPooledParser использует для парсинга пул объектов TomitaParser, одинаковым образом сконфигурированных.
Многопоточный O
вариант O
Томиты B-Technology
— O
TomitaPooledParser B-Technology
использует O
для O
парсинга B-Task
пул O
объектов O
TomitaParser B-Technology
, O
одинаковым O
образом O
сконфигурированных O
. O

# text =   Приведу только показатели качества классификации, которые были нами получены на тестах: Accuracy = 95% F1 score = 93%
Приведу O
только O
показатели O
качества O
классификации B-Task
, O
которые O
были O
нами O
получены O
на O
тестах O
: O
Accuracy B-Metric
= O
95 B-Value
% I-Value

F1 B-Metric
score I-Metric
= O
93 B-Value
% I-Value
# text =  JavaScript-библиотека для обработки текстов на русском языке
JavaScript B-Library
- I-Library
библиотека I-Library
для O
обработки B-Task
текстов I-Task
на O
русском O
языке O

# text =   Бессвязность текстов в нынешней версии «Генератора» вызвана тем, что на самом деле никакого анализа он производить не умеет.
Бессвязность O
текстов O
в O
нынешней O
версии O
« O
Генератора B-Technology
» O
вызвана O
тем O
, O
что O
на O
самом O
деле O
никакого O
анализа B-Method
он O
производить O
не O
умеет O
. O

# text =   На данный момент библиотека умеет две вещи: токенизацию и анализ морфологии.
На O
данный O
момент O
библиотека O
умеет O
две O
вещи O
: O
токенизацию B-Method
и O
анализ B-Task
морфологии I-Task
. O

# text =   Полный список граммем можно найти на странице проекта OpenCorpora.
Полный O
список O
граммем O
можно O
найти O
на O
странице O
проекта O
OpenCorpora B-InfoResource
. O

# text =   Кроме того, для анализа используется словарь OpenCorpora, упакованный в специальном формате, но об этом ниже.
Кроме O
того O
, O
для O
анализа B-Task
используется O
словарь O
OpenCorpora B-InfoResource
, O
упакованный O
в O
специальном O
формате O
, O
но O
об O
этом O
ниже O
. O

# text =   Вообще создатели проекта OpenCorpora большие молодцы и я вам рекомендую не только ознакомиться с ним, но и принять участие в коллаборативной разметке корпуса — это также поможет и другим опенсорсным проектам.
Вообще O
создатели O
проекта O
OpenCorpora B-InfoResource
большие O
молодцы O
и O
я O
вам O
рекомендую O
не O
только O
ознакомиться O
с O
ним O
, O
но O
и O
принять O
участие O
в O
коллаборативной O
разметке B-Task
корпуса I-Task
— O
это O
также O
поможет O
и O
другим O
опенсорсным O
проектам O
. O

# text =   По сути эта часть библиотеки — порт замечательного морфологического анализатора pymorphy2 за авторством kmike (на Хабре была пара статей об этой библиотеке).
По O
сути O
эта O
часть O
библиотеки O
— O
порт O
замечательного O
морфологического B-Application
анализатора I-Application
pymorphy2 B-Technology
за O
авторством O
kmike B-Person
( O
на O
Хабре B-InfoResource
была O
пара O
статей O
об O
этой O
библиотеке O
) O
. O

# text =   Анализируем тональность текстов с помощью Fast.ai
Анализируем O
тональность O
текстов O
с O
помощью O
Fast.ai B-Technology

# text =  В статье пойдет речь о классификации тональности текстовых сообщений на русском языке (а по сути любой классификации текстов, используя те же технологии).
В O
статье O
пойдет O
речь O
о O
классификации B-Task
тональности I-Task
текстовых I-Task
сообщений I-Task
на O
русском B-Lang
языке O
( O
а O
по O
сути O
любой O
классификации O
текстов O
, O
используя O
те O
же O
технологии O
) O
. O

# text =   За основу возьмем данную статью, в которой была рассмотрена классификация тональности на архитектуре CNN с использованием Word2vec модели.
За O
основу O
возьмем O
данную O
статью O
, O
в O
которой O
была O
рассмотрена O
классификация B-Task
тональности I-Task
на O
архитектуре O
CNN B-Method
с O
использованием O
Word2vec B-Model
модели o
. O

# text =   В нашем примере будем решать ту же самую задачу разделения твитов на позитивные и негативные на том же самом датасете с использованием модели ULMFit.
В O
нашем O
примере O
будем O
решать O
ту O
же O
самую O
задачу O
разделения B-Task
твитов I-Task
на O
позитивные B-Object
и O
негативные B-Object
на O
том O
же O
самом O
датасете O
с O
использованием O
модели O
ULMFit B-Model
. O

# text =   Результат из статьи (average F1-score = 0.78142) примем в качестве baseline.
Результат O
из O
статьи O
( O
average O
F1-score B-Metric
= O
0.78142 B-Value
) O
примем O
в O
качестве O
baseline O
. O

# text =   Модель ULMFIT была представлена разработчиками fast.ai (Jeremy Howard, Sebastian Ruder) в 2018 году.
Модель O
ULMFIT B-Model
была O
представлена O
разработчиками O
fast.ai B-Organization
( O
Jeremy B-Person
Howard I-Person
, O
Sebastian B-Person
Ruder I-Person
) O
в O
2018 B-Date
году O
. O

# text =   Суть подхода состоит в использовании transfer learning в задачах NLP, когда вы используете предобученные модели, сокращая время на обучение своих моделей и снижая требования к размерам размеченной тестовой выборки.
Суть O
подхода O
состоит O
в O
использовании O
transfer B-Method
learning I-Method
в O
задачах O
NLP B-ShortName_Science
, O
когда O
вы O
используете O
предобученные O
модели O
, O
сокращая O
время O
на O
обучение O
своих O
моделей O
и O
снижая O
требования O
к O
размерам O
размеченной O
тестовой O
выборки O
. O

# text =   Для задачи моделирования языка ULMFit использует архитектуру AWD-LSTM, которая предполагает активное использование dropout везде, где только можно и имеет смысл.
Для O
задачи O
моделирования B-Task
языка O
ULMFit B-Model
использует O
архитектуру O
AWD B-Method
- I-Method
LSTM I-Method
, O
которая O
предполагает O
активное O
использование O
dropout O
везде O
, O
где O
только O
можно O
и O
имеет O
смысл O
. O

# text =   Результат, показанный на тестовой выборке average F1-score = 0,80064.
Результат O
, O
показанный O
на O
тестовой O
выборке O
average O
F1-score B-Metric
= O
0,80064 B-Value
. O

# text =   Добавьте возможности IBM Watson платформы в ваши приложения, разработанные на платформе IBM Cloud, или в сторонние приложения!
Добавьте O
возможности O
IBM B-Technology
Watson I-Technology
платформы O
в O
ваши O
приложения O
, O
разработанные O
на O
платформе O
IBM B-Technology
Cloud I-Technology
, O
или O
в O
сторонние O
приложения O
! O

# text =   IBM Automation Platform для цифрового бизнеса — это интегрированная платформа с пятью возможностями автоматизации, которая помогает бизнесу быстро и масштабно управлять практически всеми типами проектов автоматизации — от повторяющихся и административных до работы на уровне экспертов.
IBM B-Technology
Automation I-Technology
Platform I-Technology
для O
цифрового B-Science
бизнеса I-Science
— O
это O
интегрированная O
платформа O
с O
пятью O
возможностями O
автоматизации O
, O
которая O
помогает O
бизнесу O
быстро O
и O
масштабно O
управлять O
практически O
всеми O
типами O
проектов O
автоматизации O
— O
от O
повторяющихся O
и O
административных O
до O
работы O
на O
уровне O
экспертов O
. O

# text =   HuggingArtists | Генерируем текст песен с трансформером за 5 минут 
HuggingArtists B-Technology
| O
Генерируем B-Task
текст B-Object
песен I-Object
с O
трансформером O
за O
5 O
минут O

# text =   В HuggingArtists, мы можем создавать тексты песен на основе конкретного исполнителя.
В O
HuggingArtists B-Technology
, O
мы O
можем O
создавать B-Task
тексты I-Task
песен O
на O
основе O
конкретного O
исполнителя O
. O

# text =   Это было сделано путем fine-tune (точной настройки) предварительно обученного трансформера HuggingFace  на собранных данных Genius.
Это O
было O
сделано O
путем O
fine B-Method
- I-Method
tune I-Method
( O
точной B-Method
настройки I-Method
) O
предварительно O
обученного O
трансформера O
HuggingFace B-Model
на O
собранных O
данных O
Genius B-Technology
. O

# text =   Кроме того, мы используем интеграцию Weights & Biases для автоматического учета производительности и прогнозов модели.
Кроме O
того O
, O
мы O
используем O
интеграцию O
Weights B-Library
& I-Library
Biases I-Library
для O
автоматического O
учета O
производительности O
и O
прогнозов O
модели O
. O

# text =  Анализ тональности текста с использованием фреймворка Lightautoml 
Анализ B-Task
тональности I-Task
текста I-Task
с O
использованием O
фреймворка O
Lightautoml B-Technology

# text =  Сентиментный анализ (анализ тональности) – это область компьютерной лингвистики, занимающаяся изучением эмоций в текстовых документах, в основе которой лежит машинное обучение.
Сентиментный B-Task
анализ I-Task
( O
анализ B-Task
тональности I-Task
) O
– O
это O
область O
компьютерной B-Science
лингвистики I-Science
, O
занимающаяся O
изучением O
эмоций O
в O
текстовых O
документах O
, O
в O
основе O
которой O
лежит O
машинное O
обучение O
. O

# text =  В этой статье я покажу, как мы использовали для этих целей внутреннюю разработку компании – фреймворк LightAutoML, в котором имеется всё для решения поставленной задачи – предобученные готовые векторные представления слов FastText и готовые текстовые пресеты, в которых необходимо только указать гиперпараметры.
В O
этой O
статье O
я O
покажу O
, O
как O
мы O
использовали O
для O
этих O
целей O
внутреннюю O
разработку O
компании O
– O
фреймворк O
LightAutoML B-Technology
, O
в O
котором O
имеется O
всё O
для O
решения O
поставленной O
задачи O
– O
предобученные B-Subject
готовые I-Subject
векторные I-Subject
представления I-Subject
слов I-Subject
FastText B-Model
и O
готовые O
текстовые O
пресеты O
, O
в O
которых O
необходимо O
только O
указать O
гиперпараметры O
. O

# text =   При обучении модели значение метрики F1-score достигло 0.894, соответственно можно сделать вывод о том, что модель хорошо справляется с задачей определения нейтральных и негативных обращений.
При O
обучении O
модели O
значение O
метрики O
F1-score B-Metric
достигло O
0.894 B-Value
, O
соответственно O
можно O
сделать O
вывод O
о O
том O
, O
что O
модель O
хорошо O
справляется O
с O
задачей O
определения B-Task
нейтральных I-Task
и I-Task
негативных I-Task
обращений I-Task
. O

# text =   Также одним из способов оценить работу модели в целом можно по кривой ROC-AUC, которая описывает площадь под кривой (Area Under Curve – Receiver Operating Characteristic).
Также O
одним O
из O
способов O
оценить O
работу O
модели O
в O
целом O
можно O
по O
кривой O
ROC B-Metric
- I-Metric
AUC I-Metric
, O
которая O
описывает O
площадь O
под O
кривой O
( O
Area B-Metric
Under I-Metric
Curve I-Metric
– I-Metric
Receiver I-Metric
Operating I-Metric
Characteristic I-Metric
) O
. O

# text =   В качестве подтверждения вышесказанного можно привести работу встроенного в LAMA модуля – LIME, который раскрывает работу модели окрашивая слова в тот или иной цвет, в зависимости от их эмоционального окраса.
В O
качестве O
подтверждения O
вышесказанного O
можно O
привести O
работу O
встроенного O
в O
LAMA B-Application
модуля O
– O
LIME B-Technology
, O
который O
раскрывает O
работу O
модели O
окрашивая O
слова B-Subject
в O
тот O
или O
иной O
цвет O
, O
в O
зависимости O
от O
их O
эмоционального O
окраса O
. O

# text =  Также фреймворк может решать задачи регрессионного анализа, целью которого является определение зависимости между переменными и оценкой функции регрессии.
Также O
фреймворк O
может O
решать O
задачи O
регрессионного B-Task
анализа I-Task
, O
целью O
которого O
является O
определение B-Task
зависимости I-Task
между I-Task
переменными I-Task
и O
оценкой B-Task
функции I-Task
регрессии I-Task
. O

# text =   Работа с текстомВ LightAutoML имеется большое количество вариантов разработки той или иной модели, работающей с текстом.
Работа O
с O
текстомВ O
LightAutoML B-Technology
имеется O
большое O
количество O
вариантов O
разработки O
той O
или O
иной O
модели O
, O
работающей O
с O
текстом O
. O

# text =   Библиотека предоставляет не только получение стандартных признаков на основе TF-IDF, но и на основе эмбеддингов:1) На основе встроенного FastText, который можно тренировать на том или ином корпусе2) Предобученных моделей Gensim3) Любой другой объект, который имеет вид словаря, где на вход подается слово, а на выходе его эмбеддинги
Библиотека O
предоставляет O
не O
только O
получение O
стандартных O
признаков O
на O
основе O
TF B-Metric
- I-Metric
IDF I-Metric
, O
но O
и O
на O
основе O
эмбеддингов O
: O
1 O
) O
На O
основе O
встроенного O
FastText B-Model
, O
который O
можно O
тренировать O
на O
том O
или O
ином O
корпусе O
2 O
) O
Предобученных O
моделей O
Gensim3 B-Model
) O
Любой O
другой O
объект O
, O
который O
имеет O
вид O
словаря O
, O
где O
на O
вход O
подается O
слово O
, O
а O
на O
выходе O
его O
эмбеддинги O

# text = Среди используемых стратегий извлечения представлений текстов из эмбеддингов слов, можно выделить:1) Weighted Average Transformer (WAT) – взвешивается каждое слово с некоторым весом
Среди O
используемых O
стратегий O
извлечения O
представлений O
текстов O
из O
эмбеддингов O
слов O
, O
можно O
выделить O
: O
1 O
) O
Weighted B-Method
Average I-Method
Transformer I-Method
( O
WAT B-ShortName_Method
) O
– O
взвешивается O
каждое O
слово O
с O
некоторым O
весом O

# text =  Bag of Random Embedding Projections (BOREP) – строится линейная модель со случайными весами  
Bag B-Method
of I-Method
Random I-Method
Embedding I-Method
Projections I-Method
( O
BOREP B-ShortName_Method
) O
– O
строится O
линейная O
модель O
со O
случайными O
весами O

# text =  Bert Pooling – получение эмбеддинга с последнего выхода модели Transformer  
Bert B-Method
Pooling I-Method
– O
получение O
эмбеддинга O
с O
последнего O
выхода O
модели O
Transformer B-Model

# text =  За препроцессинг текста отвечает класс токенайзера, по умолчанию применяется только для TF-IDF.
За O
препроцессинг O
текста O
отвечает O
класс O
токенайзера O
, O
по O
умолчанию O
применяется O
только O
для O
TF B-Metric
- I-Metric
IDF I-Metric
. O

# text =  Подводя итоги стоит сказать, что LightAutoML благодаря встроенному инструментарию способен показывать достаточно хорошие результаты в задачах бинарной или мультиклассовой классификации и регрессии.
Подводя O
итоги O
стоит O
сказать O
, O
что O
LightAutoML B-Technology
благодаря O
встроенному O
инструментарию O
способен O
показывать O
достаточно O
хорошие O
результаты O
в O
задачах O
бинарной O
или O
мультиклассовой B-Task
классификации I-Task
и O
регрессии O
. O

# text =  Конкретно в нашем случае нам удалось создать модель сентиментного анализа, которая с 89% точностью определяет эмоциональный окрас обращения и слова, которые оказывают на это наибольшее влияние.
Конкретно O
в O
нашем O
случае O
нам O
удалось O
создать O
модель O
сентиментного O
анализа O
, O
которая O
с O
89 B-Value
% I-Value
точностью B-Metric
определяет O
эмоциональный O
окрас O
обращения O
и O
слова O
, O
которые O
оказывают O
на O
это O
наибольшее O
влияние O
. O

# text =   Яндекс открывает датасеты Беспилотных автомобилей, Погоды и Переводчика, чтобы помочь решить проблему сдвига данных в ML       
Яндекс B-Organization
открывает O
датасеты O
Беспилотных O
автомобилей O
, O
Погоды O
и O
Переводчика O
, O
чтобы O
помочь O
решить O
проблему O
сдвига B-Task
данных I-Task
в O
ML B-ShortName_Science

# text =   Для современных моделей, которые используются в машинном переводе, такой язык представляет серьезную проблему, так как большинство переводчиков обучаются на чуть более формальном языке: классической литературе, юридических документах или статьях Википедии.
Для O
современных O
моделей O
, O
которые O
используются O
в O
машинном O
переводе O
, O
такой O
язык O
представляет O
серьезную O
проблему O
, O
так O
как O
большинство O
переводчиков O
обучаются O
на O
чуть O
более O
формальном O
языке O
: O
классической B-InfoResource
литературе I-InfoResource
, O
юридических B-InfoResource
документах I-InfoResource
или O
статьях B-InfoResource
Википедии B-InfoResource
. O

# text =   В треке перевода мы использовали для обучения англо-русский корпус WMT’20, который в основном состоит из государственных и новостных текстов.
В O
треке O
перевода O
мы O
использовали O
для O
обучения O
англо O
- O
русский O
корпус O
WMT’20 B-Corpus
, O
который O
в O
основном O
состоит O
из O
государственных O
и O
новостных B-InfoResource
текстов I-InfoResource
. O

# text =   Данные без сдвига взяты из англо-русского корпуса Newstest’19, а также из корпуса новостных текстов, собранных службой Global Voices и переведенных Яндексом.
Данные O
без O
сдвига O
взяты O
из O
англо B-Lang
- O
русского B-Lang
корпуса O
Newstest’19 B-Corpus
, O
а O
также O
из O
корпуса O
новостных O
текстов B-Object
, O
собранных O
службой O
Global B-Organization
Voices I-Organization
и O
переведенных O
Яндексом B-Organization
. O

# text =   Данные со сдвигом для отладки взяты из подготовленного для WMT Robustness Challenge корпуса Reddit и также переведены Яндексом.
Данные O
со O
сдвигом O
для O
отладки O
взяты O
из O
подготовленного O
для O
WMT O
Robustness O
Challenge O
корпуса O
Reddit B-Corpus
и O
также O
переведены O
Яндексом B-Organization
. O

# text =   Для проверки модели на данных со сдвигом мы также собрали, перевели и разметили дополнительные данные с Reddit.
Для O
проверки O
модели O
на O
данных O
со O
сдвигом O
мы O
также O
собрали O
, O
перевели O
и O
разметили O
дополнительные O
данные O
с O
Reddit B-Corpus
. O


# text =   Парсить комментарии мы будем с помощью официального API ВКонтакте для Python
Парсить O
комментарии O
мы O
будем O
с O
помощью O
официального O
API B-Application
ВКонтакте I-Application
для O
Python B-Environment

# text =   Необходимо убрать из комментария направление, чтобы при поиске расстояния Левенштейна меньше ошибаться.
Необходимо O
убрать O
из O
комментария O
направление O
, O
чтобы O
при O
поиске O
расстояния B-Metric
Левенштейна I-Metric
меньше O
ошибаться O
. O

# text =   Небольшая справка: расстояние Левенштейна — минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую.
Небольшая O
справка O
: O
расстояние B-Metric
Левенштейна I-Metric
— O
минимальное O
количество O
операций O
вставки O
одного O
символа O
, O
удаления O
одного O
символа O
и O
замены O
одного O
символа O
на O
другой O
, O
необходимых O
для O
превращения O
одной O
строки O
в O
другую O
. O

# text =   Его мы будем находить с помощью библиотеки fuzzywuzzy.
Его O
мы O
будем O
находить O
с O
помощью O
библиотеки O
fuzzywuzzy B-Library
. O

# text =   Для ускорения работы авторы библиотеки советуют также установить библиотеку python-Levenshtein.
Для O
ускорения O
работы O
авторы O
библиотеки O
советуют O
также O
установить O
библиотеку O
python B-Library
- I-Library
Levenshtein I-Library
. O

# text =   Его мне любезно предоставил разработчик приложения GoTrans, Александр Козлов.
Его O
мне O
любезно O
предоставил O
разработчик O
приложения O
GoTrans B-Technology
, O
Александр B-Person
Козлов I-Person
. O

# text =   Самый сложный кроссворд, составленный компьютером
Самый O
сложный O
кроссворд B-Object
, O
составленный O
компьютером O

# text =   Пример Deep Blue показывает, что программы ИИ могут участвовать в викторинах и обыгрывать людей.
Пример O
Deep B-Technology
Blue I-Technology
показывает O
, O
что O
программы O
ИИ O
могут O
участвовать O
в O
викторинах B-Object
и O
обыгрывать O
людей O
. O

# text =   Американский разработчик Мэтью Гинсберг (Matthew Ginsberg) создал программу под названием Dr Fill, которая справляется с кроссвордами гораздо лучше, чем абсолютное большинство людей, пишет New Scientist.
Американский O
разработчик O
Мэтью B-Person
Гинсберг I-Person
( O
Matthew B-Person
Ginsberg I-Person
) O
создал O
программу O
под O
названием O
Dr B-Technology
Fill I-Technology
, O
которая O
справляется O
с O
кроссвордами B-Object
гораздо O
лучше O
, O
чем O
абсолютное O
большинство O
людей O
, O
пишет O
New B-Organization
Scientist I-Organization
. O

# text =   Анализ тональности текстов с помощью сверточных нейронных сетей 
Анализ B-Task
тональности I-Task
текстов I-Task
с O
помощью O
сверточных B-Method
нейронных I-Method
сетей I-Method

# text =   Есть много способов решать такую задачу, и один из них — свёрточные нейронные сети (Convolutional Neural Networks).
Есть O
много O
способов O
решать O
такую O
задачу O
, O
и O
один O
из O
них O
— O
свёрточные B-Method
нейронные I-Method
сети I-Method
( O
Convolutional B-Method
Neural I-Method
Networks I-Method
) O
. O

# text =   CNN изначально были разработаны для обработки изображений, однако они успешно справляются с решением задач в сфере автоматической обработки текстов.
CNN B-Method
изначально O
были O
разработаны O
для O
обработки O
изображений O
, O
однако O
они O
успешно O
справляются O
с O
решением O
задач O
в O
сфере O
автоматической B-Science
обработки I-Science
текстов I-Science
. O

# text =   Я познакомлю вас с бинарным анализом тональности русскоязычных текстов с помощью свёрточной нейронной сети, для которой векторные представления слов были сформированы на основе обученной Word2Vec модели.
Я O
познакомлю O
вас O
с O
бинарным O
анализом B-Task
тональности I-Task
русскоязычных B-Lang
текстов B-Object
с O
помощью O
свёрточной B-Method
нейронной I-Method
сети I-Method
, O
для O
которой O
векторные B-Object
представления I-Object
слов I-Object
были O
сформированы O
на O
основе O
обученной O
Word2Vec B-Model
модели O
. O

# text =   Для обучения я выбрал корпус коротких текстов Юлии Рубцовой, сформированный на основе русскоязычных сообщений из Twitter [4].
Для O
обучения O
я O
выбрал O
корпус B-Corpus
коротких I-Corpus
текстов I-Corpus
Юлии B-Person
Рубцовой I-Person
, O
сформированный O
на O
основе O
русскоязычных B-Lang
сообщений B-Object
из O
Twitter B-Technology
[ O
4 O
] O
. O

# text =   Визуализация кластеров похожих слов с использование t-SNE.
Визуализация O
кластеров B-Result
похожих O
слов O
с O
использование O
t B-Method
- I-Method
SNE I-Method
. O

# text =   На следующем этапе каждый текст был отображен в массив идентификаторов токенов.
На O
следующем O
этапе O
каждый O
текст B-Object
был O
отображен O
в O
массив O
идентификаторов O
токенов B-Subject
. O

# text =   Вот пусть комментаторы поправят, но кроме модуля LanguageTool для Open Office (о нём мы ещё поговорим) даже в голову ничего не приходит.
Вот O
пусть O
комментаторы O
поправят O
, O
но O
кроме O
модуля O
LanguageTool B-Technology
для O
Open B-Technology
Office I-Technology
( O
о O
нём O
мы O
ещё O
поговорим O
) O
даже O
в O
голову O
ничего O
не O
приходит O
. O

# text =   Было бы здорово составить базу с инструкциями не для людей, а для роботов, подумали инженеры из Института искусственного интеллекта при Бременском университете (Германия), авторы проекта RoboHow.
Было O
бы O
здорово O
составить O
базу O
с O
инструкциями B-Object
не O
для O
людей O
, O
а O
для O
роботов O
, O
подумали O
инженеры O
из O
Института B-Organization
искусственного I-Organization
интеллекта I-Organization
при O
Бременском B-Organization
университете I-Organization
( O
Германия O
) O
, O
авторы O
проекта O
RoboHow B-Project
. O

# text =   С такой базой wiki-инструкций роботы смогут передавать информацию друг другу.
С O
такой O
базой O
wiki B-Database
- I-Database
инструкций I-Database
роботы O
смогут O
передавать O
информацию O
друг O
другу O
. O

# text =   Созданный в Бременском университете робот PR2 (на фото вверху) учится понимать и выполнять «человеческие» инструкции из базы WikiHow.
Созданный O
в O
Бременском B-Organization
университете I-Organization
робот O
PR2 O
( O
на O
фото O
вверху O
) O
учится O
понимать O
и O
выполнять O
« O
человеческие O
» O
инструкции B-Object
из O
базы O
WikiHow B-Database
. O

# text =   Успешно выполнив задачу, то есть усвоив урок, робот делится приобретёнными знаниями со своими собратьями через онлайновую базу Open Ease.
Успешно O
выполнив O
задачу O
, O
то O
есть O
усвоив O
урок O
, O
робот O
делится O
приобретёнными O
знаниями O
со O
своими O
собратьями O
через O
онлайновую O
базу O
Open B-Database
Ease I-Database
. O

# text =   Здесь инструкции записаны в машиночитаемом виде, на языке, похожем на язык Семантической сети.
Здесь O
инструкции B-Object
записаны O
в O
машиночитаемом O
виде O
, O
на O
языке O
, O
похожем O
на O
язык O
Семантической B-Application
сети I-Application
. O

# text =   Это очень сложная задача, которая сочетает в себе тесную интеграцию распознавания речи, интерпретации команд на естественном языке, машинного зрения и планирования сложных действий через алгоритмы осуществления отдельных манипуляций.
Это O
очень O
сложная O
задача O
, O
которая O
сочетает O
в O
себе O
тесную O
интеграцию O
распознавания B-Task
речи I-Task
, O
интерпретации B-Task
команд I-Task
на I-Task
естественном I-Task
языке I-Task
, O
машинного O
зрения O
и O
планирования O
сложных O
действий O
через O
алгоритмы O
осуществления O
отдельных O
манипуляций O
. O

# text =   «М.видео-Эльдорадо» внедряет нейросеть для ответов на вопросы покупателей 
« O
М.видео B-Organization
- I-Organization
Эльдорадо I-Organization
» O
внедряет O
нейросеть O
для O
ответов B-Task
на I-Task
вопросы I-Task
покупателей O

# text =   Президент Ассоциации больших данных Анна Серебряникова отметила, что ИИ в ретейле может применяться для прогнозирования открытия новых торговых точек, а также для персонализации предложений для клиентов и создания чат-ботов для службы поддержки.
Президент O
Ассоциации B-Organization
больших I-Organization
данных I-Organization
Анна B-Person
Серебряникова I-Person
отметила O
, O
что O
ИИ O
в O
ретейле O
может O
применяться O
для O
прогнозирования O
открытия O
новых O
торговых O
точек O
, O
а O
также O
для O
персонализации B-Task
предложений I-Task
для O
клиентов O
и O
создания B-Task
чат I-Task
- I-Task
ботов I-Task
для O
службы O
поддержки O
. O

# text =   В Facebook AI продемонстрировали прямой машинный перевод с одного языка на другой
В O
Facebook B-Organization
AI I-Organization
продемонстрировали O
прямой O
машинный O
перевод O
с O
одного O
языка O
на O
другой O

# text =  Facebook AI представила новую систему машинного перевода M2M-100 с 15 млрд параметров.
Facebook B-Organization
AI I-Organization
представила O
новую O
систему B-Application
машинного I-Application
перевода I-Application
M2M-100 B-Model
с O
15 O
млрд O
параметров O
. O

# text =   Она способна переводить с одного языка на другой напрямую, не используя английский в качестве промежуточного.
Она O
способна O
переводить B-Task
с I-Task
одного I-Task
языка I-Task
на I-Task
другой I-Task
напрямую O
, O
не O
используя O
английский O
в O
качестве O
промежуточного O
. O

# text =   Она способна осуществлять переводы между парами из ста языков.
Она O
способна O
осуществлять O
переводы B-Task
между O
парами O
из O
ста O
языков O
. O

# text =   Модель обучали на наборе данных из более чем 7,5 млрд предложений как из базы Facebook, так и из других источников.
Модель B-Object
обучали O
на O
наборе O
данных O
из O
более O
чем O
7,5 O
млрд O
предложений O
как O
из O
базы O
Facebook B-Organization
, O
так O
и O
из O
других O
источников O
. O

# text =   При разработке использовали инструмент CommonCrawl, который поддерживает открытый репозиторий данных веб-сканирования, и систему классификации текстов FastText, которую в Facebook представили несколько лет назад.
При O
разработке O
использовали O
инструмент O
CommonCrawl B-Technology
, O
который O
поддерживает O
открытый O
репозиторий O
данных O
веб O
- O
сканирования O
, O
и O
систему O
классификации O
текстов O
FastText App_system
, O
которую O
в O
Facebook B-Organization
представили O
несколько O
лет O
назад O
. O

# text =   Согласно метрикам BLEU, M2M-100 на 10 баллов опережает предшественника, где английский язык был промежуточным.
Согласно O
метрикам O
BLEU B-Metric
, O
M2M-100 B-Model
на O
10 O
баллов O
опережает O
предшественника O
, O
где O
английский O
язык  O
был O
промежуточным O
. O

# text =   Facebook AI отмечает, что эта модель может быть полезной не только при машинном переводе, но и при изучении языков.
Facebook B-Organization
AI I-Organization
отмечает O
, O
что O
эта O
модель O
может O
быть O
полезной O
не O
только O
при O
машинном B-Task
переводе I-Task
, O
но O
и O
при O
изучении O
языков O
. O

# text =   Я тестировала Google Translate на одних и тех же текстах в марте и декабре 2011, январе 2016 и декабре 2017 года.
Я O
тестировала O
Google B-Technology
Translate I-Technology
на O
одних O
и O
тех O
же O
текстах O
в O
марте O
и O
декабре O
2011 O
, O
январе O
2016 O
и O
декабре O
2017 O
года O
. O

# text =   Брала одни и те же отрывки на английском, русском, немецком, французском, украинском и польском языках и переводила каждый на остальные пять языков из выборки.
Брала O
одни O
и O
те O
же O
отрывки O
на O
английском B-Lang
, O
русском B-Lang
, O
немецком B-Lang
, O
французском B-Lang
, O
украинском B-Lang
и O
польском B-Lang
языках O
и O
переводила O
каждый O
на O
остальные O
пять O
языков O
из O
выборки O
. O

# text =   Результаты cross-verification в целом совпали с тенденциями в первоначальной выборке.
Результаты O
cross B-Method
- I-Method
verification I-Method
в O
целом O
совпали O
с O
тенденциями O
в O
первоначальной O
выборке O
. O

# text =   С марта 2017 года нейросеть стали использовать для перевода на русский.
С O
марта O
2017 B-TERM
года O
нейросеть O
стали O
использовать O
для O
перевода B-Task
на O
русский B-Lang
. O

# text =   Сервис не переводит дословно, результат стал более свободным: адекватная перефразировка, перегруппировка слов, перестановка слов из начала в конец предложения, если того требуют правила языка (в немецком это реализовано великолепно).
Сервис O
не O
переводит O
дословно O
, O
результат O
стал O
более O
свободным O
: O
адекватная O
перефразировка B-Task
, O
перегруппировка B-Task
слов I-Task
, O
перестановка B-Task
слов I-Task
из O
начала O
в O
конец O
предложения O
, O
если O
того O
требуют O
правила O
языка O
( O
в O
немецком B-Lang
это O
реализовано O
великолепно O
) O
. O

# text =   В отличие от предыдущего уровня (phrase-based translation– однократное нахождение соответствий отдельных слов и фраз), нейронный переводчик в какой-то степени трансформирует предложения, анализирует их как единое целое и устанавливает соответствия «из конца в конец» в несколько стадий(end-to-end mapping – сквозное преобразование, полного цикла, непрерывная трансформация многообразия данных со входа на выход).
В O
отличие O
от O
предыдущего O
уровня O
( O
phrase B-Method
- I-Method
based I-Method
translation I-Method
– O
однократное B-Method
нахождение I-Method
соответствий I-Method
отдельных I-Method
слов I-Method
и O
фраз O
) O
, O
нейронный O
переводчик O
в O
какой O
- O
то O
степени O
трансформирует O
предложения O
, O
анализирует O
их O
как O
единое O
целое O
и O
устанавливает O
соответствия O
« O
из O
конца O
в O
конец O
» O
в O
несколько O
стадий O
( O
end B-Method
- I-Method
to I-Method
- I-Method
end I-Method
mapping I-Method
– O
сквозное B-Method
преобразование I-Method
, O
полного O
цикла O
, O
непрерывная O
трансформация B-Method
многообразия I-Method
данных I-Method
со O
входа O
на O
выход O
) O
. O

# text =   Сейчас в Яндексе мой основной проект это Алиса, голосовой помощник, который Яндекс запустил в октябре прошлого года, и моя группа отвечает за то, что можно условно назвать мозгами Алисы.
Сейчас O
в O
Яндексе B-Organization
мой O
основной O
проект O
это O
Алиса B-Technology
, O
голосовой B-Object
помощник I-Object
, O
который O
Яндекс B-Organization
запустил O
в O
октябре O
прошлого O
года O
, O
и O
моя O
группа O
отвечает O
за O
то O
, O
что O
можно O
условно O
назвать O
мозгами O
Алисы B-Technology
. O

# text =   Мы интерпретируем то, что сказал пользователь на естественном языке и превращаем это в некоторое структурированное представление.
Мы O
интерпретируем O
то O
, O
что O
сказал O
пользователь O
на O
естественном B-Object
языке I-Object
и O
превращаем O
это O
в O
некоторое O
структурированное B-Object
представление B-Object
. O

# text =   Есть Siri, единственный голосовой помощник, который тоже понимает русский язык, но он работает только на iOS и MacOS, это как бы не самая популярная платформа в России, и к Siri как к продукту тоже есть определенные вопросы.
Есть O
Siri B-Technology
, O
единственный O
голосовой B-Object
помощник I-Object
, O
который O
тоже O
понимает O
русский B-Lang
язык O
, O
но O
он O
работает O
только O
на O
iOS B-Environment
и O
MacOS B-Environment
, O
это O
как O
бы O
не O
самая O
популярная O
платформа O
в O
России O
, O
и O
к O
Siri B-Technology
как O
к O
продукту O
тоже O
есть O
определенные O
вопросы O
. O

# text =   На самом деле у нас уже есть модель которая оценивает градацию этой оскорбительности, и если бы возникла продуктовая необходимость, мы уже могли бы сделать такой ползунок который делает ответы более или менее дерзкими.
На O
самом O
деле O
у O
нас O
уже O
есть O
модель O
которая O
оценивает B-Task
градацию I-Task
этой O
оскорбительности I-Task
, O
и O
если O
бы O
возникла O
продуктовая O
необходимость O
, O
мы O
уже O
могли O
бы O
сделать O
такой O
ползунок O
который O
делает O
ответы O
более O
или O
менее O
дерзкими O
. O

# text =   Это генеративная нейронная сеть, способная решать множество задач по обработке естествнного языка (NLP).
Это O
генеративная B-Method
нейронная I-Method
сеть I-Method
, O
способная O
решать O
множество O
задач O
по O
обработке B-Science
естествнного I-Science
языка I-Science
( O
NLP B-ShortName_Method
) O
. O

# text =   Это такие задачи как суммаризация (сделать из большого текста его резюме), понимание текста (NLU), вопросно-ответные системы, генерация (например, стихов, — на Хабре была хорошая статья) и другие.
Это O
такие O
задачи O
как O
суммаризация B-Task
( O
сделать O
из O
большого O
текста O
его O
резюме O
) O
, O
понимание B-Task
текста I-Task
( O
NLU Abbrev_Task
) O
, O
вопросно B-App_system
- I-App_system
ответные I-App_system
системы I-App_system
, O
генерация B-Task
( O
например O
, O
стихов O
, O
— O
на O
Хабре B-Organization
была O
хорошая O
статья O
) O
и O
другие O
. O

# text =   В Яндекс.Браузер внедрили машинный перевод видеороликов 
В O
Яндекс B-Technology
. I-Technology
Браузер I-Technology
внедрили O
машинный B-Science
перевод I-Science
видеороликов O

# text =   Алгоритм отслеживает темп речи говорящего, за счет чего переводчик делает паузы, замедляет или ускоряет речь, чтобы закадровый голос совпадал с картинкой.Перевод доступен в Яндекс.Браузере для Windows и macOS.
Алгоритм O
отслеживает O
темп B-Object
речи I-Object
говорящего O
, O
за O
счет O
чего O
переводчик O
делает O
паузы B-Object
, O
замедляет O
или O
ускоряет O
речь B-Subject
, O
чтобы O
закадровый O
голос B-Subject
совпадал O
с O
картинкой O
. O
Перевод O
доступен O
в O
Яндекс B-Organization
. O
Браузере O
для O
Windows B-Environment
и O
macOS B-Environment
. O

# text =   Тогда к статистической модели, которая была в «Переводчике» с момента запуска, добавили технологию перевода с помощью нейросети.
Тогда O
к O
статистической O
модели O
, O
которая O
была O
в O
« O
Переводчике B-Application
» O
с O
момента O
запуска O
, O
добавили O
технологию O
перевода B-Task
с O
помощью O
нейросети O
. O

# text =   Компания объясняла, что ее технология не разбивает текст на отдельные слова, а рассматривает его целиком, чтобы лучше передать смысл.В июне Яндекс открыл доступ к нейросети «Балабоба» для всех пользователей.
Компания O
объясняла O
, O
что O
ее O
технология O
не O
разбивает O
текст B-Object
на O
отдельные O
слова B-Subject
, O
а O
рассматривает O
его O
целиком O
, O
чтобы O
лучше O
передать O
смысл O
. O
В O
июне O
Яндекс B-Organization
открыл O
доступ O
к O
нейросети O
« O
Балабоба B-Technology
» O
для O
всех O
пользователей O
. O

# text =   Она работает на языковой модели из семейства YaLM (Yet another Language Model).
Она O
работает O
на O
языковой O
модели O
из O
семейства O
YaLM B-ShortName_Model
( O
Yet B-Model
another I-Model
Language I-Model
Model I-Model
) O
. O

# text =   Эта модель помогает нейросети запоминать правила языка, выбирать подходящие слова и связывать их по смыслу.
Эта O
модель O
помогает O
нейросети O
запоминать B-Task
правила I-Task
языка I-Task
, O
выбирать B-Task
подходящие I-Task
слова I-Task
и O
связывать O
их O
по O
смыслу O
. O

# text =   У «Балабобы» нет своего мнения, она выдает случайные продолжения и может закончить историю, придумать подпись или написать небольшой рассказ.
У O
« O
Балабобы B-Technology
» O
нет O
своего O
мнения O
, O
она O
выдает O
случайные O
продолжения O
и O
может O
закончить O
историю O
, O
придумать O
подпись O
или O
написать O
небольшой O
рассказ O
. O

# text =   AntiToxicBot — бот, распознающий токсичных пользователей в телеграм чатах.
AntiToxicBot B-Technology
— O
бот O
, O
распознающий B-Task
токсичных I-Task
пользователей I-Task
в O
телеграм O
чатах O
. O

# text =   Почему же выбрано CNN+GRU, а не просто GRU или CNN?
Почему O
же O
выбрано O
CNN+GRU B-Method
, O
а O
не O
просто O
GRU B-Method
или O
CNN B-Method
? O

# text =   Нейросеть состоит из 3-х основных частей(CNN, GRU, Linear).
Нейросеть O
состоит O
из O
3-х O
основных O
частей O
( O
CNN B-Method
, O
GRU B-Method
, O
Linear B-Method
) O
. O

# text =   Как и в классификации картинок, свёрточный слой выделяет “признаки”, но в нашем случае векторизированный текст.
Как O
и O
в O
классификации O
картинок O
, O
свёрточный O
слой O
выделяет O
“ O
признаки O
” O
, O
но O
в O
нашем O
случае O
векторизированный B-Object
текст I-Object
. O

# text =   То-есть данная часть сети учится выделять признаки токсичных и позитивных сообщений.
То O
- O
есть O
данная O
часть O
сети O
учится O
выделять B-Task
признаки I-Task
токсичных I-Task
и I-Task
позитивных I-Task
сообщений I-Task
. O

# text =   GRU - Recurrent Neural Network
GRU B-Method
- O
Recurrent B-Method
Neural I-Method
Network

# text =   Чтобы обрабатывать последовательности произвольной длины, используют рекуррентные слои.
Чтобы O
обрабатывать B-Task
последовательности I-Task
произвольной I-Task
длины I-Task
, O
используют O
рекуррентные O
слои O
. O

# text =   В архитектуре используется рекуррентный слой GRU.
В O
архитектуре O
используется O
рекуррентный O
слой O
GRU B-Method
. O

# text =  Данный слой учится делать заключительное решение по определению тональности текста на основе предыдущих слоёв.
Данный O
слой O
учится O
делать O
заключительное O
решение O
по O
определению B-Task
тональности I-Task
текста I-Task
на O
основе O
предыдущих O
слоёв O
. O

# text =   Датасет был взят с сайта kaggle.
Датасет O
был O
взят O
с O
сайта O
kaggle B-Organization
. O

# text =   Около 14000 комментариев с разметкой токсичное сообщение или нет.
Около O
14000 O
комментариев O
с O
разметкой B-Method
токсичное B-Labeling
сообщение I-Labeling
или O
нет O
. O

# text =   Для решения данной проблемы была использована библиотека Yandex Speller, которая исправляет орфографические ошибки.
Для O
решения O
данной O
проблемы O
была O
использована O
библиотека O
Yandex B-Library
Speller I-Library
, O
которая O
исправляет B-Task
орфографические I-Task
ошибки I-Task
. O

# text =   Можно было обучить собственный Word2Vec на основе данного набора данных, но лучше взять уже обученный.
Можно O
было O
обучить O
собственный O
Word2Vec B-Model
на O
основе B-Subject
данного B-Object
набора O
данных O
, O
но O
лучше O
взять O
уже O
обученный O
. O

# text =   Например: Navec.
Например O
: O
Navec B-Model
. O

# text =   Модель обучали на русской литературе (~150gb), что говорит о качественной векторизации текста.
Модель B-Object
обучали O
на O
русской B-Science
литературе I-Science
( O
~150 O
gb O
) O
, O
что O
говорит O
о O
качественной O
векторизации O
текста O
. O

# text =  Для классификации используется обыкновенная функция потерь – кросс энтропия.
Для O
классификации B-Task
используется O
обыкновенная O
функция B-Method
потерь I-Method
– I-Method
кросс I-Method
энтропия I-Method
. O

# text =  При обучении сети надо обращать внимание на основные параметры такие, как loss, precision и accuracy.
При O
обучении O
сети O
надо O
обращать O
внимание O
на O
основные O
параметры O
такие O
, O
как O
loss B-Metric
, O
precision B-Metric
и O
accuracy B-Metric
. O

# text =   В ~80% случаев нейросеть классифицирует тональность текста правильно.
В O
~80 B-Value
% O
случаев O
нейросеть O
классифицирует O
тональность O
текста O
правильно O
. O

# text =   Теперь нейронная сеть указала конкретные сцены, написанные не Шекспиром, и определила, кто на самом деле их написал.
Теперь O
нейронная B-Method
сеть I-Method
указала O
конкретные O
сцены O
, O
написанные O
не O
Шекспиром O
, O
и O
определила O
, O
кто O
на O
самом O
деле O
их O
написал O
. O

# text =   Плехач обучил алгоритм распознавать стиль Шекспира на пьесах «Кориолан», «Цимбелин», «Зимняя сказка» и «Буря».
Плехач B-Person
обучил O
алгоритм O
распознавать B-Task
стиль I-Task
Шекспира O
на O
пьесах B-Object
« O
Кориолан O
» O
, O
« O
Цимбелин O
» O
, O
« O
Зимняя O
сказка O
» O
и O
« O
Буря O
» O
. O

# text =   В результате искусственный интеллект согласился с анализом Спеддинга.
В O
результате O
искусственный O
интеллект O
согласился O
с O
анализом B-Method
Спеддинга I-Method
. O

# text =   В прошлом году учёные из Университета Торонто, Мельбурнского Университета и подразделения IBM в Австралии научили искусственный интеллект генерировать сонеты в шекспировском стиле.
В O
прошлом O
году O
учёные O
из O
Университета B-Organization
Торонто I-Organization
, O
Мельбурнского B-Organization
Университета I-Organization
и O
подразделения O
IBM B-Organization
в O
Австралии O
научили O
искусственный O
интеллект O
генерировать O
сонеты O
в O
шекспировском O
стиле O
. O

# text =   Алгоритм под названием Deepspeare обучали на 2,7 тыс. сонетов Шекспира, после чего он научился писать собственные, придерживаясь похожего стиля.
Алгоритм O
под O
названием O
Deepspeare B-Method
обучали O
на O
2,7 O
сонетов B-Object
Шекспира O
, O
после O
чего O
он O
научился O
писать O
собственные O
, O
придерживаясь O
похожего O
стиля O
. O

# text =   Как научить свою нейросеть генерировать стихи
Как O
научить O
свою O
нейросеть O
генерировать B-Task
стихи I-Task

# text =   Языковые модели определяют вероятность появления последовательности слов  в данном языке: .
Языковые O
модели O
определяют B-Task
вероятность I-Task
появления I-Task
последовательности I-Task
слов I-Task
в O
данном O
языке B-Object
: O
. O

# text =   Кажется, самым простым способом построить такую модель является использование N-граммной статистики.
Кажется O
, O
самым O
простым O
способом O
построить O
такую O
модель B-Object
является O
использование O
N B-Method
- I-Method
граммной I-Method
статистики I-Method
. O

# text =   Для решения такой проблемы используют обычно сглаживание Kneser–Ney или Katz’s backing-off.
Для O
решения O
такой O
проблемы O
используют O
обычно O
сглаживание B-Method
Kneser I-Method
– I-Method
Ney I-Method
или O
Katz B-Method
’s I-Method
backing I-Method
- I-Method
off I-Method
. O

# text =   За более подробной информацией про методы сглаживания N-грамм стоит обратиться к известной книге Кристофера Маннинга “Foundations of Statistical Natural Language Processing”.
За O
более O
подробной O
информацией O
про O
методы O
сглаживания B-Method
N I-Method
- I-Method
грамм I-Method
стоит O
обратиться O
к O
известной O
книге O
Кристофера B-Person
Маннинга I-Person
“ O
Foundations B-Publication
of I-Publication
Statistical I-Publication
Natural I-Publication
Language I-Publication
Processing I-Publication
” O
. O

# text =   Хочу заметить, что 5-граммы слов я назвал не просто так: именно их (со сглаживанием, конечно) Google демонстрирует в статье “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling” — и показывает результаты, весьма сопоставимые с результатами у рекуррентных нейронных сетей — о которых, собственно, и пойдет далее речь.
Хочу O
заметить O
, O
что O
5-граммы O
слов O
я O
назвал O
не O
просто O
так O
: O
именно O
их O
( O
со O
сглаживанием O
, O
конечно O
) O
Google B-Organization
демонстрирует O
в O
статье O
“ B-Publication
One I-Publication
Billion I-Publication
Word I-Publication
Benchmark I-Publication
for I-Publication
Measuring I-Publication
Progress I-Publication
in I-Publication
Statistical I-Publication
Language I-Publication
Modeling I-Publication
” I-Publication
— O
и O
показывает O
результаты O
, O
весьма O
сопоставимые O
с O
результатами O
у O
рекуррентных B-Method
нейронных I-Method
сетей I-Method
— O
о O
которых O
, O
собственно O
, O
и O
пойдет O
далее O
речь O
. O

# text =   Преимущество рекуррентных нейронных сетей — в возможности использовать неограниченно длинный контекст.
Преимущество O
рекуррентных B-Method
нейронных I-Method
сетей I-Method
— O
в O
возможности O
использовать O
неограниченно O
длинный O
контекст O
. O

# text =   На практике классические RNN страдают от затухания градиента — по сути, отсутствия возможности помнить контекст дальше, чем на несколько слов.
На O
практике O
классические O
RNN B-ShortName_Method
страдают O
от O
затухания O
градиента O
— O
по O
сути O
, O
отсутствия O
возможности O
помнить O
контекст O
дальше O
, O
чем O
на O
несколько O
слов O
. O

# text =   Самыми популярными являются LSTM и GRU.
Самыми O
популярными O
являются O
LSTM B-ShortName_Method
и O
GRU B-ShortName_Method
. O

# text =   В дальнейшем, говоря о рекуррентном слое, я всегда буду подразумевать LSTM.
В O
дальнейшем O
, O
говоря O
о O
рекуррентном O
слое O
, O
я O
всегда O
буду O
подразумевать O
LSTM B-ShortName_Method
. O

# text =   Вспомним теперь, что для нашей задачи языковая модель нужна для выбора наиболее подходящего следующего слова по уже сгенерированной последовательности.
Вспомним O
теперь O
, O
что O
для O
нашей O
задачи O
языковая O
модель B-Object
нужна O
для O
выбора B-Task
наиболее I-Task
подходящего I-Task
следующего I-Task
слова I-Task
по O
уже O
сгенерированной B-Object
последовательности I-Object
. O

# text =   Метрические правила определяют последовательность ударных и безударных слогов в строке.
Метрические B-Object
правила I-Object
определяют O
последовательность O
ударных O
и O
безударных O
слогов B-Subject
в O
строке O
. O

# text =   Для решения этой проблемы мы делаем лучевой поиск (beam search), выбирая на каждом шаге вместо одного сразу N путей с наивысшими вероятностями.
Для O
решения O
этой O
проблемы O
мы O
делаем O
лучевой B-Method
поиск I-Method
( O
beam B-Method
search I-Method
) O
, O
выбирая O
на O
каждом O
шаге O
вместо O
одного O
сразу O
N O
путей O
с O
наивысшими O
вероятностями O
. O

# text =  Автоматическое определение эмоций в текстовых беседах с использованием нейронных сетей
Автоматическое B-Task
определение I-Task
эмоций I-Task
в O
текстовых B-Object
беседах I-Object
с O
использованием O
нейронных B-Method
сетей I-Method

#text = Одна из основных задач диалоговых систем состоит не только в предоставлении нужной пользователю информации, но и в генерации как можно более человеческих ответов.
Одна O
из O
основных O
задач O
диалоговых O
систем O
состоит O
не O
только O
в O
предоставлении B-Task
нужной I-Task
пользователю I-Task
информации I-Task
, O
но O
и O
в O
генерации B-Task
как O
можно O
более O
человеческих O
ответов I-Task
. O

# text =   В этой статье мы рассмотрим архитектуру рекуррентной нейросети для определения эмоций в текстовых беседах, которая принимала участие в SemEval-2019 Task 3 “EmoContext”, ежегодном соревновании по компьютерной лингвистике.
В O
этой O
статье O
мы O
рассмотрим O
архитектуру O
рекуррентной B-Method
нейросети I-Method
для O
определения B-Task
эмоций I-Task
в O
текстовых B-Object
беседах I-Object
, O
которая O
принимала O
участие O
в O
SemEval-2019 O
Task O
3 O
“ O
EmoContext O
” O
, O
ежегодном O
соревновании O
по O
компьютерной B-Science
лингвистике I-Science
. O

# text =   Задача состояла в классификации эмоций (“happy”, “sad”, “angry” и “others”) в беседе из трех реплик, в которой участвовали чат-бот и человек.
Задача O
состояла O
в O
классификации B-Task
эмоций I-Task
( O
“ O
happy B-Labeling
” O
, O
“ O
sad B-Labeling
” O
, O
“ O
angry B-Labeling
” O
и O
“ O
others B-Labeling
” O
) O
в O
беседе O
из O
трех O
реплик O
, O
в O
которой O
участвовали O
чат O
- O
бот O
и O
человек O
. O

# text =   В четвёртой части мы опишем архитектуру LSTM, которую мы использовали в соревновании.
В O
четвёртой O
части O
мы O
опишем O
архитектуру O
LSTM B-ShortName_Method
, O
которую O
мы O
использовали O
в O
соревновании O
. O

# text =   Код написан на языке Python с использованием библиотеки Keras.
Код O
написан O
на O
языке O
Python B-Environment
с O
использованием O
библиотеки O
Keras B-Library
. O

# text =   Подробное описание представлено здесь: (Chatterjee et al., 2019).
Подробное O
описание O
представлено O
здесь O
: O
( O
Chatterjee B-Publication
et I-Publication
al I-Publication
. I-Publication
, I-Publication
2019 I-Publication
) O
. O

# text =   Примеры из датасета EmoContext (Chatterjee et al., 2019)
Примеры O
из O
датасета O
EmoContext B-Dataset
( O
Chatterjee B-Publication
et I-Publication
al I-Publication
. I-Publication
, I-Publication
2019 I-Publication
) O

# text =   Данные предоставлены Microsoft, скачать их можно в официальной группе в LinkedIn.
Данные O
предоставлены O
Microsoft B-Organization
, O
скачать O
их O
можно O
в O
официальной O
группе O
в O
LinkedIn B-Application
. O

# text =   В дополнение к этим данным мы собрали 900 тыс. англоязычных сообщений из Twitter, чтобы создать Distant-датасет (300 тыс. твитов на каждую эмоцию).
В O
дополнение O
к O
этим O
данным O
мы O
собрали O
900 O
тыс. O
англоязычных O
сообщений O
из O
Twitter B-Organization
, O
чтобы O
создать O
Distant B-Dataset
- O
датасет O
( O
300 O
тыс. O
твитов O
на O
каждую O
эмоцию O
) O
. O

# text =   При его создании мы придерживались стратегии Go et al. (2009), в рамках которой просто ассоциировали сообщения с наличием относящихся к эмоциям слов, таких как #angry, #annoyed, #happy, #sad, #surprised и так далее.
При O
его O
создании O
мы O
придерживались O
стратегии O
Go B-Publication
et I-Publication
al I-Publication
. I-Publication
( I-Publication
2009 I-Publication
) I-Publication
, O
в O
рамках O
которой O
просто O
ассоциировали B-Task
сообщения I-Task
с I-Task
наличием I-Task
относящихся I-Task
к I-Task
эмоциям I-Task
слов I-Task
, O
таких O
как O
# O
angry B-Labeling
, O
# O
annoyed B-Labeling
, O
# O
happy B-Labeling
, O
# O
sad B-Labeling
, O
# O
surprised B-Labeling
и O
так O
далее O
. O

# text =   Список терминов основан на терминах из SemEval-2018 AIT DISC (Duppada et al., 2018).
Список O
терминов B-Subject
основан O
на O
терминах B-Subject
из O
SemEval-2018 B-Dataset
AIT I-Dataset
DISC I-Dataset
( O
Duppada B-Publication
et I-Publication
al I-Publication
. I-Publication
, I-Publication
2018 I-Publication
) O
. O

# text =   Главной метрикой качества в соревновании EmoContext является усредненная F1-мера для трёх классов эмоций, то есть для классов «happy», «sad» и «angry».
Главной O
метрикой O
качества O
в O
соревновании O
EmoContext O
является O
усредненная O
F1-мера B-Metric
для O
трёх O
классов B-Object
эмоций O
, O
то O
есть O
для O
классов O
« O
happy B-Labeling
» O
, O
« O
sad B-Labeling
» O
и O
« O
angry B-Labeling
» O
. O

# text =   Перед обучением мы предварительно обработали тексты с помощью инструмента Ekphrasis (Baziotis et al., 2017).
Перед O
обучением O
мы O
предварительно O
обработали O
тексты B-Object
с O
помощью O
инструмента B-Object
Ekphrasis B-Application
( O
Baziotis B-Publication
et I-Publication
al I-Publication
. I-Publication
, I-Publication
2017 I-Publication
) O
. O

# text =   Он помогает исправить орфографию, нормализовать слова, сегментировать, а также определить, какие токены следует отбросить, нормализовать или аннотировать с помощью специальных тегов.
Он O
помогает O
исправить B-Task
орфографию I-Task
, O
нормализовать B-Task
слова I-Task
, O
сегментировать B-Task
, O
а O
также O
определить O
, O
какие O
токены B-Subject
следует O
отбросить O
, O
нормализовать O
или O
аннотировать O
с O
помощью O
специальных O
тегов B-Subject
. O

# text =   Кроме того, Emphasis содержит токенизатор, который может идентифицировать большинство эмодзи, эмотиконов и сложных выражений, а также даты, время, валюты и акронимы.
Кроме O
того O
, O
Emphasis B-Application
содержит O
токенизатор B-Method
, O
который O
может O
идентифицировать B-Task
большинство I-Task
эмодзи I-Task
, O
эмотиконов I-Task
и O
сложных I-Task
выражений I-Task
, O
а O
также O
даты I-Task
, O
время I-Task
, O
валюты I-Task
и O
акронимы I-Task
. O

