# text =   На основании триггера на определенные ключевые слова она сможет определять, к примеру, признаки обмана, мошенничества.
На O
основании O
триггера O
на O
определенные O
ключевые B-Subject
слова I-Subject
она O
сможет O
определять O
, O
к O
примеру O
, O
признаки O
обмана O
, O
мошенничества O
. O

# text =   То есть, сформировав некоторый корпус слов-триггеров, вполне возможно классифицировать сайты по их текстовому содержанию.
То O
есть O
, O
сформировав O
некоторый O
корпус B-InfoResource
слов I-InfoResource
- I-InfoResource
триггеров I-InfoResource
, O
вполне O
возможно O
классифицировать O
сайты O
по O
их O
текстовому O
содержанию O
. O

# text =   Задача распознавания текста относится к сфере обработки естественного языка или NLP (natural language processing).
Задача B-Task
распознавания I-Task
текста I-Task
относится O
к O
сфере O
обработки B-Science
естественного I-Science
языка I-Science
или O
NLP B-ShortName_Science
( O
natural B-Science
language I-Science
processing I-Science
) O
. O

# text =   NLP — направление искусственного интеллекта, нацеленное на обработку и анализ данных на естественном языке и обучение машин взаимодействию с людьми [1].
NLP B-Science
— O
направление O
искусственного B-Science
интеллекта I-Science
, O
нацеленное O
на O
обработку O
и O
анализ B-Method
данных I-Method
на O
естественном O
языке O
и O
обучение O
машин O
взаимодействию O
с O
людьми O
[ O
1 O
] O
. O

# text =   Такой подход называется методом вложения слов (word embedding).
Такой O
подход O
называется O
методом B-Method
вложения I-Method
слов I-Method
( O
word B-Subject
embedding I-Subject
) O
. O

# text =   Используя данные, состоящие из таких векторов, мы можем применять различные методы Machine Learning.
Используя O
данные O
, O
состоящие O
из O
таких O
векторов O
, O
мы O
можем O
применять O
различные O
методы O
Machine B-Science
Learning I-Science
. O

# text =   И поскольку искусственные нейронные сети лучшим образом справляются с векторно-матричными вычислениями, то выбор в их пользу становиться очевидным.
И O
поскольку O
искусственные B-Method
нейронные I-Method
сети I-Method
лучшим O
образом O
справляются O
с O
векторно B-Method
- I-Method
матричными I-Method
вычислениями I-Method
, O
то O
выбор O
в O
их O
пользу O
становиться O
очевидным O
. O

# text =   Искусственная нейронная сеть — это математическая модель, а также ее программное или аппаратное воплощение, построенные по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.
Искусственная B-Method
нейронная I-Method
сеть I-Method
— O
это O
математическая B-Model
модель I-Model
, O
а O
также O
ее O
программное O
или O
аппаратное O
воплощение O
, O
построенные O
по O
принципу O
организации O
и O
функционирования O
биологических O
нейронных O
сетей O
— O
сетей O
нервных O
клеток O
живого O
организма O
. O

# text =   Современные модели представляют собой так называемые глубокие модели.
Современные O
модели O
представляют O
собой O
так O
называемые O
глубокие B-Model
модели I-Model
. O

# text =   И в ее решении наилучшие метрики точности были достигнуты рекуррентными нейронными сетями, LSTM (сети с долгой краткосрочной памятью).
И O
в O
ее O
решении O
наилучшие O
метрики O
точности B-Metric
были O
достигнуты O
рекуррентными B-Model
нейронными I-Model
сетями I-Model
, O
LSTM B-ShortName_Model
( O
сети B-Model
с I-Model
долгой I-Model
краткосрочной I-Model
памятью I-Model
) O
. O

# text =   Позже свое превосходство в этой нише обрели NLP-модели-трансформеры.
Позже O
свое O
превосходство O
в O
этой O
нише O
обрели O
NLP B-Model
- I-Model
модели B-Model
- I-Model
трансформеры I-Model
. O

# text =  Описание упомянутых рекуррентных нейросетей (RNN), LSTM и GRU выходит за рамки темы статьи.
Описание O
упомянутых O
рекуррентных B-Model
нейросетей I-Model
( O
RNN B-ShortName_Model
) O
, O
LSTM B-ShortName_Model
и O
GRU B-ShortName_Model
выходит O
за O
рамки O
темы O
статьи O
. O

# text =   Однако RNN способны фиксировать зависимости только в одном направлении языка.
Однако O
RNN B-ShortName_Model
способны O
фиксировать B-Task
зависимости I-Task
только O
в O
одном O
направлении O
языка O
. O

# text =   Кроме этого, RNN не очень хороши в захвате долгосрочных зависимостей.
Кроме O
этого O
, O
RNN B-ShortName_Model
не O
очень O
хороши O
в O
захвате B-Task
долгосрочных I-Task
зависимостей I-Task
. O

# text =  LSTM избегают проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.
LSTM B-ShortName_Model
избегают O
проблемы O
долговременной O
зависимости O
, O
запоминая O
значения O
как O
на O
короткие O
, O
так O
и O
на O
длинные O
промежутки O
времени O
. O

# text =   Это объясняется тем, что LSTM не использует функцию активации внутри своих рекуррентных компонентов.
Это O
объясняется O
тем O
, O
что O
LSTM B-ShortName_Model
не O
использует O
функцию B-Method
активации I-Method
внутри O
своих O
рекуррентных O
компонентов O
. O

# text =   LSTM часто используются в машинном переводе и в задачах генерирования текстов на естественном языке.
LSTM B-ShortName_Model
часто O
используются O
в O
машинном B-Science
переводе I-Science
и O
в O
задачах O
генерирования B-Task
текстов I-Task
на O
естественном O
языке O
. O

# text =   Прежде чем использовать такой мощный и в то же время сложный инструмент, наша команда протестировала и более простые NLP-методы машинного обучения, в том числе «наивный байесовский классификатор», алгоритмы, использующие bag-of-words («мешок слов» — метод представления слов) и tf-idf (метрика определения частоты вхождения слов), а также простейшие модели нейронных сетей, состоящие из небольшого количества скрытых слоев.
Прежде O
чем O
использовать O
такой O
мощный O
и O
в O
то O
же O
время O
сложный O
инструмент O
, O
наша O
команда O
протестировала O
и O
более O
простые O
NLP O
- O
методы O
машинного O
обучения O
, O
в O
том O
числе O
« O
наивный B-Method
байесовский I-Method
классификатор I-Method
» O
, O
алгоритмы O
, O
использующие O
bag B-Method
- I-Method
of I-Method
- I-Method
words I-Method
( O
« O
мешок B-Method
слов I-Method
» O
— O
метод B-Method
представления I-Method
слов I-Method
) O
и O
tf B-Metric
- I-Metric
idf B-Metric
( O
метрика B-TERM
определения B-Subject
частоты I-TERM
вхождения I-TERM
слов O
) O
, O
а O
также O
простейшие O
модели O
нейронных O
сетей O
, O
состоящие O
из O
небольшого O
количества O
скрытых O
слоев O
. O

# text =   BERT, или Bidirectional Encoder Representations from Transformers, — нейросетевая модель-трансформер от Google, на которой сегодня строится большинство инструментов автоматической обработки языка.
BERT B-ShortName_Model
, O
или O
Bidirectional B-TEModelRM
Encoder I-Model
Representations I-Model
from I-Model
Transformers I-Model
, O
— O
нейросетевая O
модель O
- O
трансформер O
от O
Google B-Organization
, O
на O
которой O
сегодня O
строится O
большинство O
инструментов O
автоматической O
обработки O
языка O
. O

# text =  Релиз BERT в 2018 году стал некоторой переломной точкой в развитии NLP-моделей.
Релиз O
BERT B-ShortName_Model
в O
2018 B-Date
году I-Date
стал O
некоторой O
переломной O
точкой O
в O
развитии O
NLP B-Model
- I-Model
моделей I-Model
. O

# text =   Его появлению предшествовал ряд недавних разработок в области обработки естественного языка (BERT, ELMO и Ко в картинках — как в NLP пришло трансферное обучение): Semi-supervised Sequence learning (Andrew Dai и Quoc Le), ELMo (Matthew Peters и исследователи из AI2 и UW CSE), ULMFiT (Jeremy Howard и Sebastian Ruder), OpenAI Transformer (исследователи OpenAI Radford, Narasimhan, Salimans, и Sutskever) и Трансформер (Vaswani et al).
Его O
появлению O
предшествовал O
ряд O
недавних O
разработок O
в O
области O
обработки O
естественного O
языка O
( O
BERT B-ShortName_Model
, O
ELMO B-ShortName_Model
и O
Ко B-ShortName_Model
в O
картинках O
— O
как O
в O
NLP B-ShortName_Science
пришло O
трансферное B-Method
обучение I-Method
): O
Semi B-Method
- I-Method
supervised I-Method
Sequence I-Method
learning I-Method
( O
Andrew B-Person
Dai I-Person
и O
Quoc B-Person
Le I-Person
) O
, O
ELMo B-Model
( O
Matthew B-Person
Peters I-Person
и O
исследователи O
из O
AI2 B-Organization
и O
UW B-Organization
CSE B-Organization
) O
, O
ULMFiT B-Model
( O
Jeremy B-Person
Howard I-Person
и O
Sebastian B-Person
Ruder I-Person
) O
, O
OpenAI B-TERM
Transformer I-TERM
( O
исследователи O
OpenAI B-Organization
Radford I-Organization
, O
Narasimhan B-Person
, O
Salimans B-Person
, O
и O
Sutskever B-Person
) O
и O
Трансформер B-Model
( O
Vaswani B-Person
et O
al O
)O 
. O

# text =  Трансформеры в машинном обучении — это семейство архитектур нейронных сетей, общая идея которых основана на так называемом «самовнимании» (self-attention).
Трансформеры B-Model
в O
машинном B-Science
обучении I-Science
— O
это O
семейство O
архитектур O
нейронных O
сетей O
, O
общая O
идея O
которых O
основана O
на O
так O
называемом O
« O
самовнимании B-Method
» O
( O
self B-Method
- I-Method
attention I-Method
) O
. O

# text =   Однако алгоритм Self-attention не сразу поймет смысл предложения.
Однако O
алгоритм O
Self B-Method
- I-Method
attention I-Method
не O
сразу O
поймет O
смысл O
предложения O
. O

# text =   Потом результаты сетей объединяется.По своей сути BERT — это обученный стек энкодеров Трансформера.
Потом O
результаты O
сетей O
объединяется O
. O
По O
своей O
сути O
BERT B-ShortName_Model
— O
это O
обученный O
стек O
энкодеров B-Method
Трансформера I-Method
. O

# text =   Разработкой и обучением модели BERT занималась целая группа исследователей Google AI Language на многомиллионном наборе слов на разных языках (более 100).
Разработкой O
и O
обучением O
модели O
BERT B-ShortName_Model
занималась O
целая O
группа O
исследователей O
Google B-Organization
AI I-Organization
Language I-Organization
на O
многомиллионном O
наборе O
слов O
на O
разных O
языках O
( O
более O
100 O
) O
. O

# text =   И мы дообучили BERT распознавать текстовое содержимое сайтов по 63 категориям (медицина, здоровье, видео, интернет-магазины, юмористические сайты, эротика, оружие, секты, криминал и пр.).
И O
мы O
дообучили O
BERT B-ShortName_Model
распознавать O
текстовое O
содержимое O
сайтов O
по O
63 O
категориям O
( O
медицина B-Science
, O
здоровье O
, O
видео O
, O
интернет O
- O
магазины O
, O
юмористические O
сайты O
, O
эротика O
, O
оружие O
, O
секты O
, O
криминал O
) O
. O

# text =   Smart-Cat на первом этапе проводит их предобработку.
Smart B-App_system
- I-App_system
Cat I-App_system
на O
первом O
этапе O
проводит O
их O
предобработку O
. O

# text =   Для удобства работы с категоризатором Smart-Cat мы создали специальный Telegram-бот.
Для O
удобства O
работы O
с O
категоризатором O
Smart B-App_system
- I-App_system
Cat I-App_system
мы O
создали O
специальный O
Telegram B-Technology
- I-Technology
бот I-Technology
. O

# text =   После своей работы BERT-bot отправит CSV-таблицу.
После O
своей O
работы O
BERT B-Technology
- I-Technology
bot I-Technology
отправит O
CSV O
- O
таблицу O
. O

# text =   Но, как я уже сказал ранее, такие модели могут применяться не только в задачах классификации текста.
Но O
, O
как O
я O
уже O
сказал O
ранее O
, O
такие O
модели O
могут O
применяться O
не O
только O
в O
задачах O
классификации B-Task
текста I-Task
. O
